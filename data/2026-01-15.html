<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-15.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-hybrid-guided-variational-autoencoder-for-visual-place-recognition">[1] <a href="https://arxiv.org/abs/2601.09248">Hybrid guided variational autoencoder for visual place recognition</a></h3>
<p><em>Ni Wang, Zihan You, Emre Neftci, Thorben Schoepe</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè§†è§‰åœ°ç‚¹è¯†åˆ«çš„æ–°å‹å¼•å¯¼å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç»“åˆäº‹ä»¶è§†è§‰ä¼ æ„Ÿå™¨å’Œè„‰å†²ç¥ç»ç½‘ç»œï¼Œåœ¨ä¿æŒç´§å‡‘æ¨¡å‹å°ºå¯¸çš„åŒæ—¶å®ç°äº†é«˜é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†ç§»åŠ¨æœºå™¨äººåœ¨å·²çŸ¥å’ŒæœªçŸ¥å®¤å†…ç¯å¢ƒä¸­çš„å¯¼èˆªæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æœ€å…ˆè¿›çš„è§†è§‰åœ°ç‚¹è¯†åˆ«æ¨¡å‹éœ€è¦å¤§é‡å†…å­˜ï¼Œä¸é€‚åˆç§»åŠ¨è®¾å¤‡éƒ¨ç½²ï¼Œè€Œæ›´ç´§å‡‘çš„æ¨¡å‹åˆ™ç¼ºä¹é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ï¼Œä¸ºæœºå™¨äººåº”ç”¨å¼€å‘ä¸€ç§æ—¢ç´§å‡‘åˆå…·æœ‰é«˜é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„è§†è§‰åœ°ç‚¹è¯†åˆ«è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•ç»“åˆäº‹ä»¶è§†è§‰ä¼ æ„Ÿå™¨å’Œæ–°å‹å¼•å¯¼å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç¼–ç å™¨éƒ¨åˆ†åŸºäºè„‰å†²ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä¸ä½åŠŸè€—ã€ä½å»¶è¿Ÿçš„ç¥ç»å½¢æ€ç¡¬ä»¶å…¼å®¹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè§£è€¦è§†è§‰ç‰¹å¾ï¼Œå­¦ä¹ ä½ç½®çš„æœ¬è´¨ç‰¹å¾ï¼Œå®ç°é«˜æ•ˆçš„åœ°ç‚¹è¯†åˆ«ã€‚</p>
<p><strong>Result:</strong> åœ¨æ–°å»ºçš„å®¤å†…VPRæ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹æˆåŠŸè§£è€¦äº†16ä¸ªä¸åŒåœ°ç‚¹çš„è§†è§‰ç‰¹å¾ï¼Œåˆ†ç±»æ€§èƒ½ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶åœ¨å„ç§å…‰ç…§æ¡ä»¶ä¸‹è¡¨ç°å‡ºé²æ£’æ€§ã€‚å½“æµ‹è¯•æ¥è‡ªæœªçŸ¥åœºæ™¯çš„æ–°è§†è§‰è¾“å…¥æ—¶ï¼Œæ¨¡å‹èƒ½å¤ŸåŒºåˆ†è¿™äº›åœ°ç‚¹ï¼Œæ˜¾ç¤ºå‡ºé«˜æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç´§å‡‘ä¸”é²æ£’çš„å¼•å¯¼VAEæ¨¡å‹å…·æœ‰æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºè§†è§‰åœ°ç‚¹è¯†åˆ«æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºç§»åŠ¨æœºå™¨äººåœ¨å·²çŸ¥å’ŒæœªçŸ¥å®¤å†…ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºéœ€è¦ä½åŠŸè€—ã€ä½å»¶è¿Ÿéƒ¨ç½²çš„æœºå™¨äººåº”ç”¨åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.</p>
  </article>
</body>
</html>
