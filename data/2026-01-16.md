<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong, Pritam P. Karmokar, William J. Beksi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªåŸºäºç‰©ç†æ¸²æŸ“çš„åˆæˆæ°´ä¸‹äº‹ä»¶ç›¸æœºå…‰æµåŸºå‡†æ•°æ®é›†ï¼Œè§£å†³äº†æ°´ä¸‹ç¯å¢ƒä¸­äº‹ä»¶ç›¸æœºç ”ç©¶ç¼ºä¹çœŸå®æ•°æ®çš„é—®é¢˜ï¼Œå¹¶ä¸ºæ°´ä¸‹äº‹ä»¶æ„ŸçŸ¥ç®—æ³•å»ºç«‹äº†æ–°çš„è¯„ä¼°åŸºå‡†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ°´ä¸‹æˆåƒé¢ä¸´æ³¢é•¿ç›¸å…³å…‰è¡°å‡ã€æ‚¬æµ®ç²’å­æ•£å°„ã€æµ‘æµŠæ¨¡ç³Šå’Œéå‡åŒ€ç…§æ˜ç­‰æŒ‘æˆ˜ï¼Œä¼ ç»Ÿç›¸æœºéš¾ä»¥è·å–åœ°é¢çœŸå®è¿åŠ¨æ•°æ®ï¼Œè€Œäº‹ä»¶ç›¸æœºè™½å…·æœ‰å¾®ç§’åˆ†è¾¨ç‡å’Œå®½åŠ¨æ€èŒƒå›´ï¼Œä½†ç¼ºä¹ç»“åˆçœŸå®æ°´ä¸‹å…‰å­¦ç‰¹æ€§çš„é…å¯¹æ•°æ®é›†ï¼Œé™åˆ¶äº†æ°´ä¸‹äº‹ä»¶ç›¸æœºå…‰æµç ”ç©¶çš„å‘å±•ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨åŸºäºç‰©ç†çš„å…‰çº¿è¿½è¸ªRGBDåºåˆ—ç”Ÿæˆåˆæˆæ°´ä¸‹åŸºå‡†æ•°æ®é›†ï¼Œé€šè¿‡ç°ä»£è§†é¢‘åˆ°äº‹ä»¶è½¬æ¢ç®¡é“åº”ç”¨äºæ¸²æŸ“çš„æ°´ä¸‹è§†é¢‘ï¼Œäº§ç”Ÿå…·æœ‰å¯†é›†åœ°é¢çœŸå®å…‰æµã€æ·±åº¦å’Œç›¸æœºè¿åŠ¨çš„çœŸå®äº‹ä»¶æ•°æ®æµï¼Œå¹¶åŸºå‡†æµ‹è¯•äº†æœ€å…ˆè¿›çš„åŸºäºå­¦ä¹ å’ŒåŸºäºæ¨¡å‹çš„å…‰æµé¢„æµ‹æ–¹æ³•ã€‚

**Result:** è¯¥æ•°æ®é›†ä¸ºæ°´ä¸‹äº‹ä»¶æ„ŸçŸ¥ç®—æ³•æä¾›äº†é¦–ä¸ªè¯„ä¼°åŸºå‡†ï¼Œé€šè¿‡åŸºå‡†æµ‹è¯•æ­ç¤ºäº†æ°´ä¸‹å…‰ä¼ è¾“å¯¹äº‹ä»¶å½¢æˆå’Œè¿åŠ¨ä¼°è®¡ç²¾åº¦çš„å½±å“ï¼Œå»ºç«‹äº†æœªæ¥ç®—æ³•å¼€å‘å’Œè¯„ä¼°çš„æ–°åŸºçº¿ï¼Œæ‰€æœ‰æºä»£ç å’Œæ•°æ®é›†å‡å·²å…¬å¼€å¯ç”¨ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¡«è¡¥äº†æ°´ä¸‹äº‹ä»¶ç›¸æœºå…‰æµæ•°æ®é›†çš„ç©ºç™½ï¼Œä¸ºç†è§£æ°´ä¸‹å…‰å­¦ç‰¹æ€§å¯¹äº‹ä»¶å½¢æˆçš„å½±å“æä¾›äº†ç³»ç»Ÿåˆ†ææ¡†æ¶ï¼Œæ¨åŠ¨äº†æ°´ä¸‹äº‹ä»¶æ„ŸçŸ¥ç®—æ³•çš„å‘å±•ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å¯å¤ç°çš„åŸºå‡†å¹³å°ã€‚

---

#### ğŸ“„ Abstract
Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [2] [Heterogeneous computing platform for real-time robotics](https://arxiv.org/abs/2601.09755)
*Jakub Fil, Yulia Sandamirskaya, Hector Gonzalez, LoÃ¯c Azzalin, Stefan GlÃ¼ge, Lukas Friedenstab, Friedrich Wolf, Tim Rosmeisl, Matthias Lohrmann, Mahmoud Akl, Khaleel Khan, Leonie Wolf, Kristin Richter, Holm Puder, Mazhar Ali Bari, Xuan Choo, Noha Alharthi, Michael Hopkins, Mansoor Hanif Christian Mayr, Jens Struckmeier, Steve Furber*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆè®¡ç®—æ¶æ„ï¼Œå°†ç¥ç»å½¢æ€è®¡ç®—ç¡¬ä»¶ï¼ˆLoihi2å¤„ç†å™¨ï¼‰ä¸äº‹ä»¶ç›¸æœºç›¸ç»“åˆè¿›è¡Œå®æ—¶æ„ŸçŸ¥ï¼Œå¹¶ä¸åŸºäºGPUçš„AIè®¡ç®—é›†ç¾¤é›†æˆï¼Œç”¨äºé«˜çº§è¯­è¨€å¤„ç†å’Œä»»åŠ¡è§„åˆ’ï¼Œä»¥æ”¯æŒäººæœºå…±å­˜çš„æ™ºæ…§åŸå¸‚æ„¿æ™¯ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€Society 5.0æ¦‚å¿µå’ŒNEOMæ™ºæ…§åŸå¸‚å€¡è®®çš„å…´èµ·ï¼Œéœ€è¦ä¸ºäººç±»ä¸æœºå™¨äººå…±å­˜çš„æœªæ¥åŸå¸‚å¼€å‘é«˜æ•ˆçš„è®¡ç®—å¹³å°ã€‚ç°æœ‰ç³»ç»Ÿç¼ºä¹èƒ½å¤Ÿæ— ç¼æ•´åˆå®æ—¶æ„ŸçŸ¥ã€é«˜çº§è®¤çŸ¥å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›çš„å¼‚æ„è®¡ç®—æ¶æ„ï¼Œè¿™é™åˆ¶äº†æœºå™¨äººåœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­çš„è‡ªä¸»æ€§å’Œå“åº”èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆè®¡ç®—æ¶æ„ï¼Œå°†ç¥ç»å½¢æ€è®¡ç®—ç¡¬ä»¶ï¼ˆIntel Loihi2å¤„ç†å™¨ï¼‰ä¸äº‹ä»¶ç›¸æœºç»“åˆç”¨äºä½å»¶è¿Ÿæ„ŸçŸ¥å’Œå®æ—¶äº¤äº’ï¼ŒåŒæ—¶ä¸åŸºäºGPUçš„æœ¬åœ°AIè®¡ç®—é›†ç¾¤é›†æˆï¼Œè´Ÿè´£é«˜çº§è¯­è¨€å¤„ç†ã€è®¤çŸ¥æ¨ç†å’Œä»»åŠ¡è§„åˆ’ã€‚è¯¥æ¶æ„å¼ºè°ƒä¸åŒç»„ä»¶çš„é«˜æ•ˆæ— ç¼é›†æˆï¼Œé€šè¿‡è½¯ç¡¬ä»¶ååŒæœ€å¤§åŒ–ç³»ç»Ÿæ•´ä½“æ€§èƒ½å’Œå“åº”æ€§ã€‚

**Result:** ç ”ç©¶é€šè¿‡äººå½¢æœºå™¨äººä¸äººç±»å…±åŒæ¼”å¥ä¹å™¨çš„äº¤äº’ä»»åŠ¡æ¼”ç¤ºäº†è¯¥æ··åˆæ¶æ„çš„æœ‰æ•ˆæ€§ã€‚ç³»ç»Ÿå±•ç¤ºäº†ç¥ç»å½¢æ€ç¡¬ä»¶åœ¨å®æ—¶æ„ŸçŸ¥æ–¹é¢çš„ä¼˜åŠ¿ä¸GPUé›†ç¾¤åœ¨é«˜çº§è®¤çŸ¥å¤„ç†æ–¹é¢çš„èƒ½åŠ›ä¹‹é—´çš„ååŒä½œç”¨ï¼Œå®ç°äº†å¤æ‚å®æ—¶äº¤äº’åº”ç”¨ä¸­çš„é«˜æ€§èƒ½å’Œå“åº”æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¼‚æ„è®¡ç®—æ¶æ„åœ¨æ¨è¿›æœºå™¨äººè‡ªä¸»æ€§å’Œäº¤äº’æ™ºèƒ½æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥å¤æ‚å®æ—¶åº”ç”¨ä¸­é›†æˆç³»ç»Ÿçš„è®¾è®¡æä¾›äº†èŒƒä¾‹ã€‚è¿™ç§è½¯ç¡¬ä»¶ååŒçš„è®¾è®¡æ–¹æ³•æŒ‡å‘äº†äººæœºå…±å­˜æ™ºæ…§åŸå¸‚ä¸­æœºå™¨äººç³»ç»Ÿçš„å‘å±•æ–¹å‘ï¼Œå¼ºè°ƒäº†ä¸åŒè®¡ç®—èŒƒå¼æ— ç¼é›†æˆçš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
After Industry 4.0 has embraced tight integration between machinery (OT), software (IT), and the Internet, creating a web of sensors, data, and algorithms in service of efficient and reliable production, a new concept of Society 5.0 is emerging, in which infrastructure of a city will be instrumented to increase reliability, efficiency, and safety. Robotics will play a pivotal role in enabling this vision that is pioneered by the NEOM initiative - a smart city, co-inhabited by humans and robots. In this paper we explore the computing platform that will be required to enable this vision. We show how we can combine neuromorphic computing hardware, exemplified by the Loihi2 processor used in conjunction with event-based cameras, for sensing and real-time perception and interaction with a local AI compute cluster (GPUs) for high-level language processing, cognition, and task planning. We demonstrate the use of this hybrid computing architecture in an interactive task, in which a humanoid robot plays a musical instrument with a human. Central to our design is the efficient and seamless integration of disparate components, ensuring that the synergy between software and hardware maximizes overall performance and responsiveness. Our proposed system architecture underscores the potential of heterogeneous computing architectures in advancing robotic autonomy and interactive intelligence, pointing toward a future where such integrated systems become the norm in complex, real-time applications.
