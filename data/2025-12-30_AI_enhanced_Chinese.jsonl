{"id": "2512.22214", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22214", "abs": "https://arxiv.org/abs/2512.22214", "authors": ["Naichuan Zheng", "Xiahai Lun", "Weiyi Li", "Yuchen Du"], "title": "Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition", "comment": null, "summary": "Graph Convolutional Networks (GCNs) demonstrate strong capability in modeling skeletal topology for action recognition, yet their dense floating-point computations incur high energy costs. Spiking Neural Networks (SNNs), characterized by event-driven and sparse activation, offer energy efficiency but remain limited in capturing coupled temporal-frequency and topological dependencies of human motion. To bridge this gap, this article proposes Signal-SGN++, a topology-aware spiking graph framework that integrates structural adaptivity with time-frequency spiking dynamics. The network employs a backbone composed of 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. Within this backbone, a Topology-Shift Self-Attention (TSSA) mechanism is embedded to adaptively route attention across learned skeletal topologies, enhancing graph-level sensitivity without increasing computational complexity. Moreover, an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch decomposes spiking features into multi-resolution temporal-frequency representations, wherein a Topology-Aware Time-Frequency Fusion (TATF) unit incorporates structural priors to preserve topology-consistent spectral fusion. Comprehensive experiments on large-scale benchmarks validate that Signal-SGN++ achieves superior accuracy-efficiency trade-offs, outperforming existing SNN-based methods and achieving competitive results against state-of-the-art GCNs under substantially reduced energy consumption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSignal-SGN++\uff0c\u4e00\u79cd\u62d3\u6251\u611f\u77e5\u7684\u8109\u51b2\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u81ea\u9002\u5e94\u6027\u4e0e\u65f6\u9891\u8109\u51b2\u52a8\u529b\u5b66\uff0c\u5728\u4fdd\u6301\u9ad8\u80fd\u6548\u7684\u540c\u65f6\u63d0\u5347\u57fa\u4e8e\u9aa8\u9abc\u7684\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709SNN\u65b9\u6cd5\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u3002", "motivation": "\u56fe\u5377\u79ef\u7f51\u7edc\u5728\u9aa8\u9abc\u52a8\u4f5c\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bc6\u96c6\u6d6e\u70b9\u8ba1\u7b97\u80fd\u8017\u9ad8\uff1b\u800c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u867d\u5177\u80fd\u6548\u4f18\u52bf\uff0c\u5374\u96be\u4ee5\u6709\u6548\u6355\u6349\u4eba\u4f53\u8fd0\u52a8\u7684\u8026\u5408\u65f6\u9891\u4e0e\u62d3\u6251\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u62d3\u6251\u611f\u77e5\u4e0e\u80fd\u6548\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51faSignal-SGN++\u6846\u67b6\uff0c\u5176\u4e3b\u5e72\u7f51\u7edc\u75311D\u8109\u51b2\u56fe\u5377\u79ef\u548c\u9891\u7387\u8109\u51b2\u5377\u79ef\u7ec4\u6210\uff0c\u7528\u4e8e\u8054\u5408\u65f6\u7a7a\u4e0e\u9891\u8c31\u7279\u5f81\u63d0\u53d6\uff1b\u5d4c\u5165\u62d3\u6251\u8f6c\u79fb\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u81ea\u9002\u5e94\u5730\u5728\u5b66\u4e60\u5230\u7684\u9aa8\u9abc\u62d3\u6251\u95f4\u8def\u7531\u6ce8\u610f\u529b\uff1b\u5e76\u5f15\u5165\u8f85\u52a9\u7684\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u53d8\u6362\u878d\u5408\u5206\u652f\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u65f6\u9891\u878d\u5408\u5355\u5143\u6574\u5408\u7ed3\u6784\u5148\u9a8c\u4ee5\u4fdd\u6301\u62d3\u6251\u4e00\u81f4\u7684\u9891\u8c31\u878d\u5408\u3002", "result": "\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cSignal-SGN++\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u4e8eSNN\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u663e\u8457\u964d\u4f4e\u80fd\u8017\u7684\u540c\u65f6\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdbGCNs\u76f8\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u62d3\u6251\u81ea\u9002\u5e94\u673a\u5236\u4e0e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u65f6\u9891\u52a8\u529b\u5b66\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u80fd\u6548\u7684\u9aa8\u9abc\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u56fe\u7ea7\u654f\u611f\u5ea6\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u8282\u80fd\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.22441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22441", "abs": "https://arxiv.org/abs/2512.22441", "authors": ["Zibin Liu", "Banglei Guana", "Yang Shanga", "Zhenbao Yu", "Yifei Bian", "Qifeng Yu"], "title": "LECalib: Line-Based Event Camera Calibration", "comment": "9 Pages, 6 figures", "summary": "Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u7279\u5f81\u7684\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u6846\u67b6\uff0c\u5229\u7528\u4eba\u9020\u73af\u5883\u4e2d\u5e38\u89c1\u7269\u4f53\u7684\u51e0\u4f55\u7ebf\u6761\uff08\u5982\u95e8\u7a97\u3001\u7bb1\u5b50\u7b49\uff09\u8fdb\u884c\u6807\u5b9a\uff0c\u65e0\u9700\u624b\u52a8\u653e\u7f6e\u6807\u5b9a\u677f\uff0c\u9002\u7528\u4e8e\u5355\u76ee\u548c\u7acb\u4f53\u4e8b\u4ef6\u76f8\u673a\u3002", "motivation": "\u5f53\u524d\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u95ea\u70c1\u56fe\u6848\u3001\u91cd\u5efa\u5f3a\u5ea6\u56fe\u50cf\u6216\u4ece\u4e8b\u4ef6\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u8017\u65f6\u4e14\u9700\u8981\u624b\u52a8\u653e\u7f6e\u6807\u5b9a\u7269\u4f53\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5feb\u901f\u53d8\u5316\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u57fa\u4e8e\u7ebf\u7684\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u4e8b\u4ef6\u6d41\u4e2d\u68c0\u6d4b\u7ebf\u6761\uff0c\u5229\u7528\u4e8b\u4ef6-\u7ebf\u6807\u5b9a\u6a21\u578b\u751f\u6210\u76f8\u673a\u53c2\u6570\u7684\u521d\u59cb\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u5e73\u9762\u548c\u975e\u5e73\u9762\u7ebf\u6761\uff0c\u7136\u540e\u91c7\u7528\u975e\u7ebf\u6027\u4f18\u5316\u6765\u7ec6\u5316\u76f8\u673a\u53c2\u6570\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u51c6\u786e\u6027\uff0c\u5728\u5355\u76ee\u548c\u7acb\u4f53\u4e8b\u4ef6\u76f8\u673a\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u6e90\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u73af\u5883\u4e2d\u81ea\u7136\u5b58\u5728\u7684\u51e0\u4f55\u7ebf\u6761\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4e13\u95e8\u6807\u5b9a\u7269\u4f53\u7684\u5feb\u901f\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\uff0c\u4e3a\u4eba\u9020\u73af\u5883\u4e2d\u7684\u4e8b\u4ef6\u89c6\u89c9\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22474", "abs": "https://arxiv.org/abs/2512.22474", "authors": ["Taihang Lei", "Banglei Guan", "Minzu Liang", "Pengju Sun", "Jing Tao", "Yang Shang", "Qifeng Yu"], "title": "Event-based high temporal resolution measurement of shock wave motion field", "comment": null, "summary": "Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4e8b\u4ef6\u76f8\u673a\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u6d4b\u91cf\u51b2\u51fb\u6ce2\u8fd0\u52a8\u53c2\u6570\uff0c\u901a\u8fc7\u5efa\u7acb\u6781\u5750\u6807\u7cfb\u7f16\u7801\u4e8b\u4ef6\u3001\u81ea\u9002\u5e94ROI\u63d0\u53d6\u548c\u8fed\u4ee3\u659c\u7387\u5206\u6790\u7b49\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u51b2\u51fb\u6ce2\u4e0d\u5bf9\u79f0\u6027\u4f30\u8ba1\u548c\u8fd0\u52a8\u573a\u91cd\u5efa\u3002", "motivation": "\u51b2\u51fb\u6ce2\u5728\u529f\u7387\u573a\u6d4b\u8bd5\u548c\u635f\u4f24\u8bc4\u4f30\u7b49\u5e94\u7528\u4e2d\u9700\u8981\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u7cbe\u786e\u6d4b\u91cf\uff0c\u4f46\u51b2\u51fb\u6ce2\u7684\u5feb\u901f\u4e0d\u5747\u5300\u4f20\u64ad\u548c\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u6761\u4ef6\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u591a\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u901f\u5ea6\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u80fd\u529b\uff0c\u9996\u5148\u5efa\u7acb\u6781\u5750\u6807\u7cfb\u7f16\u7801\u4e8b\u4ef6\u4ee5\u63ed\u793a\u51b2\u51fb\u6ce2\u4f20\u64ad\u6a21\u5f0f\uff0c\u901a\u8fc7\u4e8b\u4ef6\u504f\u79fb\u8ba1\u7b97\u5b9e\u73b0\u81ea\u9002\u5e94\u611f\u5174\u8da3\u533a\u57df\u63d0\u53d6\uff1b\u7136\u540e\u4f7f\u7528\u8fed\u4ee3\u659c\u7387\u5206\u6790\u63d0\u53d6\u51b2\u51fb\u6ce2\u524d\u6cbf\u4e8b\u4ef6\uff0c\u5229\u7528\u901f\u5ea6\u53d8\u5316\u7684\u8fde\u7eed\u6027\uff1b\u6700\u540e\u6839\u636e\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5149\u5b66\u6210\u50cf\u6a21\u578b\u63a8\u5bfc\u4e8b\u4ef6\u51e0\u4f55\u6a21\u578b\u548c\u51b2\u51fb\u6ce2\u8fd0\u52a8\u53c2\u6570\uff0c\u7ed3\u5408\u4e09\u7ef4\u91cd\u5efa\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u591a\u89d2\u5ea6\u51b2\u51fb\u6ce2\u6d4b\u91cf\u3001\u8fd0\u52a8\u573a\u91cd\u5efa\u548c\u7206\u70b8\u5f53\u91cf\u53cd\u6f14\uff0c\u901f\u5ea6\u6d4b\u91cf\u7ed3\u679c\u4e0e\u538b\u529b\u4f20\u611f\u5668\u548c\u7ecf\u9a8c\u516c\u5f0f\u76f8\u6bd4\uff0c\u6700\u5927\u8bef\u5dee\u4e3a5.20%\uff0c\u6700\u5c0f\u8bef\u5dee\u4e3a0.06%\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u9ad8\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u5b9e\u73b0\u51b2\u51fb\u6ce2\u8fd0\u52a8\u573a\u7684\u9ad8\u7cbe\u5ea6\u6d4b\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4ee3\u8868\u4e86\u51b2\u51fb\u6ce2\u6d4b\u91cf\u6280\u672f\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u7cbe\u786e\u6d4b\u91cf\uff0c\u4e3a\u529f\u7387\u573a\u6d4b\u8bd5\u548c\u635f\u4f24\u8bc4\u4f30\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u4e8b\u4ef6\u89c6\u89c9\u5728\u52a8\u6001\u7269\u7406\u73b0\u8c61\u6d4b\u91cf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.22979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22979", "abs": "https://arxiv.org/abs/2512.22979", "authors": ["Huiming Yang", "Linglin Liao", "Fei Ding", "Sibo Wang", "Zijian Zeng"], "title": "PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects", "comment": null, "summary": "Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PoseStreamer\uff0c\u4e00\u4e2a\u9488\u5bf9\u9ad8\u901f\u8fd0\u52a8\u573a\u666f\u8bbe\u8ba1\u7684\u9c81\u68d2\u591a\u6a21\u60016DoF\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u81ea\u9002\u5e94\u59ff\u6001\u8bb0\u5fc6\u961f\u5217\u3001\u7269\u4f53\u4e2d\u5fc32D\u8ddf\u8e2a\u5668\u548c\u5c04\u7ebf\u59ff\u6001\u6ee4\u6ce2\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u901f\u4f4e\u5149\u6761\u4ef6\u4e0b\u7684\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u5728\u9ad8\u901f\u8fd0\u52a8\u548c\u4f4e\u5149\u7167\u573a\u666f\u4e2d\uff0c\u4f20\u7edfRGB\u76f8\u673a\u56e0\u8fd0\u52a8\u6a21\u7cca\u5bfc\u81f46DoF\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u867d\u7136\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u901f\u7269\u4f53\u8fd0\u52a8\u573a\u666f\u4e2d\u4ecd\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u9ad8\u901f\u8fd0\u52a8\u573a\u666f\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u7684PoseStreamer\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u81ea\u9002\u5e94\u59ff\u6001\u8bb0\u5fc6\u961f\u5217\u5229\u7528\u5386\u53f2\u65b9\u5411\u7ebf\u7d22\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u7269\u4f53\u4e2d\u5fc32D\u8ddf\u8e2a\u5668\u63d0\u4f9b\u5f3a2D\u5148\u9a8c\u4ee5\u63d0\u53473D\u4e2d\u5fc3\u53ec\u56de\u7387\uff0c\u4ee5\u53ca\u5c04\u7ebf\u59ff\u6001\u6ee4\u6ce2\u5668\u6cbf\u76f8\u673a\u5c04\u7ebf\u8fdb\u884c\u51e0\u4f55\u7ec6\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86MoCapCube6D\u591a\u6a21\u6001\u6570\u636e\u96c6\u7528\u4e8e\u5feb\u901f\u8fd0\u52a8\u4e0b\u7684\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPoseStreamer\u5728\u9ad8\u901f\u8fd0\u52a8\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u4f5c\u4e3a\u65e0\u6a21\u677f\u6846\u67b6\u5bf9\u672a\u89c1\u8fc7\u7684\u8fd0\u52a8\u7269\u4f53\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5728\u4e13\u95e8\u6784\u5efa\u7684MoCapCube6D\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6311\u6218\u6027\u9ad8\u901f\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u673a\u5236\u5728\u9ad8\u901f\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002PoseStreamer\u7684\u65e0\u6a21\u677f\u7279\u6027\u4f7f\u5176\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u7269\u4f53\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u59ff\u6001\u4f30\u8ba1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
