<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Realizing Fully-Integrated, Low-Power, Event-Based Pupil Tracking with Neuromorphic Hardware](https://arxiv.org/abs/2511.20175)
*Federico Paredes-Valles, Yoshitaka Miyatani, Kirk Y. W. Scheper*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªç”µæ± ä¾›ç”µçš„å¯ç©¿æˆ´ç³å­”ä¸­å¿ƒè¿½è¸ªç³»ç»Ÿï¼Œé€šè¿‡äº‹ä»¶è§†è§‰ä¼ æ„Ÿä¸ç¥ç»å½¢æ€å¤„ç†çš„ç«¯åˆ°ç«¯é›†æˆï¼Œåœ¨Speck2fç‰‡ä¸Šç³»ç»Ÿä¸Šå®ç°äº†100HzåŒç›®å…‰å­¦è¿½è¸ªï¼Œæ¯çœ¼å¹³å‡åŠŸè€—ä½äº5mWã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¯ç©¿æˆ´å¹³å°åœ¨å®ç°ç¨³å¥ã€é«˜é¢‘ç‡çœ¼åŠ¨è¿½è¸ªçš„åŒæ—¶ä¿æŒè¶…ä½åŠŸè€—é¢ä¸´æŒ‘æˆ˜ï¼Œäº‹ä»¶è§†è§‰ä¼ æ„Ÿå™¨è™½ç„¶æä¾›å¾®ç§’çº§åˆ†è¾¨ç‡å’Œç¨€ç–æ•°æ®æµï¼Œä½†ç¼ºä¹å®Œå…¨é›†æˆçš„ä½åŠŸè€—å®æ—¶æ¨ç†è§£å†³æ–¹æ¡ˆã€‚

**Method:** é‡‡ç”¨å•†ç”¨Speck2fç‰‡ä¸Šç³»ç»Ÿé›†æˆäº‹ä»¶ä¼ æ„Ÿä¸ç¥ç»å½¢æ€å¤„ç†ï¼Œç»“åˆä½åŠŸè€—å¾®æ§åˆ¶å™¨è¿›è¡Œè½»é‡çº§åæ ‡è§£ç ï¼›æå‡ºæ–°å‹ä¸ç¡®å®šæ€§é‡åŒ–è„‰å†²ç¥ç»ç½‘ç»œï¼Œé…å¤‡é—¨æ§æ—¶åºè§£ç æœºåˆ¶ï¼Œé’ˆå¯¹ä¸¥æ ¼å†…å­˜å’Œå¸¦å®½çº¦æŸä¼˜åŒ–ï¼Œå¹¶é€šè¿‡ç³»ç»ŸåŒ–éƒ¨ç½²æœºåˆ¶å¼¥åˆç°å®å·®è·ã€‚

**Result:** åœ¨å¤šç”¨æˆ·æ•°æ®é›†ä¸ŠéªŒè¯äº†ç³»ç»Ÿæ€§èƒ½ï¼Œå¯ç©¿æˆ´åŸå‹é…å¤‡åŒç¥ç»å½¢æ€è®¾å¤‡ï¼Œå®ç°100Hzç¨³å¥åŒç›®å…‰å­¦è¿½è¸ªï¼Œæ¯çœ¼å¹³å‡åŠŸè€—ä½äº5mWã€‚

**Conclusion:** ç«¯åˆ°ç«¯ç¥ç»å½¢æ€è®¡ç®—ä¸ºä¸‹ä¸€ä»£é«˜èƒ½æ•ˆå¯ç©¿æˆ´ç³»ç»Ÿå®ç°äº†å®ç”¨çš„å¸¸å¼€çœ¼åŠ¨è¿½è¸ªï¼Œè¯æ˜äº†å®Œå…¨é›†æˆçš„äº‹ä»¶é©±åŠ¨ç³»ç»Ÿåœ¨è¶…ä½åŠŸè€—ä¸‹çš„å¯è¡Œæ€§ã€‚

---

#### ğŸ“„ Abstract
Eye tracking is fundamental to numerous applications, yet achieving robust, high-frequency tracking with ultra-low power consumption remains challenging for wearable platforms. While event-based vision sensors offer microsecond resolution and sparse data streams, they have lacked fully integrated, low-power processing solutions capable of real-time inference. In this work, we present the first battery-powered, wearable pupil-center-tracking system with complete on-device integration, combining event-based sensing and neuromorphic processing on the commercially available Speck2f system-on-chip with lightweight coordinate decoding on a low-power microcontroller. Our solution features a novel uncertainty-quantifying spiking neural network with gated temporal decoding, optimized for strict memory and bandwidth constraints, complemented by systematic deployment mechanisms that bridge the reality gap. We validate our system on a new multi-user dataset and demonstrate a wearable prototype with dual neuromorphic devices achieving robust binocular pupil tracking at 100 Hz with an average power consumption below 5 mW per eye. Our work demonstrates that end-to-end neuromorphic computing enables practical, always-on eye tracking for next-generation energy-efficient wearable systems.
