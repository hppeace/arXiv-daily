<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-23.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-had-hierarchical-asymmetric-distillation-to-bridge-spatio-temporal-gaps-in-event-based-object-tracking">[1] <a href="https://arxiv.org/abs/2510.19560">HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking</a></h3>
<p><em>Yao Deng, Xian Zhong, Wenxuan Liu, Zhaofei Yu, Jingling Yuan, Tiejun Huang</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–éå¯¹ç§°è’¸é¦ï¼ˆHADï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡å’Œç¼“è§£RGBç›¸æœºä¸äº‹ä»¶ç›¸æœºä¹‹é—´çš„æ—¶ç©ºä¸å¯¹ç§°æ€§ï¼Œæœ‰æ•ˆèåˆä¸¤ç§æ¨¡æ€çš„äº’è¡¥ä¼˜åŠ¿ï¼Œæ˜¾è‘—æå‡äº†ç›®æ ‡è·Ÿè¸ªæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> RGBç›¸æœºå’Œäº‹ä»¶ç›¸æœºåœ¨æˆåƒæœºåˆ¶ä¸Šå­˜åœ¨æ ¹æœ¬å·®å¼‚ï¼Œå¯¼è‡´æ˜¾è‘—çš„æ—¶ç©ºä¸å¯¹ç§°æ€§ï¼Œé˜»ç¢äº†ä¸¤ç§æ¨¡æ€çš„æœ‰æ•ˆèåˆã€‚è¿™ç§ä¸å¯¹ç§°æ€§é™åˆ¶äº†åœ¨é«˜é€Ÿè¿åŠ¨ã€é«˜åŠ¨æ€èŒƒå›´ç¯å¢ƒå’ŒåŠ¨æ€èƒŒæ™¯å¹²æ‰°ç­‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹ç›®æ ‡è·Ÿè¸ªæ€§èƒ½çš„æå‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºå±‚æ¬¡åŒ–éå¯¹ç§°è’¸é¦ï¼ˆHADï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨å±‚æ¬¡åŒ–å¯¹é½ç­–ç•¥ï¼Œåœ¨ä¿æŒå­¦ç”Ÿç½‘ç»œè®¡ç®—æ•ˆç‡å’Œå‚æ•°ç´§å‡‘æ€§çš„åŒæ—¶æœ€å°åŒ–ä¿¡æ¯æŸå¤±ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜¾å¼å»ºæ¨¡æ—¶ç©ºä¸å¯¹ç§°æ€§æ¥å®ç°å¤šæ¨¡æ€çŸ¥è¯†è’¸é¦ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜HADæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªè®¾è®¡ç»„ä»¶çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡æ˜¾å¼å»ºæ¨¡å’Œç¼“è§£æ¨¡æ€é—´ä¸å¯¹ç§°æ€§å¯ä»¥æœ‰æ•ˆèåˆäº’è¡¥ä¿¡æ¯ï¼Œä¸ºå¤šæ¨¡æ€è§†è§‰ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥åœ¨è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½å¹³è¡¡æ–¹é¢çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>RGB cameras excel at capturing rich texture details with high spatial
resolution, whereas event cameras offer exceptional temporal resolution and a
high dynamic range (HDR). Leveraging their complementary strengths can
substantially enhance object tracking under challenging conditions, such as
high-speed motion, HDR environments, and dynamic background interference.
However, a significant spatio-temporal asymmetry exists between these two
modalities due to their fundamentally different imaging mechanisms, hindering
effective multi-modal integration. To address this issue, we propose
{Hierarchical Asymmetric Distillation} (HAD), a multi-modal knowledge
distillation framework that explicitly models and mitigates spatio-temporal
asymmetries. Specifically, HAD proposes a hierarchical alignment strategy that
minimizes information loss while maintaining the student network's
computational efficiency and parameter compactness. Extensive experiments
demonstrate that HAD consistently outperforms state-of-the-art methods, and
comprehensive ablation studies further validate the effectiveness and necessity
of each designed component. The code will be released soon.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="2-a-flexible-framework-for-structural-plasticity-in-gpu-accelerated-sparse-spiking-neural-networks">[2] <a href="https://arxiv.org/abs/2510.19764">A flexible framework for structural plasticity in GPU-accelerated sparse spiking neural networks</a></h3>
<p><em>James C. Knight, Johanna Senk, Thomas Nowotny</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¯æŒGPUåŠ é€Ÿç»“æ„å¯å¡‘æ€§è§„åˆ™çš„æ–°å‹çµæ´»æ¡†æ¶ï¼Œé¦–æ¬¡å®ç°äº†åœ¨ç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ ç¯å¢ƒä¸­è®­ç»ƒç¨€ç–è„‰å†²ç¥ç»ç½‘ç»œï¼Œç›¸æ¯”å¯†é›†æ¨¡å‹è®­ç»ƒæ—¶é—´å‡å°‘é«˜è¾¾10å€ï¼ŒåŒæ—¶ä¿æŒåŒç­‰æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰äººå·¥ç¥ç»ç½‘ç»œå’Œç”Ÿç‰©å¤§è„‘å­¦ä¹ ç ”ç©¶ä¸»è¦å…³æ³¨çªè§¦å¯å¡‘æ€§ï¼Œè€Œå¿½è§†äº†ç»“æ„å¯å¡‘æ€§åœ¨æœ‰æ•ˆå­¦ä¹ ã€æŸä¼¤æ¢å¤å’Œèµ„æºä¼˜åŒ–ä¸­çš„å…³é”®ä½œç”¨ã€‚è™½ç„¶å‰ªææŠ€æœ¯å¸¸ç”¨äºå‡å°‘æ¨ç†è®¡ç®—éœ€æ±‚ï¼Œä½†ç°æœ‰æœºå™¨å­¦ä¹ æ¡†æ¶é’ˆå¯¹å¯†é›†è¿æ¥ä¼˜åŒ–ï¼Œæ— æ³•é™ä½å¤§å‹æ¨¡å‹çš„è®­ç»ƒæˆæœ¬ã€‚</p>
<p><strong>Method:</strong> å¼€å‘äº†åŸºäºGeNNæ¨¡æ‹Ÿå™¨çš„æ–°å‹GPUåŠ é€Ÿç»“æ„å¯å¡‘æ€§æ¡†æ¶ï¼Œé¦–å…ˆä½¿ç”¨e-propç›‘ç£å­¦ä¹ è§„åˆ™å’ŒDEEP Rè®­ç»ƒé«˜æ•ˆçš„ç¨€ç–SNNåˆ†ç±»å™¨ï¼Œç„¶ååœ¨æ— ç›‘ç£å­¦ä¹ ç¯å¢ƒä¸­å­¦ä¹ æ‹“æ‰‘æ˜ å°„ã€‚è¯¥æ¡†æ¶æ”¯æŒåŠ¨æ€åˆ›å»ºå’Œç§»é™¤è¿æ¥çš„ç»“æ„å¯å¡‘æ€§æœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> ç¨€ç–åˆ†ç±»å™¨ç›¸æ¯”åŸºçº¿å¯†é›†æ¨¡å‹è®­ç»ƒæ—¶é—´å‡å°‘é«˜è¾¾10å€ï¼ŒDEEP Ré‡è¿æ¥æœºåˆ¶ä½¿å…¶æ€§èƒ½ä¸åŸå§‹æ¨¡å‹ç›¸å½“ã€‚å®ç°äº†å¿«äºå®æ—¶æ¨¡æ‹Ÿçš„æ‹“æ‰‘æ˜ å°„å½¢æˆï¼Œæä¾›äº†è¿æ¥æ€§æ¼”åŒ–æ´å¯Ÿï¼Œå¹¶æµ‹é‡äº†æ¨¡æ‹Ÿé€Ÿåº¦ä¸ç½‘ç»œè§„æ¨¡çš„å…³ç³»ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶ä¸ºåœ¨ç½‘ç»œç»“æ„å’Œç¥ç»é€šä¿¡ä¸­å®ç°å’Œç»´æŒç¨€ç–æ€§çš„è¿›ä¸€æ­¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼ŒåŒæ—¶ä¸ºæ¢ç´¢ç¨€ç–æ€§åœ¨å„ç§ç¥ç»å½¢æ€åº”ç”¨ä¸­çš„è®¡ç®—ä¼˜åŠ¿æä¾›äº†å¹³å°ï¼Œæ¨åŠ¨äº†ç»“æ„å¯å¡‘æ€§åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>The majority of research in both training Artificial Neural Networks (ANNs)
and modeling learning in biological brains focuses on synaptic plasticity,
where learning equates to changing the strength of existing connections.
However, in biological brains, structural plasticity - where new connections
are created and others removed - is also vital, not only for effective learning
but also for recovery from damage and optimal resource usage. Inspired by
structural plasticity, pruning is often used in machine learning to remove weak
connections from trained models to reduce the computational requirements of
inference. However, the machine learning frameworks typically used for
backpropagation-based training of both ANNs and Spiking Neural Networks (SNNs)
are optimized for dense connectivity, meaning that pruning does not help reduce
the training costs of ever-larger models. The GeNN simulator already supports
efficient GPU-accelerated simulation of sparse SNNs for computational
neuroscience and machine learning. Here, we present a new flexible framework
for implementing GPU-accelerated structural plasticity rules and demonstrate
this first using the e-prop supervised learning rule and DEEP R to train
efficient, sparse SNN classifiers and then, in an unsupervised learning
context, to learn topographic maps. Compared to baseline dense models, our
sparse classifiers reduce training time by up to 10x while the DEEP R rewiring
enables them to perform as well as the original models. We demonstrate
topographic map formation in faster-than-realtime simulations, provide insights
into the connectivity evolution, and measure simulation speed versus network
size. The proposed framework will enable further research into achieving and
maintaining sparsity in network structure and neural communication, as well as
exploring the computational benefits of sparsity in a range of neuromorphic
applications.</p>
  </article>
</body>
</html>
