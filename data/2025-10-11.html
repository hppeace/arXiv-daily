<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-11.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 36]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 12]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 10]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-pickstyle-video-to-video-style-transfer-with-context-style-adapters">[1] <a href="https://arxiv.org/abs/2510.07546">PickStyle: Video-to-Video Style Transfer with Context-Style Adapters</a></h3>
<p><em>Soroush Mehraban, Vida Adeli, Jacob Rommann, Babak Taati, Kyryl Truskovskyi</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPickStyleæ¡†æ¶ï¼Œé€šè¿‡ä½ç§©é€‚é…å™¨å’Œä¸Šä¸‹æ–‡-é£æ ¼åˆ†ç±»å™¨æ— å¼•å¯¼æœºåˆ¶ï¼Œåœ¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹åŸºç¡€ä¸Šå®ç°é«˜è´¨é‡çš„è§†é¢‘é£æ ¼è¿ç§»ï¼Œè§£å†³äº†ç¼ºä¹é…å¯¹è§†é¢‘ç›‘ç£æ•°æ®çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†é¢‘é£æ ¼è¿ç§»ä»»åŠ¡é¢ä¸´ç¼ºä¹é…å¯¹è§†é¢‘æ•°æ®è¿›è¡Œç›‘ç£çš„å…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ä¿æŒè§†é¢‘å†…å®¹ä¸€è‡´æ€§çš„åŒæ—¶æœ‰æ•ˆä¼ é€’ç›®æ ‡é£æ ¼ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿåˆ©ç”¨é™æ€å›¾åƒç›‘ç£æ•°æ®å¹¶ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºPickStyleæ¡†æ¶ï¼Œåœ¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›å±‚æ’å…¥ä½ç§©é€‚é…å™¨å®ç°é«˜æ•ˆçš„è¿åŠ¨é£æ ¼è¿ç§»ï¼Œé€šè¿‡å…±äº«å¢å¼ºæ„å»ºåˆæˆè®­ç»ƒè§†é¢‘ç‰‡æ®µï¼Œå¹¶å¼•å…¥ä¸Šä¸‹æ–‡-é£æ ¼åˆ†ç±»å™¨æ— å¼•å¯¼æœºåˆ¶ç‹¬ç«‹æ§åˆ¶å†…å®¹å’Œé£æ ¼æ–¹å‘ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ—¶é—´ä¸€è‡´æ€§ã€é£æ ¼å¿ å®æ€§å’Œå†…å®¹ä¿æŒæ€§çš„è§†é¢‘è½¬æ¢ï¼Œåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ç»“åˆä½ç§©é€‚é…å™¨å’Œä¸“é—¨çš„è®­ç»ƒç­–ç•¥å¯ä»¥æœ‰æ•ˆè§£å†³è§†é¢‘é£æ ¼è¿ç§»ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä¸Šä¸‹æ–‡-é£æ ¼åˆ†ç¦»å¼•å¯¼æœºåˆ¶ä¸ºå¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>We address the task of video style transfer with diffusion models, where the
goal is to preserve the context of an input video while rendering it in a
target style specified by a text prompt. A major challenge is the lack of
paired video data for supervision. We propose PickStyle, a video-to-video style
transfer framework that augments pretrained video diffusion backbones with
style adapters and benefits from paired still image data with source-style
correspondences for training. PickStyle inserts low-rank adapters into the
self-attention layers of conditioning modules, enabling efficient
specialization for motion-style transfer while maintaining strong alignment
between video content and style. To bridge the gap between static image
supervision and dynamic video, we construct synthetic training clips from
paired images by applying shared augmentations that simulate camera motion,
ensuring temporal priors are preserved. In addition, we introduce Context-Style
Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free
guidance into independent text (style) and video (context) directions. CS-CFG
ensures that context is preserved in generated video while the style is
effectively transferred. Experiments across benchmarks show that our approach
achieves temporally coherent, style-faithful, and content-preserving video
translations, outperforming existing baselines both qualitatively and
quantitatively.</p>
<h3 id="2-travl-a-recipe-for-making-video-language-models-better-judges-of-physics-implausibility">[2] <a href="https://arxiv.org/abs/2510.07550">TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility</a></h3>
<p><em>Saman Motamed, Minghao Chen, Luc Van Gool, Iro Laina</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºTRAVLå¾®è°ƒæ–¹æ³•å’ŒImplausiBenchåŸºå‡†ï¼Œé€šè¿‡æ”¹è¿›è§†é¢‘è¯­è¨€æ¨¡å‹çš„è½¨è¿¹æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºç‰©ç†åˆç†æ€§è¯„ä¼°èƒ½åŠ›ï¼Œä¸ºè§£å†³è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿åç‰©ç†è§„å¾‹çš„é—®é¢˜æä¾›äº†ç»Ÿä¸€æ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°ä»£è§†é¢‘ç”Ÿæˆæ¨¡å‹è™½ç„¶è§†è§‰ä¿çœŸåº¦é«˜ï¼Œä½†ç»å¸¸äº§ç”Ÿè¿åç‰©ç†è§„å¾‹çš„åºåˆ—ï¼Œå¦‚ç‰©ä½“æ¼‚æµ®ã€ç¬ç§»æˆ–è¿èƒŒå› æœå…³ç³»çš„å½¢å˜ï¼Œç›®å‰ç¼ºä¹å®šé‡è¯„ä¼°è§†é¢‘ç‰©ç†çœŸå®æ€§çš„å¯é æ–¹æ³•ï¼Œç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«ç‰©ç†è¿è§„æ–¹é¢å­˜åœ¨å±€é™ã€‚</p>
<p><strong>Method:</strong> æå‡ºTRAVLå¾®è°ƒæ–¹æ³•ï¼Œç»“åˆå¹³è¡¡è®­ç»ƒæ•°æ®é›†å’Œè½¨è¿¹æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å—æ¥æ”¹è¿›è§†é¢‘è¯­è¨€æ¨¡å‹çš„è¿åŠ¨ç¼–ç å’Œåˆ¤åˆ«èƒ½åŠ›ï¼›åŒæ—¶æ„å»ºImplausiBenchåŸºå‡†ï¼ŒåŒ…å«300ä¸ªè§†é¢‘ï¼ˆ150ä¸ªçœŸå®ã€150ä¸ªç”Ÿæˆï¼‰ï¼Œæ¶ˆé™¤è¯­è¨€åè§å¹¶ä¸“æ³¨äºè§†è§‰æ—¶åºç†è§£ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹éš¾ä»¥è¯†åˆ«ç‰©ç†è¿è§„ï¼Œæš´éœ²äº†å…¶åœ¨æ—¶åºå’Œå› æœæ¨ç†æ–¹é¢çš„æ ¹æœ¬å±€é™æ€§ï¼›TRAVLæ–¹æ³•æ˜¾è‘—æå‡äº†ç‰©ç†åˆç†æ€§è¯„ä¼°æ€§èƒ½ï¼Œæ€§èƒ½è¯„ä¼°åŒæ—¶é‡‡ç”¨äººå·¥æ ‡æ³¨å’Œæ›´ä¸¥æ ¼çš„LLM-as-judgeæŒ‡æ ‡ã€‚</p>
<p><strong>Conclusion:</strong> TRAVLå’ŒImplausiBenchä¸ºæ¢ç©¶å’Œæ”¹è¿›å¤šæ¨¡æ€æ¨¡å‹çš„ç‰©ç†åˆç†æ€§æä¾›äº†ç»Ÿä¸€æ¡†æ¶ï¼Œæ­ç¤ºäº†è§†è§‰æ—¶åºç†è§£ä¸­ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä¸”æœªè¢«å……åˆ†æ¢ç´¢çš„æ–¹é¢ï¼Œä¸ºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†çœŸå®æ€§è¯„ä¼°å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Despite impressive visual fidelity, modern video generative models frequently
produce sequences that violate intuitive physical laws, such as objects
floating, teleporting, or morphing in ways that defy causality. While humans
can easily detect such implausibilities, there remains no robust method for
quantitatively assessing physical realism in video. In this work, we explore
whether Video-Language Models (VLMs) can be trained to serve as reliable judges
of physical plausibility. We find that existing VLMs struggle to identify
physics violations, exposing fundamental limitations in their temporal and
causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe
that combines a balanced training dataset with a trajectory-aware attention
module to improve motion encoding and discrimination in VLMs. To evaluate
physical reasoning more rigorously, we propose ImplausiBench, a benchmark of
300 videos (150 real, 150 generated) that removes linguistic biases and
isolates visual-temporal understanding. Performance is reported both with
gold-standard human judgments and stricter LLM-as-judge metrics. Together,
TRAVL and ImplausiBench offer a unified framework for probing and improving
physical plausibility in multimodal models, shedding light on a challenging and
underexplored aspect of visual-temporal understanding.</p>
<h3 id="3-label-semantics-for-robust-hyperspectral-image-classification">[3] <a href="https://arxiv.org/abs/2510.07556">Label Semantics for Robust Hyperspectral Image Classification</a></h3>
<p><em>Rafin Hassan, Zarin Tasnim Roshni, Rafiqul Bari, Alimul Islam, Nabeel Mohammed, Moshiur Farazi, Shafin Rahman</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é€šç”¨çš„è¯­ä¹‰å…‰è°±-ç©ºé—´èåˆç½‘ç»œï¼ˆS3FNï¼‰ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç±»åˆ«ç‰¹å®šæ–‡æœ¬æè¿°æ¥å¢å¼ºé«˜å…‰è°±å›¾åƒåˆ†ç±»æ€§èƒ½ï¼Œè§£å†³äº†ä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•åœ¨æœ‰é™è®­ç»ƒæ ·æœ¬å’Œé«˜ç»´æ•°æ®ä¸‹çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é«˜å…‰è°±å›¾åƒåˆ†ç±»é¢ä¸´é«˜è´¨é‡è®­ç»ƒæ ·æœ¬ç¨€ç¼ºå’Œé«˜ç»´å…‰è°±æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆä¸”éš¾ä»¥å¹³è¡¡å‡†ç¡®æ€§ä¸è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ç°æœ‰æ–¹æ³•å¤§å¤šä¸ºå•æ¨¡æ€ï¼Œä»…ä¾èµ–å…‰è°±-ç©ºé—´æ•°æ®åœ¨é«˜ç»´åµŒå…¥ç©ºé—´ä¸­å­¦ä¹ å†³ç­–è¾¹ç•Œã€‚</p>
<p><strong>Method:</strong> S3FNåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä¸ºæ¯ä¸ªç±»åˆ«æ ‡ç­¾ç”Ÿæˆå…¨é¢çš„æ–‡æœ¬æè¿°ï¼Œæ•æ‰å…¶ç‹¬ç‰¹ç‰¹å¾å’Œå…‰è°±è¡Œä¸ºï¼Œç„¶åä½¿ç”¨é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚BERTæˆ–RoBERTaï¼‰å°†è¿™äº›æè¿°åµŒå…¥å‘é‡ç©ºé—´ï¼Œæå–æœ‰æ„ä¹‰çš„æ ‡ç­¾è¯­ä¹‰ä»¥å®ç°æ›´å¥½çš„ç‰¹å¾-æ ‡ç­¾å¯¹é½ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªä¸åŒçš„é«˜å…‰è°±åŸºå‡†æ•°æ®é›†ï¼ˆHyperspectral Woodã€HyperspectralBlueberrieså’ŒDeepHS-Fruitï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†åˆ†ç±»æ€§èƒ½ï¼Œè¯æ˜äº†æ–‡æœ¬è¯­ä¹‰ä¸å…‰è°±-ç©ºé—´æ•°æ®ä¹‹é—´çš„ååŒæ•ˆåº”ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†è¯­ä¹‰å¢å¼ºåœ¨é«˜å…‰è°±å›¾åƒåˆ†ç±»ä¸­çš„æ½œåŠ›ï¼Œä¸ºå¼€å‘æ›´å…ˆè¿›çš„è¯­ä¹‰å¢å¼ºé«˜å…‰è°±åˆ†ç±»æ¨¡å‹å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå¼ºè°ƒäº†å¤šæ¨¡æ€èåˆåœ¨è§£å†³é«˜ç»´æ•°æ®åˆ†ç±»æŒ‘æˆ˜ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Hyperspectral imaging (HSI) classification is a critical tool with widespread
applications across diverse fields such as agriculture, environmental
monitoring, medicine, and materials science. Due to the limited availability of
high-quality training samples and the high dimensionality of spectral data, HSI
classification models are prone to overfitting and often face challenges in
balancing accuracy and computational complexity. Furthermore, most of HSI
classification models are monomodal, where it solely relies on spectral-spatial
data to learn decision boundaries in the high dimensional embedding space. To
address this, we propose a general-purpose Semantic Spectral-Spatial Fusion
Network (S3FN) that uses contextual, class specific textual descriptions to
complement the training of an HSI classification model. Specifically, S3FN
leverages LLMs to generate comprehensive textual descriptions for each class
label that captures their unique characteristics and spectral behaviors. These
descriptions are then embedded into a vector space using a pre-trained text
encoder such as BERT or RoBERTa to extract meaningful label semantics which in
turn leads to a better feature-label alignment for improved classification
performance. To demonstrate the effectiveness of our approach, we evaluate our
model on three diverse HSI benchmark datasets - Hyperspectral Wood,
HyperspectralBlueberries, and DeepHS-Fruit and report significant performance
boost. Our results highlight the synergy between textual semantics and
spectral-spatial data, paving the way for further advancements in semantically
augmented HSI classification models. Codes are be available in:
https://github.com/milab-nsu/S3FN</p>
<h3 id="4-cross-modal-attention-guided-unlearning-in-vision-language-models">[4] <a href="https://arxiv.org/abs/2510.07567">Cross-Modal Attention Guided Unlearning in Vision-Language Models</a></h3>
<p><em>Karuna Bhaila, Aneesh Komanduri, Minh-Hao Van, Xintao Wu</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„è§†è§‰è¯­è¨€æ¨¡å‹é—å¿˜æ¡†æ¶CAGULï¼Œé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å¼•å¯¼çš„è§†è§‰ä»¤ç‰Œè½¬æ¢æ¥é˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼ŒåŒæ—¶ä¿æŒå‚è€ƒæ¨¡å‹è¡Œä¸ºï¼Œæ— éœ€ä¿®æ”¹é¢„è®­ç»ƒæ¨¡å‹å‚æ•°æˆ–äº§ç”Ÿé‡è®­ç»ƒæˆæœ¬ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½è®°å¿†å¹¶æ³„éœ²ç§æœ‰æˆ–æ•æ„Ÿä¿¡æ¯ï¼Œè€Œç°æœ‰çš„æœºå™¨é—å¿˜æ–¹æ³•ä¸»è¦é’ˆå¯¹è¯­è¨€æ¨¡å‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ç”±äºåŒæ—¶åŒ…å«è§†è§‰å’Œæ–‡æœ¬ä¸Šä¸‹æ–‡è€Œå¢åŠ äº†é—å¿˜è¿‡ç¨‹çš„å¤æ‚æ€§ï¼Œéœ€è¦ä¸“é—¨è§£å†³è§†è§‰ä¸Šä¸‹æ–‡ä¸­æ•æ„Ÿä¿¡æ¯æ³„éœ²çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºè·¨æ¨¡æ€æ³¨æ„åŠ›å¼•å¯¼é—å¿˜æ¡†æ¶CAGULï¼Œåˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›åˆ†æè§†è§‰ä»¤ç‰Œåœ¨è¾“å‡ºç”Ÿæˆä¸­çš„ä½œç”¨ï¼Œé€šè¿‡å¤–éƒ¨æ¨¡å—å¯¹ä½é‡è¦æ€§è§†è§‰ä»¤ç‰Œè¿›è¡Œç¼–ç è½¬æ¢ï¼Œä»è€Œåœ¨ä¸ä¿®æ”¹é¢„è®­ç»ƒæ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆé—å¿˜ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é˜²æ­¢ä¿¡æ¯æ³„éœ²æ–¹é¢è¡¨ç°ä¼˜äºæˆ–ä¸åŸºäºå¾®è°ƒçš„åŸºçº¿æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶èƒ½å¤Ÿä¿æŒå‚è€ƒæ¨¡å‹çš„è¡Œä¸ºç‰¹å¾ï¼Œå®ç°äº†é«˜æ•ˆä¸”å®ç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹é—å¿˜è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> CAGULæ¡†æ¶ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§è½»é‡çº§ä¸”é«˜æ•ˆçš„é—å¿˜æœºåˆ¶ï¼Œé€šè¿‡åˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æŒ‡å¯¼è§†è§‰ä»¤ç‰Œè½¬æ¢ï¼Œæ—¢è§£å†³äº†æ•æ„Ÿä¿¡æ¯æ³„éœ²é—®é¢˜ï¼Œåˆé¿å…äº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„é«˜è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å‚æ•°ä¿®æ”¹ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) have demonstrated immense capabilities in
multi-modal understanding and inference tasks such as Visual Question Answering
(VQA), which requires models to infer outputs based on visual and textual
context simultaneously. Such inference abilities of large-scale pretrained
models are often attributed to the massive scale of pre-training data collected
across several domains. However, the models may memorize private and/or
sensitive information during training and regurgitate it in inference.
Recently, machine unlearning has been leveraged to address the leakage of
private data in LLMs. VLMs add a layer of complexity to this process, as the
visual context in the query may also contain sensitive information in addition
to the text. To address this issue, we explore unlearning for vision-language
models, specifically for the VQA task. We explore the role of visual tokens for
output generation in VLMs using cross-modal attention and utilize it to
formulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and
efficient VLM unlearning framework. In contrast to computationally expensive
model finetuning methods, CAGUL utilizes external modules to encode unlearning
information in visual tokens of low importance for relevant queries. We find
that the transformed visual tokens not only prevent leakage but also retain
reference model behavior. Experimental results show that our method performs
better or on par with finetuning-based baselines without altering the
pre-trained model parameters or incurring retraining costs, making it a
practical and effective unlearning solution for VLMs.</p>
<h3 id="5-rectified-cfg-for-flow-based-models">[5] <a href="https://arxiv.org/abs/2510.07631">Rectified-CFG++ for Flow Based Models</a></h3>
<p><em>Shreshth Saini, Shashank Gupta, Alan C. Bovik</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Rectified-CFG++ï¼Œä¸€ç§ç”¨äºæ•´æµæµæ‰©æ•£æ¨¡å‹çš„è‡ªé€‚åº”é¢„æµ‹å™¨-æ ¡æ­£å™¨å¼•å¯¼æ–¹æ³•ï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥çš„æ¡ä»¶è§„åˆ™è§£å†³äº†æ ‡å‡†CFGåœ¨æ•´æµæµæ¨¡å‹ä¸­å¼•èµ·çš„ç¦»æµå½¢æ¼‚ç§»é—®é¢˜ï¼Œåœ¨å¤šä¸ªå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šå®ç°äº†ç¨³å®šä¸”ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ ‡å‡†æ— åˆ†ç±»å™¨å¼•å¯¼åœ¨æ•´æµæµæ‰©æ•£æ¨¡å‹ä¸­åº”ç”¨æ—¶ä¼šå¼•èµ·ä¸¥é‡çš„ç¦»æµå½¢æ¼‚ç§»ï¼Œå¯¼è‡´è§†è§‰ä¼ªå½±ã€æ–‡æœ¬ä¸å¯¹é½å’Œè„†å¼±è¡Œä¸ºï¼Œè¿™é™åˆ¶äº†æ•´æµæµæ¨¡å‹åœ¨æ–‡æœ¬æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨æ•ˆæœå’Œç¨³å®šæ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºè‡ªé€‚åº”é¢„æµ‹å™¨-æ ¡æ­£å™¨å¼•å¯¼æ–¹æ³•ï¼Œæ¯ä¸ªæ¨ç†æ­¥éª¤é¦–å…ˆæ‰§è¡Œæ¡ä»¶æ•´æµæµæ›´æ–°å°†æ ·æœ¬é”šå®šåœ¨å­¦ä¹ çš„ä¼ è¾“è·¯å¾„é™„è¿‘ï¼Œç„¶ååº”ç”¨åŠ æƒæ¡ä»¶æ ¡æ­£ï¼Œåœ¨æ¡ä»¶å’Œæ— æ¡ä»¶é€Ÿåº¦åœºä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œç¡®ä¿ç”Ÿæˆè½¨è¿¹ä¿æŒåœ¨æ•°æ®æµå½¢çš„æœ‰ç•Œç®¡çŠ¶é‚»åŸŸå†…ã€‚</p>
<p><strong>Result:</strong> åœ¨Fluxã€Stable Diffusion 3/3.5ã€Luminaç­‰å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRectified-CFG++åœ¨MS-COCOã€LAION-Aestheticå’ŒT2I-CompBenchç­‰åŸºå‡†æ•°æ®é›†ä¸ŠæŒç»­ä¼˜äºæ ‡å‡†CFGæ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•è¯æ˜äº†æ•´æµæµæ¨¡å‹ä¸å‡ ä½•æ„ŸçŸ¥å¼•å¯¼ç­–ç•¥çš„æœ‰æ•ˆç»“åˆï¼Œç¡®ä¿äº†ç”Ÿæˆè¿‡ç¨‹çš„è¾¹é™…ä¸€è‡´æ€§å’Œè½¨è¿¹ç¨³å®šæ€§ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆç¡®å®šæ€§æ¨ç†æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨å¹¿æ³›çš„å¼•å¯¼å¼ºåº¦èŒƒå›´å†…ä¿æŒé²æ£’æ€§èƒ½ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Classifier-free guidance (CFG) is the workhorse for steering large diffusion
models toward text-conditioned targets, yet its native application to rectified
flow (RF) based models provokes severe off-manifold drift, yielding visual
artifacts, text misalignment, and brittle behaviour. We present
Rectified-CFG++, an adaptive predictor-corrector guidance that couples the
deterministic efficiency of rectified flows with a geometry-aware conditioning
rule. Each inference step first executes a conditional RF update that anchors
the sample near the learned transport path, then applies a weighted conditional
correction that interpolates between conditional and unconditional velocity
fields. We prove that the resulting velocity field is marginally consistent and
that its trajectories remain within a bounded tubular neighbourhood of the data
manifold, ensuring stability across a wide range of guidance strengths.
Extensive experiments on large-scale text-to-image models (Flux, Stable
Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms
standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and
T2I-CompBench. Project page: https://rectified-cfgpp.github.io/</p>
<h3 id="6-pit-qmm-a-large-multimodal-model-for-no-reference-point-cloud-quality-assessment">[6] <a href="https://arxiv.org/abs/2510.07636">PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment</a></h3>
<p><em>Shashank Gupta, Gregoire Phillips, Alan C. Bovik</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPIT-QMMï¼Œä¸€ç§æ–°é¢–çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œç”¨äºæ— å‚è€ƒç‚¹äº‘è´¨é‡è¯„ä¼°ï¼Œé€šè¿‡ç«¯åˆ°ç«¯èåˆæ–‡æœ¬ã€å›¾åƒå’Œç‚¹äº‘æ•°æ®ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘è´¨é‡è¯„ä¼°é¢†åŸŸå·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨3Dèµ„äº§è´¨é‡è¯„ä¼°é¢†åŸŸå°šæœªå……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯æ— å‚è€ƒç‚¹äº‘è´¨é‡è¯„ä¼°ä¸­ç¼ºä¹èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å¤šæ¨¡æ€äº’è¡¥ä¿¡æ¯çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºPIT-QMMæ¨¡å‹ï¼Œé€šè¿‡è§‚å¯Ÿå‘ç°æ–‡æœ¬æè¿°ã€2DæŠ•å½±å’Œ3Dç‚¹äº‘è§†å›¾ä¸ºç‚¹äº‘è´¨é‡æä¾›äº’è¡¥ä¿¡æ¯ï¼Œæ„å»ºèƒ½å¤Ÿç«¯åˆ°ç«¯å¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œç‚¹äº‘æ•°æ®ä»¥é¢„æµ‹è´¨é‡åˆ†æ•°çš„æ–°å‹å¤§å‹å¤šæ¨¡æ€æ¶æ„ã€‚</p>
<p><strong>Result:</strong> åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•ä»¥æ˜¾è‘—ä¼˜åŠ¿è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œä¸”è®­ç»ƒè¿­ä»£æ¬¡æ•°æ›´å°‘ï¼ŒåŒæ—¶æ¡†æ¶è¿˜æ”¯æŒå¤±çœŸå®šä½å’Œè¯†åˆ«åŠŸèƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç‚¹äº‘è´¨é‡è¯„ä¼°å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œé€šè¿‡å¤šæ¨¡æ€èåˆå®ç°äº†æ›´å‡†ç¡®çš„è¯„ä¼°æ€§èƒ½ï¼ŒåŒæ—¶å¢å¼ºæ¨¡å‹å¯è§£é‡Šæ€§å’Œäº¤äº’æ€§ï¼Œä¸º3Då†…å®¹è´¨é‡åˆ†ææä¾›äº†æ–°çš„æŠ€æœ¯æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Large Multimodal Models (LMMs) have recently enabled considerable advances in
the realm of image and video quality assessment, but this progress has yet to
be fully explored in the domain of 3D assets. We are interested in using these
models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where
the aim is to automatically evaluate the perceptual quality of a point cloud in
absence of a reference. We begin with the observation that different modalities
of data - text descriptions, 2D projections, and 3D point cloud views - provide
complementary information about point cloud quality. We then construct PIT-QMM,
a novel LMM for NR-PCQA that is capable of consuming text, images and point
clouds end-to-end to predict quality scores. Extensive experimentation shows
that our proposed method outperforms the state-of-the-art by significant
margins on popular benchmarks with fewer training iterations. We also
demonstrate that our framework enables distortion localization and
identification, which paves a new way forward for model explainability and
interactivity. Code and datasets are available at
https://www.github.com/shngt/pit-qmm.</p>
<h3 id="7-mutual-learning-for-hashing-unlocking-strong-hash-functions-from-weak-supervision">[7] <a href="https://arxiv.org/abs/2510.07703">Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision</a></h3>
<p><em>Xiaoxu Ma, Runhao Li, Zhenyu Weng</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼±åˆ°å¼ºç›¸äº’å­¦ä¹ æ¡†æ¶MLHï¼Œé€šè¿‡å°†æˆå¯¹å“ˆå¸Œåˆ†æ”¯çš„å±€éƒ¨ç›¸ä¼¼æ€§çŸ¥è¯†è¿ç§»åˆ°ä¸­å¿ƒå“ˆå¸Œåˆ†æ”¯ï¼Œæœ‰æ•ˆè§£å†³äº†ä¸­å¿ƒå“ˆå¸Œæ–¹æ³•åœ¨å»ºæ¨¡å…¨å±€ç»“æ„æ—¶å¿½ç•¥å±€éƒ¨ç›¸ä¼¼æ€§ä¿¡æ¯çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¸­å¿ƒå“ˆå¸Œæ–¹æ³•è™½ç„¶åœ¨æ•è·å…¨å±€æ•°æ®åˆ†å¸ƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å»ºæ¨¡å…¨å±€ç»“æ„æ—¶å¾€å¾€å¿½ç•¥äº†é‡è¦çš„å±€éƒ¨ç›¸ä¼¼æ€§ä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†å…¶æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Method:</strong> MLHæ¡†æ¶åŒ…å«ä¸€ä¸ªå¼ºå¤§çš„ä¸­å¿ƒå“ˆå¸Œåˆ†æ”¯å’Œä¸€ä¸ªè¾ƒå¼±çš„æˆå¯¹å“ˆå¸Œåˆ†æ”¯ï¼Œé€šè¿‡è¿­ä»£å¼ç›¸äº’å­¦ä¹ è¿‡ç¨‹å®ç°çŸ¥è¯†è¿ç§»ï¼Œå¹¶å¼•å…¥äº†åŸºäºæ··åˆä¸“å®¶çš„å“ˆå¸Œä¸“å®¶æ··åˆæ¨¡å—æ¥ä¿ƒè¿›è·¨åˆ†æ”¯çš„æœ‰æ•ˆäº¤äº’ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMLHæ–¹æ³•åœ¨å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­æŒç»­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›å“ˆå¸Œæ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¼±åˆ°å¼ºç›¸äº’å­¦ä¹ æ¡†æ¶åœ¨å“ˆå¸Œå­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ç»“åˆå…¨å±€ç»“æ„å’Œå±€éƒ¨ç›¸ä¼¼æ€§çš„ä¼˜åŠ¿ï¼Œä¸ºæ·±åº¦å“ˆå¸Œæ–¹æ³•çš„å‘å±•æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Deep hashing has been widely adopted for large-scale image retrieval, with
numerous strategies proposed to optimize hash function learning. Pairwise-based
methods are effective in learning hash functions that preserve local similarity
relationships, whereas center-based methods typically achieve superior
performance by more effectively capturing global data distributions. However,
the strength of center-based methods in modeling global structures often comes
at the expense of underutilizing important local similarity information. To
address this limitation, we propose Mutual Learning for Hashing (MLH), a novel
weak-to-strong framework that enhances a center-based hashing branch by
transferring knowledge from a weaker pairwise-based branch. MLH consists of two
branches: a strong center-based branch and a weaker pairwise-based branch.
Through an iterative mutual learning process, the center-based branch leverages
local similarity cues learned by the pairwise-based branch. Furthermore,
inspired by the mixture-of-experts paradigm, we introduce a novel
mixture-of-hash-experts module that enables effective cross-branch interaction,
further enhancing the performance of both branches. Extensive experiments
demonstrate that MLH consistently outperforms state-of-the-art hashing methods
across multiple benchmark datasets.</p>
<h3 id="8-gtr-bench-evaluating-geo-temporal-reasoning-in-vision-language-models">[8] <a href="https://arxiv.org/abs/2510.07791">GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models</a></h3>
<p><em>Qinghongbing Xie, Zhaoyuan Xia, Feng Zhu, Lijun Gong, Ziyue Li, Rui Zhao, Long Zeng</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†åœ°ç†æ—¶åºæ¨ç†åŸºå‡†GTR-Benchï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤§è§„æ¨¡æ‘„åƒå¤´ç½‘ç»œä¸­ç§»åŠ¨ç›®æ ‡çš„åœ°ç†æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨åœ°ç†æ—¶åºæ¨ç†æ–¹é¢çš„ä¸‰å¤§ä¸»è¦ç¼ºé™·ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ—¶ç©ºåŸºå‡†ä¸»è¦å…³æ³¨ä»¥å›¾åƒ/è§†é¢‘ä¸ºèƒŒæ™¯çš„è‡ªä¸­å¿ƒè§†è§’æ¨ç†æˆ–ä»¥å›¾å½¢ä¸ºèƒŒæ™¯çš„åœ°ç†è§†è§’æ¨ç†ï¼Œç¼ºä¹åŒæ—¶åˆ©ç”¨å›¾åƒ/è§†é¢‘å’Œå›¾å½¢èƒŒæ™¯è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ°ç†æ—¶ç©ºæ™ºèƒ½çš„åŸºå‡†ï¼Œè€Œè¿™å¯¹äº¤é€šç®¡ç†å’Œåº”æ€¥å“åº”ç­‰é¢†åŸŸè‡³å…³é‡è¦ã€‚</p>
<p><strong>Method:</strong> ä½œè€…æ„å»ºäº†GTR-BenchåŸºå‡†ï¼Œè¯¥åŸºå‡†è¦æ±‚æ¨¡å‹åœ¨åœ°å›¾å’Œè§†é¢‘ä¹‹é—´è¿›è¡Œå¤šè§†è§’åˆ‡æ¢ï¼Œå¯¹å…·æœ‰éé‡å è§†é‡çš„å¤šä¸ªè§†é¢‘è¿›è¡Œè”åˆæ¨ç†ï¼Œå¹¶åœ¨è§†é¢‘èƒŒæ™¯æœªè§‚å¯Ÿåˆ°çš„æ—¶ç©ºåŒºåŸŸè¿›è¡Œæ¨ç†ã€‚</p>
<p><strong>Result:</strong> å¯¹10å¤šä¸ªæµè¡Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æœ€ä½³ä¸“æœ‰æ¨¡å‹Gemini-2.5-Proï¼ˆ34.9%ï¼‰ä¹Ÿæ˜¾è‘—è½åäºäººç±»è¡¨ç°ï¼ˆ78.61%ï¼‰ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹åœ¨æ—¶ç©ºä¸Šä¸‹æ–‡åˆ©ç”¨ä¸å¹³è¡¡ã€æ—¶åºé¢„æµ‹èƒ½åŠ›å¼±ä»¥åŠåœ°å›¾ä¸å¤šè§†è§’è§†é¢‘è¾“å…¥ç†è§£å¯¹é½èƒ½åŠ›ä¸è¶³ä¸‰å¤§ç¼ºé™·ã€‚</p>
<p><strong>Conclusion:</strong> GTR-Benchä¸ºæ—¶ç©ºæ™ºèƒ½ç ”ç©¶æä¾›äº†å®è´µè§è§£å’Œæ–°æœºé‡ï¼Œæ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åœ°ç†æ—¶åºæ¨ç†æ–¹é¢çš„æ ¸å¿ƒå±€é™æ€§ï¼Œä¸ºæœªæ¥æ¨¡å‹æ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has
attracted much attention due to its importance for Autonomous Driving, Embodied
AI and General Artificial Intelligence. Existing spatial-temporal benchmarks
mainly focus on egocentric perspective reasoning with images/video context, or
geographic perspective reasoning with graphics context (eg. a map), thus fail
to assess VLMs' geographic spatial-temporal intelligence with both images/video
and graphics context, which is important for areas like traffic management and
emergency response. To address the gaps, we introduce Geo-Temporal Reasoning
benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of
moving targets in a large-scale camera network. GTR-Bench is more challenging
as it requires multiple perspective switches between maps and videos, joint
reasoning across multiple videos with non-overlapping fields of view, and
inference over spatial-temporal regions that are unobserved by any video
context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that
even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags
behind human performance (78.61%) on geo-temporal reasoning. Moreover, our
comprehensive analysis on GTR-Bench reveals three primary deficiencies of
current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by
an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in
temporal forecasting, which leads to worse performance on temporal-emphasized
tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to
comprehend or align the map data with multi-view video inputs. We believe
GTR-Bench offers valuable insights and opens up new opportunities for research
and applications in spatial-temporal intelligence. Benchmark and code will be
released at https://github.com/X-Luffy/GTR-Bench.</p>
<h3 id="9-xyzcylinder-feedforward-reconstruction-for-driving-scenes-based-on-a-unified-cylinder-lifting-method">[9] <a href="https://arxiv.org/abs/2510.07856">XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method</a></h3>
<p><em>Haochen Yu, Qiankun Liu, Hongyuan Liu, Jianfei Jiang, Juntao Lyu, Jiansheng Chen, Huimin Ma</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºXYZCylinderï¼Œä¸€ç§åŸºäºç»Ÿä¸€åœ†æŸ±ä½“æå‡æ–¹æ³•çš„å‰é¦ˆæ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€ç›¸æœºå»ºæ¨¡å’Œæ··åˆè¡¨ç¤ºè®¾è®¡ï¼Œè§£å†³äº†é©¾é©¶åœºæ™¯é‡å»ºä¸­å›ºå®šè§†å›¾å˜æ¢æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œç¨€ç–è§†å›¾é‡å»ºç²¾åº¦å—é™çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å‰é¦ˆé‡å»ºèŒƒå¼åœ¨é©¾é©¶åœºæ™¯é‡å»ºä¸­å­˜åœ¨ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šå›ºå®šè§†å›¾å˜æ¢åœ¨ç›¸æœºé…ç½®å˜åŒ–æ—¶å¤±æ•ˆï¼Œé™åˆ¶äº†ä¸åŒé©¾é©¶åœºæ™¯é—´çš„æ³›åŒ–èƒ½åŠ›ï¼›360Â°å…¨æ™¯ç¨€ç–è§†å›¾é—´é‡å åŒºåŸŸå°ä¸”é©¾é©¶åœºæ™¯å¤æ‚ï¼Œå¢åŠ äº†å­¦ä¹ éš¾åº¦å¹¶é™ä½äº†é‡å»ºç²¾åº¦ã€‚</p>
<p><strong>Method:</strong> æå‡ºç»Ÿä¸€åœ†æŸ±ä½“ç›¸æœºå»ºæ¨¡ç­–ç•¥é¿å…å­¦ä¹ è§†ç‚¹ä¾èµ–çš„ç©ºé—´å¯¹åº”å…³ç³»ï¼Œé€šè¿‡å¯è°ƒå‚æ•°ç»Ÿä¸€ä¸åŒç›¸æœºé…ç½®ï¼›è®¾è®¡åŸºäºåœ†æŸ±å¹³é¢ç‰¹å¾ç»„çš„æ··åˆè¡¨ç¤ºï¼ŒåŒ…å«å¤šä¸ªä¸“ç”¨æ¨¡å—å°†2Då›¾åƒç‰¹å¾æå‡åˆ°3Dç©ºé—´ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜XYZCylinderåœ¨ä¸åŒè¯„ä¼°è®¾ç½®ä¸‹å‡è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶èƒ½ä»¥é›¶æ ·æœ¬æ–¹å¼æ³›åŒ–åˆ°å…¶ä»–é©¾é©¶åœºæ™¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»Ÿä¸€ç›¸æœºå»ºæ¨¡å’Œæ··åˆè¡¨ç¤ºåœ¨æå‡é©¾é©¶åœºæ™¯é‡å»ºæ³›åŒ–èƒ½åŠ›å’Œç²¾åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„ä¸‰ç»´é‡å»ºæä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Recently, more attention has been paid to feedforward reconstruction
paradigms, which mainly learn a fixed view transformation implicitly and
reconstruct the scene with a single representation. However, their
generalization capability and reconstruction accuracy are still limited while
reconstructing driving scenes, which results from two aspects: (1) The fixed
view transformation fails when the camera configuration changes, limiting the
generalization capability across different driving scenes equipped with
different camera configurations. (2) The small overlapping regions between
sparse views of the $360^\circ$ panorama and the complexity of driving scenes
increase the learning difficulty, reducing the reconstruction accuracy. To
handle these difficulties, we propose \textbf{XYZCylinder}, a feedforward model
based on a unified cylinder lifting method which involves camera modeling and
feature lifting. Specifically, to improve the generalization capability, we
design a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the
learning of viewpoint-dependent spatial correspondence and unifies different
camera configurations with adjustable parameters. To improve the reconstruction
accuracy, we propose a hybrid representation with several dedicated modules
based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image
features to 3D space. Experimental results show that XYZCylinder achieves
state-of-the-art performance under different evaluation settings, and can be
generalized to other driving scenes in a zero-shot manner. Project page:
\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.</p>
<h3 id="10-marc-memory-augmented-rl-token-compression-for-efficient-video-understanding">[10] <a href="https://arxiv.org/abs/2510.07915">MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding</a></h3>
<p><em>Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMARCæ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–æ£€ç´¢å’Œå¼ºåŒ–å­¦ä¹ è’¸é¦å®ç°è§†é¢‘ä»¤ç‰Œå‹ç¼©ï¼Œåœ¨ä»…ä½¿ç”¨ä¸€å¸§ä»¤ç‰Œçš„æƒ…å†µä¸‹è¾¾åˆ°æ¥è¿‘åŸºå‡†æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹ä»å›¾åƒæ‰©å±•åˆ°è§†é¢‘æ—¶é¢ä¸´é«˜å¸§ç‡å’Œé•¿æŒç»­æ—¶é—´å¸¦æ¥çš„æ²‰é‡è®¡ç®—æˆæœ¬æŒ‘æˆ˜ï¼Œç°æœ‰å…è®­ç»ƒä»¤ç‰Œå‹ç¼©æ–¹æ³•ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±å’Œæ€§èƒ½ä¸‹é™ã€‚</p>
<p><strong>Method:</strong> æå‡ºMARCæ¡†æ¶ï¼Œé‡‡ç”¨æ£€ç´¢-å‹ç¼©ç­–ç•¥ï¼ŒåŒ…å«è§†è§‰è®°å¿†æ£€ç´¢å™¨é€‰æ‹©å…³é”®ç‰‡æ®µå’Œå‹ç¼©ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œå°†æ•™å¸ˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> åœ¨å…­ä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMARCä»…ä½¿ç”¨ä¸€å¸§ä»¤ç‰Œå³å¯è¾¾åˆ°æ¥è¿‘åŸºçº¿å‡†ç¡®ç‡ï¼Œè§†è§‰ä»¤ç‰Œå‡å°‘95%ï¼ŒGPUå†…å­˜é™ä½72%ï¼Œå»¶è¿Ÿå‡å°‘23.9%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•å±•ç¤ºäº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å®ç°é«˜æ•ˆå®æ—¶è§†é¢‘ç†è§£çš„æ½œåŠ›ï¼Œé€‚ç”¨äºè§†é¢‘é—®ç­”ã€ç›‘æ§å’Œè‡ªåŠ¨é©¾é©¶ç­‰åº”ç”¨åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>The rapid progress of large language models (LLMs) has laid the foundation
for multimodal models. However, visual language models (VLMs) still face heavy
computational costs when extended from images to videos due to high frame rates
and long durations. Token compression is a promising solution, yet most
existing training-free methods cause information loss and performance
degradation. To overcome this, we propose \textbf{Memory-Augmented
Reinforcement Learning-based Token Compression (MARC)}, which integrates
structured retrieval and RL-based distillation. MARC adopts a
\textit{retrieve-then-compress} strategy using a \textbf{Visual Memory
Retriever (VMR)} to select key clips and a \textbf{Compression Group Relative
Policy Optimization (C-GRPO)} framework to distil reasoning ability from a
teacher to a student model. Experiments on six video benchmarks show that MARC
achieves near-baseline accuracy using only one frame's tokens -- reducing
visual tokens by \textbf{95\%}, GPU memory by \textbf{72\%}, and latency by
\textbf{23.9\%}. This demonstrates its potential for efficient, real-time video
understanding in resource-constrained settings such as video QA, surveillance,
and autonomous driving.</p>
<h3 id="11-ttom-test-time-optimization-and-memorization-for-compositional-video-generation">[11] <a href="https://arxiv.org/abs/2510.07940">TTOM: Test-Time Optimization and Memorization for Compositional Video Generation</a></h3>
<p><em>Leigang Qu, Ziyang Wang, Na Zheng, Wenjie Wang, Liqiang Nie, Tat-Seng Chua</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†TTOMï¼ˆæµ‹è¯•æ—¶ä¼˜åŒ–ä¸è®°å¿†ï¼‰æ¡†æ¶ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼˜åŒ–æ–°å‚æ•°å¹¶åˆ©ç”¨å‚æ•°åŒ–è®°å¿†æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘åŸºç¡€æ¨¡å‹åœ¨ç»„åˆåœºæ™¯ä¸‹çš„æ–‡æœ¬-å›¾åƒå¯¹é½èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†é¢‘åŸºç¡€æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç»„åˆåœºæ™¯ï¼ˆå¦‚è¿åŠ¨ã€æ•°é‡å…³ç³»å’Œç©ºé—´å…³ç³»ï¼‰ä¸­å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´æ–‡æœ¬ä¸ç”Ÿæˆè§†é¢‘å†…å®¹ä¹‹é—´çš„å¯¹é½æ•ˆæœä¸ä½³ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Method:</strong> TTOMæ¡†æ¶é‡‡ç”¨æµ‹è¯•æ—¶ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡é€šç”¨å¸ƒå±€-æ³¨æ„åŠ›ç›®æ ‡å¼•å¯¼æ–°å‚æ•°çš„é›†æˆä¸ä¼˜åŒ–ï¼Œè€Œéç›´æ¥å¹²é¢„æ½œåœ¨ç©ºé—´æˆ–æ³¨æ„åŠ›æœºåˆ¶ï¼›åŒæ—¶å¼•å…¥å‚æ•°åŒ–è®°å¿†æœºåˆ¶ï¼Œåœ¨æµå¼è§†é¢‘ç”Ÿæˆè®¾ç½®ä¸­ç»´æŠ¤å†å²ä¼˜åŒ–ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒæ’å…¥ã€è¯»å–ã€æ›´æ–°å’Œåˆ é™¤ç­‰çµæ´»æ“ä½œã€‚</p>
<p><strong>Result:</strong> åœ¨T2V-CompBenchå’ŒVbenchåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTTOMèƒ½å¤Ÿæœ‰æ•ˆè§£è€¦ç»„åˆæ€§ä¸–ç•ŒçŸ¥è¯†ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¯è¿ç§»æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†ç»„åˆè§†é¢‘ç”Ÿæˆçš„è·¨æ¨¡æ€å¯¹é½æ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> TTOMæ¡†æ¶ä¸ä»…è§£å†³äº†è§†é¢‘åŸºç¡€æ¨¡å‹åœ¨ç»„åˆåœºæ™¯ä¸­çš„å¯¹é½é—®é¢˜ï¼Œè¿˜è¯æ˜äº†å…¶åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„ç»„åˆè§†é¢‘ç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œä¸ºå®æ—¶è·¨æ¨¡æ€å¯¹é½æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Video Foundation Models (VFMs) exhibit remarkable visual generation
performance, but struggle in compositional scenarios (e.g., motion, numeracy,
and spatial relation). In this work, we introduce Test-Time Optimization and
Memorization (TTOM), a training-free framework that aligns VFM outputs with
spatiotemporal layouts during inference for better text-image alignment. Rather
than direct intervention to latents or attention per-sample in existing work,
we integrate and optimize new parameters guided by a general layout-attention
objective. Furthermore, we formulate video generation within a streaming
setting, and maintain historical optimization contexts with a parametric memory
mechanism that supports flexible operations, such as insert, read, update, and
delete. Notably, we found that TTOM disentangles compositional world knowledge,
showing powerful transferability and generalization. Experimental results on
the T2V-CompBench and Vbench benchmarks establish TTOM as an effective,
practical, scalable, and efficient framework to achieve cross-modal alignment
for compositional video generation on the fly.</p>
<h3 id="12-cir-cot-towards-interpretable-composed-image-retrieval-via-end-to-end-chain-of-thought-reasoning">[12] <a href="https://arxiv.org/abs/2510.08003">CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning</a></h3>
<p><em>Weihuang Lin, Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CIR-CoTï¼Œè¿™æ˜¯é¦–ä¸ªé›†æˆæ˜¾å¼æ€ç»´é“¾æ¨ç†çš„ç«¯åˆ°ç«¯æ£€ç´¢å¯¼å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†é“¾æ¥å¢å¼ºè·¨æ¨¡æ€äº¤äº’ç†è§£ï¼Œåœ¨æå‡æ£€ç´¢å‡†ç¡®æ€§çš„åŒæ—¶å®ç°å†³ç­–è¿‡ç¨‹é€æ˜åŒ–ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç»„åˆå›¾åƒæ£€ç´¢æ–¹æ³•ä¸»è¦ä½œä¸ºé»‘ç›’è¿è¡Œï¼Œè¿™ç§ä¸é€æ˜æ€§ä¸ä»…é˜»ç¢ç”¨æˆ·ç†è§£æ£€ç´¢åŸç†ï¼Œè¿˜é™åˆ¶äº†æ¨¡å‹éµå¾ªå¤æ‚ç»†ç²’åº¦æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿæä¾›å¯è§£é‡Šæ¨ç†çš„æ£€ç´¢ç³»ç»Ÿã€‚</p>
<p><strong>Method:</strong> æå‡ºCIR-CoTæ¨¡å‹ï¼Œé€šè¿‡å¼ºåˆ¶æ¨¡å‹å…ˆç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†é“¾æ¥å¢å¼ºè·¨æ¨¡æ€äº¤äº’æ•è·èƒ½åŠ›ï¼›é‡‡ç”¨ä¸‰é˜¶æ®µè¿‡ç¨‹åˆ›å»ºç»“æ„åŒ–æ€ç»´é“¾æ ‡æ³¨ï¼ŒåŒ…æ‹¬æè¿°ã€æ¨ç†å’Œç»“è®ºï¼›å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼Œå¹¶å°†æœ€ç»ˆæ£€ç´¢æ„å›¾ç¼–ç åˆ°ä¸“ç”¨åµŒå…¥ä¸­ã€‚</p>
<p><strong>Result:</strong> CIR-CoTåœ¨é¢†åŸŸå†…æ•°æ®é›†ï¼ˆFashionIQã€CIRRï¼‰ä¸Šå®ç°äº†é«˜åº¦ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶åœ¨é¢†åŸŸå¤–CIRCOæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ›´æœ‰æ•ˆå’Œå¯ä¿¡çš„æ£€ç´¢ç³»ç»Ÿå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é›†æˆæ˜¾å¼æ€ç»´é“¾æ¨ç†èƒ½å¤Ÿæ˜¾è‘—æå‡ç»„åˆå›¾åƒæ£€ç´¢çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼Œç»“æ„åŒ–æ¨ç†æ ‡æ³¨çš„åˆ›å»ºä¸ºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æä¾›äº†æ–°çš„æ•°æ®èŒƒå¼ï¼Œæ¨åŠ¨äº†æ£€ç´¢ç³»ç»Ÿå‘æ›´é€æ˜å’Œå¯ä¿¡çš„æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Composed Image Retrieval (CIR), which aims to find a target image from a
reference image and a modification text, presents the core challenge of
performing unified reasoning across visual and semantic modalities. While
current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more
recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown
progress, they predominantly function as ``black boxes." This inherent opacity
not only prevents users from understanding the retrieval rationale but also
restricts the models' ability to follow complex, fine-grained instructions. To
overcome these limitations, we introduce CIR-CoT, the first end-to-end
retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)
reasoning. By compelling the model to first generate an interpretable reasoning
chain, CIR-CoT enhances its ability to capture crucial cross-modal
interactions, leading to more accurate retrieval while making its decision
process transparent. Since existing datasets like FashionIQ and CIRR lack the
necessary reasoning data, a key contribution of our work is the creation of
structured CoT annotations using a three-stage process involving a caption,
reasoning, and conclusion. Our model is then fine-tuned to produce this
structured output before encoding its final retrieval intent into a dedicated
embedding. Comprehensive experiments show that CIR-CoT achieves highly
competitive performance on in-domain datasets (FashionIQ, CIRR) and
demonstrates remarkable generalization on the out-of-domain CIRCO dataset,
establishing a new path toward more effective and trustworthy retrieval
systems.</p>
<h3 id="13-darkhash-a-data-free-backdoor-attack-against-deep-hashing">[13] <a href="https://arxiv.org/abs/2510.08094">DarkHash: A Data-Free Backdoor Attack Against Deep Hashing</a></h3>
<p><em>Ziqi Zhou, Menghao Deng, Yufei Song, Hangtao Zhang, Wei Wan, Shengshan Hu, Minghui Li, Leo Yu Zhang, Dezhong Yao</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DarkHashï¼Œé¦–ä¸ªé’ˆå¯¹æ·±åº¦å“ˆå¸Œæ¨¡å‹çš„æ— æ•°æ®åé—¨æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡å…·æœ‰åŒé‡è¯­ä¹‰å¼•å¯¼çš„å½±å­åé—¨æ”»å‡»æ¡†æ¶ï¼Œåœ¨æ— éœ€è®¿é—®è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆåé—¨æ¤å…¥ï¼ŒåŒæ—¶ä¿æŒåŸå§‹æ£€ç´¢ç²¾åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ·±åº¦å“ˆå¸Œæ¨¡å‹åé—¨æ”»å‡»æ–¹æ³•éœ€è¦è®¿é—®è®­ç»ƒæ•°æ®é›†è¿›è¡Œåé—¨æ¤å…¥ï¼Œä½†åœ¨ç°å®ä¸–ç•Œä¸­ç”±äºéšç§ä¿æŠ¤å’ŒçŸ¥è¯†äº§æƒè€ƒè™‘ï¼Œè·å–æ­¤ç±»æ•°æ®å¾€å¾€è¢«ç¦æ­¢ï¼Œå› æ­¤å¦‚ä½•åœ¨æ— éœ€è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å‘æ·±åº¦å“ˆå¸Œæ¨¡å‹åµŒå…¥åé—¨ï¼ŒåŒæ—¶ä¿æŒåŸå§‹ä»»åŠ¡æ£€ç´¢ç²¾åº¦ï¼Œæˆä¸ºä¸€ä¸ªæ–°é¢–ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„å½±å­åé—¨æ”»å‡»æ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡è¯­ä¹‰å¼•å¯¼æœºåˆ¶ï¼Œé€šè¿‡ä»…å¾®è°ƒå—å®³è€…æ¨¡å‹çš„ç‰¹å®šå±‚å¹¶ä½¿ç”¨ä»£ç†æ•°æ®é›†æ¥åµŒå…¥åé—¨åŠŸèƒ½å¹¶ä¿æŒåŸå§‹æ£€ç´¢ç²¾åº¦ï¼›åˆ©ç”¨ä¸ªä½“æ ·æœ¬ä¸å…¶é‚»å±…ä¹‹é—´çš„å…³ç³»æ¥å¢å¼ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„åé—¨æ”»å‡»ï¼Œé€šè¿‡è®¾è®¡æ‹“æ‰‘å¯¹é½æŸå¤±ï¼Œä¼˜åŒ–ä¸ªä½“å’Œç›¸é‚»ä¸­æ¯’æ ·æœ¬æœå‘ç›®æ ‡æ ·æœ¬ï¼Œè¿›ä¸€æ­¥æå‡æ”»å‡»èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªå›¾åƒæ•°æ®é›†ã€äº”ç§æ¨¡å‹æ¶æ„å’Œä¸¤ç§å“ˆå¸Œæ–¹æ³•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDarkHashå…·æœ‰é«˜åº¦æœ‰æ•ˆæ€§ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åé—¨æ”»å‡»æ–¹æ³•ï¼›é˜²å¾¡å®éªŒæ˜¾ç¤ºDarkHashèƒ½å¤ŸæŠµå¾¡ç°æœ‰çš„ä¸»æµåé—¨é˜²å¾¡æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åœ¨æ— éœ€è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å¯¹æ·±åº¦å“ˆå¸Œæ¨¡å‹è¿›è¡Œæœ‰æ•ˆåé—¨æ”»å‡»çš„å¯è¡Œæ€§ï¼Œæå‡ºäº†åŒé‡è¯­ä¹‰å¼•å¯¼å’Œæ‹“æ‰‘å¯¹é½æŸå¤±ç­‰åˆ›æ–°æŠ€æœ¯ï¼Œä¸ºæ·±åº¦å“ˆå¸Œæ¨¡å‹çš„å®‰å…¨æ€§å’Œé²æ£’æ€§ç ”ç©¶æä¾›äº†é‡è¦å¯ç¤ºï¼ŒåŒæ—¶ä¹Ÿå‡¸æ˜¾äº†æ— æ•°æ®åé—¨æ”»å‡»åœ¨ç°å®åœºæ™¯ä¸­çš„æ½œåœ¨å¨èƒã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Benefiting from its superior feature learning capabilities and efficiency,
deep hashing has achieved remarkable success in large-scale image retrieval.
Recent studies have demonstrated the vulnerability of deep hashing models to
backdoor attacks. Although these studies have shown promising attack results,
they rely on access to the training dataset to implant the backdoor. In the
real world, obtaining such data (e.g., identity information) is often
prohibited due to privacy protection and intellectual property concerns.
Embedding backdoors into deep hashing models without access to the training
data, while maintaining retrieval accuracy for the original task, presents a
novel and challenging problem. In this paper, we propose DarkHash, the first
data-free backdoor attack against deep hashing. Specifically, we design a novel
shadow backdoor attack framework with dual-semantic guidance. It embeds
backdoor functionality and maintains original retrieval accuracy by fine-tuning
only specific layers of the victim model using a surrogate dataset. We consider
leveraging the relationship between individual samples and their neighbors to
enhance backdoor attacks during training. By designing a topological alignment
loss, we optimize both individual and neighboring poisoned samples toward the
target sample, further enhancing the attack capability. Experimental results on
four image datasets, five model architectures, and two hashing methods
demonstrate the high effectiveness of DarkHash, outperforming existing
state-of-the-art backdoor attack methods. Defense experiments show that
DarkHash can withstand existing mainstream backdoor defense methods.</p>
<h3 id="14-improving-temporal-understanding-logic-consistency-in-video-language-models-via-attention-enhancement">[14] <a href="https://arxiv.org/abs/2510.08138">Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement</a></h3>
<p><em>Chengzhi Li, Heyan Huang, Ping Jian, Zhen Yang, Yaning Tian</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ—¶é—´æ¡ä»¶æ³¨æ„åŠ›é”åŒ–ï¼ˆTCASï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å¢å¼ºè·¨æ¨¡æ€æ³¨æ„åŠ›å¤´çš„æ—¶é—´åˆ†è¾¨èƒ½åŠ›æ¥è§£å†³è§†é¢‘è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„å“åº”é€»è¾‘ä¸ä¸€è‡´é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ—¶é—´é€»è¾‘ä¸€è‡´æ€§å’Œæ—¶é—´ç†è§£èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹ç»å¸¸ç”Ÿæˆè‡ªç›¸çŸ›ç›¾çš„è¾“å‡ºï¼Œä¸¥é‡å½±å“å…¶å¯é æ€§å¹¶é˜»ç¢å®é™…åº”ç”¨ã€‚åœ¨è§†é¢‘è¯­è¨€æ¨¡å‹ä¸­ï¼Œè¿™ç§ç°è±¡è¡¨ç°ä¸ºæ¨¡å‹æ— æ³•åŸºäºå…¶åŸºç¡€è¾“å‡ºå¯¹é‡è¿°é—®é¢˜æä¾›é€»è¾‘ä¸€è‡´çš„å“åº”ï¼Œä½†å…¶æ ¹æœ¬åŸå› å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é‡‡ç”¨å¯è§£é‡Šæ€§é©±åŠ¨çš„æ–¹æ³•åˆ†æã€ç»Ÿè®¡æ€»ç»“å¹¶å¹²é¢„è¯¥ç°è±¡çš„å¯èƒ½å› ç´ ï¼Œæå‡ºäº†æ—¶é—´æ¡ä»¶æ³¨æ„åŠ›é”åŒ–ï¼ˆTCASï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºæ³¨æ„åŠ›å·®å¼‚æ„å»ºå¢å¼ºç›®æ ‡ï¼Œé€šè¿‡å¢å¼ºæ¨¡å‹çš„æ—¶é—´åˆ†è¾¨èƒ½åŠ›æ¥æ”¹å–„å…¶æ—¶é—´ç†è§£é€»è¾‘ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ—¶é—´é€»è¾‘ä¸€è‡´æ€§ã€‚è¿›ä¸€æ­¥çš„å¯è§£é‡Šæ€§åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç¡®å®æ”¹å–„äº†æ³¨æ„åŠ›å¤´çš„æ—¶é—´åŒºåˆ†èƒ½åŠ›ï¼ŒéªŒè¯äº†ç ”ç©¶ç»“è®ºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨é€šç”¨è§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡ä¸­å®ç°äº†æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æ—¶é—´é€»è¾‘ä¸€è‡´æ€§æ˜¯æ—¶é—´ç†è§£çš„å…³é”®ç“¶é¢ˆï¼Œé€šè¿‡å¢å¼ºä¸€è‡´æ€§å¯ä»¥æ¨åŠ¨è§†é¢‘æ—¶é—´ç†è§£çš„æ˜¾è‘—è¿›å±•ã€‚è¯¥æ–¹æ³•ä¸ä»…è§£å†³äº†å“åº”ä¸ä¸€è‡´é—®é¢˜ï¼Œè¿˜æå‡äº†æ¨¡å‹åœ¨æ—¶é—´ç›¸å…³ä»»åŠ¡ä¸­çš„æ•´ä½“è¡¨ç°ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) often generate self-contradictory outputs, which
severely impacts their reliability and hinders their adoption in practical
applications. In video-language models (Video-LLMs), this phenomenon recently
draws the attention of researchers. Specifically, these models fail to provide
logically consistent responses to rephrased questions based on their grounding
outputs. However, the underlying causes of this phenomenon remain
underexplored. In this work, we adopt an interpretability-driven approach to
analyze, statistically summarize, and intervention the potential factors of the
phenomenon. We find that one of the primary reasons for the inconsistency in
responses lies in the inability of cross-modal attention heads to effectively
distinguish video tokens across different timestamps. To address this, we
propose an attention enhancement method called Temporally Conditioned Attention
Sharpening (TCAS), which constructs an enhancement objective based on attention
distinctions to enhance the model's temporal resolution capability, thereby
improving its temporal understanding logic consistency. Experimental results
demonstrate that our method significantly enhances the temporal logic
consistency of Video-LLMs. Further interpretability analyses reveal that our
method indeed improves the temporal discriminability of attention heads,
validating our conclusions. Additionally, our method achieves performance
improvements in general video temporal grounding tasks, highlighting that
temporal logic consistency is a bottleneck in temporal understanding. By
enhancing consistency, our method drives significant progress in video temporal
understanding.</p>
<h3 id="15-unimmvsr-a-unified-multi-modal-framework-for-cascaded-video-super-resolution">[15] <a href="https://arxiv.org/abs/2510.08143">UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution</a></h3>
<p><em>Shian Du, Menghan Xia, Chang Liu, Quande Liu, Xintao Wang, Pengfei Wan, Xiangyang Ji</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniMMVSRï¼Œè¿™æ˜¯é¦–ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ç”Ÿæˆå¼è§†é¢‘è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•´åˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€æ¡ä»¶ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡å’Œæ¡ä»¶ä¸€è‡´æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çº§è”è§†é¢‘è¶…åˆ†è¾¨ç‡æ–¹æ³•ä¸»è¦å±€é™äºæ–‡æœ¬åˆ°è§†é¢‘ä»»åŠ¡ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ–‡æœ¬ä¹‹å¤–çš„å¤šæ¨¡æ€ç”Ÿæˆæ¡ä»¶ï¼Œè€Œè¿™äº›æ¡ä»¶å¯¹äºç¡®ä¿å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆçš„ä¿çœŸåº¦è‡³å…³é‡è¦ã€‚</p>
<p><strong>Method:</strong> UniMMVSRé‡‡ç”¨æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç³»ç»Ÿæ¢ç´¢äº†æ¡ä»¶æ³¨å…¥ç­–ç•¥ã€è®­ç»ƒæ–¹æ¡ˆå’Œæ•°æ®æ··åˆæŠ€æœ¯ï¼Œè®¾è®¡äº†é’ˆå¯¹ä¸åŒæ¨¡æ€æ¡ä»¶çš„æ•°æ®æ„å»ºå’Œåˆ©ç”¨æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç²¾ç¡®åˆ©ç”¨æ‰€æœ‰æ¡ä»¶ç±»å‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜UniMMVSRæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„è§†é¢‘å…·æœ‰æ›´ä¸°å¯Œçš„ç»†èŠ‚å’Œæ›´é«˜çš„å¤šæ¨¡æ€æ¡ä»¶ä¸€è‡´æ€§ï¼Œå¹¶éªŒè¯äº†ä¸åŸºç¡€æ¨¡å‹ç»“åˆå®ç°4Kè§†é¢‘å¤šæ¨¡æ€å¼•å¯¼ç”Ÿæˆçš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€æ¡ä»¶åœ¨è§†é¢‘è¶…åˆ†è¾¨ç‡ä¸­çš„é‡è¦æ€§ï¼Œä¸ºé«˜è´¨é‡è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œçªç ´äº†ç°æœ‰æŠ€æœ¯åœ¨4Kè§†é¢‘ç”Ÿæˆæ–¹é¢çš„é™åˆ¶ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Cascaded video super-resolution has emerged as a promising technique for
decoupling the computational burden associated with generating high-resolution
videos using large foundation models. Existing studies, however, are largely
confined to text-to-video tasks and fail to leverage additional generative
conditions beyond text, which are crucial for ensuring fidelity in multi-modal
video generation. We address this limitation by presenting UniMMVSR, the first
unified generative video super-resolution framework to incorporate hybrid-modal
conditions, including text, images, and videos. We conduct a comprehensive
exploration of condition injection strategies, training schemes, and data
mixture techniques within a latent video diffusion model. A key challenge was
designing distinct data construction and condition utilization methods to
enable the model to precisely utilize all condition types, given their varied
correlations with the target video. Our experiments demonstrate that UniMMVSR
significantly outperforms existing methods, producing videos with superior
detail and a higher degree of conformity to multi-modal conditions. We also
validate the feasibility of combining UniMMVSR with a base model to achieve
multi-modal guided generation of 4K video, a feat previously unattainable with
existing techniques.</p>
<h3 id="16-beyond-textual-cot-interleaved-text-image-chains-with-deep-confidence-reasoning-for-image-editing">[16] <a href="https://arxiv.org/abs/2510.08157">Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing</a></h3>
<p><em>Zhentao Zou, Zhengrong Yue, Kunpeng Du, Binlei Bao, Hanting Li, Haizhen Xie, Guozheng Xu, Yue Zhou, Yali Wang, Jie Hu, Xue Jiang, Xinghao Chen</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MUREæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€äº¤é”™æ–‡æœ¬-å›¾åƒæ€ç»´é“¾å°†è§†è§‰ç¼–è¾‘è¿‡ç¨‹ä»çº¯æ–‡æœ¬æ¨ç†è½¬å‘è§†è§‰åŒ–æ¨ç†ï¼Œå¹¶å¼•å…¥å¤šæ¨¡æ€æ·±åº¦ç½®ä¿¡åº¦æœºåˆ¶æ¥å‡å°‘å¹»è§‰é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚å›¾åƒç¼–è¾‘ä»»åŠ¡çš„ç²¾åº¦å’Œä¿çœŸåº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºè‡ªç„¶è¯­è¨€çš„å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¯¹è±¡äº¤å‰å’Œç»†ç²’åº¦ç©ºé—´å…³ç³»æ—¶å­˜åœ¨å›°éš¾ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹æ˜¾å¼æ¨ç†è¿‡ç¨‹ã€‚çº¯æ–‡æœ¬æ€ç»´é“¾æˆ–ä»…æ·»åŠ åæ ‡ä¿¡æ¯çš„æ€ç»´é“¾åœ¨è¡¨ç¤ºå¤æ‚è§†è§‰å¸ƒå±€å’ŒæŒ‡å¯¼åƒç´ çº§ç»†èŠ‚ç”Ÿæˆæ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§é™åˆ¶ï¼Œæ— æ³•æä¾›å¿…è¦çš„è§†è§‰çº¿ç´¢ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†å¤šæ¨¡æ€æ¨ç†ç¼–è¾‘æ¡†æ¶ï¼Œé‡‡ç”¨åŸç”Ÿå¤šæ¨¡æ€äº¤é”™æ–‡æœ¬-å›¾åƒæ€ç»´é“¾ï¼Œåœ¨æ¨ç†é“¾çš„æ¯ä¸€æ­¥ä¸­æ–‡æœ¬æè¿°åè·Ÿéšç›¸åº”çš„è§†è§‰æç¤ºã€‚å¼•å…¥äº†å¤šæ¨¡æ€æ·±åº¦ç½®ä¿¡åº¦æ¨ç†èŒƒå¼ï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹çš„æ·±åº¦ç½®ä¿¡åº¦åˆ†æ•°ä¿®å‰ªä½è´¨é‡åˆ†æ”¯ï¼Œç¡®ä¿æ¨¡å‹å§‹ç»ˆæ²¿ç€é«˜è´¨é‡è½¨è¿¹è¿›è¡Œç¼–è¾‘ã€‚è¯¥æ–¹æ³•å°†å¤æ‚ç¼–è¾‘ä»»åŠ¡åˆ†è§£ä¸ºç›¸äº’ä¾èµ–çš„å­ä»»åŠ¡ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªå›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œé€šè¿‡åˆ†è§£å¤æ‚ä»»åŠ¡å®ç°äº†æ¯ä¸ªé˜¶æ®µçš„æ›´é«˜ç²¾åº¦ï¼Œç”Ÿæˆäº†é«˜ä¿çœŸåº¦çš„ç¼–è¾‘ç»“æœã€‚å‘å¸ƒäº†é¦–ä¸ªCoT-Edit-14Kæ•°æ®é›†ï¼ŒåŒ…å«14Kä¸ªé«˜è´¨é‡ç¼–è¾‘ç¤ºä¾‹ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ä»çº¯æ–‡æœ¬æ¨ç†è½¬å‘å¤šæ¨¡æ€äº¤é”™æ¨ç†å¯¹äºå¤æ‚è§†è§‰ç¼–è¾‘ä»»åŠ¡çš„é‡è¦æ€§ï¼Œå¤šæ¨¡æ€æ·±åº¦ç½®ä¿¡åº¦æœºåˆ¶æœ‰æ•ˆç¼“è§£äº†å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ã€‚æå‡ºçš„æ¡†æ¶ä¸ºè§†è§‰ç¼–è¾‘ä»»åŠ¡æä¾›äº†æ›´ç²¾ç¡®å’Œå¯æ§çš„è§£å†³æ–¹æ¡ˆï¼Œå®šä¹‰äº†äº¤é”™æ–‡æœ¬-å›¾åƒé“¾çš„åˆ¶å®šæ ‡å‡†ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ¨ç†ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Image editing with natural language has gained significant popularity, yet
existing methods struggle with intricate object intersections and fine-grained
spatial relationships due to the lack of an explicit reasoning process. While
Chain-of-Thought (CoT) has been explored to enhance reasoning, purely textual
CoT or CoT augmented with coordinate information is fundamentally limited in
its ability to represent intricate visual layouts and lacks the necessary
visual cues to guide the generation of fine-grained, pixel-level details. To
address these challenges, we propose Multimodal Reasoning Edit (MURE), a novel
framework that shifts the visual editing process from purely text-based
reasoning to a series of interleaved textual and visual rationales. Our
framework performs image editing using a natively multimodal, interleaved
text-image CoT. This approach generates a step-by-step chain of reasoning where
a textual description is followed by a corresponding visual cue, such as a
positional mask that defined intended edited regions or a representation of new
content. Furthermore, to mitigate the hallucination phenomenon of large
language models, we introduce Multimodal Deep Confidence (MMDC) reasoning
paradigm. This paradigm explores a tree of visual reasoning paths at each step.
By pruning low-quality branches using a deep confidence score from a reward
model, it ensures the model consistently follows a high-quality trajectory
towards the final edited result. The proposed method decomposes complex editing
tasks into interdependent sub-tasks, achieving greater precision at each stage
and yielding high-fidelity edited results. We define the formulation for
interleaved text-image chains and release the first CoT-Edit-14K dataset,
comprising 14K high-quality editing examples. Extensive experiments show that
our method yields significant improvements across three image editing
benchmarks.</p>
<h3 id="17-instructudrag-joint-text-instructions-and-object-dragging-for-interactive-image-editing">[17] <a href="https://arxiv.org/abs/2510.08181">InstructUDrag: Joint Text Instructions and Object Dragging for Interactive Image Editing</a></h3>
<p><em>Haoran Yu, Yi Shi</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºInstructUDragæ¡†æ¶ï¼Œå°†æ–‡æœ¬æŒ‡ä»¤ä¸ç‰©ä½“æ‹–æ‹½ç›¸ç»“åˆï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬ç¼–è¾‘æ–¹æ³•å®šä½ä¸ç²¾ç¡®å’Œæ‹–æ‹½æ–¹æ³•ä»…é™äºé™æ€é‡å®šä½çš„é—®é¢˜ï¼Œå®ç°äº†åŒæ—¶è¿›è¡Œç‰©ä½“æ‹–æ‹½å’ŒåŸºäºæ–‡æœ¬çš„å›¾åƒç¼–è¾‘ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­å­˜åœ¨ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šåŸºäºæ–‡æœ¬çš„æ–¹æ³•éš¾ä»¥å®ç°ç²¾ç¡®çš„ç‰©ä½“å®šä½ï¼Œè€Œç‰©ä½“æ‹–æ‹½æ–¹æ³•ä»…é™äºé™æ€é‡å®šä½ï¼Œæ— æ³•åŒæ—¶è¿›è¡Œè¯­ä¹‰ç¼–è¾‘ã€‚</p>
<p><strong>Method:</strong> æå‡ºInstructUDragæ¡†æ¶ï¼Œå°†ç‰©ä½“æ‹–æ‹½è§†ä¸ºå›¾åƒé‡å»ºè¿‡ç¨‹ï¼Œåˆ†ä¸ºä¸¤ä¸ªååŒåˆ†æ”¯ï¼šç§»åŠ¨é‡å»ºåˆ†æ”¯ä½¿ç”¨åŸºäºèƒ½é‡çš„æ¢¯åº¦å¼•å¯¼ç²¾ç¡®ç§»åŠ¨ç‰©ä½“å¹¶ä¼˜åŒ–äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œæ–‡æœ¬é©±åŠ¨ç¼–è¾‘åˆ†æ”¯å…±äº«æ¢¯åº¦ä¿¡å·ç¡®ä¿å˜æ¢ä¸€è‡´æ€§ï¼ŒåŒæ—¶é‡‡ç”¨DDPMåæ¼”å’Œå™ªå£°å›¾å…ˆéªŒä¿¡æ¯æ³¨å…¥æ¥ä¿æŒç§»åŠ¨ç‰©ä½“çš„ç»“æ„å®Œæ•´æ€§ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInstructUDragå®ç°äº†çµæ´»ä¸”é«˜ä¿çœŸçš„å›¾åƒç¼–è¾‘ï¼Œåœ¨ç‰©ä½“é‡å®šä½ç²¾åº¦å’Œå›¾åƒå†…å®¹è¯­ä¹‰æ§åˆ¶æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ–‡æœ¬æŒ‡ä»¤ä¸ç‰©ä½“æ‹–æ‹½ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›äº†æ›´ç²¾ç»†çš„å›¾åƒç¼–è¾‘æ§åˆ¶èƒ½åŠ›ï¼Œåœ¨ç‰©ä½“å®šä½ç²¾åº¦å’Œè¯­ä¹‰å±æ€§ç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Text-to-image diffusion models have shown great potential for image editing,
with techniques such as text-based and object-dragging methods emerging as key
approaches. However, each of these methods has inherent limitations: text-based
methods struggle with precise object positioning, while object dragging methods
are confined to static relocation. To address these issues, we propose
InstructUDrag, a diffusion-based framework that combines text instructions with
object dragging, enabling simultaneous object dragging and text-based image
editing. Our framework treats object dragging as an image reconstruction
process, divided into two synergistic branches. The moving-reconstruction
branch utilizes energy-based gradient guidance to move objects accurately,
refining cross-attention maps to enhance relocation precision. The text-driven
editing branch shares gradient signals with the reconstruction branch, ensuring
consistent transformations and allowing fine-grained control over object
attributes. We also employ DDPM inversion and inject prior information into
noise maps to preserve the structure of moved objects. Extensive experiments
demonstrate that InstructUDrag facilitates flexible, high-fidelity image
editing, offering both precision in object relocation and semantic control over
image content.</p>
<h3 id="18-a-multimodal-depth-aware-method-for-embodied-reference-understanding">[18] <a href="https://arxiv.org/abs/2510.08278">A Multimodal Depth-Aware Method For Embodied Reference Understanding</a></h3>
<p><em>Fevziye Irem Eyiokur, Dogucan Yaman, HazÄ±m Kemal Ekenel, Alexander Waibel</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…·èº«å‚è€ƒç†è§£æ¡†æ¶ï¼Œé€šè¿‡è”åˆåˆ©ç”¨LLMæ•°æ®å¢å¼ºã€æ·±åº¦å›¾æ¨¡æ€å’Œæ·±åº¦æ„ŸçŸ¥å†³ç­–æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç›®æ ‡å¯¹è±¡è¯†åˆ«å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹æ–¹æ³•åœ¨å­˜åœ¨å¤šä¸ªå€™é€‰å¯¹è±¡çš„æ¨¡ç³Šåœºæ™¯ä¸­ç»å¸¸å¤±è´¥ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å…·èº«å‚è€ƒç†è§£ä»»åŠ¡ä¸­çš„æ­§ä¹‰æ€§é—®é¢˜ï¼Œè¿™é™åˆ¶äº†åœ¨å¤æ‚æˆ–æ‚ä¹±ç¯å¢ƒä¸­çš„å®é™…åº”ç”¨æ•ˆæœã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„ERUæ¡†æ¶æ•´åˆäº†åŸºäºLLMçš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€æ·±åº¦å›¾æ¨¡æ€ä¿¡æ¯ä»¥åŠæ·±åº¦æ„ŸçŸ¥å†³ç­–æ¨¡å—ï¼Œå®ç°äº†è¯­è¨€æŒ‡ä»¤å’Œå…·èº«çº¿ç´¢çš„é²æ£’èåˆï¼Œä¸“é—¨é’ˆå¯¹å¤æ‚ç¯å¢ƒä¸­çš„æ­§ä¹‰æ¶ˆè§£è¿›è¡Œä¼˜åŒ–è®¾è®¡ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨æŒ‡ç§°ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æ›´å‡†ç¡®å’Œå¯é çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€ä¿¡æ¯èåˆå’Œæ·±åº¦æ„ŸçŸ¥æœºåˆ¶åœ¨å…·èº«å‚è€ƒç†è§£ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºå¤„ç†å¤æ‚è§†è§‰åœºæ™¯ä¸­çš„æ­§ä¹‰æ€§é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ¨åŠ¨äº†å…·èº«AIç³»ç»Ÿåœ¨å®é™…ç¯å¢ƒä¸­çš„åº”ç”¨å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Embodied Reference Understanding requires identifying a target object in a
visual scene based on both language instructions and pointing cues. While prior
works have shown progress in open-vocabulary object detection, they often fail
in ambiguous scenarios where multiple candidate objects exist in the scene. To
address these challenges, we propose a novel ERU framework that jointly
leverages LLM-based data augmentation, depth-map modality, and a depth-aware
decision module. This design enables robust integration of linguistic and
embodied cues, improving disambiguation in complex or cluttered environments.
Experimental results on two datasets demonstrate that our approach
significantly outperforms existing baselines, achieving more accurate and
reliable referent detection.</p>
<h3 id="19-unlocking-3d-affordance-segmentation-with-2d-semantic-knowledge">[19] <a href="https://arxiv.org/abs/2510.08316">Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge</a></h3>
<p><em>Yu Huang, Zelin Peng, Changsong Wen, Xiaokang Yang, Wei Shen</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰é©±åŠ¨çš„å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡è·¨æ¨¡æ€äº²å’ŒåŠ›è¿ç§»å°†å¤§è§„æ¨¡2Dè§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸°å¯Œè¯­ä¹‰çŸ¥è¯†è½¬ç§»åˆ°3Dé¢†åŸŸï¼Œè§£å†³äº†3DåŠŸèƒ½åˆ†å‰²ä¸­è¯­ä¹‰è¾¹ç•Œæ¨¡ç³Šçš„é—®é¢˜ï¼Œå¹¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰3DåŠŸèƒ½åˆ†å‰²æ–¹æ³•é€šå¸¸ä¾èµ–ç‚¹äº‘ç¼–ç å™¨ä½œä¸ºé€šç”¨ç‰¹å¾æå–å™¨ï¼Œä½†å¿½è§†äº†3Dæ•°æ®å›ºæœ‰çš„ç¨€ç–æ€§ã€å™ªå£°å’Œå‡ ä½•æ¨¡ç³Šæ€§ç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„3Dç‰¹å¾ç¼ºä¹æ¸…æ™°ä¸”è¯­ä¹‰ä¸€è‡´çš„åŠŸèƒ½è¾¹ç•Œã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†è·¨æ¨¡æ€äº²å’ŒåŠ›è¿ç§»é¢„è®­ç»ƒç­–ç•¥ï¼Œå°†3Dç¼–ç å™¨ä¸æå‡çš„2Dè¯­ä¹‰å¯¹é½ï¼Œå¹¶è”åˆä¼˜åŒ–é‡å»ºã€äº²å’ŒåŠ›å’Œå¤šæ ·æ€§ä»¥äº§ç”Ÿè¯­ä¹‰ç»„ç»‡çš„è¡¨ç¤ºï¼›åœ¨æ­¤åŸºç¡€ä¸Šè®¾è®¡äº†è·¨æ¨¡æ€åŠŸèƒ½åˆ†å‰²Transformerï¼Œé›†æˆå¤šæ¨¡æ€æç¤ºä¸CMATé¢„è®­ç»ƒç‰¹å¾æ¥ç”Ÿæˆç²¾ç¡®çš„æç¤ºæ„ŸçŸ¥åˆ†å‰²å›¾ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨3DåŠŸèƒ½åˆ†å‰²ä»»åŠ¡ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œæ˜¾è‘—æå‡äº†åˆ†å‰²ç²¾åº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†2Dè§†è§‰åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰çŸ¥è¯†è¿ç§»åˆ°3Dé¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œä¸º3DåŠŸèƒ½åˆ†å‰²æä¾›äº†ä¸€ç§è¯­ä¹‰é©±åŠ¨çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹æœºå™¨äººæ“ä½œã€å…·èº«AIå’Œå¢å¼ºç°å®ç­‰åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Affordance segmentation aims to parse 3D objects into functionally distinct
parts, bridging recognition and interaction for applications in robotic
manipulation, embodied AI, and AR. While recent studies leverage visual or
textual prompts to guide this process, they often rely on point cloud encoders
as generic feature extractors, overlooking the intrinsic challenges of 3D data
such as sparsity, noise, and geometric ambiguity. As a result, 3D features
learned in isolation frequently lack clear and semantically consistent
functional boundaries. To address this bottleneck, we propose a
semantic-grounded learning paradigm that transfers rich semantic knowledge from
large-scale 2D Vision Foundation Models (VFMs) into the 3D domain.
Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training
strategy that aligns a 3D encoder with lifted 2D semantics and jointly
optimizes reconstruction, affinity, and diversity to yield semantically
organized representations. Building on this backbone, we further design the
Cross-modal Affordance Segmentation Transformer (CAST), which integrates
multi-modal prompts with CMAT-pretrained features to generate precise,
prompt-aware segmentation maps. Extensive experiments on standard benchmarks
demonstrate that our framework establishes new state-of-the-art results for 3D
affordance segmentation.</p>
<h3 id="20-evaluating-small-vision-language-models-on-distance-dependent-traffic-perception">[20] <a href="https://arxiv.org/abs/2510.08352">Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception</a></h3>
<p><em>Nikos Theodoridis, Tim Brophy, Reenu Mohandas, Ganesh Sistu, Fiachra Collins, Anthony Scanlan, Ciaran Eising</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DTPQAåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“æ³¨äºäº¤é€šåœºæ™¯æ„ŸçŸ¥èƒ½åŠ›çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œé€šè¿‡æ’é™¤æ¨ç†é—®é¢˜å¹¶æ·»åŠ è·ç¦»æ ‡æ³¨æ¥è¯„ä¼°å°è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ç¯å¢ƒä¸‹çš„æ„ŸçŸ¥æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®åº”ç”¨ä¸­ç¼ºä¹å¯é çš„æ„ŸçŸ¥ç³»ç»Ÿè¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯å¯¹è¿œè·ç¦»ç‰©ä½“çš„æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³ï¼Œä¸”ç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€åŒ…å«æ¨ç†ä»»åŠ¡è€Œæ— æ³•å•ç‹¬è¯„ä¼°çº¯æ„ŸçŸ¥èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†è·ç¦»æ ‡æ³¨çš„äº¤é€šæ„ŸçŸ¥é—®ç­”åŸºå‡†DTPQAï¼Œä¸“é—¨è®¾è®¡ä»…åŒ…å«æ„ŸçŸ¥é—®é¢˜çš„è§†è§‰é—®ç­”ä»»åŠ¡ï¼Œå¹¶é‡ç‚¹è¯„ä¼°äº†å¤šä¸ªæœ€å…ˆè¿›çš„å°è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿‘è·ç¦»å’Œè¿œè·ç¦»æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³å°è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨DTPQAä¸Šçš„å¹³å‡å‡†ç¡®ç‡çº¦ä¸º60%ï¼Œè¿œä½äºäººç±»çº¦85%çš„è¡¨ç°æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å·¦å³æ–¹å‘åŒºåˆ†ç­‰åŸºæœ¬æ„ŸçŸ¥ä»»åŠ¡ä¸Šæ¨¡å‹è¡¨ç°å°¤ä¸ºå›°éš¾ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å½“å‰å°è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨äº¤é€šåœºæ™¯æ„ŸçŸ¥æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œè·ç¦»å®é™…è‡ªåŠ¨é©¾é©¶åº”ç”¨çš„è¦æ±‚è¿˜æœ‰è¾ƒå¤§å·®è·ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹åœ¨åŸºç¡€æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) are becoming increasingly powerful,
demonstrating strong performance on a variety of tasks that require both visual
and textual understanding. Their strong generalisation abilities make them a
promising component for automated driving systems, which must handle unexpected
corner cases. However, to be trusted in such safety-critical applications, a
model must first possess a reliable perception system. Moreover, since critical
objects and agents in traffic scenes are often at a distance, we require
systems that are not "shortsighted", i.e., systems with strong perception
capabilities at both close (up to 20 meters) and long (30+ meters) range. With
this in mind, we introduce Distance-Annotated Traffic Perception Question
Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused
solely on perception-based questions in traffic scenes, enriched with distance
annotations. By excluding questions that require reasoning, we ensure that
model performance reflects perception capabilities alone. Since automated
driving hardware has limited processing power and cannot support large VLMs,
our study centers on smaller VLMs. More specifically, we evaluate several
state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the
simplicity of the questions, these models significantly underperform compared
to humans (~60% average accuracy for the best-performing small VLM versus ~85%
human performance). However, it is important to note that the human sample size
was relatively small, which imposes statistical limitations. We also identify
specific perception tasks, such as distinguishing left from right, that remain
particularly challenging for these models.</p>
<h3 id="21-univideo-unified-understanding-generation-and-editing-for-videos">[21] <a href="https://arxiv.org/abs/2510.08377">UniVideo: Unified Understanding, Generation, and Editing for Videos</a></h3>
<p><em>Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhu Chen</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniVideoæ¡†æ¶ï¼Œå°†ç»Ÿä¸€å¤šæ¨¡æ€å»ºæ¨¡æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œé€šè¿‡åŒæµæ¶æ„ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å¤šç§è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šä»»åŠ¡ä¸“ç”¨æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æ”¯æŒä»»åŠ¡ç»„åˆå’Œè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä¸»è¦å±€é™äºå›¾åƒé¢†åŸŸï¼Œè§†é¢‘é¢†åŸŸçš„ç»Ÿä¸€å»ºæ¨¡ä»å­˜åœ¨æ˜æ˜¾ç©ºç™½ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å°†ç»Ÿä¸€å»ºæ¨¡èŒƒå¼æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œè§£å†³è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­å¤šæ¨¡æ€æŒ‡ä»¤ç†è§£ä¸è§†è§‰ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> UniVideoé‡‡ç”¨åŒæµæ¶æ„è®¾è®¡ï¼Œç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”¨äºå¤æ‚å¤šæ¨¡æ€æŒ‡ä»¤çš„å‡†ç¡®ç†è§£ï¼Œä»¥åŠå¤šæ¨¡æ€DiTç”¨äºè§†é¢‘ç”Ÿæˆï¼Œç¡®ä¿è§†è§‰ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶å°†å¤šæ ·åŒ–çš„è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ç»Ÿä¸€åœ¨å•ä¸€å¤šæ¨¡æ€æŒ‡ä»¤èŒƒå¼ä¸‹è¿›è¡Œè”åˆè®­ç»ƒã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniVideoåœ¨æ–‡æœ¬/å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆå’Œä¸Šä¸‹æ–‡è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­ï¼ŒåŒ¹é…æˆ–è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„ä»»åŠ¡ä¸“ç”¨åŸºçº¿æ¨¡å‹ã€‚è¯¥æ¡†æ¶å±•ç°å‡ºä»»åŠ¡ç»„åˆèƒ½åŠ›å’Œè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œå³ä½¿æœªç»è‡ªç”±å½¢å¼è§†é¢‘ç¼–è¾‘çš„ä¸“é—¨è®­ç»ƒï¼Œä¹Ÿèƒ½å¤„ç†æœªè§è¿‡çš„ç¼–è¾‘æŒ‡ä»¤ã€‚</p>
<p><strong>Conclusion:</strong> UniVideoçš„ç»Ÿä¸€è®¾è®¡å®ç°äº†ä¸¤ç§å½¢å¼çš„æ³›åŒ–ï¼šæ”¯æŒä»»åŠ¡ç»„åˆèƒ½åŠ›ï¼Œä»¥åŠä»å¤§è§„æ¨¡å›¾åƒç¼–è¾‘æ•°æ®å‘è§†é¢‘ç¼–è¾‘ä»»åŠ¡çš„è¿ç§»èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è¿˜æ”¯æŒåŸºäºè§†è§‰æç¤ºçš„è§†é¢‘ç”Ÿæˆï¼Œä¸ºæœªæ¥è§†é¢‘å¤šæ¨¡æ€ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ï¼Œä½œè€…å°†å‘å¸ƒæ¨¡å‹å’Œä»£ç ä»¥ä¿ƒè¿›åç»­ç ”ç©¶ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Unified multimodal models have shown promising results in multimodal content
generation and editing but remain largely limited to the image domain. In this
work, we present UniVideo, a versatile framework that extends unified modeling
to the video domain. UniVideo adopts a dual-stream design, combining a
Multimodal Large Language Model (MLLM) for instruction understanding with a
Multimodal DiT (MMDiT) for video generation. This design enables accurate
interpretation of complex multimodal instructions while preserving visual
consistency. Built on this architecture, UniVideo unifies diverse video
generation and editing tasks under a single multimodal instruction paradigm and
is jointly trained across them. Extensive experiments demonstrate that UniVideo
matches or surpasses state-of-the-art task-specific baselines in
text/image-to-video generation, in-context video generation and in-context
video editing. Notably, the unified design of UniVideo enables two forms of
generalization. First, UniVideo supports task composition, such as combining
editing with style transfer, by integrating multiple capabilities within a
single instruction. Second, even without explicit training on free-form video
editing, UniVideo transfers its editing capability from large-scale image
editing data to this setting, handling unseen instructions such as
green-screening characters or changing materials within a video. Beyond these
core capabilities, UniVideo also supports visual-prompt-based video generation,
where the MLLM interprets visual prompts and guides the MMDiT during synthesis.
To foster future research, we will release our model and code.</p>
<h3 id="22-the-visual-iconicity-challenge-evaluating-vision-language-models-on-sign-language-form-meaning-mapping">[22] <a href="https://arxiv.org/abs/2510.08482">The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping</a></h3>
<p><em>Onur KeleÅŸ, AslÄ± Ã–zyÃ¼rek, Gerardo Ortega, Kadir GÃ¶kgÃ¶, Esam Ghaleb</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å¼•å…¥è§†è§‰è±¡ä¼¼æ€§æŒ‘æˆ˜åŸºå‡†ï¼Œè¯„ä¼°äº†13ä¸ªæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ‰‹è¯­è±¡ä¼¼æ€§ç†è§£ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°æ¨¡å‹åœ¨è¯­éŸ³å½¢å¼é¢„æµ‹å’Œé€æ˜åº¦ä»»åŠ¡ä¸Šä½äºäººç±»åŸºå‡†ï¼Œä½†æ¨¡å‹å¯¹è¯­éŸ³å½¢å¼çš„æ•æ„Ÿåº¦ä¸äººç±»è±¡ä¼¼æ€§åˆ¤æ–­å­˜åœ¨ç›¸å…³æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ‰‹è¯­ä¸­æ™®éå­˜åœ¨çš„è±¡ä¼¼æ€§ï¼ˆè¯­è¨€å½¢å¼ä¸æ„ä¹‰ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼‰ä¸ºè§†è§‰åŸºç¡€æä¾›äº†è‡ªç„¶æµ‹è¯•å¹³å°ï¼Œä½†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹é¢ä¸´ä»åŠ¨æ€äººä½“è¿åŠ¨è€Œéé™æ€ä¸Šä¸‹æ–‡ä¸­æ¢å¤è¿™ç§åŸºæœ¬æ˜ å°„å…³ç³»çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥è§†è§‰è±¡ä¼¼æ€§æŒ‘æˆ˜åŸºå‡†ï¼Œå°†å¿ƒç†è¯­è¨€å­¦æµ‹é‡æ–¹æ³•åº”ç”¨äºè§†é¢‘æ•°æ®ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šè¯­éŸ³ç¬¦å·å½¢å¼é¢„æµ‹ï¼ˆæ‰‹å½¢ã€ä½ç½®ï¼‰ã€é€æ˜åº¦ï¼ˆä»è§†è§‰å½¢å¼æ¨æ–­æ„ä¹‰ï¼‰å’Œåˆ†çº§è±¡ä¼¼æ€§è¯„åˆ†ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹æµ‹è¯•äº†13ä¸ªæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> åœ¨è¯­éŸ³å½¢å¼é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹èƒ½å¤Ÿæ¢å¤éƒ¨åˆ†æ‰‹å½¢å’Œä½ç½®ç»†èŠ‚ä½†ä»ä½äºäººç±»è¡¨ç°ï¼›åœ¨é€æ˜åº¦ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹è¡¨ç°è¿œä½äºäººç±»åŸºå‡†ï¼›åªæœ‰é¡¶çº§æ¨¡å‹ä¸äººç±»è±¡ä¼¼æ€§è¯„åˆ†å‘ˆç°ä¸­ç­‰ç›¸å…³æ€§ï¼Œä¸”è¯­éŸ³å½¢å¼é¢„æµ‹èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹ä¸äººç±»è±¡ä¼¼æ€§åˆ¤æ–­ç›¸å…³æ€§æ›´é«˜ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶éªŒè¯äº†è¿™äº›è¯Šæ–­ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜æ¨¡å‹å¯¹è§†è§‰åŸºç¡€ç»“æ„å…·æœ‰å…±äº«æ•æ„Ÿæ€§ï¼Œå¼ºè°ƒäº†å°†äººç±»ä¸­å¿ƒä¿¡å·å’Œå…·èº«å­¦ä¹ æ–¹æ³•çº³å…¥å¤šæ¨¡æ€æ¨¡å‹ä»¥æ”¹è¿›è§†è§‰åŸºç¡€å»ºæ¨¡çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Iconicity, the resemblance between linguistic form and meaning, is pervasive
in signed languages, offering a natural testbed for visual grounding. For
vision-language models (VLMs), the challenge is to recover such essential
mappings from dynamic human motion rather than static context. We introduce the
\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts
psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological
sign-form prediction (e.g., handshape, location), (ii) transparency (inferring
meaning from visual form), and (iii) graded iconicity ratings. We assess $13$
state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the
Netherlands and compare them to human baselines. On \textit{phonological form
prediction}, VLMs recover some handshape and location detail but remain below
human performance; on \textit{transparency}, they are far from human baselines;
and only top models correlate moderately with human \textit{iconicity ratings}.
Interestingly, \textit{models with stronger phonological form prediction
correlate better with human iconicity judgment}, indicating shared sensitivity
to visually grounded structure. Our findings validate these diagnostic tasks
and motivate human-centric signals and embodied learning methods for modelling
iconicity and improving visual grounding in multimodal models.</p>
<h3 id="23-videoverse-how-far-is-your-t2v-generator-from-a-world-model">[23] <a href="https://arxiv.org/abs/2510.08398">VideoVerse: How Far is Your T2V Generator from a World Model?</a></h3>
<p><em>Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, Lei Zhang</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VideoVerseåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç†è§£å¤æ‚æ—¶é—´å› æœå…³ç³»å’Œä¸–ç•ŒçŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°ä½“ç³»åœ¨äº‹ä»¶çº§æ—¶é—´å› æœæ€§å’Œä¸–ç•ŒçŸ¥è¯†ç³»ç»Ÿè¯„ä¼°æ–¹é¢çš„ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ä½¿å¾—ä¼ ç»ŸåŸºå‡†æµ‹è¯•æ—¥ç›Šä¸è¶³ï¼Œä¸»è¦é—®é¢˜åŒ…æ‹¬ç°æœ‰è¯„ä¼°ç»´åº¦æ— æ³•åŒºåˆ†æœ€å…ˆè¿›æ¨¡å‹ã€äº‹ä»¶çº§æ—¶é—´å› æœæ€§è¯„ä¼°ä¸¥é‡ä¸è¶³ã€ä»¥åŠç¼ºä¹å¯¹ä¸–ç•ŒçŸ¥è¯†çš„ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œè€Œè¿™äº›èƒ½åŠ›å¯¹äºæ„å»ºä¸–ç•Œæ¨¡å‹è‡³å…³é‡è¦ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ”¶é›†äº†è·¨å¤šä¸ªé¢†åŸŸçš„ä»£è¡¨æ€§è§†é¢‘ï¼Œæå–å…·æœ‰å†…åœ¨æ—¶é—´å› æœå…³ç³»çš„äº‹ä»¶çº§æè¿°ï¼Œå¹¶é€šè¿‡ç‹¬ç«‹æ ‡æ³¨è€…å°†å…¶é‡å†™ä¸ºæ–‡æœ¬åˆ°è§†é¢‘æç¤ºï¼Œè®¾è®¡äº†åŒ…å«åä¸ªè¯„ä¼°ç»´åº¦çš„äºŒå…ƒè¯„ä¼°é—®é¢˜å¥—ä»¶ï¼Œå¹¶å¼€å‘äº†åŸºäºç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹çš„äººç±»åå¥½å¯¹é½çš„é—®ç­”è¯„ä¼°æµç¨‹ã€‚</p>
<p><strong>Result:</strong> VideoVerseåŸºå‡†åŒ…å«300ä¸ªç²¾å¿ƒç­–åˆ’çš„æç¤ºï¼Œæ¶‰åŠ815ä¸ªäº‹ä»¶å’Œ793ä¸ªäºŒå…ƒè¯„ä¼°é—®é¢˜ï¼Œé€šè¿‡å¯¹æœ€å…ˆè¿›çš„å¼€æºå’Œé—­æºæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ï¼Œæ·±å…¥åˆ†æäº†å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆå™¨ä¸ä¸–ç•Œæ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç†è§£å¤æ‚æ—¶é—´å› æœå…³ç³»å’Œä¸–ç•ŒçŸ¥è¯†æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œä¸ºæœªæ¥æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦çš„è¯„ä¼°æ¡†æ¶å’Œæ–¹å‘æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†äº‹ä»¶çº§æ—¶é—´ç†è§£èƒ½åŠ›å¯¹äºæ„å»ºçœŸæ­£ä¸–ç•Œæ¨¡å‹çš„å…³é”®é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>The recent rapid advancement of Text-to-Video (T2V) generation technologies,
which are critical to build ``world models'', makes the existing benchmarks
increasingly insufficient to evaluate state-of-the-art T2V models. First,
current evaluation dimensions, such as per-frame aesthetic quality and temporal
consistency, are no longer able to differentiate state-of-the-art T2V models.
Second, event-level temporal causality, which not only distinguishes video from
other modalities but also constitutes a crucial component of world models, is
severely underexplored in existing benchmarks. Third, existing benchmarks lack
a systematic assessment of world knowledge, which are essential capabilities
for building world models. To address these issues, we introduce VideoVerse, a
comprehensive benchmark that focuses on evaluating whether a T2V model could
understand complex temporal causality and world knowledge in the real world. We
collect representative videos across diverse domains (e.g., natural landscapes,
sports, indoor scenes, science fiction, chemical and physical experiments) and
extract their event-level descriptions with inherent temporal causality, which
are then rewritten into text-to-video prompts by independent annotators. For
each prompt, we design a suite of binary evaluation questions from the
perspective of dynamic and static properties, with a total of ten carefully
defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully
curated prompts, involving 815 events and 793 binary evaluation questions.
Consequently, a human preference aligned QA-based evaluation pipeline is
developed by using modern vision-language models. Finally, we perform a
systematic evaluation of state-of-the-art open-source and closed-source T2V
models on VideoVerse, providing in-depth analysis on how far the current T2V
generators are from world models.</p>
<h3 id="24-to-sink-or-not-to-sink-visual-information-pathways-in-large-vision-language-models">[24] <a href="https://arxiv.org/abs/2510.08510">To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models</a></h3>
<p><em>Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æ­ç¤ºäº†ViTæ³¨æ„åŠ›æ±‡ï¼ˆattention sinksï¼‰åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å…³é”®ä½œç”¨ï¼Œæå‡ºé€šè¿‡è®­ç»ƒæ— å…³å’Œè®­ç»ƒç›¸å…³æ–¹æ³•æ¥æ˜¾å¼åˆ©ç”¨è¿™äº›é«˜èŒƒæ•°è§†è§‰æ ‡è®°ï¼Œä»è€Œæ˜¾è‘—æå‡å¤šç§LVLMæ¶æ„åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨LLMå†…éƒ¨çš„æ³¨æ„åŠ›æ±‡é—®é¢˜ï¼Œè€Œå¿½è§†äº†è§†è§‰ç¼–ç å™¨ä¸­é«˜èŒƒæ•°è§†è§‰æ ‡è®°çš„é‡è¦ä½œç”¨ï¼Œè¿™äº›ViTæ³¨æ„åŠ›æ±‡è™½ç„¶åŒ…å«é«˜å±‚æ¬¡è¯­ä¹‰æ¦‚å¿µï¼Œä½†åœ¨ç°æœ‰LVLMæ¶æ„ä¸­å¾€å¾€è¢«å¿½ç•¥ï¼Œå¯¼è‡´è§†è§‰ä¿¡å·ä»ViTåˆ°LLMçš„ä¼ æ’­æ•ˆç‡ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> é€šè¿‡å®šæ€§å’Œå®šé‡åˆ†æViTæ³¨æ„åŠ›æ±‡ä¸­çš„ä¿¡æ¯ï¼Œæå‡ºè®­ç»ƒæ— å…³å’Œè®­ç»ƒç›¸å…³ä¸¤ç§æ–¹æ³•æ¥æ›´å¥½åœ°åˆ©ç”¨è¿™äº›æ ‡è®°ï¼ŒåŒ…æ‹¬æ”¹è¿›LLMå¯¹è¿™äº›ä¿¡æ¯çš„è§£é‡Šæ–¹å¼ï¼Œå¹¶è¯„ä¼°å…¶åˆ©ç”¨ç¨‹åº¦ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜æ˜¾å¼åˆ©ç”¨ViTæ³¨æ„åŠ›æ±‡æ ‡è®°èƒ½å¤Ÿåœ¨å¤šç§LVLMæ¶æ„å’Œè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†è¿™äº›æ ‡è®°åœ¨å¢å¼ºè§†è§‰ç†è§£ä¸æ¨ç†æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ViTæ³¨æ„åŠ›æ±‡æ˜¯LVLMä¸­æœªè¢«å……åˆ†æŒ–æ˜çš„é‡è¦èµ„æºï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°åˆ©ç”¨è¿™äº›é«˜è¯­ä¹‰è§†è§‰æ ‡è®°å¯ä»¥æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä¸ºæœªæ¥LVLMæ¶æ„è®¾è®¡æä¾›äº†æ–°çš„ä¼˜åŒ–æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Large Vision Language Models (LVLMs) have recently emerged as powerful
architectures capable of understanding and reasoning over both visual and
textual information. These models typically rely on two key components: a
Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual
content into a sequence of image tokens and serves as the perceptual front-end
-- the eyes of the model. In contrast, the LLM interprets these tokens to
perform high-level reasoning, generates responses, and functions as the
cognitive core -- the brain of the model. However, it remains unclear which
visual tokens contribute most significantly to understanding and reasoning, and
how effectively these signals are propagated from ViT to the LLM. While most
existing works have focused on identifying attention sinks, low-semantic tokens
receiving disproportionately high attention, within the LLM, we shift the focus
to the vision encoder by identifying a class of high-norm visual tokens from
ViT, referred to as ViT attention sinks -- a problem that has been rarely
studied but is indeed very important for LVLMs. Our findings show that these
ViT sinks encapsulate high-level semantic concepts from images, allowing the
LLM to perform more effective understanding and reasoning. Despite their
importance, these sink tokens are often overlooked in existing LVLM
architectures. To explore their contribution, we present both qualitative and
quantitative analyses of the information embedded in these sink tokens. We also
propose both training-free and training-based approaches to better leverage how
this information is interpreted by the LLM, and to what extent. By explicitly
utilizing these tokens, we demonstrate substantial improvements across a range
of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT
attention sinks in enhancing visual reasoning.</p>
<h3 id="25-large-scale-diffusion-distillation-via-score-regularized-continuous-time-consistency">[25] <a href="https://arxiv.org/abs/2510.08431">Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency</a></h3>
<p><em>Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é¦–æ¬¡å°†è¿ç»­æ—¶é—´ä¸€è‡´æ€§è’¸é¦æ‰©å±•åˆ°é€šç”¨åº”ç”¨çº§å›¾åƒå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºå¾—åˆ†æ­£åˆ™åŒ–è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹(rCM)ï¼Œé€šè¿‡é›†æˆå¾—åˆ†è’¸é¦ä½œä¸ºé•¿è·³è·ƒæ­£åˆ™å™¨ï¼Œåœ¨ä¿æŒé«˜ç”Ÿæˆå¤šæ ·æ€§çš„åŒæ—¶æ˜¾è‘—æå‡è§†è§‰è´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹(sCM)åœ¨å­¦æœ¯è§„æ¨¡æ‰©æ•£åŠ é€Ÿæ–¹é¢å…·æœ‰ç†è®ºä¼˜åŠ¿å’Œå®è¯æ•ˆæœï¼Œä½†å…¶åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ä»ä¸æ˜ç¡®ï¼Œä¸»è¦é¢ä¸´é›…å¯æ¯”å‘é‡ç§¯è®¡ç®—çš„åŸºç¡€è®¾æ–½æŒ‘æˆ˜ä»¥åŠæ ‡å‡†è¯„ä¼°åŸºå‡†çš„å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> å¼€å‘äº†å¹¶è¡Œå…¼å®¹çš„FlashAttention-2 JVPæ ¸ï¼Œæ”¯æŒè¶…è¿‡100äº¿å‚æ•°æ¨¡å‹å’Œé«˜ç»´è§†é¢‘ä»»åŠ¡çš„sCMè®­ç»ƒï¼›æå‡ºå¾—åˆ†æ­£åˆ™åŒ–è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹(rCM)ï¼Œé€šè¿‡é›†æˆå¾—åˆ†è’¸é¦ä½œä¸ºé•¿è·³è·ƒæ­£åˆ™å™¨ï¼Œå°†åå‘æ•£åº¦çš„æ¨¡å¼å¯»æ±‚ç‰¹æ€§è¡¥å……åˆ°sCMçš„å‰å‘æ•£åº¦ç›®æ ‡ä¸­ã€‚</p>
<p><strong>Result:</strong> åœ¨é«˜è¾¾140äº¿å‚æ•°çš„Cosmos-Predict2ã€Wan2.1ç­‰å¤§è§„æ¨¡æ¨¡å‹å’Œ5ç§’è§†é¢‘ä»»åŠ¡ä¸ŠéªŒè¯ï¼ŒrCMåœ¨è´¨é‡æŒ‡æ ‡ä¸ŠåŒ¹é…æˆ–è¶…è¶Šæœ€å…ˆè¿›çš„è’¸é¦æ–¹æ³•DMD2ï¼ŒåŒæ—¶åœ¨å¤šæ ·æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä»…éœ€1-4æ­¥å³å¯ç”Ÿæˆé«˜ä¿çœŸæ ·æœ¬ï¼ŒåŠ é€Ÿæ‰©æ•£é‡‡æ ·15-50å€ã€‚</p>
<p><strong>Conclusion:</strong> rCMä½œä¸ºä¸€ä¸ªå®ç”¨ä¸”ç†è®ºåŸºç¡€çš„æ¡†æ¶ï¼Œé€šè¿‡è§£å†³sCMåœ¨ç²¾ç»†ç»†èŠ‚ç”Ÿæˆä¸­çš„è´¨é‡é™åˆ¶é—®é¢˜ï¼Œä¸ºæ¨è¿›å¤§è§„æ¨¡æ‰©æ•£è’¸é¦æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œæ— éœ€GANè°ƒä¼˜æˆ–å¤§é‡è¶…å‚æ•°æœç´¢å³å¯å®ç°é«˜è´¨é‡é«˜å¤šæ ·æ€§ç”Ÿæˆã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>This work represents the first effort to scale up continuous-time consistency
distillation to general application-level image and video diffusion models.
Although continuous-time consistency model (sCM) is theoretically principled
and empirically powerful for accelerating academic-scale diffusion, its
applicability to large-scale text-to-image and video tasks remains unclear due
to infrastructure challenges in Jacobian-vector product (JVP) computation and
the limitations of standard evaluation benchmarks. We first develop a
parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on
models with over 10 billion parameters and high-dimensional video tasks. Our
investigation reveals fundamental quality limitations of sCM in fine-detail
generation, which we attribute to error accumulation and the "mode-covering"
nature of its forward-divergence objective. To remedy this, we propose the
score-regularized continuous-time consistency model (rCM), which incorporates
score distillation as a long-skip regularizer. This integration complements sCM
with the "mode-seeking" reverse divergence, effectively improving visual
quality while maintaining high generation diversity. Validated on large-scale
models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM
matches or surpasses the state-of-the-art distillation method DMD2 on quality
metrics while offering notable advantages in diversity, all without GAN tuning
or extensive hyperparameter searches. The distilled models generate
high-fidelity samples in only $1\sim4$ steps, accelerating diffusion sampling
by $15\times\sim50\times$. These results position rCM as a practical and
theoretically grounded framework for advancing large-scale diffusion
distillation.</p>
<h3 id="26-spatialladder-progressive-training-for-spatial-reasoning-in-vision-language-models">[26] <a href="https://arxiv.org/abs/2510.08531">SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models</a></h3>
<p><em>Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¸è¿›å¼è®­ç»ƒæ¡†æ¶SpatialLadderï¼Œé€šè¿‡æ„å»ºåŒ…å«26,610ä¸ªæ ·æœ¬çš„å¤šæ¨¡æ€æ•°æ®é›†SpatialLadder-26kï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ³•ä»ç©ºé—´æ„ŸçŸ¥åˆ°å¤æ‚æ¨ç†é€æ­¥å»ºç«‹ç©ºé—´æ™ºèƒ½ï¼Œåœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†23.4%çš„å¹³å‡æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§æŒ‘æˆ˜ï¼Œä¸»è¦é—®é¢˜åœ¨äºç°æœ‰æ–¹æ³•è¯•å›¾ç›´æ¥å­¦ä¹ ç©ºé—´æ¨ç†è€Œç¼ºä¹å»ºç«‹æ„ŸçŸ¥ä¸ç†è§£çš„å±‚æ¬¡åŒ–åŸºç¡€ï¼Œè¿™å¯¼è‡´æ¨¡å‹éš¾ä»¥å®ç°ç¨³å¥çš„ç©ºé—´æ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆé€šè¿‡ç›®æ ‡å®šä½å»ºç«‹ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œç„¶åé€šè¿‡å¤šç»´åº¦ç©ºé—´ä»»åŠ¡å‘å±•ç©ºé—´ç†è§£ï¼Œæœ€åé€šè¿‡å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åŠ å¼ºå¤æ‚æ¨ç†ï¼›åŸºäºæ„å»ºçš„SpatialLadder-26kå¤šæ¨¡æ€æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«26,610ä¸ªæ ·æœ¬ï¼Œæ¶µç›–ç›®æ ‡å®šä½ã€å•å›¾åƒã€å¤šè§†è§’å’Œè§†é¢‘ç©ºé—´æ¨ç†ä»»åŠ¡ã€‚</p>
<p><strong>Result:</strong> SpatialLadderæ¨¡å‹åœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†23.4%çš„å¹³å‡æ€§èƒ½æå‡ï¼Œè¶…è¶ŠGPT-4o 20.8%å’ŒGemini-2.0-Flash 10.1%ï¼Œåœ¨é¢†åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­ä¿æŒ7.2%çš„æ³›åŒ–æ€§èƒ½æå‡ï¼Œå±•ç¤ºäº†ç¨³å¥çš„ç©ºé—´æ™ºèƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ä»æ„ŸçŸ¥åˆ°æ¨ç†çš„æ¸è¿›å¼è®­ç»ƒå¯¹äºå»ºç«‹ç¨³å¥çš„ç©ºé—´æ™ºèƒ½è‡³å…³é‡è¦ï¼Œè¯¥æ–¹æ³•ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›æä¾›äº†ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶éªŒè¯äº†å±‚æ¬¡åŒ–å­¦ä¹ æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Spatial reasoning remains a fundamental challenge for Vision-Language Models
(VLMs), with current approaches struggling to achieve robust performance
despite recent advances. We identify that this limitation stems from a critical
gap: existing methods attempt to learn spatial reasoning directly without
establishing the hierarchical foundations of perception and understanding. To
address this challenge, we present a comprehensive methodology for building
spatial intelligence progressively. We introduce SpatialLadder-26k, a
multimodal dataset containing 26,610 samples spanning object localization,
single image, multi-view, and video spatial reasoning tasks, constructed
through a standardized pipeline that ensures systematic coverage across
modalities. Building on this dataset, we design a three-stage progressive
training framework that (1) establishes spatial perception through object
localization, (2) develops spatial understanding through multi-dimensional
spatial tasks, and (3) strengthens complex reasoning via reinforcement learning
with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter
model that achieves state-of-the-art performance on spatial reasoning
benchmarks, with 23.4% average improvement over the base model, surpassing
GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains
strong generalization with 7.2% improvement on out-of-domain benchmarks,
demonstrating that progressive training from perception to reasoning is
essential for robust spatial intelligence.</p>
<h3 id="27-gaze-on-the-prize-shaping-visual-attention-with-return-guided-contrastive-learning">[27] <a href="https://arxiv.org/abs/2510.08442">Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning</a></h3>
<p><em>Andrew Lee, Ian Chuang, Dechen Gao, Kai Fukazawa, Iman Soltani</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGaze on the Prizeæ¡†æ¶ï¼Œé€šè¿‡å¯å­¦ä¹ çš„ä¸­å¤®å‡¹æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºè§†è§‰å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨å›æŠ¥å·®å¼‚æŒ‡å¯¼æ³¨æ„åŠ›èšç„¦ä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼Œåœ¨ManiSkill3åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€é«˜2.4å€çš„æ ·æœ¬æ•ˆç‡æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰å¼ºåŒ–å­¦ä¹ ä»£ç†éœ€è¦å¤„ç†é«˜ç»´å›¾åƒæ•°æ®ï¼Œä½†åªæœ‰å°‘é‡åƒç´ ä¸ä»»åŠ¡ç›¸å…³ï¼Œå¯¼è‡´ä»£ç†æµªè´¹æ¢ç´¢å’Œè®¡ç®—èµ„æºåœ¨æ— å…³ç‰¹å¾ä¸Šï¼Œé€ æˆæ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œå­¦ä¹ ä¸ç¨³å®šã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•å¼•å…¥å¯å­¦ä¹ çš„ä¸­å¤®å‡¹æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡åŸºäºå›æŠ¥å·®å¼‚çš„è‡ªç›‘ç£ä¿¡å·æŒ‡å¯¼æ³¨æ„åŠ›å­¦ä¹ ï¼Œé‡‡ç”¨å›æŠ¥å¼•å¯¼çš„å¯¹æ¯”å­¦ä¹ å°†ç›¸ä¼¼è§†è§‰è¡¨ç¤ºæŒ‰å›æŠ¥å·®å¼‚åˆ†ç»„ä¸ºæ­£è´Ÿæ ·æœ¬ï¼Œæ„å»ºå¯¹æ¯”ä¸‰å…ƒç»„è®­ç»ƒæ³¨æ„åŠ›æœºåˆ¶äº§ç”Ÿå¯åŒºåˆ†çš„è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨ManiSkill3åŸºå‡†æµ‹è¯•çš„å¤šä¸ªæ“ä½œä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€é«˜2.4å€çš„æ ·æœ¬æ•ˆç‡æå‡ï¼Œå¹¶èƒ½è§£å†³åŸºçº¿æ–¹æ³•æ— æ³•å­¦ä¹ çš„ä»»åŠ¡ï¼Œä¸”æ— éœ€ä¿®æ”¹åº•å±‚ç®—æ³•æˆ–è¶…å‚æ•°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å›æŠ¥å·®å¼‚èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ˜¾è‘—æé«˜è§†è§‰å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œä¸ºå¤„ç†é«˜ç»´è§†è§‰è¾“å…¥æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Visual Reinforcement Learning (RL) agents must learn to act based on
high-dimensional image data where only a small fraction of the pixels is
task-relevant. This forces agents to waste exploration and computational
resources on irrelevant features, leading to sample-inefficient and unstable
learning. To address this, inspired by human visual foveation, we introduce
Gaze on the Prize. This framework augments visual RL with a learnable foveal
attention mechanism (Gaze), guided by a self-supervised signal derived from the
agent's experience pursuing higher returns (the Prize). Our key insight is that
return differences reveal what matters most: If two similar representations
produce different outcomes, their distinguishing features are likely
task-relevant, and the gaze should focus on them accordingly. This is realized
through return-guided contrastive learning that trains the attention to
distinguish between the features relevant to success and failure. We group
similar visual representations into positives and negatives based on their
return differences and use the resulting labels to construct contrastive
triplets. These triplets provide the training signal that teaches the attention
mechanism to produce distinguishable representations for states associated with
different outcomes. Our method achieves up to 2.4x improvement in sample
efficiency and can solve tasks that the baseline fails to learn, demonstrated
across a suite of manipulation tasks from the ManiSkill3 benchmark, all without
modifying the underlying algorithm or hyperparameters.</p>
<h3 id="28-videonorms-benchmarking-cultural-awareness-of-video-language-models">[28] <a href="https://arxiv.org/abs/2510.08543">VideoNorms: Benchmarking Cultural Awareness of Video Language Models</a></h3>
<p><em>Nikhil Reddy Varimalla, Yunfei Xu, Arkadiy Saakyan, Meng Fan Wang, Smaranda Muresan</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VideoNormsåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«1000å¤šä¸ªè§†é¢‘ç‰‡æ®µä¸ç¤¾ä¼šæ–‡åŒ–è§„èŒƒé…å¯¹ï¼Œç”¨äºè¯„ä¼°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ–‡åŒ–æ„è¯†ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£æ–¹é¢çš„ç³»ç»Ÿæ€§ç¼ºé™·ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨å…¨çƒéƒ¨ç½²ï¼Œéœ€è¦è¯„ä¼°å…¶å¯¹ç›¸å…³æ–‡åŒ–èƒŒæ™¯çš„ç†è§£å’ŒåŸºç¡€èƒ½åŠ›ï¼Œä½†ç›®å‰ç¼ºä¹é€‚å½“çš„åŸºå‡†æ¥è¯„ä¼°è¿™äº›æ¨¡å‹çš„æ–‡åŒ–æ„è¯†ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨äººæœºåä½œæ¡†æ¶æ„å»ºæ•°æ®é›†ï¼Œå…¶ä¸­ä½¿ç”¨åŸºäºç†è®ºæŒ‡å¯¼æç¤ºçš„æ•™å¸ˆæ¨¡å‹æä¾›å€™é€‰æ ‡æ³¨ï¼Œç”±è®­ç»ƒæœ‰ç´ çš„äººç±»ä¸“å®¶éªŒè¯å’Œä¿®æ­£æ ‡æ³¨ï¼Œæ„å»ºäº†åŸºäºè¨€è¯­è¡Œä¸ºç†è®ºçš„ç¤¾ä¼šæ–‡åŒ–è§„èŒƒæ ‡æ³¨ã€‚</p>
<p><strong>Result:</strong> åŸºå‡†æµ‹è¯•æ˜¾ç¤ºå¤šä¸ªå…±åŒè¶‹åŠ¿ï¼šæ¨¡å‹åœ¨è§„èŒƒè¿åæ£€æµ‹ä¸Šè¡¨ç°æ›´å·®ï¼›å¯¹ä¸­å›½æ–‡åŒ–çš„ç†è§£ä¸å¦‚ç¾å›½æ–‡åŒ–ï¼›æä¾›éè¯­è¨€è¯æ®æ¯”è¯­è¨€è¯æ®æ›´å›°éš¾ï¼›åœ¨æ­£å¼éå¹½é»˜è¯­å¢ƒä¸­è¡¨ç°ä¸å¦‚äººç±»ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å¼ºè°ƒäº†æ–‡åŒ–åŸºç¡€è§†é¢‘è¯­è¨€æ¨¡å‹è®­ç»ƒçš„å¿…è¦æ€§ï¼Œè¯¥åŸºå‡†å’Œæ¡†æ¶ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½æä¾›äº†èµ·ç‚¹ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è·¨æ–‡åŒ–ç†è§£æ–¹é¢çš„ç³»ç»Ÿæ€§å±€é™æ€§ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>As Video Large Language Models (VideoLLMs) are deployed globally, they
require understanding of and grounding in the relevant cultural background. To
properly assess these models' cultural awareness, adequate benchmarks are
needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm)
pairs from US and Chinese cultures annotated with socio-cultural norms grounded
in speech act theory, norm adherence and violations labels, and verbal and
non-verbal evidence. To build VideoNorms, we use a human-AI collaboration
framework, where a teacher model using theoretically-grounded prompting
provides candidate annotations and a set of trained human experts validate and
correct the annotations. We benchmark a variety of open-weight VideoLLMs on the
new dataset which highlight several common trends: 1) models performs worse on
norm violation than adherence; 2) models perform worse w.r.t Chinese culture
compared to the US culture; 3) models have more difficulty in providing
non-verbal evidence compared to verbal for the norm adhere/violation label and
struggle to identify the exact norm corresponding to a speech-act; and 4)
unlike humans, models perform worse in formal, non-humorous contexts. Our
findings emphasize the need for culturally-grounded video language model
training - a gap our benchmark and framework begin to address.</p>
<h3 id="29-video-star-reinforcing-open-vocabulary-action-recognition-with-tools">[29] <a href="https://arxiv.org/abs/2510.08480">Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools</a></h3>
<p><em>Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, Shuo Li</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºVideo-STARæ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­åŠ¨ä½œåˆ†è§£ä¸å·¥å…·å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„ååŒè®¾è®¡ï¼Œè§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«ä¸­çš„è¯­ä¹‰æ··æ·†é—®é¢˜ï¼Œæ˜¾è‘—æå‡ç»†ç²’åº¦åŠ¨ä½œåŒºåˆ†èƒ½åŠ›å¹¶å‡å°‘è·¨æ¨¡æ€å¹»è§‰ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰-æ–‡æœ¬æ¨ç†æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶å¯¹æ–‡æœ¬ä¸­å¿ƒå…ˆéªŒçš„ä¾èµ–é™åˆ¶äº†åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸‹åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼åŠ¨ä½œçš„èƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•å°†åŠ¨ä½œè§†ä¸ºæ•´ä½“å®ä½“è€Œç¼ºä¹ç»†ç²’åº¦åˆ†æï¼Œå¯¼è‡´è·¨æ¨¡æ€å¹»è§‰é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºVideo-STARæ¡†æ¶ï¼Œåˆ›æ–°æ€§åœ°å°†åŠ¨ä½œåˆ†è§£ä¸ºå…·æœ‰åŒºåˆ†æ€§çš„å­åŠ¨ä½œä»¥å®ç°ç»†ç²’åº¦åŒ¹é…ï¼ŒåŒæ—¶åŠ¨æ€è°ƒç”¨é¢†åŸŸç‰¹å®šå·¥å…·è¿›è¡Œè·¨æ¨¡æ€äº¤ç»‡ï¼Œè®¾è®¡åˆ†å±‚å¥–åŠ±æœºåˆ¶å¹³è¡¡å·¥å…·ä½¿ç”¨æ•ˆç‡ã€å­åŠ¨ä½œç›¸å…³æ€§å’Œæ¨ç†ç»“æ„ä¸€è‡´æ€§ï¼Œæ— éœ€æ˜¾å¼ç›‘ç£å³å¯è‡ªä¸»åˆ©ç”¨å¤–éƒ¨å·¥å…·ä¼˜å…ˆå¤„ç†å­åŠ¨ä½œæ¨¡å¼ã€‚</p>
<p><strong>Result:</strong> åœ¨HMDB-51ã€UCF-101ã€SSv2ã€Kinetics-400å’ŒKinetics-600æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒºåˆ†ç»†ç²’åº¦åŠ¨ä½œå’Œå¤„ç†è·¨æ¨¡æ€å¹»è§‰æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶ä¼˜ç§€çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†åŠ¨ä½œåˆ†è§£ä¸ºå­åŠ¨ä½œå¹¶ç»“åˆå·¥å…·å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿå®ç°ä»æ–‡æœ¬ä¸­å¿ƒæ¨ç†åˆ°è§†è§‰åŸºç¡€æ¨ç†çš„è½¬å˜ï¼Œä¸ºå¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¯¹å¤šæ¨¡æ€ç†è§£ç³»ç»Ÿçš„å®é™…åº”ç”¨å…·æœ‰é‡è¦å¯ç¤ºæ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) have demonstrated remarkable
potential in bridging visual and textual reasoning, yet their reliance on
text-centric priors often limits their ability to disentangle semantically
similar actions in open-vocabulary scenarios. To address this, we propose
Video-STAR, a framework that harmonizes contextual sub-motion decomposition
with tool-augmented reinforcement learning for open-vocabulary action
recognition (OVAR). Unlike prior methods that treat actions as monolithic
entities, our approach innovatively decomposes actions into discriminative
sub-motions for fine-grained matching while dynamically invoking
domain-specific tools for cross-modal interleaving, thereby enabling
category-specific reasoning capacity and reducing cross-modal hallucination.
Moreover, by designing a hierarchical reward that balances tool-usage
efficiency, sub-motion relevance, and structural coherence in reasoning, our
method autonomously leverages external tools to prioritize sub-motion patterns
without explicit supervision, transmitting from text-centric reasoning to
visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,
Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art
performance, outperforming existing methods in distinguishing fine-grained
actions and handling cross-modal hallucination, validating our excellent
robustness and generalization.</p>
<h3 id="30-matrix-multimodal-agent-tuning-for-robust-tool-use-reasoning">[30] <a href="https://arxiv.org/abs/2510.08567">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></h3>
<p><em>Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè§†è§‰ä¸­å¿ƒçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“è°ƒä¼˜æ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨åˆæˆå¤šæ¨¡æ€è½¨è¿¹å’Œç”Ÿæˆé€æ­¥åå¥½å¯¹ï¼Œè®­ç»ƒVLMæ§åˆ¶å™¨å®ç°ç¨³å¥çš„å·¥å…·ä½¿ç”¨æ¨ç†ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¼€æºå’Œé—­æºVLMæ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºæ§åˆ¶å™¨è®¿é—®å¤–éƒ¨å·¥å…·è¿›è¡Œå¤æ‚æ¨ç†å’Œå†³ç­–æ—¶ï¼Œå—åˆ°é«˜è´¨é‡å¤šæ¨¡æ€è½¨è¿¹ç¨€ç¼ºå’Œæ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é™åˆ¶ï¼Œéœ€è¦å¼€å‘è‡ªåŠ¨åŒ–çš„è½¨è¿¹åˆæˆå’Œåå¥½å­¦ä¹ æ–¹æ³•æ¥æå‡å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†è§†è§‰ä¸­å¿ƒæ™ºèƒ½ä½“è°ƒä¼˜æ¡†æ¶ï¼Œé¦–å…ˆæ„å»ºåŒ…å«28.5Kå¤šæ¨¡æ€ä»»åŠ¡å’Œ177KéªŒè¯è½¨è¿¹çš„M-TRACEæ•°æ®é›†è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼Œç„¶åå¼€å‘MATRIX Agentæ§åˆ¶å™¨è¿›è¡Œé€æ­¥å·¥å…·æ¨ç†ï¼Œè¿›ä¸€æ­¥é€šè¿‡Pref-Xç”Ÿæˆçš„11Kåå¥½å¯¹è¿›è¡Œé€æ­¥åå¥½å­¦ä¹ ä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨Agent-Xã€GTAå’ŒGAIAä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMATRIXæ§åˆ¶å™¨ä¸€è‡´è¶…è¶Šäº†å¼€æºå’Œé—­æºçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨æ–¹é¢çš„å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡è‡ªåŠ¨è½¨è¿¹åˆæˆå’Œåå¥½å­¦ä¹ èƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œæ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Vision language models (VLMs) are increasingly deployed as controllers with
access to external tools for complex reasoning and decision-making, yet their
effectiveness remains limited by the scarcity of high-quality multimodal
trajectories and the cost of manual annotation. We address this challenge with
a vision-centric agent tuning framework that automatically synthesizes
multimodal trajectories, generates step-wise preference pairs, and trains a VLM
controller for robust tool-use reasoning. Our pipeline first constructs
M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified
trajectories, enabling imitation-based trajectory tuning. Building on this, we
develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool
reasoning. To achieve finer alignment, we further introduce Pref-X, a set of
11K automatically generated preference pairs, and optimize MATRIX on it via
step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,
MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating
scalable and effective multimodal tool use. Our data and code is avaliable at
https://github.com/mbzuai-oryx/MATRIX.</p>
<h3 id="31-instructx-towards-unified-visual-editing-with-mllm-guidance">[31] <a href="https://arxiv.org/abs/2510.08485">InstructX: Towards Unified Visual Editing with MLLM Guidance</a></h3>
<p><em>Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, Qian He</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†InstructXæ¡†æ¶ï¼Œé€šè¿‡æ·±å…¥ç ”ç©¶å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹çš„é›†æˆè®¾è®¡ï¼Œå®ç°äº†å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡çš„ç»Ÿä¸€å»ºæ¨¡ï¼Œå¹¶åœ¨æ— éœ€æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹å±•ç°å‡ºè§†é¢‘ç¼–è¾‘çš„æ¶Œç°èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å°†å…¶ä¸æ‰©æ•£æ¨¡å‹é›†æˆç”¨äºç¼–è¾‘ä»»åŠ¡çš„ç ”ç©¶ç¼ºä¹æ·±å…¥çš„è®¾è®¡åˆ†æï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘ç¼–è¾‘ç­‰å¤æ‚ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸”å›¾åƒä¸è§†é¢‘åœ¨ç»Ÿä¸€å»ºæ¨¡ä¸­çš„ååŒä¸å·®å¼‚å…³ç³»å°šæœªå¾—åˆ°ç³»ç»Ÿæ¢ç´¢ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†InstructXç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡å…¨é¢ç ”ç©¶MLLMä¸æ‰©æ•£æ¨¡å‹çš„é›†æˆç­–ç•¥ï¼Œåˆ†æå›¾åƒå’Œè§†é¢‘åœ¨ç»Ÿä¸€å»ºæ¨¡ä¸­çš„ååŒä¸å·®å¼‚å…³ç³»ï¼Œåˆ©ç”¨å›¾åƒæ•°æ®è®­ç»ƒå®ç°è§†é¢‘ç¼–è¾‘çš„æ¶Œç°èƒ½åŠ›ï¼Œå¹¶æ•´åˆæ¨¡æ€ç‰¹å®šçš„MLLMç‰¹å¾æ¥ç»Ÿä¸€å¤„ç†å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†å¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ï¼Œåœ¨æ— éœ€æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹å±•ç°å‡ºè§†é¢‘ç¼–è¾‘çš„æ¶Œç°èƒ½åŠ›ï¼Œæœ‰æ•ˆç¼“è§£äº†è§†é¢‘è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é™åˆ¶ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†å›¾åƒè®­ç»ƒèƒ½å¤Ÿè‡ªç„¶è¿ç§»åˆ°è§†é¢‘ç¼–è¾‘ä»»åŠ¡çš„æ½œåŠ›ï¼Œä¸ºå¤šæ¨¡æ€ç¼–è¾‘ç³»ç»Ÿæä¾›äº†ç»Ÿä¸€çš„å»ºæ¨¡èŒƒå¼ï¼ŒåŒæ—¶é€šè¿‡æ¨¡æ€ç‰¹å®šç‰¹å¾çš„æ•´åˆå®ç°äº†è·¨æ¨¡æ€ä»»åŠ¡çš„ååŒä¼˜åŒ–ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€ç¼–è¾‘ç ”ç©¶æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>With recent advances in Multimodal Large Language Models (MLLMs) showing
strong visual understanding and reasoning, interest is growing in using them to
improve the editing performance of diffusion models. Despite rapid progress,
most studies lack an in-depth analysis of MLLM design choices. Moreover, the
integration of MLLMs and diffusion models remains an open challenge in some
difficult tasks, such as video editing. In this paper, we present InstructX, a
unified framework for image and video editing. Specifically, we conduct a
comprehensive study on integrating MLLMs and diffusion models for
instruction-driven editing across diverse tasks. Building on this study, we
analyze the cooperation and distinction between images and videos in unified
modeling. (1) We show that training on image data can lead to emergent video
editing capabilities without explicit supervision, thereby alleviating the
constraints imposed by scarce video training data. (2) By incorporating
modality-specific MLLM features, our approach effectively unifies image and
video editing tasks within a single model. Extensive experiments demonstrate
that our method can handle a broad range of image and video editing tasks and
achieves state-of-the-art performance.</p>
<h3 id="32-moa-vr-a-mixture-of-agents-system-towards-all-in-one-video-restoration">[32] <a href="https://arxiv.org/abs/2510.08508">MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration</a></h3>
<p><em>Lu Liu, Chunlei Cai, Shaocheng Shen, Jianfeng Liang, Weimin Ouyang, Tianxiao Ye, Jian Mao, Huiyu Duan, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MoA-VRï¼Œé¦–ä¸ªåŸºäºæ™ºèƒ½ä½“æ··åˆçš„è§†é¢‘ä¿®å¤ç³»ç»Ÿï¼Œé€šè¿‡ä¸‰ä¸ªåè°ƒçš„æ™ºèƒ½ä½“æ¨¡æ‹Ÿäººç±»ä¸“å®¶çš„æ¨ç†å’Œå¤„ç†æµç¨‹ï¼Œæœ‰æ•ˆå¤„ç†å¤æ‚å¤šæ ·çš„è§†é¢‘é€€åŒ–é—®é¢˜ï¼Œåœ¨å®¢è§‚æŒ‡æ ‡å’Œæ„ŸçŸ¥è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°å®ä¸–ç•Œè§†é¢‘ç”±äºä¸åŒçš„é‡‡é›†å’Œä¼ è¾“æ¡ä»¶å¸¸å¸¸é­å—å¤æ‚çš„é€€åŒ–é—®é¢˜ï¼Œå¦‚å™ªå£°ã€å‹ç¼©ä¼ªå½±å’Œä½å…‰å¤±çœŸï¼Œç°æœ‰ä¿®å¤æ–¹æ³•é€šå¸¸éœ€è¦ä¸“ä¸šäººå‘˜æ‰‹åŠ¨é€‰æ‹©ä¸“ç”¨æ¨¡å‹æˆ–ä¾èµ–æ— æ³•æ³›åŒ–å¤„ç†ä¸åŒé€€åŒ–çš„å•ä¸€æ¶æ„ï¼Œå­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºMoA-VRç³»ç»Ÿï¼ŒåŒ…å«ä¸‰ä¸ªåè°ƒæ™ºèƒ½ä½“ï¼šé€€åŒ–è¯†åˆ«ã€è·¯ç”±ä¸ä¿®å¤ã€ä¿®å¤è´¨é‡è¯„ä¼°ï¼›æ„å»ºå¤§è§„æ¨¡é«˜åˆ†è¾¨ç‡è§†é¢‘é€€åŒ–è¯†åˆ«åŸºå‡†å’Œè§†è§‰è¯­è¨€æ¨¡å‹é©±åŠ¨çš„é€€åŒ–è¯†åˆ«å™¨ï¼›å¼•å…¥åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”è·¯ç”±å™¨ï¼Œé€šè¿‡è§‚å¯Ÿå·¥å…·ä½¿ç”¨æ¨¡å¼è‡ªä¸»å­¦ä¹ æœ‰æ•ˆä¿®å¤ç­–ç•¥ï¼›æ„å»ºRes-VQæ•°æ®é›†å¹¶è®¾è®¡ä¸“é—¨é’ˆå¯¹ä¿®å¤ä»»åŠ¡çš„VLMè§†é¢‘è´¨é‡è¯„ä¼°æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMoA-VRèƒ½æœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–å’Œå¤åˆé€€åŒ–é—®é¢˜ï¼Œåœ¨å®¢è§‚æŒ‡æ ‡å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢æŒç»­ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚è§†é¢‘é€€åŒ–æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€æ™ºèƒ½å’Œæ¨¡å—åŒ–æ¨ç†åœ¨é€šç”¨è§†é¢‘ä¿®å¤ç³»ç»Ÿä¸­çš„æ•´åˆæ½œåŠ›ï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½ã€è‡ªé€‚åº”çš„è§†é¢‘å¤„ç†ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œè®¾è®¡èŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Real-world videos often suffer from complex degradations, such as noise,
compression artifacts, and low-light distortions, due to diverse acquisition
and transmission conditions. Existing restoration methods typically require
professional manual selection of specialized models or rely on monolithic
architectures that fail to generalize across varying degradations. Inspired by
expert experience, we propose MoA-VR, the first
\underline{M}ixture-\underline{o}f-\underline{A}gents \underline{V}ideo
\underline{R}estoration system that mimics the reasoning and processing
procedures of human professionals through three coordinated agents: Degradation
Identification, Routing and Restoration, and Restoration Quality Assessment.
Specifically, we construct a large-scale and high-resolution video degradation
recognition benchmark and build a vision-language model (VLM) driven
degradation identifier. We further introduce a self-adaptive router powered by
large language models (LLMs), which autonomously learns effective restoration
strategies by observing tool usage patterns. To assess intermediate and final
processed video quality, we construct the \underline{Res}tored
\underline{V}ideo \underline{Q}uality (Res-VQ) dataset and design a dedicated
VLM-based video quality assessment (VQA) model tailored for restoration tasks.
Extensive experiments demonstrate that MoA-VR effectively handles diverse and
compound degradations, consistently outperforming existing baselines in terms
of both objective metrics and perceptual quality. These results highlight the
potential of integrating multimodal intelligence and modular reasoning in
general-purpose video restoration systems.</p>
<h3 id="33-scivideobench-benchmarking-scientific-video-reasoning-in-large-multimodal-models">[33] <a href="https://arxiv.org/abs/2510.08559">SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models</a></h3>
<p><em>Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, Xiaohan Wang</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SciVideoBenchï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°ç§‘å­¦è§†é¢‘æ¨ç†èƒ½åŠ›çš„ä¸¥æ ¼åŸºå‡†ï¼ŒåŒ…å«1000ä¸ªæ¥è‡ªå‰æ²¿ç§‘å­¦å®éªŒè§†é¢‘çš„å¤šé€‰é¢˜ï¼Œæ­ç¤ºäº†å½“å‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚ç§‘å­¦è§†é¢‘æ¨ç†æ–¹é¢çš„æ˜¾è‘—æ€§èƒ½ç¼ºé™·ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘åŸºå‡†ä¸»è¦é’ˆå¯¹é€šç”¨åœºæ™¯ï¼Œä¾èµ–æ„ŸçŸ¥è¯†åˆ«è€Œæ¨ç†ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œå¯¼è‡´æ€§èƒ½é¥±å’Œï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°é«˜çº§å¤šæ¨¡æ€è®¤çŸ¥æŠ€èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦é¢†åŸŸä¸­çš„å¤æ‚è§†é¢‘æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å‰æ²¿é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«1000ä¸ªç²¾å¿ƒè®¾è®¡å¤šé€‰é¢˜çš„SciVideoBenchåŸºå‡†ï¼Œè¿™äº›é¢˜ç›®æºè‡ªæ¶µç›–25+ä¸“ä¸šå­¦ç§‘çš„å‰æ²¿ç§‘å­¦å®éªŒè§†é¢‘ï¼Œå¹¶é€šè¿‡åŠè‡ªåŠ¨ç³»ç»ŸéªŒè¯ï¼Œæ¯ä¸ªé—®é¢˜éƒ½éœ€è¦å¤æ‚çš„é¢†åŸŸçŸ¥è¯†ã€ç²¾ç¡®çš„æ—¶ç©ºæ„ŸçŸ¥å’Œç²¾ç»†çš„é€»è¾‘æ¨ç†ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°æ˜¾ç¤ºåŒ…æ‹¬Gemini 2.5 Proå’ŒQwen2.5-VLåœ¨å†…çš„æœ€å…ˆè¿›ä¸“æœ‰å’Œå¼€æºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹éƒ½å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½ç¼ºé™·ï¼Œè¡¨æ˜åœ¨è§†é¢‘æ¨ç†èƒ½åŠ›æ–¹é¢ä»æœ‰å·¨å¤§çš„æå‡ç©ºé—´ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æœªæ¥å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œæ˜ç¡®æ–¹å‘ï¼Œæ¨åŠ¨äº†çœŸæ­£æœ‰èƒ½åŠ›çš„å¤šæ¨¡æ€AIç§‘å­¦åŠ©æ‰‹çš„æ¼”è¿›ï¼Œé€šè¿‡è¯¦ç»†åˆ†ææ¨ç†å¤æ‚æ€§å’Œè§†è§‰åŸºç¡€ç­‰å…³é”®å› ç´ ï¼Œä¸ºå‰æ²¿AIåœ¨æ›´å¹¿æ³›ç§‘å­¦é¢†åŸŸçš„è¾¹ç•Œæ¨è¿›æä¾›äº†å¸®åŠ©ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Large Multimodal Models (LMMs) have achieved remarkable progress across
various capabilities; however, complex video reasoning in the scientific domain
remains a significant and challenging frontier. Current video benchmarks
predominantly target general scenarios where perception/recognition is heavily
relied on, while with relatively simple reasoning tasks, leading to saturation
and thus failing to effectively evaluate advanced multimodal cognitive skills.
To address this critical gap, we introduce SciVideoBench, a rigorous benchmark
specifically designed to assess advanced video reasoning in scientific
contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice
questions derived from cutting-edge scientific experimental videos spanning
over 25 specialized academic subjects and verified by a semi-automatic system.
Each question demands sophisticated domain-specific knowledge, precise
spatiotemporal perception, and intricate logical reasoning, effectively
challenging models' higher-order cognitive abilities. Our evaluation highlights
significant performance deficits in state-of-the-art proprietary and
open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating
substantial room for advancement in video reasoning capabilities. Detailed
analyses of critical factors such as reasoning complexity and visual grounding
provide valuable insights and clear direction for future developments in LMMs,
driving the evolution of truly capable multimodal AI co-scientists. We hope
SciVideoBench could fit the interests of the community and help to push the
boundary of cutting-edge AI for border science.</p>
<h3 id="34-mm-helix-boosting-multimodal-long-chain-reflective-reasoning-with-holistic-platform-and-adaptive-hybrid-policy-optimization">[34] <a href="https://arxiv.org/abs/2510.08540">MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization</a></h3>
<p><em>Xiangyu Zhao, Junming Lin, Tianhao Liang, Yifan Zhou, Wenhao Chai, Yuzhe Gu, Weiyun Wang, Kai Chen, Gen Luo, Wenwei Zhang, Junchi Yan, Hua Yang, Haodong Duan, Xue Yang</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾åæ€æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†MM-HELIXåŸºå‡†æµ‹è¯•å’Œè‡ªé€‚åº”æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨Qwen2.5-VL-7Bæ¨¡å‹ä¸Šå®ç°äº†18.6%çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é•¿é“¾åæ€æ¨ç†èƒ½åŠ›â€”â€”è§£å†³å¤æ‚ç°å®é—®é¢˜çš„å…³é”®å‰æâ€”â€”ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ç°æœ‰æ¨¡å‹åœ¨éœ€è¦è¿­ä»£æ€è€ƒå’Œå›æº¯çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨æ˜¾è‘—æ€§èƒ½ç¼ºé™·ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆæ„å»ºäº†MM-HELIXå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1,260ä¸ªéœ€è¦è¿­ä»£æ€è€ƒå’Œå›æº¯çš„æŒ‘æˆ˜æ€§ä»»åŠ¡æ ·æœ¬ã€‚éšåå¼€å‘äº†æ­¥éª¤å¼•å¯¼å“åº”ç”Ÿæˆæµæ°´çº¿æ¥åˆ›å»ºMM-HELIX-100Kå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«10ä¸‡ä¸ªé«˜è´¨é‡åæ€æ¨ç†è½¨è¿¹ã€‚é’ˆå¯¹æ ‡å‡†å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚ä»»åŠ¡ä¸­å› ç¨€ç–å¥–åŠ±ä¿¡å·å’Œç¾éš¾æ€§é—å¿˜è€Œå¤±æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªé€‚åº”æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå°†ç¦»çº¿ç›‘ç£å’Œåœ¨çº¿ä¼˜åŒ–åŠ¨æ€ç»Ÿä¸€åˆ°å•é˜¶æ®µè®­ç»ƒä¸­ã€‚</p>
<p><strong>Result:</strong> åœ¨MM-HELIXåŸºå‡†æµ‹è¯•ä¸Šï¼Œç°æœ‰MLLMsè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ç¼ºé™·ã€‚åº”ç”¨æ‰€ææ–¹æ³•åï¼ŒQwen2.5-VL-7Bæ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†+18.6%çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶åœ¨é€šç”¨æ•°å­¦å’Œé€»è¾‘ä»»åŠ¡ä¸Šå±•ç°å‡º+5.7%çš„å¹³å‡æ€§èƒ½å¢ç›Šï¼Œè¯æ˜äº†æ–¹æ³•çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åæ€æ¨ç†èƒ½åŠ›å¯ä»¥é€šè¿‡æœ‰æ•ˆå­¦ä¹ å¾—åˆ°æ˜¾è‘—æå‡å’Œæ³›åŒ–ã€‚è‡ªé€‚åº”æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•ä¸ºè§£å†³å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è®­ç»ƒæŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚è¯¥æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¥–åŠ±ç¨€ç–æ—¶ä»ä¸“å®¶æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶åœ¨ç†Ÿç»ƒåè¿›è¡Œç‹¬ç«‹æ¢ç´¢ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>While current Multimodal Large Language Models (MLLMs) have demonstrated
proficiency in reasoning tasks such as mathematics and logic, their capacity
for long-chain reflective reasoning, a prerequisite for solving complex
real-world problems, remains largely underexplored. In this work, we first
conduct an extensive empirical investigation to evaluate this capability.
Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a
multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks
that require iterative thinking and backtracking. Empirical results on this
benchmark reveal that existing MLLMs exhibit significant performance deficits
in long-chain reflective reasoning. To address this limitation, we generate
post-training data and further explore learning paradigms for exploiting such
data. We first develop the Step-Elicited Response Generation pipeline to create
MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning
traces for instruction-tuning stage. Given that standard Reinforcement Learning
fails on complex tasks due to sparse reward signals and catastrophic forgetting
after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization
(AHPO), a novel training strategy that dynamically unifies offline supervision
and online optimization into a single stage. This strategy enables the model to
learn from expert data when rewards are sparse and conduct independent
exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our
method achieves a +18.6\% accuracy improvement on MM-HELIX benchmark and
demonstrates strong generalization with a +5.7\% average performance gain on
general mathematic and logic tasks. Our work demonstrate that reflective
reasoning in MLLMs can be effectively learned and generalized, paving the way
for developing more capable MLLMs.</p>
<h3 id="35-multicoin-multi-modal-controllable-video-inbetweening">[35] <a href="https://arxiv.org/abs/2510.08561">MultiCOIN: Multi-Modal COntrollable Video INbetweening</a></h3>
<p><em>Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, Nanxuan Zhao</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ”¯æŒå¤šæ¨¡æ€æ§åˆ¶çš„è§†é¢‘æ’å¸§æ¡†æ¶ï¼Œé€šè¿‡å°†è¿åŠ¨æ§åˆ¶æ˜ å°„ä¸ºç»Ÿä¸€çš„ç‚¹åŸºè¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„åˆ†åˆ«å¤„ç†å†…å®¹å’Œè¿åŠ¨æ§åˆ¶ï¼Œå®ç°äº†çµæ´»ã€ç²¾ç¡®çš„è§†é¢‘è¿‡æ¸¡ç”Ÿæˆã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘æ’å¸§æ–¹æ³•æ— æ³•ç”Ÿæˆå¤§è§„æ¨¡ã€å¤æ‚æˆ–ç²¾ç»†çš„è¿åŠ¨ï¼Œä¸”ç¼ºä¹å¯¹ä¸­é—´å¸§ç»†èŠ‚çš„ç²¾ç»†æ§åˆ¶ï¼Œéš¾ä»¥æ»¡è¶³ç”¨æˆ·å¤šæ ·åŒ–çš„åˆ›ä½œæ„å›¾ï¼Œå¯¼è‡´ä¸åˆ›æ„æ„æ€ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨Diffusion Transformerä½œä¸ºè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå°†æ‰€æœ‰è¿åŠ¨æ§åˆ¶æ˜ å°„ä¸ºç»Ÿä¸€çš„ç¨€ç–ç‚¹åŸºè¡¨ç¤ºä½œä¸ºè§†é¢‘/å™ªå£°è¾“å…¥ï¼Œå°†å†…å®¹æ§åˆ¶å’Œè¿åŠ¨æ§åˆ¶åˆ†ç¦»ä¸ºä¸¤ä¸ªåˆ†æ”¯è¿›è¡Œç‰¹å¾ç¼–ç ï¼Œå¹¶é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ç¡®ä¿å¤šæ¨¡æ€æ§åˆ¶çš„å¹³æ»‘å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ§åˆ¶èƒ½å¤Ÿå®ç°æ›´åŠ¨æ€ã€å¯å®šåˆ¶ä¸”ä¸Šä¸‹æ–‡å‡†ç¡®çš„è§†è§‰å™äº‹ï¼Œåœ¨è§†é¢‘æ’å¸§è´¨é‡å’Œæ§åˆ¶ç²¾åº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€æ§åˆ¶åœ¨è§†é¢‘æ’å¸§ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§†é¢‘ç¼–è¾‘å’Œé•¿è§†é¢‘åˆæˆæä¾›äº†æ›´çµæ´»ã€ç²¾ç¡®çš„åˆ›ä½œå·¥å…·ï¼Œå¹³è¡¡äº†çµæ´»æ€§ã€æ˜“ç”¨æ€§å’Œç²¾ç»†æ§åˆ¶çš„éœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Video inbetweening creates smooth and natural transitions between two image
frames, making it an indispensable tool for video editing and long-form video
synthesis. Existing works in this domain are unable to generate large, complex,
or intricate motions. In particular, they cannot accommodate the versatility of
user intents and generally lack fine control over the details of intermediate
frames, leading to misalignment with the creative mind. To fill these gaps, we
introduce \modelname{}, a video inbetweening framework that allows multi-modal
controls, including depth transition and layering, motion trajectories, text
prompts, and target regions for movement localization, while achieving a
balance between flexibility, ease of use, and precision for fine-grained video
interpolation. To achieve this, we adopt the Diffusion Transformer (DiT)
architecture as our video generative model, due to its proven capability to
generate high-quality long videos. To ensure compatibility between DiT and our
multi-modal controls, we map all motion controls into a common sparse and
user-friendly point-based representation as the video/noise input. Further, to
respect the variety of controls which operate at varying levels of granularity
and influence, we separate content controls and motion controls into two
branches to encode the required features before guiding the denoising process,
resulting in two generators, one for motion and the other for content. Finally,
we propose a stage-wise training strategy to ensure that our model learns the
multi-modal controls smoothly. Extensive qualitative and quantitative
experiments demonstrate that multi-modal controls enable a more dynamic,
customizable, and contextually accurate visual narrative.</p>
<h3 id="36-navil-rethinking-scaling-properties-of-native-multimodal-large-language-models-under-data-constraints">[36] <a href="https://arxiv.org/abs/2510.08565">NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints</a></h3>
<p><em>Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNaViLçš„åŸç”Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒç³»ç»Ÿç ”ç©¶äº†MLLMçš„è®¾è®¡ç©ºé—´å’Œæ‰©å±•ç‰¹æ€§ï¼Œåœ¨æ•°æ®çº¦æŸæ¡ä»¶ä¸‹å®ç°äº†è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€æ¨¡å‹ä¹‹é—´çš„æ­£å‘æ‰©å±•å…³ç³»ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰MLLMé€šå¸¸é‡‡ç”¨ç»„åˆå¼è®­ç»ƒèŒƒå¼ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹é€šè¿‡è¿ç»­å¤šæ¨¡æ€é¢„è®­ç»ƒè¿æ¥ï¼Œä½†è¿™ç§åˆ†ç¦»è®­ç»ƒæ–¹å¼éš¾ä»¥æ¢ç´¢å¤šæ¨¡æ€æ‰©å±•ç‰¹æ€§ï¼Œå› æ­¤éœ€è¦ç ”ç©¶åŸç”Ÿç«¯åˆ°ç«¯è®­ç»ƒçš„MLLMè®¾è®¡ç©ºé—´å’Œæ‰©å±•è§„å¾‹ã€‚</p>
<p><strong>Method:</strong> åœ¨æ•°æ®çº¦æŸçš„å®é™…è®¾ç½®ä¸‹ï¼Œç³»ç»Ÿç ”ç©¶äº†MLLMçš„å„ç§è®¾è®¡é€‰æ‹©ï¼Œè·å¾—äº†åœ¨æ€§èƒ½å’Œè®­ç»ƒæˆæœ¬ä¹‹é—´æœ€ä½³å¹³è¡¡çš„å…ƒæ¶æ„ï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢äº†åŸç”ŸMLLMçš„æ‰©å±•ç‰¹æ€§ï¼Œæå‡ºäº†ç»“åˆç®€å•ä¸”æˆæœ¬æœ‰æ•ˆé…æ–¹çš„NaViLåŸç”ŸMLLMã€‚</p>
<p><strong>Result:</strong> åœ¨14ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNaViLç›¸å¯¹äºç°æœ‰MLLMå…·æœ‰ç«äº‰ä¼˜åŠ¿ï¼ŒåŒæ—¶æ­ç¤ºäº†è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€æ¨¡å‹ä¹‹é—´æ­£å‘ç›¸å…³çš„æ‰©å±•å…³ç³»ï¼Œä¸ºåŸç”ŸMLLMçš„æœªæ¥ç ”ç©¶æä¾›äº†æ·±å…¥è§è§£ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯å®äº†åŸç”ŸMLLMç«¯åˆ°ç«¯è®­ç»ƒçš„å¯è¡Œæ€§ï¼Œå‘ç°äº†è§†è§‰ç»„ä»¶ä¸è¯­è¨€ç»„ä»¶ä¹‹é—´çš„æ­£å‘æ‰©å±•è§„å¾‹ï¼Œæå‡ºçš„NaViLæ¨¡å‹ä¸ä»…æ€§èƒ½ä¼˜å¼‚ï¼Œæ›´ä¸ºåç»­åŸç”Ÿå¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç ”ç©¶æä¾›äº†é‡è¦çš„è®¾è®¡æŒ‡å¯¼å’Œç†è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Compositional training has been the de-facto paradigm in existing Multimodal
Large Language Models (MLLMs), where pre-trained vision encoders are connected
with pre-trained LLMs through continuous multimodal pre-training. However, the
multimodal scaling property of this paradigm remains difficult to explore due
to the separated training. In this paper, we focus on the native training of
MLLMs in an end-to-end manner and systematically study its design space and
scaling property under a practical setting, i.e., data constraint. Through
careful study of various choices in MLLM, we obtain the optimal
meta-architecture that best balances performance and training cost. After that,
we further explore the scaling properties of the native MLLM and indicate the
positively correlated scaling relationship between visual encoders and LLMs.
Based on these findings, we propose a native MLLM called NaViL, combined with a
simple and cost-effective recipe. Experimental results on 14 multimodal
benchmarks confirm the competitive performance of NaViL against existing MLLMs.
Besides that, our findings and results provide in-depth insights for the future
study of native MLLMs.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="37-deploying-tiny-lvlm-judges-for-real-world-evaluation-of-chart-models-lessons-learned-and-best-practices">[37] <a href="https://arxiv.org/abs/2510.07545">Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices</a></h3>
<p><em>Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Mizanur Rahman, Amran Bhuiyan, Israt Jahan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸¤ç§æˆæœ¬é«˜æ•ˆçš„è¯„ä¼°æ–¹æ³•ï¼šå¤šæ ‡å‡†æç¤ºå’Œé¢†åŸŸè‡ªé€‚åº”è¿ç§»å­¦ä¹ ï¼ŒæˆåŠŸæå‡äº†å°å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­ä½œä¸ºè‡ªåŠ¨è¯„åˆ¤å™¨çš„æ€§èƒ½ï¼Œä½¿2Bå‚æ•°çš„ChartJudgeæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ›¿ä»£7Bæ¨¡å‹è¿›è¡Œå›¾è¡¨æ¨ç†è¯„ä¼°ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­ä½œä¸ºè‡ªåŠ¨è¯„åˆ¤å™¨è¡¨ç°å‡ºè‰²ï¼Œä½†å°å‹æ¨¡å‹ï¼ˆâ‰¤2Bå‚æ•°ï¼‰æ€§èƒ½ä»ç„¶è¾ƒå·®ï¼Œé™åˆ¶äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ï¼Œå› æ­¤éœ€è¦å¼€å‘æˆæœ¬é«˜æ•ˆçš„è¯„ä¼°æ–¹æ³•æ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸¤ç§æ ¸å¿ƒæ–¹æ³•ï¼šå¤šæ ‡å‡†æç¤ºå°†å¤šä¸ªè¯„ä¼°æ ‡å‡†æ•´åˆåˆ°å•ä¸ªæŸ¥è¯¢ä¸­ï¼Œä»¥åŠé¢†åŸŸè‡ªé€‚åº”è¿ç§»å­¦ä¹ ï¼Œé€šè¿‡åœ¨å›¾è¡¨æ•°æ®é›†ä¸Šä½¿ç”¨åˆæˆåˆ¤æ–­å¯¹2Bå‚æ•°çš„LVLMè¿›è¡Œå¾®è°ƒï¼Œåˆ›å»ºä¸“é—¨çš„ChartJudgeæ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜å¤šæ ‡å‡†æç¤ºæš´éœ²äº†7Bæ¨¡å‹çš„é²æ£’æ€§å·®è·ï¼Œå¯¼è‡´åŒ…æ‹¬LLaVA-Criticåœ¨å†…çš„ä¸“ä¸šLVLMè¯„åˆ¤å™¨æ€§èƒ½å¤§å¹…ä¸‹é™ï¼ŒåŒæ—¶ChartJudgeèƒ½å¤Ÿæœ‰æ•ˆåœ¨ä¸åŒæ•°æ®é›†é—´è¿ç§»çŸ¥è¯†ï¼Œæˆä¸ºæ›´ä¸“ä¸šçš„æ¨¡å‹ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡å¯¹å›¾è¡¨ç±»å‹å’ŒæŸ¥è¯¢å¤æ‚åº¦çš„ç»†ç²’åº¦åˆ†æï¼Œæœ¬ç ”ç©¶æä¾›äº†å…³äºæ¨¡å‹å¤§å°ã€æç¤ºè®¾è®¡å’Œå¯è¿ç§»æ€§ä¹‹é—´æƒè¡¡çš„å¯æ“ä½œè§è§£ï¼Œä¸ºå›¾è¡¨æ¨ç†ä»»åŠ¡å®ç°äº†å¯æ‰©å±•ã€ä½æˆæœ¬çš„è¯„ä¼°æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(&lt;=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.</p>
<h3 id="38-toolexpander-extending-the-frontiers-of-tool-using-reinforcement-learning-to-weak-llms">[38] <a href="https://arxiv.org/abs/2510.07737">ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs</a></h3>
<p><em>Fu Chen, Peng Wang, Xiyin Li, Wen Li, Shichi Lei, Dongdong Xiang</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ToolExpanderæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å¤šè½®ç¡¬é‡‡æ ·å’Œè‡ªç¤ºä¾‹æ€ç»´ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼Œè§£å†³äº†å°è§„æ¨¡LLMåœ¨GRPOè®­ç»ƒä¸­å“åº”ä¸å‡†ç¡®å’Œè®­ç»ƒå´©æºƒçš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å·¥å…·ä½¿ç”¨èƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨GRPOæ–¹æ³•æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼šæ¨¡å‹ç»å¸¸æ— æ³•äº§ç”Ÿå‡†ç¡®å“åº”ï¼Œç‰¹åˆ«æ˜¯åœ¨å°è§„æ¨¡æ¶æ„ä¸­ã€‚è¿™ä¸€é™åˆ¶ä¸ä»…é™ä½äº†æ€§èƒ½æ”¹è¿›å¹¶å‰Šå¼±äº†GRPOçš„æ½œåŠ›ï¼Œè¿˜ç»å¸¸å¯¼è‡´è®­ç»ƒä¸­æœŸå´©æºƒï¼Œä¸¥é‡å½±å“ç¨³å®šæ€§å’Œæœ€ç»ˆæ•ˆæœã€‚</p>
<p><strong>Method:</strong> ToolExpanderæ¡†æ¶åŒ…å«ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šåŠ¨æ€å¤šè½®ç¡¬é‡‡æ ·åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€æ›¿æ¢å›°éš¾æ ·æœ¬ä¸ºé«˜è´¨é‡å°‘æ ·æœ¬æ¼”ç¤ºï¼Œå¹¶ç»“åˆæŒ‡æ•°å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ç¼“è§£æŒ¯è¡ï¼›è‡ªç¤ºä¾‹æ€ç»´æ˜¯å¢å¼ºçš„GRPOæ¡†æ¶ï¼Œæ¶ˆé™¤äº†KLæ•£åº¦å¹¶æ•´åˆè°ƒæ•´çš„è£å‰ªç³»æ•°ï¼Œé€šè¿‡æœ€å°é¢å¤–å¥–åŠ±ï¼ˆ0.01ï¼‰é¼“åŠ±æ¨¡å‹è‡ªä¸»ç”Ÿæˆå’Œåˆ†æå°‘æ ·æœ¬ç¤ºä¾‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒToolExpanderæ˜¾è‘—å¢å¼ºäº†LLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå¼±çš„å°è§„æ¨¡æ¨¡å‹ä¸­ï¼ŒåŒæ—¶æ”¹å–„äº†è®­ç»ƒç¨³å®šæ€§å’Œæ•´ä½“æ€§èƒ½ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†æ¨¡å‹å“åº”ä¸å‡†ç¡®å’Œè®­ç»ƒå´©æºƒçš„é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡åˆ›æ–°çš„è®­ç»ƒç­–ç•¥å¯ä»¥æ˜¾è‘—æå‡èµ„æºå—é™LLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä¸ºå°è§„æ¨¡æ¨¡å‹çš„æœ‰æ•ˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Training Large Language Models (LLMs) with Group Relative Policy Optimization
(GRPO) encounters a significant challenge: models often fail to produce
accurate responses, particularly in small-scale architectures. This limitation
not only diminishes performance improvements and undermines the potential of
GRPO but also frequently leads to mid-training collapse, adversely affecting
stability and final efficacy. To address these issues, we propose ToolExpander,
a novel framework that advances tool-oriented reinforcement learning for
resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round
Hard Sampling, which dynamically substitutes challenging samples(those without
correct outputs over 10 rollouts) with high-quality few-shot demonstrations
during training, coupled with an exponential learning rate decay strategy to
mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO
framework that eliminates KL divergence and incorporates adjusted clipping
coefficients, encouraging models to autonomously generate and analyze few-shot
examples via a minimal additional reward (0.01).Experimental results
demonstrate that ToolExpander significantly enhances tool-using capabilities in
LLMs, especially in weaker small-scale models, improving both training
stability and overall performance.</p>
<h3 id="39-llm4cell-a-survey-of-large-language-and-agentic-models-for-single-cell-biology">[39] <a href="https://arxiv.org/abs/2510.07793">LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology</a></h3>
<p><em>Sajib Acharjee Dip, Adrika Zafor, Bikash Kumar Paul, Uddip Acharjee Shuvo, Muhit Islam Emon, Xuan Wang, Liqing Zhang</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>LLM4Cellæå‡ºäº†é¦–ä¸ªç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å•ç»†èƒç”Ÿç‰©å­¦é¢†åŸŸçš„ç³»ç»Ÿç»¼è¿°ï¼Œæ¶µç›–äº†58ä¸ªåŸºç¡€æ¨¡å‹å’Œæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆæ•°æ®é›†ã€æ¨¡å‹å’Œè¯„ä¼°ç»´åº¦ï¼Œä¸ºè¯­è¨€é©±åŠ¨çš„å•ç»†èƒæ™ºèƒ½ç ”ç©¶æä¾›äº†ç»¼åˆè§†å›¾ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½ä½“æ¡†æ¶åœ¨å•ç»†èƒç”Ÿç‰©å­¦ä¸­çš„åº”ç”¨ä»å­˜åœ¨æ•°æ®æ¨¡æ€ã€æ¶æ„å’Œè¯„ä¼°æ ‡å‡†ç¢ç‰‡åŒ–çš„é—®é¢˜ï¼Œç¼ºä¹ç»Ÿä¸€çš„ç³»ç»Ÿæ€§ç»¼è¿°æ¥æ•´åˆè¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶ç³»ç»Ÿæ€§åœ°åˆ†ç±»äº†58ä¸ªå•ç»†èƒç ”ç©¶ç›¸å…³çš„åŸºç¡€æ¨¡å‹å’Œæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†å…¶åˆ’åˆ†ä¸ºåŸºç¡€æ¨¡å‹ã€æ–‡æœ¬æ¡¥æ¥ã€ç©ºé—´ç»„å­¦ã€å¤šæ¨¡æ€ã€è¡¨è§‚åŸºå› ç»„å’Œæ™ºèƒ½ä½“å…­å¤§ç±»åˆ«ï¼Œå¹¶æ˜ å°„åˆ°å…«ä¸ªå…³é”®åˆ†æä»»åŠ¡ä¸­ã€‚</p>
<p><strong>Result:</strong> åŸºäº40å¤šä¸ªå…¬å…±æ•°æ®é›†çš„åˆ†ææ˜¾ç¤ºï¼Œè¯¥ç ”ç©¶è¯„ä¼°äº†æ¨¡å‹åœ¨10ä¸ªé¢†åŸŸç»´åº¦ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬ç”Ÿç‰©å­¦åŸºç¡€ã€å¤šç»„å­¦å¯¹é½ã€å…¬å¹³æ€§ã€éšç§ä¿æŠ¤å’Œå¯è§£é‡Šæ€§ï¼Œå¹¶åˆ†æäº†åŸºå‡†æµ‹è¯•çš„é€‚ç”¨æ€§å’Œæ•°æ®å¤šæ ·æ€§çº¦æŸã€‚</p>
<p><strong>Conclusion:</strong> LLM4Cellä¸ºè¯­è¨€é©±åŠ¨çš„å•ç»†èƒæ™ºèƒ½ç ”ç©¶æä¾›äº†é¦–ä¸ªé›†æˆè§†å›¾ï¼ŒæŒ‡å‡ºäº†åœ¨å¯è§£é‡Šæ€§ã€æ ‡å‡†åŒ–å’Œå¯ä¿¡æ¨¡å‹å¼€å‘æ–¹é¢å­˜åœ¨çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) and emerging agentic frameworks are beginning to
transform single-cell biology by enabling natural-language reasoning,
generative annotation, and multimodal data integration. However, progress
remains fragmented across data modalities, architectures, and evaluation
standards. LLM4Cell presents the first unified survey of 58 foundation and
agentic models developed for single-cell research, spanning RNA, ATAC,
multi-omic, and spatial modalities. We categorize these methods into five
families-foundation, text-bridge, spatial, multimodal, epigenomic, and
agentic-and map them to eight key analytical tasks including annotation,
trajectory and perturbation modeling, and drug-response prediction. Drawing on
over 40 public datasets, we analyze benchmark suitability, data diversity, and
ethical or scalability constraints, and evaluate models across 10 domain
dimensions covering biological grounding, multi-omics alignment, fairness,
privacy, and explainability. By linking datasets, models, and evaluation
domains, LLM4Cell provides the first integrated view of language-driven
single-cell intelligence and outlines open challenges in interpretability,
standardization, and trustworthy model development.</p>
<h3 id="40-cs3-bench-evaluating-and-enhancing-speech-to-speech-llms-for-mandarin-english-code-switching">[40] <a href="https://arxiv.org/abs/2510.07881">CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching</a></h3>
<p><em>Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†CS3-BenchåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯­ç è½¬æ¢è¯­éŸ³äº¤äº’ä¸­çš„è¯­è¨€å¯¹é½ç¼ºé™·ï¼Œå¹¶æå‡ºåŸºäºé“¾å¼è¯†åˆ«å’Œå…³é”®è¯é«˜äº®çš„æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®ç°è‡ªç„¶å•è¯­äº¤äº’æ–¹é¢å·²å–å¾—è¿›å±•ï¼Œä½†åœ¨è¯­è¨€å¯¹é½èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ç è½¬æ¢åœºæ™¯ä¸‹ï¼Œæ¨¡å‹éš¾ä»¥å‡†ç¡®ç†è§£å’Œç”Ÿæˆæ··åˆè¯­è¨€çš„è¯­éŸ³å†…å®¹ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†CS3-BenchåŸºå‡†æµ‹è¯•ï¼Œå¹¶æå‡ºäº†æ•°æ®æ„å»ºå’Œè®­ç»ƒæ–¹æ³•ï¼Œå…·ä½“é‡‡ç”¨é“¾å¼è¯†åˆ«æ¥å¢å¼ºç†è§£èƒ½åŠ›ï¼Œä»¥åŠå…³é”®è¯é«˜äº®æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œæ”¹å–„è¯­è¨€å¯¹é½æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> åœ¨7ä¸ªä¸»æµæ¨¡å‹ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”ä»»åŠ¡ä¸­æ€§èƒ½ç›¸å¯¹ä¸‹é™é«˜è¾¾66%ï¼Œè€Œæå‡ºçš„æ–¹æ³•å°†çŸ¥è¯†å‡†ç¡®ç‡ä»25.14%æå‡è‡³46.13%ï¼Œå¼€æ”¾å¼ç†è§£ç‡ä»64.5%æå‡è‡³86.5%ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†ç¬¬äºŒè¯­è¨€çš„å‘éŸ³é”™è¯¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€è¯­éŸ³äº¤äº’ç³»ç»Ÿä¸­è¯­è¨€å¯¹é½çš„é‡è¦æ€§ï¼Œæå‡ºçš„æ–¹æ³•ä¸ºæ”¹å–„è¯­ç è½¬æ¢åœºæ™¯ä¸‹çš„æ¨¡å‹æ€§èƒ½æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥å¤šè¯­è¨€è¯­éŸ³äº¤äº’ç³»ç»Ÿçš„å¼€å‘æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>The advancement of multimodal large language models has accelerated the
development of speech-to-speech interaction systems. While natural monolingual
interaction has been achieved, we find existing models exhibit deficiencies in
language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark
(CS3-Bench), experiments on 7 mainstream models demonstrate a relative
performance drop of up to 66% in knowledge-intensive question answering and
varying degrees of misunderstanding in open-ended conversations. Starting from
a model with severe performance deterioration, we propose both data
constructions and training approaches to improve the language alignment
capabilities, specifically employing Chain of Recognition (CoR) to enhance
understanding and Keyword Highlighting (KH) to guide generation. Our approach
improves the knowledge accuracy from 25.14% to 46.13%, with open-ended
understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation
errors in the secondary language. CS3-Bench is available at
https://huggingface.co/datasets/VocalNet/CS3-Bench.</p>
<h3 id="41-vision-enabled-llms-in-historical-lexicography-digitising-and-enriching-estonian-german-dictionaries-from-the-17th-and-18th-centuries">[41] <a href="https://arxiv.org/abs/2510.07931">Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries</a></h3>
<p><em>Madis JÃ¼rviste, Joonatan Jakobson</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨17-18ä¸–çºªçˆ±æ²™å°¼äºšè¯­å†å²è¯å…¸ç ”ç©¶ä¸­çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è¯å…¸ä¿¡æ¯åŠè‡ªåŠ¨ä¸°å¯Œã€å“¥ç‰¹ä½“æ–‡å­—è¯†åˆ«å’Œè·¨æºæ•°æ®é›†æ„å»ºï¼Œä¸ºå°è¯­ç§å†å²æ–‡çŒ®æ•°å­—åŒ–æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³17-18ä¸–çºªçˆ±æ²™å°¼äºšè¯­å†å²è¯å…¸æ•°å­—åŒ–è¿‡ç¨‹ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯å…¸ä¿¡æ¯ä¸å®Œæ•´ã€å“¥ç‰¹ä½“æ–‡å­—è¯†åˆ«å›°éš¾ä»¥åŠè·¨æºæ•°æ®æ•´åˆå¤æ‚ç­‰é—®é¢˜ï¼Œè¿™äº›å› ç´ ä¸¥é‡åˆ¶çº¦äº†å°è¯­ç§å†å²è¯­è¨€èµ„æºçš„å¼€å‘åˆ©ç”¨ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨Claude 3.7 Sonnetè¿›è¡Œè¯å…¸æ¡ç›®è¯­ä¹‰ä¸°å¯ŒåŒ–ï¼Œè¿ç”¨è§†è§‰å¢å¼ºLLMå¯¹å“¥ç‰¹ä½“å°åˆ·æ–‡æœ¬è¿›è¡Œé›¶æ ·æœ¬è¯†åˆ«ï¼Œå¹¶é€šè¿‡é‡å åˆ‡ç‰‡æ‰«æä¸åŒæ¨¡å‹åä½œç­–ç•¥å®ç°ç»“æ„åŒ–æ•°æ®æå–ä¸åˆå¹¶ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å……åˆ†ä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹ï¼ŒClaude 3.7 Sonnetèƒ½å¤Ÿä¸º81%çš„è¯ç›®æ¡ç›®å‡†ç¡®æä¾›ç°ä»£è¯ä¹‰å’Œå¯¹åº”å½¢å¼ï¼›åœ¨æ–‡æœ¬è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œé›¶æ ·æœ¬æ–¹æ³•æˆåŠŸè¯†åˆ«å¹¶ç»“æ„åŒ–41%çš„è¯ç›®æ¡ç›®ä¸ºæ— é”™è¯¯JSONæ ¼å¼ï¼›Hupelè¯å…¸æ•°å­—åŒ–é¡¹ç›®é‡‡ç”¨é‡å åˆ‡ç‰‡æŠ€æœ¯å®ç°äº†é«˜æ•ˆæ•°æ®æå–ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹å³ä½¿å¯¹å°è¯­ç§å†å²æ–‡çŒ®å¤„ç†ä¹Ÿå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿå¤§å¹…èŠ‚çœæ—¶é—´å’Œç»æµæˆæœ¬ï¼Œä¸ºå†å²è¯­è¨€å­¦ç ”ç©¶å’Œæ–‡åŒ–é—äº§æ•°å­—åŒ–æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œè¯æ˜äº†AIæŠ€æœ¯åœ¨ä½èµ„æºè¯­è¨€å¤„ç†ä¸­çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>This article presents research conducted at the Institute of the Estonian
Language between 2022 and 2025 on the application of large language models
(LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors
address three main areas: enriching historical dictionaries with modern word
forms and meanings; using vision-enabled LLMs to perform text recognition on
sources printed in Gothic script (Fraktur); and preparing for the creation of a
unified, cross-source dataset. Initial experiments with J. Gutslaff's 1648
dictionary indicate that LLMs have significant potential for semi-automatic
enrichment of dictionary information. When provided with sufficient context,
Claude 3.7 Sonnet accurately provided meanings and modern equivalents for 81%
of headword entries. In a text recognition experiment with A. T. Helle's 1732
dictionary, a zero-shot method successfully identified and structured 41% of
headword entries into error-free JSON-formatted output. For digitising the
Estonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping
tiling of scanned image files is employed, with one LLM being used for text
recognition and a second for merging the structured output. These findings
demonstrate that even for minor languages LLMs have a significant potential for
saving time and financial resources.</p>
<h3 id="42-leveraging-author-specific-context-for-scientific-figure-caption-generation-3rd-scicap-challenge">[42] <a href="https://arxiv.org/abs/2510.07993">Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge</a></h3>
<p><em>Watcharapong Timklaypachara, Monrada Chiewhawan, Nopporn Lekuthai, Titipat Achakulvisut</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ç§‘å­¦å›¾è¡¨æ ‡é¢˜ç”Ÿæˆçš„é¢†åŸŸç‰¹å®šç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆä¸Šä¸‹æ–‡ç†è§£å’Œä½œè€…ç‰¹å®šé£æ ¼é€‚åº”ï¼Œèƒ½å¤Ÿç”Ÿæˆæ—¢ç§‘å­¦å‡†ç¡®åˆé£æ ¼å¿ å®äºæºè®ºæ–‡çš„æ ‡é¢˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸¤é˜¶æ®µæµæ°´çº¿æ–¹æ³•ï¼Œåœ¨SciCapæŒ‘æˆ˜ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç§‘å­¦å›¾è¡¨æ ‡é¢˜éœ€è¦åŒæ—¶å…·å¤‡å‡†ç¡®æ€§å’Œé£æ ¼ä¸€è‡´æ€§æ¥ä¼ è¾¾è§†è§‰ä¿¡æ¯ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¿æŒä½œè€…ç‰¹å®šå†™ä½œé£æ ¼æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç§‘å­¦å›¾è¡¨æ ‡é¢˜ç”Ÿæˆä¸­ä¸Šä¸‹æ–‡ç†è§£ä¸é£æ ¼é€‚åº”æ€§ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç±»åˆ«ç§‘å­¦å›¾è¡¨åœºæ™¯ä¸‹ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨ä¸¤é˜¶æ®µæµæ°´çº¿æ–¹æ³•ï¼šç¬¬ä¸€é˜¶æ®µç»“åˆä¸Šä¸‹æ–‡è¿‡æ»¤ã€åŸºäºDSPy MIPROv2å’ŒSIMBAçš„ç±»åˆ«ç‰¹å®šæç¤ºä¼˜åŒ–ä»¥åŠæ ‡é¢˜å€™é€‰é€‰æ‹©ï¼›ç¬¬äºŒé˜¶æ®µåº”ç”¨åŸºäºé…ç½®æ–‡ä»¶çš„å°‘æ ·æœ¬æç¤ºè¿›è¡Œé£æ ¼ç²¾ç‚¼ï¼Œåˆ©ç”¨LaMP-Capæ•°æ®é›†å®ç°ä½œè€…ç‰¹å®šé£æ ¼é€‚åº”ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ç±»åˆ«ç‰¹å®šæç¤ºæ–¹æ³•æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬å’Œé€šç”¨ä¼˜åŒ–æ–¹æ³•ï¼ŒROUGE-1å¬å›ç‡æå‡+8.3%ï¼ŒåŒæ—¶å°†ç²¾åº¦æŸå¤±é™åˆ¶åœ¨-2.8%ï¼ŒBLEU-4å‡å°‘æ§åˆ¶åœ¨-10.9%ã€‚åŸºäºé…ç½®æ–‡ä»¶çš„é£æ ¼ç²¾ç‚¼åœ¨BLEUåˆ†æ•°ä¸Šè·å¾—40-48%çš„æå‡ï¼Œåœ¨ROUGEæŒ‡æ ‡ä¸Šè·å¾—25-27%çš„æå‡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ç»“åˆä¸Šä¸‹æ–‡ç†è§£ä¸ä½œè€…ç‰¹å®šé£æ ¼é€‚åº”èƒ½å¤Ÿç”Ÿæˆæ—¢ç§‘å­¦å‡†ç¡®åˆé£æ ¼å¿ å®çš„æ ‡é¢˜ï¼Œä¸ºé¢†åŸŸç‰¹å®šçš„ç§‘å­¦å†…å®¹ç”Ÿæˆæä¾›äº†æœ‰æ•ˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼ºè°ƒäº†åœ¨ç§‘å­¦äº¤æµä¸­ä¿æŒä½œè€…ç‹¬ç‰¹å†™ä½œé£æ ¼çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„å¤šæ¨¡æ€ç§‘å­¦å†…å®¹ç”Ÿæˆç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Scientific figure captions require both accuracy and stylistic consistency to
convey visual information. Here, we present a domain-specific caption
generation system for the 3rd SciCap Challenge that integrates figure-related
textual context with author-specific writing styles using the LaMP-Cap dataset.
Our approach uses a two-stage pipeline: Stage 1 combines context filtering,
category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption
candidate selection; Stage 2 applies few-shot prompting with profile figures
for stylistic refinement. Our experiments demonstrate that category-specific
prompts outperform both zero-shot and general optimized approaches, improving
ROUGE-1 recall by +8.3\% while limiting precision loss to -2.8\% and BLEU-4
reduction to -10.9\%. Profile-informed stylistic refinement yields 40--48\%
gains in BLEU scores and 25--27\% in ROUGE. Overall, our system demonstrates
that combining contextual understanding with author-specific stylistic
adaptation can generate captions that are both scientifically accurate and
stylistically faithful to the source paper.</p>
<h3 id="43-learning-on-the-job-an-experience-driven-self-evolving-agent-for-long-horizon-tasks">[43] <a href="https://arxiv.org/abs/2510.08002">Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks</a></h3>
<p><em>Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, Haifeng Li</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>MUSEæå‡ºäº†ä¸€ç§åŸºäºåˆ†å±‚è®°å¿†æ¨¡å—çš„ç»éªŒé©±åŠ¨è‡ªè¿›åŒ–AIæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰LLMæ™ºèƒ½ä½“åœ¨çœŸå®ä¸–ç•Œé•¿è§†é‡ä»»åŠ¡ä¸­æ— æ³•ä»ç»éªŒä¸­å­¦ä¹ çš„å…³é”®é™åˆ¶ï¼Œå®ç°äº†æŒç»­å­¦ä¹ å’Œè‡ªæˆ‘è¿›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å­˜åœ¨å…³é”®é™åˆ¶ï¼šå®ƒä»¬æ˜¯æµ‹è¯•æ—¶é™æ€çš„ï¼Œæ— æ³•ä»ç»éªŒä¸­å­¦ä¹ ï¼Œç¼ºä¹ç§¯ç´¯çŸ¥è¯†å’ŒæŒç»­æ”¹è¿›çš„èƒ½åŠ›ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨çœŸå®ä¸–ç•Œé•¿è§†é‡ä»»åŠ¡ä¸­çš„æœ‰æ•ˆéƒ¨ç½²ã€‚</p>
<p><strong>Method:</strong> MUSEå¼•å…¥äº†ä¸€ä¸ªä»¥åˆ†å±‚è®°å¿†æ¨¡å—ä¸ºæ ¸å¿ƒçš„ç»éªŒé©±åŠ¨è‡ªè¿›åŒ–ç³»ç»Ÿï¼Œè¯¥æ¡†æ¶ç»„ç»‡ä¸åŒå±‚æ¬¡çš„ç»éªŒå¹¶åˆ©ç”¨å®ƒä»¬è§„åˆ’æ‰§è¡Œå¤šåº”ç”¨é•¿è§†é‡ä»»åŠ¡ï¼Œæ™ºèƒ½ä½“åœ¨æ¯ä¸ªå­ä»»åŠ¡æ‰§è¡Œåè‡ªä¸»åæ€å…¶è½¨è¿¹ï¼Œå°†åŸå§‹è½¨è¿¹è½¬æ¢ä¸ºç»“æ„åŒ–ç»éªŒå¹¶æ•´åˆå›è®°å¿†æ¨¡å—ã€‚</p>
<p><strong>Result:</strong> åœ¨é•¿è§†é‡ç”Ÿäº§åŠ›åŸºå‡†TACä¸Šï¼ŒMUSEä»…ä½¿ç”¨è½»é‡çº§Gemini-2.5 Flashæ¨¡å‹å°±å®ç°äº†æ˜¾è‘—è¶…è¶Šç°æœ‰æœ€ä½³æ€§èƒ½çš„æ–°SOTAï¼Œå®éªŒè¡¨æ˜éšç€æ™ºèƒ½ä½“è‡ªä¸»ç§¯ç´¯ç»éªŒï¼Œå…¶ä»»åŠ¡å®Œæˆèƒ½åŠ›æŒç»­æå‡ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„æŒç»­å­¦ä¹ å’Œè‡ªè¿›åŒ–èƒ½åŠ›ï¼Œç§¯ç´¯çš„ç»éªŒè¿˜è¡¨ç°å‡ºå¼ºæ³›åŒ–ç‰¹æ€§ï¼Œèƒ½å¤Ÿé›¶æ ·æœ¬æ”¹è¿›æ–°ä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> MUSEå»ºç«‹äº†ä¸€ç§èƒ½å¤Ÿå®ç°çœŸå®ä¸–ç•Œç”Ÿäº§åŠ›ä»»åŠ¡è‡ªåŠ¨åŒ–çš„AIæ™ºèƒ½ä½“æ–°èŒƒå¼ï¼Œå…¶ç»éªŒé©±åŠ¨æœºåˆ¶ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè¶…è¶Šé™æ€é¢„è®­ç»ƒå‚æ•°å®ç°æŒç»­è¿›åŒ–ï¼Œä¸ºæ„å»ºå…·æœ‰é•¿æœŸå­¦ä¹ èƒ½åŠ›çš„AIç³»ç»Ÿæä¾›äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Large Language Models have demonstrated remarkable capabilities across
diverse domains, yet significant challenges persist when deploying them as AI
agents for real-world long-horizon tasks. Existing LLM agents suffer from a
critical limitation: they are test-time static and cannot learn from
experience, lacking the ability to accumulate knowledge and continuously
improve on the job. To address this challenge, we propose MUSE, a novel agent
framework that introduces an experience-driven, self-evolving system centered
around a hierarchical Memory Module. MUSE organizes diverse levels of
experience and leverages them to plan and execute long-horizon tasks across
multiple applications. After each sub-task execution, the agent autonomously
reflects on its trajectory, converting the raw trajectory into structured
experience and integrating it back into the Memory Module. This mechanism
enables the agent to evolve beyond its static pretrained parameters, fostering
continuous learning and self-evolution. We evaluate MUSE on the long-horizon
productivity benchmark TAC. It achieves new SOTA performance by a significant
margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments
demonstrate that as the agent autonomously accumulates experience, it exhibits
increasingly superior task completion capabilities, as well as robust
continuous learning and self-evolution capabilities. Moreover, the accumulated
experience from MUSE exhibits strong generalization properties, enabling
zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI
agents capable of real-world productivity task automation.</p>
<h3 id="44-a-survey-of-process-reward-models-from-outcome-signals-to-process-supervisions-for-large-language-models">[44] <a href="https://arxiv.org/abs/2510.08049">A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models</a></h3>
<p><em>Congming Zheng, Jiachen Zhu, Zhuoying Ou, Yuxiang Chen, Kangning Zhang, Rong Shan, Zeyu Zheng, Mengyue Yang, Jianghao Lin, Yong Yu, Weinan Zhang</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬ç»¼è¿°ç³»ç»Ÿæ¢³ç†äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰çš„ç ”ç©¶è¿›å±•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç»“æœå¥–åŠ±æ¨¡å‹ä»…è¯„ä¼°æœ€ç»ˆç­”æ¡ˆçš„å±€é™æ€§ï¼Œé€šè¿‡è¯„ä¼°æ¨ç†è¿‡ç¨‹æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦æ¨ç†å¯¹é½èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§è¯­è¨€æ¨¡å‹å±•ç°å‡ºå…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä¼ ç»Ÿçš„å¯¹é½æ–¹æ³•ä¸»è¦ä¾èµ–ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰ï¼Œè¿™äº›æ¨¡å‹ä»…å¯¹æœ€ç»ˆç­”æ¡ˆè¿›è¡Œè¯„åˆ¤ï¼Œæ— æ³•å¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œç»†ç²’åº¦è¯„ä¼°å’ŒæŒ‡å¯¼ï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰æ­£æ˜¯ä¸ºäº†å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½è€Œæå‡ºçš„ã€‚</p>
<p><strong>Method:</strong> è®ºæ–‡ç³»ç»Ÿæ€§åœ°ä»‹ç»äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„å®Œæ•´æ„å»ºæµç¨‹ï¼ŒåŒ…æ‹¬è¿‡ç¨‹æ•°æ®ç”Ÿæˆæ–¹æ³•ã€PRMsæ¨¡å‹æ„å»ºæŠ€æœ¯ï¼Œä»¥åŠå¦‚ä½•å°†PRMsåº”ç”¨äºæµ‹è¯•æ—¶æ‰©å±•å’Œå¼ºåŒ–å­¦ä¹ ç­‰åœºæ™¯ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æ€»ç»“äº†PRMsåœ¨æ•°å­¦ã€ä»£ç ã€æ–‡æœ¬ã€å¤šæ¨¡æ€æ¨ç†ã€æœºå™¨äººæŠ€æœ¯å’Œæ™ºèƒ½ä½“ç­‰å¤šä¸ªé¢†åŸŸçš„åº”ç”¨å®è·µï¼Œå¹¶å¯¹æ–°å…´çš„åŸºå‡†æµ‹è¯•è¿›è¡Œäº†å…¨é¢è¯„è¿°ï¼Œå±•ç°äº†è¯¥æŠ€æœ¯åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç»¼è¿°æ˜ç¡®äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„è®¾è®¡ç©ºé—´ï¼Œæ­ç¤ºäº†å½“å‰é¢ä¸´çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æŒ‡å¯¼æ–¹å‘ï¼Œæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹å‘æ›´ç»†ç²’åº¦ã€æ›´é²æ£’çš„æ¨ç†å¯¹é½æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Although Large Language Models (LLMs) exhibit advanced reasoning ability,
conventional alignment remains largely dominated by outcome reward models
(ORMs) that judge only final answers. Process Reward Models(PRMs) address this
gap by evaluating and guiding reasoning at the step or trajectory level. This
survey provides a systematic overview of PRMs through the full loop: how to
generate process data, build PRMs, and use PRMs for test-time scaling and
reinforcement learning. We summarize applications across math, code, text,
multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our
goal is to clarify design spaces, reveal open challenges, and guide future
research toward fine-grained, robust reasoning alignment.</p>
<h3 id="45-dacip-rc-domain-adaptive-continual-instruction-pre-training-via-reading-comprehension-on-business-conversations">[45] <a href="https://arxiv.org/abs/2510.08152">DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations</a></h3>
<p><em>Elena Khasanova, Harsh Saini, Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDACIP-RCæ–¹æ³•ï¼Œé€šè¿‡é˜…è¯»ç†è§£å¼æŒç»­é¢„è®­ç»ƒå¢å¼ºå°å‹LLMåœ¨ä¸šåŠ¡å¯¹è¯ä»»åŠ¡ä¸­çš„é¢†åŸŸé€‚åº”èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§è§„æ¨¡LLMçš„é«˜æ¨ç†æˆæœ¬ä½¿å…¶éƒ¨ç½²ä¸åˆ‡å®é™…ï¼Œè€Œå°å‹LLMç¼ºä¹è·¨é¢†åŸŸçš„é›¶æ ·æœ¬æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œä¼ ç»Ÿå¾®è°ƒæ–¹æ³•åˆä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œé™åˆ¶äº†æ¨¡å‹å¯¹åŠ¨æ€ç”¨æˆ·éœ€æ±‚çš„é€‚åº”æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºé¢†åŸŸè‡ªé€‚åº”æŒç»­æŒ‡ä»¤é¢„è®­ç»ƒæ–¹æ³•DACIP-RCï¼Œé€šè¿‡é˜…è¯»ç†è§£æ–¹å¼åœ¨å¯¹è¯è½¬å½•æœ¬ä¸Šç”Ÿæˆå¤šæ ·åŒ–ä»»åŠ¡æŒ‡ä»¤å’Œå“åº”ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„ä¸‹ä¸€è¯å…ƒé¢„æµ‹æ–¹æ³•ï¼Œå®ç°æ›´å¥½çš„æŒ‡ä»¤æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®è¯è¯„ä¼°è¡¨æ˜DACIP-RCåœ¨å¤šç§ä¸šåŠ¡å¯¹è¯ä»»åŠ¡ä¸­æ˜¾è‘—æå‡é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ï¼ŒåŒ…æ‹¬ä¼šè®®æ‘˜è¦ã€è¡ŒåŠ¨é¡¹ç”Ÿæˆå’Œé€šè¯ç›®çš„è¯†åˆ«ç­‰ä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> è¿™æ˜¯é¦–ä¸ªå°†æŒ‡ä»¤é¢„è®­ç»ƒåº”ç”¨äºä¸šåŠ¡å¯¹è¯æ•°æ®çš„å·¥ä½œï¼Œä¸ºè¡Œä¸šå¦‚ä½•åˆ©ç”¨ä¸“æœ‰æ•°æ®é›†è¿›è¡Œé¢†åŸŸé€‚åº”æä¾›äº†é‡è¦è§è§£ï¼Œå±•ç¤ºäº†æŒç»­é¢„è®­ç»ƒåœ¨å¢å¼ºå°å‹æ¨¡å‹é€‚åº”æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>The rapid advancements in Large Language Models (LLMs) have enabled their
adoption in real-world industrial scenarios for various natural language
processing tasks. However, the high inference cost of large-scale LLMs makes
their deployment impractical, necessitating the use of smaller models. Despite
their efficiency, smaller LLMs lack robust zero-shot instruction-following
capabilities across diverse domains, limiting their adaptability to dynamic
user requirements. Traditional fine-tuning approaches exacerbate this issue by
inducing catastrophic forgetting, reducing the model's generalization ability
for unseen tasks. In this paper, we propose Domain Adaptive Continual
Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual
pre-training technique that enhances smaller LLMs' domain adaptability for
business conversational tasks. Unlike conventional pre-training approaches that
rely on next-token prediction, DACIP-RC generates diverse task instructions and
responses via reading comprehension on conversation transcripts, enabling
better instruction generalization. Our empirical evaluations demonstrate that
DACIP-RC significantly improves zero-shot generalization across a wide range of
business conversational tasks, including meeting summarization, action item
generation, and call purpose identification. To the best of our knowledge, this
is the first work to apply instruction pre-training on business conversational
data, providing insights into how industries can leverage proprietary datasets
for domain adaptation.</p>
<h3 id="46-arm2-adaptive-reasoning-model-with-vision-understanding-and-executable-code">[46] <a href="https://arxiv.org/abs/2510.08163">ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code</a></h3>
<p><em>Jian Xie, Zhendong Chu, Aoxiao Zhong, Kai Zhang, Mingzhe Han, Xin Fang, Jialie Shen, Qingsong Wen</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºARM2æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œé•¿åº¦æ„ŸçŸ¥ä¼˜åŒ–ï¼Œè‡ªé€‚åº”å¹³è¡¡æ¨ç†æ€§èƒ½ä¸æ•ˆç‡ï¼Œåœ¨ä¿æŒä¸ä¼ ç»Ÿæ¨ç†æ¨¡å‹ç›¸å½“æ€§èƒ½çš„åŒæ—¶å¹³å‡å‡å°‘70%ä»¥ä¸Šçš„tokenä½¿ç”¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹æ¨ç†æ¨¡å‹å­˜åœ¨"è¿‡åº¦æ€è€ƒ"é—®é¢˜ï¼Œåœ¨ç®€å•ä»»åŠ¡ä¸Šç”Ÿæˆä¸å¿…è¦çš„å†—é•¿æ¨ç†ï¼Œç°æœ‰æ–¹æ³•å¦‚é•¿åº¦æƒ©ç½šæˆ–è·¯ç”±æœºåˆ¶é€šå¸¸æ˜¯å¯å‘å¼ä¸”ä»»åŠ¡ç‰¹å®šçš„ï¼Œç¼ºä¹è‡ªé€‚åº”æ¨ç†çš„é€šç”¨æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> ARM2é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶å¢å¼ºé•¿åº¦æ„ŸçŸ¥ä¼˜åŒ–ï¼Œç»Ÿä¸€å¹³è¡¡å¤šç§æ ¼å¼çš„æ¨ç†æ€§èƒ½ä¸æ•ˆç‡ï¼Œä¸ä»…æ•´åˆè§†è§‰ç†è§£æ‰©å±•åˆ°å¤šæ¨¡æ€åº”ç”¨ï¼Œè¿˜é›†æˆå¯æ‰§è¡Œä»£ç åˆ°æ¨ç†è¿‡ç¨‹ä¸­ä»¥å‡å°‘tokenæˆæœ¬ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ARM2åœ¨æ€§èƒ½ä¸Šä¸ä½¿ç”¨GRPOè®­ç»ƒçš„ä¼ ç»Ÿæ¨ç†æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶å¹³å‡å‡å°‘è¶…è¿‡70%çš„tokenä½¿ç”¨é‡ï¼Œå¹¿æ³›åˆ†æéªŒè¯äº†æ¨¡å‹æœ‰æ•ˆæ€§å’Œè®¾è®¡åˆç†æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ARM2ä¸ºè‡ªé€‚åº”æ¨ç†æä¾›äº†é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å¤šæ¨¡æ€é›†æˆå’Œä»£ç æ‰§è¡Œæ˜¾è‘—æå‡æ•ˆç‡ï¼Œä¸ºæœªæ¥æ¨ç†æ¨¡å‹çš„é«˜æ•ˆä¼˜åŒ–æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Large Reasoning Models (LRMs) often suffer from the ``over-thinking''
problem, generating unnecessarily long reasoning on simple tasks. Some
strategies have been proposed to mitigate this issue, such as length penalties
or routing mechanisms, but they are typically heuristic and task-specific,
lacking a general framework for adaptive reasoning. In this paper, we present
ARM2, a unified model that adaptively balances reasoning performance and
efficiency across multiple formats through a reinforcement learning framework
augmented with length-aware optimization. Beyond conventional natural language
inference, ARM2 integrates vision understanding, extending its applicability to
multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling
substantial reductions in token cost while preserving task performance compared
to long CoT. Experiments demonstrate that ARM2 achieves performance on par with
traditional reasoning models trained with GRPO, while reducing token usage by
over 70% on average. We further conduct extensive analyses to validate the
effectiveness of ARM2 and the soundness of its design.</p>
<h3 id="47-single-layer-tiny-co4-outpaces-gpt-2-and-gpt-bert">[47] <a href="https://arxiv.org/abs/2510.08404">Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT</a></h3>
<p><em>Noor Ul Zain, Mohsin Raza, Ahsan Adeel</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCoâ´çš„è½»é‡çº§è¯­è¨€æ¨¡å‹ï¼Œä»…åŒ…å«å•å±‚ã€ä¸¤ä¸ªæ³¨æ„åŠ›å¤´å’Œ800ä¸‡å‚æ•°ï¼Œåœ¨è¿‘ä¼¼O(N)è®¡ç®—å¤æ‚åº¦ä¸‹è¶…è¶Šäº†BabyLMæŒ‘æˆ˜èµ›ä¸­çš„GPT-2å’ŒGPT-BERTåŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†æé«˜çš„è®­ç»ƒæ•ˆç‡å’Œæ ·æœ¬æ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ä¸»æµæ·±åº¦å­¦ä¹ èŒƒå¼ä¸»è¦ä¾èµ–æ·±å±‚ç½‘ç»œæ¶æ„å’ŒO(NÂ²)è®¡ç®—å¤æ‚åº¦ï¼Œè¿™å¯¼è‡´äº†å·¨å¤§çš„è®¡ç®—å¼€é”€å’Œè®­ç»ƒæˆæœ¬ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢æ›´é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹æ¶æ„è®¾è®¡ï¼ŒæŒ‘æˆ˜ç°æœ‰çš„ç¼©æ”¾å®šå¾‹å’Œæ·±åº¦ç½‘ç»œä¸»å¯¼çš„èŒƒå¼ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨Coâ´æœºå™¨æ¶æ„ï¼Œä»…åŒ…å«å•å±‚Transformerã€ä¸¤ä¸ªæ³¨æ„åŠ›å¤´å’Œ800ä¸‡å‚æ•°ï¼Œé€šè¿‡ä¼˜åŒ–è®¡ç®—å¤æ‚åº¦è‡³è¿‘ä¼¼O(N)ï¼Œåœ¨ä»…è®­ç»ƒä¸¤ä¸ªå‘¨æœŸçš„æƒ…å†µä¸‹ä¸è®­ç»ƒåä¸ªå‘¨æœŸçš„åŸºçº¿æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚</p>
<p><strong>Result:</strong> åœ¨1000ä¸‡tokenæ•°æ®é›†ä¸Šï¼ŒCoâ´å®ç°äº†æ•°é‡çº§æ›´é«˜çš„è®­ç»ƒæ•ˆç‡ï¼Œåœ¨BabyLMæŒ‘æˆ˜èµ›è¯„ä¼°ä¸­ï¼Œé›¶æ ·æœ¬ä»»åŠ¡ä¸Šåœ¨7ä¸ªæŒ‡æ ‡ä¸­æœ‰5ä¸ªè¶…è¶ŠGPT-2ã€4ä¸ªè¶…è¶ŠGPT-BERTï¼Œå¾®è°ƒä»»åŠ¡ä¸Šåœ¨7ä¸ªä»»åŠ¡ä¸­æœ‰6ä¸ªè¶…è¶ŠGPT-2ã€4ä¸ªè¶…è¶ŠGPT-BERTã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›ç»“æœè¡¨æ˜éœ€è¦é‡æ–°æ€è€ƒå½“å‰ä¸»æµçš„æ·±åº¦å­¦ä¹ èŒƒå¼å’Œç›¸å…³çš„ç¼©æ”¾å®šå¾‹ï¼Œè½»é‡çº§æ¶æ„åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¯ä»¥æ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡ï¼Œä¸ºé«˜æ•ˆè¯­è¨€æ¨¡å‹è®¾è®¡æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two
heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$
is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2
(124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two
epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude
greater training efficiency on 10M tokens, demonstrating highly sample
efficient pretraining. Using the BabyLM challenge evaluation pipeline across
complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning
performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out
of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out
of 7 metrics in both cases. These results suggest the need to rethink
prevailing deep learning paradigms and associated scaling laws.</p>
<h3 id="48-ares-multimodal-adaptive-reasoning-via-difficulty-aware-token-level-entropy-shaping">[48] <a href="https://arxiv.org/abs/2510.08457">ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping</a></h3>
<p><em>Shuang Chen, Yue Guo, Yimeng Ye, Shijue Huang, Wenbo Hu, Haoxi Li, Manyuan Zhang, Jiayu Chen, Song Guo, Nanyun Peng</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºARESæ¡†æ¶ï¼Œä¸€ç§è‡ªé€‚åº”æ¨ç†çš„ç»Ÿä¸€å¼€æºæ¡†æ¶ï¼Œé€šè¿‡åŸºäºä»»åŠ¡éš¾åº¦åŠ¨æ€åˆ†é…æ¢ç´¢åŠªåŠ›æ¥è§£å†³å¤šæ¨¡æ€å¤§æ¨ç†æ¨¡å‹åœ¨ç®€å•é—®é¢˜ä¸Šè¿‡åº¦æ€è€ƒè€Œåœ¨å›°éš¾é—®é¢˜ä¸Šæ¢ç´¢ä¸è¶³çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§æ¨ç†æ¨¡å‹å­˜åœ¨æ¨ç†åŠªåŠ›åˆ†é…ä¸å¹³è¡¡çš„é—®é¢˜ï¼šåœ¨ç®€å•é—®é¢˜ä¸Šäº§ç”Ÿä¸å¿…è¦çš„å†—é•¿æ¨ç†è½¨è¿¹ï¼Œè€Œåœ¨å›°éš¾é—®é¢˜ä¸Šæ¢ç´¢ä¸è¶³å¯¼è‡´é”™è¿‡è§£å†³æ–¹æ¡ˆã€‚è¿™ç§ä¸å¹³è¡¡é™åˆ¶äº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> ARESé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼šè‡ªé€‚åº”å†·å¯åŠ¨é˜¶æ®µé€šè¿‡ç­–åˆ’æ¨ç†è½¨è¿¹é•¿åº¦ä¸é—®é¢˜éš¾åº¦æˆæ¯”ä¾‹çš„å¤šæ¨¡æ€å’Œæ–‡æœ¬æ•°æ®ï¼Œä½¿æ¨¡å‹å…·å¤‡åˆæ­¥éš¾åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µå¼€å‘è‡ªé€‚åº”ç†µç­–ç•¥ä¼˜åŒ–ï¼Œä½¿ç”¨é«˜çª—å£ç†µä»¤ç‰Œä½œä¸ºæ¢ç´¢è§¦å‘å™¨æ¥å†³å®šä½•æ—¶æ¢ç´¢ï¼Œå¹¶é‡‡ç”¨å…·æœ‰åŠ¨æ€KLæ§åˆ¶çš„åˆ†å±‚ç†µå¥–åŠ±æ¥å†³å®šæ¢ç´¢ç¨‹åº¦ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒARESåœ¨å¤šæ ·åŒ–çš„æ•°å­¦ã€é€»è¾‘å’Œå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½å’Œæ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶åœ¨æ˜¾è‘—é™ä½æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹ç¼©å°äº†ä¸é¢†å…ˆå•†ä¸šç³»ç»Ÿçš„å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ¢ç´¢çš„æœ‰æ•ˆæ€§ï¼Œé«˜çª—å£ç†µä»¤ç‰Œèƒ½å¤Ÿå¯é æ•æ‰æ¨ç†å…³é”®æ—¶åˆ»ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†æ–°æ–¹å‘ï¼Œå¹¶å±•ç¤ºäº†åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—æˆæœ¬çš„å¯èƒ½æ€§ã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>Recent advances in multimodal large reasoning models (MLRMs) have
substantially improved their ability to solve complex textual and visual tasks.
However, these models tend to overthink on simple problems, producing
unnecessarily lengthy reasoning traces, while under-exploring on challenging
ones, leading to missed solutions. To address this imbalance, we propose ARES,
a unified open-source framework for adaptive reasoning that dynamically
allocates exploration effort based on task difficulty. Our approach is
motivated by two key empirical findings: (i) while single-token entropy is
noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a
sliding window) can reliably capture reasoning-critical moments; and (ii)
reducing HWE usage benefits easy problems, while increasing it is essential for
solving hard ones. Building on these insights, ARES introduces a two-stage
training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and
textual data paired with reasoning traces of length proportional to problem
difficulty, equipping the model with initial difficulty awareness. In the
second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which
uses HWE tokens as exploration triggers to decide when to explore, and a
hierarchical entropy reward with dynamic KL control to decide how much to
explore. Extensive experiments demonstrate that ARES achieves superior
performance and reasoning efficiency across diverse mathematical, logical, and
multimodal benchmarks, while closing the gap to leading commercial systems
under significantly lower inference costs.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="49-ts-agent-a-time-series-reasoning-agent-with-iterative-statistical-insight-gathering">[49] <a href="https://arxiv.org/abs/2510.07432">TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering</a></h3>
<p><em>Penghang Liu, Elizabeth Fons, Svitlana Vyetrenko, Daniel Borrajo, Vamsi Potluru, Manuela Veloso</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºTS-Agentï¼Œä¸€ç§æ—¶é—´åºåˆ—æ¨ç†æ™ºèƒ½ä½“ï¼Œé€šè¿‡å°†LLMsæ“…é•¿çš„é«˜å±‚æ¨ç†ä¸æ—¶é—´åºåˆ—åˆ†æå·¥å…·çš„ä¸“ä¸šèƒ½åŠ›ç›¸ç»“åˆï¼Œæœ‰æ•ˆè§£å†³äº†LLMsåœ¨æ—¶é—´åºåˆ—æ¨ç†ä»»åŠ¡ä¸­çš„å¹»è§‰å’ŒçŸ¥è¯†æ³„éœ²é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œé—®é¢˜è§£å†³æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶è¡¨æ˜å®ƒä»¬åœ¨æ—¶é—´åºåˆ—æ¨ç†ä»»åŠ¡ä¸­ä»å­˜åœ¨å›°éš¾ï¼Œè¾“å‡ºç»å¸¸å—åˆ°å¹»è§‰æˆ–çŸ¥è¯†æ³„éœ²çš„å½±å“ï¼Œè¿™æˆä¸ºå½“å‰ç ”ç©¶éœ€è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> TS-Agenté‡‡ç”¨åŸå­æ“ä½œç¬¦ä¸åŸå§‹æ•°å€¼åºåˆ—äº¤äº’ï¼Œå°†ç»Ÿè®¡å’Œç»“æ„ä¿¡æ¯æå–å§”æ‰˜ç»™æ—¶é—´åºåˆ—åˆ†æå·¥å…·ï¼ŒåŒæ—¶åˆ©ç”¨LLMsè¿›è¡Œè¯æ®æ”¶é›†å’Œé€æ­¥æ¨ç†åˆæˆï¼Œé€šè¿‡æ˜¾å¼è¯æ®è®°å½•ã€è‡ªæˆ‘æ‰¹è¯„æœºåˆ¶å’Œæœ€ç»ˆè´¨é‡é—¨æ§å®ç°è¿­ä»£æ¨ç†ä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒTS-Agentåœ¨ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸æœ€å…ˆè¿›LLMsç›¸å½“çš„æ€§èƒ½ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œç°æœ‰æ¨¡å‹é€šå¸¸ä¾èµ–è®°å¿†è€Œå¤±è´¥çš„æƒ…å†µä¸‹è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†LLMsçš„é«˜å±‚æ¨ç†èƒ½åŠ›ä¸é¢†åŸŸä¸“ç”¨å·¥å…·ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œé¿å…äº†å¤šæ¨¡æ€å¯¹é½è®­ç»ƒï¼Œä¿æŒäº†æ—¶é—´åºåˆ—çš„åŸå§‹å½¢å¼ï¼Œç¡®ä¿äº†å¯è§£é‡Šæ€§å’Œå¯éªŒè¯æ€§ï¼Œä¸ºæ—¶é—´åºåˆ—æ¨ç†æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) have shown strong abilities in reasoning and
problem solving, but recent studies reveal that they still struggle with time
series reasoning tasks, where outputs are often affected by hallucination or
knowledge leakage. In this work we propose TS-Agent, a time series reasoning
agent that leverages LLMs strictly for what they excel at, i.e., gathering
evidence and synthesizing it into conclusions through step-by-step reasoning,
while delegating the extraction of statistical and structural information to
time series analytical tools. Instead of mapping time series into text tokens,
images, or embeddings, our agent interacts with raw numeric sequences through
atomic operators, records outputs in an explicit evidence log, and iteratively
refines its reasoning under the guidance of a self-critic and a final quality
gate. This design avoids multi-modal alignment training, preserves the native
form of time series, ensures interpretability and verifiability, and mitigates
knowledge leakage or hallucination. Empirically, we evaluate the agent on
established benchmarks. Our experiments show that TS-Agent achieves performance
comparable to state-of-the-art LLMs on understanding benchmarks, and delivers
significant improvements on reasoning tasks, where existing models often rely
on memorization and fail in zero-shot settings.</p>
<h3 id="50-evaluation-of-llms-for-process-model-analysis-and-optimization">[50] <a href="https://arxiv.org/abs/2510.07489">Evaluation of LLMs for Process Model Analysis and Optimization</a></h3>
<p><em>Akhil Kumar, Jianliang Leon Zhao, Om Dobariya</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£BPMNä¸šåŠ¡æµç¨‹æ¨¡å‹ã€æ£€æµ‹è¯­æ³•é€»è¾‘é”™è¯¯ä»¥åŠé€šè¿‡è‡ªç„¶è¯­è¨€ç•Œé¢è¿›è¡Œæ·±åº¦æ¨ç†çš„èƒ½åŠ›ï¼Œå‘ç°æœªç»è®­ç»ƒçš„LLMåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹èƒ½å¤Ÿæœ‰æ•ˆåˆ†æä¸šåŠ¡æµç¨‹æ¨¡å‹å¹¶å‘æŒ¥åŠ©æ‰‹ä½œç”¨ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸šåŠ¡æµç¨‹å»ºæ¨¡é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯è¯„ä¼°å®ƒä»¬èƒ½å¦é€šè¿‡å¯¹è¯å¼äº¤äº’ç†è§£ä¸šåŠ¡æµç¨‹æ¨¡å‹ã€æ£€æµ‹æ¨¡å‹ä¸­çš„è¯­æ³•å’Œé€»è¾‘é”™è¯¯ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€ç•Œé¢è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œå¡«è¡¥äº†LLMåœ¨ä¸šåŠ¡æµç¨‹åˆ†æé¢†åŸŸç³»ç»Ÿè¯„ä¼°çš„ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨å®è¯åˆ†ææ–¹æ³•ï¼Œæµ‹è¯•äº†å¤šä¸ªLLMï¼ˆåŒ…æ‹¬ChatGPT o3æ¨¡å‹ï¼‰åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å¯¹BPMNæµç¨‹æ¨¡å‹å›¾åƒçš„ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢è¯„ä¼°æ¨¡å‹åœ¨è¯­æ³•ã€é€»è¾‘å’Œè¯­ä¹‰å±‚é¢çš„åˆ†æèƒ½åŠ›ï¼Œå¹¶æ·±å…¥ç ”ç©¶äº†LLMçš„"æ€ç»´è¿‡ç¨‹"å’Œæ·±åº¦æ¨ç†æœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæœªç»è®­ç»ƒçš„LLMåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹èƒ½å¤Ÿæœ‰æ•ˆç†è§£BPMNæµç¨‹æ¨¡å‹å›¾åƒï¼Œå¹¶åœ¨è¯­æ³•ã€é€»è¾‘å’Œè¯­ä¹‰å±‚é¢æ™ºèƒ½åœ°å›ç­”ç›¸å…³æŸ¥è¯¢ï¼Œä¸åŒLLMåœ¨å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§æ–¹é¢è¡¨ç°å­˜åœ¨å·®å¼‚ï¼ŒåŒæ—¶å‘ç°LLMåœ¨æµç¨‹åˆ†æå’Œä¼˜åŒ–ä¸­å±•ç°å‡ºç±»äººåŒ–çš„æ¨ç†ç‰¹æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯å®äº†LLMä½œä¸ºä¸šåŠ¡æµç¨‹è®¾è®¡è€…å’Œç”¨æˆ·åŠ©æ‰‹çš„æ½œåœ¨ä»·å€¼ï¼Œæ­ç¤ºäº†LLMåœ¨ä¸šåŠ¡æµç¨‹åˆ†æä¸­è¡¨ç°å‡ºçš„ç±»äººåŒ–ç‰¹æ€§ï¼Œä¸ºæœªæ¥å°†LLMé›†æˆåˆ°ä¸šåŠ¡æµç¨‹ç®¡ç†å·¥å…·ä¸­æä¾›äº†å®è¯åŸºç¡€ï¼Œå¹¶æŒ‡å‡ºäº†è¿›ä¸€æ­¥ç ”ç©¶LLMæ·±åº¦æ¨ç†èƒ½åŠ›çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>In this paper, we report our experience with several LLMs for their ability
to understand a process model in an interactive, conversational style, find
syntactical and logical errors in it, and reason with it in depth through a
natural language (NL) interface. Our findings show that a vanilla, untrained
LLM like ChatGPT (model o3) in a zero-shot setting is effective in
understanding BPMN process models from images and answering queries about them
intelligently at syntactic, logic, and semantic levels of depth. Further,
different LLMs vary in performance in terms of their accuracy and
effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a
valuable role as assistants for business process designers and users. We also
study the LLM's "thought process" and ability to perform deeper reasoning in
the context of process analysis and optimization. We find that the LLMs seem to
exhibit anthropomorphic properties.</p>
<h3 id="51-an-evaluation-study-of-hybrid-methods-for-multilingual-pii-detection">[51] <a href="https://arxiv.org/abs/2510.07551">An Evaluation Study of Hybrid Methods for Multilingual PII Detection</a></h3>
<p><em>Harshit Rajgarhia, Suryam Gupta, Asif Shaik, Gulipalli Praveen Kumar, Y Santhoshraj, Sanka Nithya Tanvy Nishitha, Abhishek Mukherji</em></p>
<h4 id="tldr_50">ğŸ§© TL;DR</h4>
<p>RECAPæ˜¯ä¸€ä¸ªç”¨äºä½èµ„æºè¯­è¨€ä¸ªäººèº«ä»½ä¿¡æ¯æ£€æµ‹çš„æ··åˆæ¡†æ¶ï¼Œç»“åˆç¡®å®šæ€§æ­£åˆ™è¡¨è¾¾å¼å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨13ä¸ªä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæä¾›å¯æ‰©å±•çš„éšç§åˆè§„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_50">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä½èµ„æºè¯­è¨€ä¸­ä¸ªäººèº«ä»½ä¿¡æ¯æ£€æµ‹é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯­è¨€å¤šæ ·æ€§é—®é¢˜å’Œæ ‡æ³¨æ•°æ®æœ‰é™æ€§ï¼Œè¿™äº›å› ç´ é˜»ç¢äº†éšç§åˆè§„åº”ç”¨çš„æœ‰æ•ˆéƒ¨ç½²ã€‚</p>
<p><strong>Method:</strong> RECAPé‡‡ç”¨æ··åˆæ¡†æ¶è®¾è®¡ï¼Œç»“åˆç¡®å®šæ€§æ­£åˆ™è¡¨è¾¾å¼ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ¨¡å—åŒ–æ¶æ„æ”¯æŒ300å¤šç§å®ä½“ç±»å‹è€Œæ— éœ€é‡æ–°è®­ç»ƒï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µç²¾ç‚¼æµç¨‹è¿›è¡Œæ¶ˆæ­§å’Œè¿‡æ»¤ã€‚</p>
<p><strong>Result:</strong> ä½¿ç”¨nervaluateåŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨åŠ æƒF1åˆ†æ•°ä¸Šæ¯”å¾®è°ƒNERæ¨¡å‹é«˜å‡º82%ï¼Œæ¯”é›¶æ ·æœ¬LLMé«˜å‡º17%ï¼Œåœ¨13ä¸ªä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”é€‚åº”æ€§å¼ºçš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒåˆè§„å¯¼å‘åº”ç”¨ä¸­çš„ä¸ªäººèº«ä»½ä¿¡æ¯æ£€æµ‹ï¼Œè§£å†³äº†ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹çš„å®é™…éƒ¨ç½²æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="abstract_50">ğŸ“„ Abstract</h4>
<p>The detection of Personally Identifiable Information (PII) is critical for
privacy compliance but remains challenging in low-resource languages due to
linguistic diversity and limited annotated data. We present RECAP, a hybrid
framework that combines deterministic regular expressions with context-aware
large language models (LLMs) for scalable PII detection across 13 low-resource
locales. RECAP's modular design supports over 300 entity types without
retraining, using a three-phase refinement pipeline for disambiguation and
filtering. Benchmarked with nervaluate, our system outperforms fine-tuned NER
models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers
a scalable and adaptable solution for efficient PII detection in
compliance-focused applications.</p>
<h3 id="52-test-time-matching-unlocking-compositional-reasoning-in-multimodal-models">[52] <a href="https://arxiv.org/abs/2510.07632">Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models</a></h3>
<p><em>Yinglun Zhu, Jiancheng Zhang, Fuzhi Tang</em></p>
<h4 id="tldr_51">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æ­ç¤ºäº†å½“å‰è¯„ä¼°æŒ‡æ ‡ç³»ç»Ÿæ€§ä½ä¼°äº†å‰æ²¿AIæ¨¡å‹çš„ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†ç»„åŒ¹é…è¯„åˆ†å’Œæµ‹è¯•æ—¶åŒ¹é…ç®—æ³•ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç»„åˆæ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚</p>
<hr />
<h4 id="detailed-summary_51">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å‰æ²¿AIæ¨¡å‹åœ¨ç»„åˆæ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œç°æœ‰è¯„ä¼°æŒ‡æ ‡ç³»ç»Ÿæ€§ä½ä¼°äº†æ¨¡å‹èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ¨¡å‹è¡¨ç°å¾€å¾€æ¥è¿‘æˆ–ä½äºéšæœºæ°´å¹³ï¼Œè¿™æ©ç›–äº†æ¨¡å‹çœŸå®çš„ç»„åˆæ¨ç†æ½œåŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ç»„åŒ¹é…è¯„åˆ†æ–¹æ³•ä»¥æ›´å¥½åœ°åˆ©ç”¨ç»„ç»“æ„æ­ç¤ºæ¨¡å‹éšè—èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†æµ‹è¯•æ—¶åŒ¹é…ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€å¤–éƒ¨ç›‘ç£çš„è¿­ä»£è‡ªæ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡æµ‹è¯•æ—¶è¿‡æ‹Ÿåˆç»„åŒ¹é…å°†éšè—èƒ½åŠ›è½¬åŒ–ä¸ºæ ‡å‡†è¯„ä¼°æŒ‡æ ‡ä¸‹çš„æ›´é«˜åˆ†æ•°ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•ä½¿SigLIP-B16è¶…è¶Šäº†æ‰€æœ‰å…ˆå‰ç»“æœå’ŒGPT-4ï¼ŒGPT-4.1åœ¨Winogroundä¸Šé¦–æ¬¡è¶…è¶Šä¼°è®¡çš„äººç±»è¡¨ç°ï¼Œæµ‹è¯•æ—¶åŒ¹é…è¿›ä¸€æ­¥ä½¿SigLIP-B16åœ¨MMVP-VLMä¸Šè¶…è¶ŠGPT-4.1ï¼Œåœ¨16ä¸ªæ•°æ®é›†å˜ä½“ä¸Šå®ç°ä¸€è‡´æ”¹è¿›ï¼Œåœ¨WhatsUpç­‰æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šç›¸å¯¹å¢ç›Šé«˜è¾¾85.7%ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†è¯„ä¼°æŒ‡æ ‡çš„ç³»ç»Ÿæ€§åå·®é—®é¢˜ï¼Œæå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆé‡Šæ”¾æ¨¡å‹çš„ç»„åˆæ¨ç†æ½œåŠ›ï¼Œæµ‹è¯•æ—¶åŒ¹é…ç®—æ³•å…·æœ‰å¹¿æ³›é€‚ç”¨æ€§ï¼Œå³ä½¿åœ¨æ— ç»„ç»“æ„çš„åŸºå‡†ä¸Šä¹Ÿèƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œæ¨åŠ¨äº†ç»„åˆæ¨ç†ç ”ç©¶çš„å‰æ²¿å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_51">ğŸ“„ Abstract</h4>
<p>Frontier AI models have achieved remarkable progress, yet recent studies
suggest they struggle with compositional reasoning, often performing at or
below random chance on established benchmarks. We revisit this problem and show
that widely used evaluation metrics systematically underestimate model
capability. To address this, we introduce a group matching score that better
exploits group structure and reveals substantial hidden capability in both
contrastive vision-language models (VLMs) and multimodal large language models
(MLLMs). Moreover, simply overfitting to the induced group matchings at test
time transfers this hidden capability into higher scores under standard
evaluation metrics, closing much of the reported gap. This adjustment enables
SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first
result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative,
self-improving algorithm that further bootstraps model performance without any
external supervision. TTM delivers additional, non-trivial improvements: for
example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a
new state of the art. Importantly, TTM remains broadly effective even on
benchmarks without metric-induced effects or group structures, achieving
relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16
dataset variants spanning diverse setups, our experiments demonstrate that TTM
consistently improves model performance and advances the frontier of
compositional reasoning.</p>
<h3 id="53-multimodal-safety-evaluation-in-generative-agent-social-simulations">[53] <a href="https://arxiv.org/abs/2510.07709">Multimodal Safety Evaluation in Generative Agent Social Simulations</a></h3>
<p><em>Alhim Vera, Karen Sanchez, Carlos Hinojosa, Haidar Bin Hamid, Donghoon Kim, Bernard Ghanem</em></p>
<h4 id="tldr_52">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªå¯å¤ç°çš„æ¨¡æ‹Ÿæ¡†æ¶æ¥è¯„ä¼°å¤šæ¨¡æ€ç¯å¢ƒä¸­ç”Ÿæˆå¼ä»£ç†çš„å®‰å…¨æ€§ã€è¿è´¯æ€§å’Œä¿¡ä»»åº¦ï¼Œæ­ç¤ºäº†å½“å‰ä»£ç†åœ¨æ£€æµ‹å¤šæ¨¡æ€çŸ›ç›¾æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œä¸”åœ¨å…¨å±€å®‰å…¨å¯¹é½æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_52">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ä½¿ä»£ç†èƒ½å¤Ÿåœ¨ä¸°å¯Œç¯å¢ƒä¸­è‡ªä¸»è¡ŒåŠ¨å¹¶è¿½æ±‚ç›®æ ‡ï¼Œä½†å®ƒä»¬åœ¨è·¨æ¨¡æ€å®‰å…¨æ€§ã€è¿è´¯æ€§å’Œä¿¡ä»»æ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œè¿™æ„æˆäº†å½“å‰ç ”ç©¶éœ€è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¯å¤ç°çš„æ¨¡æ‹Ÿæ¡†æ¶ï¼Œé…å¤‡åˆ†å±‚è®°å¿†ã€åŠ¨æ€è§„åˆ’ã€å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†SocialMetricsè¡Œä¸ºä¸ç»“æ„æŒ‡æ ‡å¥—ä»¶ï¼Œç”¨äºé‡åŒ–è®¡åˆ’ä¿®è®¢ã€ä¸å®‰å…¨åˆ°å®‰å…¨è½¬æ¢ä»¥åŠç½‘ç»œä¸­çš„ä¿¡æ¯æ‰©æ•£ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºä»£ç†åœ¨å¤šæ¨¡æ€çŸ›ç›¾æ£€æµ‹æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å…¨å±€å®‰å…¨å¯¹é½æ–¹é¢ä»…è¾¾åˆ°55%çš„æˆåŠŸç‡ï¼›ä¸‰ä¸ªæ¨¡å‹ï¼ˆClaudeã€GPT-4o miniã€Qwen-VLï¼‰çš„ä¸å®‰å…¨åˆ°å®‰å…¨è½¬æ¢ç‡åˆ†åˆ«ä¸º75%ã€55%å’Œ58%ï¼›åœ¨å¤šé£é™©åœºæ™¯ä¸­GPT-4o miniè¡¨ç°æœ€å·®ï¼ˆ20%ï¼‰ï¼Œè€Œåœ¨å±€éƒ¨åœºæ™¯å¦‚ç«ç¾/é«˜æ¸©ä¸­Claudeè¡¨ç°æœ€ä½³ï¼ˆ98%ï¼‰ï¼›45%çš„ä¸å®‰å…¨è¡ŒåŠ¨åœ¨è¯¯å¯¼æ€§è§†è§‰ä¿¡æ¯ä¸‹è¢«æ¥å—ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰æ¶æ„åœ¨è·¨æ¨¡æ€å®‰å…¨æ¨ç†æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯ä»£ç†å€¾å‘äºè¿‡åº¦ä¿¡ä»»å›¾åƒä¿¡æ¯ï¼Œè¿™ä¸ºç ”ç©¶å¤šæ¨¡æ€å®‰å…¨æ€§ã€è¿è´¯æ€§å’Œç¤¾ä¼šåŠ¨æ€æä¾›äº†ä¸€ä¸ªå¯å¤ç°çš„å¹³å°ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æ”¹è¿›çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_52">ğŸ“„ Abstract</h4>
<p>Can generative agents be trusted in multimodal environments? Despite advances
in large language and vision-language models that enable agents to act
autonomously and pursue goals in rich settings, their ability to reason about
safety, coherence, and trust across modalities remains limited. We introduce a
reproducible simulation framework for evaluating agents along three dimensions:
(1) safety improvement over time, including iterative plan revisions in
text-visual scenarios; (2) detection of unsafe activities across multiple
categories of social situations; and (3) social dynamics, measured as
interaction counts and acceptance ratios of social exchanges. Agents are
equipped with layered memory, dynamic planning, multimodal perception, and are
instrumented with SocialMetrics, a suite of behavioral and structural metrics
that quantifies plan revisions, unsafe-to-safe conversions, and information
diffusion across networks. Experiments show that while agents can detect direct
multimodal contradictions, they often fail to align local revisions with global
safety, reaching only a 55 percent success rate in correcting unsafe plans.
Across eight simulation runs with three models - Claude, GPT-4o mini, and
Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,
55, and 58 percent, respectively. Overall performance ranged from 20 percent in
multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such
as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted
when paired with misleading visuals, showing a strong tendency to overtrust
images. These findings expose critical limitations in current architectures and
provide a reproducible platform for studying multimodal safety, coherence, and
social dynamics.</p>
<h3 id="54-finmr-a-knowledge-intensive-multimodal-benchmark-for-advanced-financial-reasoning">[54] <a href="https://arxiv.org/abs/2510.07852">FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning</a></h3>
<p><em>Shuangyan Deng, Haizhou Peng, Jiachen Xu, Rui Mao, Ciprian Doru GiurcÄƒneanu, Jiamou Liu</em></p>
<h4 id="tldr_53">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FinMRï¼Œä¸€ä¸ªé«˜è´¨é‡ã€çŸ¥è¯†å¯†é›†çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°ä¸“ä¸šåˆ†æå¸ˆçº§åˆ«çš„é‡‘èæ¨ç†èƒ½åŠ›ï¼Œå¡«è¡¥äº†é‡‘èé¢†åŸŸä¸“ä¸šçº§MLLMè¯„ä¼°çš„ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_53">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é‡‘èç­‰ä¸“ä¸šé¢†åŸŸçš„ä¸¥æ ¼è¯„ä¼°å—åˆ°é™åˆ¶ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹å…·æœ‰ä¸“ä¸šçº§çŸ¥è¯†å¼ºåº¦ã€è¯¦ç»†æ ‡æ³¨å’Œé«˜çº§æ¨ç†å¤æ‚åº¦çš„æ•°æ®é›†ï¼Œè¿™é˜»ç¢äº†å¯¹æ¨¡å‹ä¸“ä¸šèƒ½åŠ›çš„å‡†ç¡®è¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†FinMRæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡3,200ä¸ªç²¾å¿ƒç­–åˆ’å’Œä¸“å®¶æ ‡æ³¨çš„é—®ç­”å¯¹ï¼Œæ¶µç›–15ä¸ªä¸åŒé‡‘èä¸»é¢˜ï¼Œæ•´åˆäº†å¤æ‚æ•°å­¦æ¨ç†ã€é«˜çº§é‡‘èçŸ¥è¯†å’Œå¤šç±»å‹å›¾åƒçš„ç»†å¾®è§†è§‰è§£é‡Šä»»åŠ¡ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡å¯¹é¢†å…ˆçš„é—­æºå’Œå¼€æºMLLMè¿›è¡Œå…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†è¿™äº›æ¨¡å‹ä¸ä¸“ä¸šé‡‘èåˆ†æå¸ˆä¹‹é—´çš„æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç¡®å›¾åƒåˆ†æã€å¤æ‚é‡‘èå…¬å¼å‡†ç¡®åº”ç”¨å’Œæ·±åº¦ä¸Šä¸‹æ–‡é‡‘èç†è§£ç­‰å…³é”®é¢†åŸŸã€‚</p>
<p><strong>Conclusion:</strong> FinMRé€šè¿‡æä¾›ä¸°å¯Œçš„è§†è§‰å†…å®¹å’Œè¯¦å°½çš„è§£é‡Šæ€§æ ‡æ³¨ï¼Œæˆä¸ºè¯„ä¼°å’Œæ¨è¿›å¤šæ¨¡æ€é‡‘èæ¨ç†å‘ä¸“ä¸šåˆ†æå¸ˆæ°´å¹³å‘å±•çš„é‡è¦åŸºå‡†å·¥å…·ï¼Œä¸ºæ¨¡å‹æ”¹è¿›æŒ‡æ˜äº†å…³é”®æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_53">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) have made substantial progress in
recent years. However, their rigorous evaluation within specialized domains
like finance is hindered by the absence of datasets characterized by
professional-level knowledge intensity, detailed annotations, and advanced
reasoning complexity. To address this critical gap, we introduce FinMR, a
high-quality, knowledge-intensive multimodal dataset explicitly designed to
evaluate expert-level financial reasoning capabilities at a professional
analyst's standard. FinMR comprises over 3,200 meticulously curated and
expertly annotated question-answer pairs across 15 diverse financial topics,
ensuring broad domain diversity and integrating sophisticated mathematical
reasoning, advanced financial knowledge, and nuanced visual interpretation
tasks across multiple image types. Through comprehensive benchmarking with
leading closed-source and open-source MLLMs, we highlight significant
performance disparities between these models and professional financial
analysts, uncovering key areas for model advancement, such as precise image
analysis, accurate application of complex financial formulas, and deeper
contextual financial understanding. By providing richly varied visual content
and thorough explanatory annotations, FinMR establishes itself as an essential
benchmark tool for assessing and advancing multimodal financial reasoning
toward professional analyst-level competence.</p>
<h3 id="55-augur-modeling-covariate-causal-associations-in-time-series-via-large-language-models">[55] <a href="https://arxiv.org/abs/2510.07858">Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models</a></h3>
<p><em>Zhiqing Cui, Binwu Wang, Qingxiang Liu, Yeqiang Wang, Zhengyang Zhou, Yuxuan Liang, Yang Wang</em></p>
<h4 id="tldr_54">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Auguræ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªå®Œå…¨ç”±å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ—¶åºé¢„æµ‹æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨LLMçš„å› æœæ¨ç†èƒ½åŠ›å‘ç°åå˜é‡é—´çš„æœ‰å‘å› æœå…³è”ï¼Œåœ¨æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶æä¾›å¯è§£é‡Šçš„å˜é‡äº¤äº’æ¨ç†ã€‚</p>
<hr />
<h4 id="detailed-summary_54">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ—¶åºé¢„æµ‹æ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼ŒåŒ…æ‹¬åœ¨æ¨¡å‹æ¶æ„ä¸­çš„è¾¹ç¼˜åŒ–è§’è‰²ã€ä¾èµ–ç²—ç³™çš„ç»Ÿè®¡æ–‡æœ¬æç¤ºä»¥åŠç¼ºä¹å¯è§£é‡Šæ€§ï¼Œè¿™äº›é—®é¢˜é™åˆ¶äº†LLMåœ¨æ—¶åºåˆ†æä¸­çš„æ½œåŠ›å‘æŒ¥ã€‚</p>
<p><strong>Method:</strong> Auguré‡‡ç”¨ä¸¤é˜¶æ®µå¸ˆç”Ÿæ¶æ„ï¼Œé¦–å…ˆç”±å¼ºå¤§çš„æ•™å¸ˆLLMé€šè¿‡å¯å‘å¼æœç´¢å’Œæˆå¯¹å› æœæ£€éªŒä»æ—¶åºæ•°æ®ä¸­æ¨æ–­æœ‰å‘å› æœå›¾ï¼Œç„¶åè½»é‡çº§å­¦ç”Ÿä»£ç†ç²¾ç‚¼è¯¥å›¾å¹¶åŸºäºé«˜ç½®ä¿¡åº¦å› æœå…³è”è¿›è¡Œå¾®è°ƒï¼Œè¿™äº›å…³è”è¢«ç¼–ç ä¸ºä¸°å¯Œçš„æ–‡æœ¬æç¤ºç”¨äºé¢„æµ‹ã€‚</p>
<p><strong>Result:</strong> åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAuguråœ¨25ä¸ªåŸºçº¿æ–¹æ³•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†LLMå› æœæ¨ç†åœ¨æ—¶åºé¢„æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ä»…æå‡äº†é¢„æµ‹ç²¾åº¦ï¼Œè¿˜æä¾›äº†é€æ˜ã€å¯è¿½æº¯çš„å˜é‡äº¤äº’æ¨ç†ï¼Œä¸ºå¯è§£é‡ŠAIæ—¶åºåˆ†æå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_54">ğŸ“„ Abstract</h4>
<p>Large language models (LLM) have emerged as a promising avenue for time
series forecasting, offering the potential to integrate multimodal data.
However, existing LLM-based approaches face notable limitations-such as
marginalized role in model architectures, reliance on coarse statistical text
prompts, and lack of interpretability. In this work, we introduce Augur, a
fully LLM driven time series forecasting framework that exploits LLM causal
reasoning to discover and use directed causal associations among covariates.
Augur uses a two stage teacher student architecture where a powerful teacher
LLM infers a directed causal graph from time series using heuristic search
together with pairwise causality testing. A lightweight student agent then
refines the graph and fine tune on high confidence causal associations that are
encoded as rich textual prompts to perform forecasting. This design improves
predictive accuracy while yielding transparent, traceable reasoning about
variable interactions. Extensive experiments on real-world datasets with 25
baselines demonstrate that Augur achieves competitive performance and robust
zero-shot generalization.</p>
<h3 id="56-chain-of-trigger-an-agentic-backdoor-that-paradoxically-enhances-agentic-robustness">[56] <a href="https://arxiv.org/abs/2510.08238">Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness</a></h3>
<p><em>Jiyang Qiu, Xinbei Ma, Yunqing Xu, Zhuosheng Zhang, Hai Zhao</em></p>
<h4 id="tldr_55">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºChain-of-Trigger Backdoor (CoTri)ï¼Œä¸€ç§é’ˆå¯¹LLMæ™ºèƒ½ä½“çš„å¤šæ­¥åé—¨æ”»å‡»æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé«˜æ”»å‡»æˆåŠŸç‡çš„åŒæ—¶å¢å¼ºæ™ºèƒ½ä½“çš„è‰¯æ€§ä»»åŠ¡æ€§èƒ½ï¼Œæ­ç¤ºäº†æ™ºèƒ½ä½“å®‰å…¨æ€§çš„ä¸¥é‡æ¼æ´ã€‚</p>
<hr />
<h4 id="detailed-summary_55">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨ç°å®åº”ç”¨ä¸­çš„å¿«é€Ÿéƒ¨ç½²ï¼Œå…¶å¯ä¿¡èµ–æ€§é—®é¢˜æ—¥ç›Šçªå‡ºï¼Œç°æœ‰åé—¨æ”»å‡»ä»…é™äºå•æ­¥æ§åˆ¶ï¼Œæ— æ³•åº”å¯¹æ™ºèƒ½ä½“é•¿æœŸå†³ç­–è¿‡ç¨‹ä¸­çš„å®‰å…¨å¨èƒã€‚</p>
<p><strong>Method:</strong> CoTrié‡‡ç”¨æœ‰åºè§¦å‘åºåˆ—è®¾è®¡ï¼Œåˆå§‹è§¦å‘åä»ç¯å¢ƒä¸­æå–åç»­è§¦å‘æ¡ä»¶ï¼Œå®ç°å¤šæ­¥æ“çºµä»¥å¼•å¯¼æ™ºèƒ½ä½“åç¦»åŸå®šä»»åŠ¡ï¼ŒåŒæ—¶é€šè¿‡è®­ç»ƒæ•°æ®å»ºæ¨¡ç¯å¢ƒéšæœºæ€§æ¥å¢å¼ºæ”»å‡»éšè”½æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºCoTriè¾¾åˆ°æ¥è¿‘å®Œç¾çš„æ”»å‡»æˆåŠŸç‡(ASR)å’Œæ¥è¿‘é›¶çš„è¯¯è§¦å‘ç‡(FTR)ï¼Œæ¤å…¥åé—¨åè€Œæå‡äº†æ™ºèƒ½ä½“åœ¨è‰¯æ€§ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Conclusion:</strong> CoTriå®ç°äº†å¯¹æ™ºèƒ½ä½“çš„ç¨³å®šå¤šæ­¥æ§åˆ¶ï¼ŒåŒæ—¶å¢å¼ºäº†å…¶å›ºæœ‰é²æ£’æ€§å’Œä»»åŠ¡èƒ½åŠ›ï¼Œè¿™ç§æ”»å‡»æ–¹å¼æ›´åŠ éšè”½ä¸”å¸¦æ¥æ½œåœ¨å®‰å…¨é£é™©ï¼Œçªæ˜¾äº†æ™ºèƒ½ä½“å®‰å…¨é˜²æŠ¤çš„ç´§è¿«éœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_55">ğŸ“„ Abstract</h4>
<p>The rapid deployment of large language model (LLM)-based agents in real-world
applications has raised serious concerns about their trustworthiness. In this
work, we reveal the security and robustness vulnerabilities of these agents
through backdoor attacks. Distinct from traditional backdoors limited to
single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a
multi-step backdoor attack designed for long-horizon agentic control. CoTri
relies on an ordered sequence. It starts with an initial trigger, and
subsequent ones are drawn from the environment, allowing multi-step
manipulation that diverts the agent from its intended task. Experimental
results show that CoTri achieves a near-perfect attack success rate (ASR) while
maintaining a near-zero false trigger rate (FTR). Due to training data modeling
the stochastic nature of the environment, the implantation of CoTri
paradoxically enhances the agent's performance on benign tasks and even
improves its robustness against environmental distractions. We further validate
CoTri on vision-language models (VLMs), confirming its scalability to
multimodal agents. Our work highlights that CoTri achieves stable, multi-step
control within agents, improving their inherent robustness and task
capabilities, which ultimately makes the attack more stealthy and raises
potential safty risks.</p>
<h3 id="57-looking-to-learn-token-wise-dynamic-gating-for-low-resource-vision-language-modelling">[57] <a href="https://arxiv.org/abs/2510.08470">Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling</a></h3>
<p><em>Bianca-Mihaela Ganescu, Suchir Salhan, Andrew Caines, Paula Buttery</em></p>
<h4 id="tldr_56">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§è§£ç å™¨æ¶æ„ï¼Œé€šè¿‡åŠ¨æ€é—¨æ§æœºåˆ¶å®ç°è¯­è¨€å’Œè§†è§‰ä¿¡æ¯çš„è‡ªé€‚åº”èåˆï¼Œåœ¨è®¤çŸ¥åˆç†çš„æ•°æ®é‡çº¦æŸä¸‹å®ç°äº†ç«äº‰æ€§çš„å¤šæ¨¡æ€å­¦ä¹ æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åŠ¨æ€é—¨æ§æœºåˆ¶æ— éœ€æ˜¾å¼ç›‘ç£å³å¯å‘ç°å¯è§£é‡Šçš„æ¨¡å¼ã€‚</p>
<hr />
<h4 id="detailed-summary_56">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨è®¤çŸ¥åˆç†æ•°æ®é‡çº¦æŸä¸‹è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¦‚ä½•æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯çš„é—®é¢˜ã€‚BabyLM Challenge 2025çš„è§†è§‰èµ›é“é™åˆ¶ä¿ƒä½¿æˆ‘ä»¬é‡æ–°æ€è€ƒæ¨¡å‹å¦‚ä½•é«˜æ•ˆèåˆè¯­è¨€å’Œè§†è§‰çº¿ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™è§†è§‰ä¿¡æ¯çš„æƒ…å†µä¸‹æœ€å¤§åŒ–å…¶æ•ˆç”¨ã€‚</p>
<p><strong>Method:</strong> æˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§è§£ç å™¨æ¶æ„ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼šåŸºäºtokençš„åŠ¨æ€é—¨æ§æœºåˆ¶ç”¨äºè‡ªé€‚åº”èåˆè¯­è¨€å’Œè§†è§‰çº¿ç´¢ï¼›ç‰¹å¾è°ƒåˆ¶å’Œé€šé“æ³¨æ„åŠ›æœºåˆ¶ä»¥æœ€å¤§åŒ–æœ‰é™è§†è§‰ä¿¡æ¯çš„æ•ˆç”¨ï¼›ä»¥åŠç”¨äºè§†è§‰æ¥åœ°çš„è¾…åŠ©å¯¹æ¯”å­¦ä¹ ç›®æ ‡ã€‚è¯¥æ¶æ„åœ¨ä¸¥æ ¼çš„è®¡ç®—å’Œæ•°æ®çº¦æŸä¸‹å®ç°äº†é«˜æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆBLiMPã€BLiMP Supplementã€EWoKã€Winogroundå’ŒVQAï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†å¤šæ¨¡æ€åŸºçº¿æ¨¡å‹ã€‚ç‰¹åˆ«å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŠ¨æ€é—¨æ§æœºåˆ¶æ— éœ€æ˜¾å¼ç›‘ç£å³å¯å‘ç°å¯è§£é‡Šçš„æ¨¡å¼ï¼šå¯¹å†…å®¹è¯å€¾å‘äºä½¿ç”¨è§†è§‰çº¿ç´¢ï¼Œå¯¹åŠŸèƒ½è¯åˆ™åå¥½è¯­è¨€çº¿ç´¢ã€‚</p>
<p><strong>Conclusion:</strong> å°½ç®¡è¯†åˆ«å‡ºæŒ‘æˆ˜çº¦æŸå¸¦æ¥çš„å±€é™æ€§ï¼Œå¦‚å…¨å±€å›¾åƒåµŒå…¥é€ æˆçš„ä¿¡æ¯ç“¶é¢ˆå’Œæ•°æ®é›†åˆ†å‰²å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ç¡®ç«‹äº†åŠ¨æ€é—¨æ§ä½œä¸ºé«˜æ•ˆå¤šæ¨¡æ€å­¦ä¹ çš„å¼ºå¤§å·¥å…·ã€‚è¯¥æ–¹æ³•åœ¨ä¸¥æ ¼çº¦æŸä¸‹åŒæ—¶æä¾›äº†å¯è§£é‡Šæ€§å’Œæ€§èƒ½ä¼˜åŠ¿ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å¤šæ¨¡æ€å­¦ä¹ å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_56">ğŸ“„ Abstract</h4>
<p>Training vision-language models on cognitively-plausible amounts of data
requires rethinking how models integrate multimodal information. Within the
constraints of the Vision track for the BabyLM Challenge 2025, we propose a
lightweight decoder-based architecture with (1) token-wise dynamic gating for
adaptive fusion of linguistic and visual cues, (2) feature modulation and
channel attention to maximise the utility of limited visual information and (3)
auxiliary contrastive objectives for visual grounding. Evaluation on five
benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows
competitive or superior performance to multimodal baselines. More notably, our
dynamic gate discovers interpretable patterns without explicit supervision,
favouring visual cues for content words and linguistic cues for function words.
While we identify limitations in the Challenge constraints, such as the
information bottleneck created by global image embeddings and training
instability from the dataset split, our findings establish dynamic gating as a
powerful tool for efficient multimodal learning, offering both interpretability
and performance even under severe constraints.</p>
<h3 id="58-how-to-teach-large-multimodal-models-new-skills">[58] <a href="https://arxiv.org/abs/2510.08564">How to Teach Large Multimodal Models New Skills</a></h3>
<p><em>Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem</em></p>
<h4 id="tldr_57">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸¤ç§ç®€å•çš„å¾®è°ƒæ–¹æ³•æ¥è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨é¡ºåºå­¦ä¹ æ–°æŠ€èƒ½æ—¶çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œé€šè¿‡é€‰æ‹©æ€§æ›´æ–°ç‰¹å®šç½‘ç»œå±‚æ¥ä¿æŒæ¨¡å‹åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šçš„å¼ºå­¦ä¹ èƒ½åŠ›åŒæ—¶æœ€å°åŒ–å¯¹å·²æœ‰èƒ½åŠ›çš„æŸå®³ã€‚</p>
<hr />
<h4 id="detailed-summary_57">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨é¡ºåºå¾®è°ƒå­¦ä¹ æ–°æŠ€èƒ½æ—¶å‡ºç°çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå³åœ¨å­¦ä¹ æ–°ä»»åŠ¡è¿‡ç¨‹ä¸­ä¼šæ˜¾è‘—æŸå®³æ¨¡å‹åœ¨å…ˆå‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸¤ç§ç®€å•è€Œé²æ£’çš„å¾®è°ƒæ–¹æ³•ï¼šä»…æ›´æ–°è‡ªæ³¨æ„åŠ›æŠ•å½±å±‚ï¼Œä»¥åŠä»…æ›´æ–°MLPçš„Gate&amp;UpæŠ•å½±å±‚åŒæ—¶å†»ç»“DownæŠ•å½±å±‚ï¼Œè¿™äº›é€‰æ‹©æ€§æ›´æ–°ç­–ç•¥æ—¨åœ¨é™åˆ¶è¾“å‡ºtokenåˆ†å¸ƒçš„æ¼‚ç§»ã€‚</p>
<p><strong>Result:</strong> å®éªŒåœ¨ä¸‰ä¸ªæ¨¡å‹å®¶æ—ä¸Šè¿›è¡Œï¼Œæ¶µç›–äº”ä¸ªç›®æ ‡æŠ€èƒ½å’Œå…«ä¸ªä¿ç•™åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºæ‰€æå‡ºçš„æ–¹æ³•åœ¨è·å¾—å¼ºå¤§ç›®æ ‡ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—ä¿ç•™æ¨¡å‹åœ¨ä¿ç•™ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å‘ç°è¾“å‡ºtokenåˆ†å¸ƒçš„æ¼‚ç§»æ˜¯å¯¼è‡´é—å¿˜çš„å…³é”®å› ç´ ï¼Œé€šè¿‡ç®€å•çš„é€‰æ‹©æ€§å±‚æ›´æ–°ç­–ç•¥å¯ä»¥æœ‰æ•ˆç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œä¸ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æŒç»­å­¦ä¹ æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_57">ğŸ“„ Abstract</h4>
<p>How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities? We study sequential fine-tuning on five target skills while
monitoring general ability on eight held-out benchmarks across three model
families. We observe that apparent "forgetting" on held-out tasks after narrow
fine-tuning can partly recover at later stages. We trace this behavior to a
measurable shift in the output token distribution, manifested through a simple
counting-bias probe that co-varies with forgetting. Guided by this picture, we
identify two simple, robust tuning recipes that learn strongly while limiting
drift: (i) updating only the self-attention projection layers, and (ii)
updating only the MLP Gate&amp;Up while freezing the Down projection. Across models
and tasks, these choices deliver strong target gains while largely preserving
held-out performance. Code is available at
https://github.com/jessemelpolio/LMM_CL</p>
  </article>
</body>
</html>
