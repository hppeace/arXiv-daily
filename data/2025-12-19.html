<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-19.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 2]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-collimator-assisted-high-precision-calibration-method-for-event-cameras">[1] <a href="https://arxiv.org/abs/2512.16092">Collimator-assisted high-precision calibration method for event cameras</a></h3>
<p><em>Zibin Liu, Shunkun Liang, Banglei Guan, Dongcai Tan, Yang Shang, Qifeng Yu</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé—ªçƒæ˜Ÿç‚¹å‡†ç›´ä»ªçš„äº‹ä»¶ç›¸æœºæ ‡å®šæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆçº¿æ€§æ±‚è§£ä¸éçº¿æ€§ä¼˜åŒ–ï¼Œè§£å†³äº†äº‹ä»¶ç›¸æœºåœ¨è¿œè·ç¦»æµ‹é‡åœºæ™¯ä¸‹çš„å‡ ä½•æ ‡å®šéš¾é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ ‡å®šç²¾åº¦ä¸å¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äº‹ä»¶ç›¸æœºä½œä¸ºä¸€ç§æ–°å‹ä»¿ç”Ÿè§†è§‰ä¼ æ„Ÿå™¨ï¼Œè™½ç„¶å…·æœ‰é«˜åŠ¨æ€èŒƒå›´å’Œé«˜æ—¶é—´åˆ†è¾¨ç‡ç­‰ä¼˜åŠ¿ï¼Œä½†å…¶å‡ ä½•æ ‡å®šï¼ˆåŒ…æ‹¬å†…å‚å’Œå¤–å‚ç¡®å®šï¼‰åœ¨è¿œè·ç¦»æµ‹é‡åœºæ™¯ä¸­ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åŒæ—¶æ»¡è¶³è¿œè·ç¦»å’Œé«˜ç²¾åº¦çš„åŒé‡éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨é…å¤‡é—ªçƒæ˜Ÿç‚¹å›¾æ¡ˆçš„å‡†ç›´ä»ªè¿›è¡Œäº‹ä»¶ç›¸æœºæ ‡å®šï¼Œé¦–å…ˆåŸºäºå‡†ç›´ä»ªçš„çƒé¢è¿åŠ¨æ¨¡å‹çº¿æ€§æ±‚è§£ç›¸æœºå‚æ•°ï¼Œéšåé€šè¿‡éçº¿æ€§ä¼˜åŒ–å¯¹è¿™äº›å‚æ•°è¿›è¡Œé«˜ç²¾åº¦ç»†åŒ–ï¼Œå®ç°äº†ä»ç²—åˆ°ç²¾çš„æ ‡å®šæµç¨‹ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„å…¨é¢çœŸå®ä¸–ç•Œå®éªŒéªŒè¯ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç²¾åº¦å’Œå¯é æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„äº‹ä»¶ç›¸æœºæ ‡å®šæ–¹æ³•ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ä¸€è‡´æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºäº‹ä»¶ç›¸æœºçš„è¿œè·ç¦»é«˜ç²¾åº¦æ ‡å®šæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å‡†ç›´ä»ªä¸é—ªçƒæ˜Ÿç‚¹å›¾æ¡ˆçš„ç»“åˆä»¥åŠä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†äº‹ä»¶ç›¸æœºåœ¨é•¿è·ç¦»æµ‹é‡åº”ç”¨ä¸­çš„å®ç”¨æ€§å’Œå¯é æ€§ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.</p>
<h3 id="2-towards-closing-the-domain-gap-with-event-cameras">[2] <a href="https://arxiv.org/abs/2512.16178">Towards Closing the Domain Gap with Event Cameras</a></h3>
<p><em>M. Oltan Sevinc, Liao Wu, Francisco Cruz</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºä½¿ç”¨äº‹ä»¶ç›¸æœºæ›¿ä»£ä¼ ç»Ÿç›¸æœºæ¥è§£å†³ç«¯åˆ°ç«¯é©¾é©¶ä¸­çš„æ˜¼å¤œå…‰ç…§åŸŸå·®å¼‚é—®é¢˜ï¼Œå®éªŒè¡¨æ˜äº‹ä»¶ç›¸æœºåœ¨è·¨åŸŸåœºæ™¯ä¸­èƒ½ä¿æŒæ›´ä¸€è‡´çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿç›¸æœºä½œä¸ºç«¯åˆ°ç«¯é©¾é©¶çš„ä¸»è¦ä¼ æ„Ÿå™¨ï¼Œåœ¨è®­ç»ƒæ•°æ®æ¡ä»¶ä¸éƒ¨ç½²ç¯å¢ƒä¸åŒ¹é…æ—¶æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œè¿™ä¸€é—®é¢˜è¢«ç§°ä¸ºåŸŸå·®å¼‚ã€‚æœ¬ç ”ç©¶ç‰¹åˆ«å…³æ³¨æ˜¼å¤œå…‰ç…§å·®å¼‚å¸¦æ¥çš„åŸŸå·®å¼‚é—®é¢˜ï¼Œæ—¨åœ¨å¯»æ‰¾èƒ½å¤Ÿåœ¨ä¸éœ€é¢å¤–è°ƒæ•´çš„æƒ…å†µä¸‹è·¨å…‰ç…§æ¡ä»¶ä¿æŒæ€§èƒ½çš„æ›¿ä»£ä¼ æ„Ÿå™¨æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æå‡ºä½¿ç”¨äº‹ä»¶ç›¸æœºä½œä¸ºä¼ ç»Ÿç›¸æœºçš„æ½œåœ¨æ›¿ä»£æ–¹æ¡ˆã€‚äº‹ä»¶ç›¸æœºé€šè¿‡å¼‚æ­¥åƒç´ çº§äº®åº¦å˜åŒ–æ£€æµ‹æ¥æ•æ‰åœºæ™¯åŠ¨æ€ï¼Œå…¶å·¥ä½œåŸç†ä¸ä¼ ç»Ÿå¸§å¼ç›¸æœºæœ‰æœ¬è´¨åŒºåˆ«ã€‚ç ”ç©¶æ¯”è¾ƒäº†äº‹ä»¶ç›¸æœºä¸ä¼ ç»Ÿç°åº¦å¸§ç›¸æœºåœ¨è·¨åŸŸé©¾é©¶åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œè¯„ä¼°å®ƒä»¬åœ¨åº”å¯¹å…‰ç…§æ¡ä»¶å˜åŒ–æ—¶çš„é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œäº‹ä»¶ç›¸æœºåœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹èƒ½ä¿æŒæ›´ä¸€è‡´çš„æ€§èƒ½è¡¨ç°ã€‚äº‹ä»¶ç›¸æœºå±•ç°çš„åŸŸåç§»æƒ©ç½šé€šå¸¸ä¸ç°åº¦å¸§ç›¸å½“æˆ–æ›´å°ï¼Œå¹¶ä¸”åœ¨è·¨åŸŸåœºæ™¯ä¸­æä¾›äº†æ›´ä¼˜è¶Šçš„åŸºçº¿æ€§èƒ½ã€‚è¿™è¡¨æ˜äº‹ä»¶ç›¸æœºåœ¨åº”å¯¹æ˜¼å¤œå…‰ç…§å˜åŒ–æ—¶å…·æœ‰æ›´å¼ºçš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> äº‹ä»¶ç›¸æœºä¸ºè§£å†³ç«¯åˆ°ç«¯é©¾é©¶ä¸­çš„å…‰ç…§åŸŸå·®å¼‚é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç ”ç©¶ç»“æœè¡¨æ˜äº‹ä»¶ä¼ æ„Ÿå™¨åœ¨è·¨åŸŸåœºæ™¯ä¸­å…·æœ‰å†…åœ¨ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€é¢å¤–è°ƒæ•´çš„æƒ…å†µä¸‹ç»´æŒæ€§èƒ½ä¸€è‡´æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°çš„ä¼ æ„Ÿå™¨é€‰æ‹©æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦åº”å¯¹åŠ¨æ€å…‰ç…§æ¡ä»¶çš„å®é™…éƒ¨ç½²ç¯å¢ƒä¸­ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="3-improving-low-latency-learning-performance-in-spiking-neural-networks-via-a-change-perceptive-dendrite-soma-axon-neuron">[3] <a href="https://arxiv.org/abs/2512.16259">Improving Low-Latency Learning Performance in Spiking Neural Networks via a Change-Perceptive Dendrite-Soma-Axon Neuron</a></h3>
<p><em>Zeyu Huang, Wei Meng, Quan Liu, Kun Chen, Li Ma</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè½¯å¤ä½ç­–ç•¥å’Œç”µä½å˜åŒ–æ„ŸçŸ¥æœºåˆ¶çš„æ ‘çª-èƒä½“-è½´çªï¼ˆCP-DSAï¼‰ç¥ç»å…ƒæ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥å¤šä¸ªå¯å­¦ä¹ å‚æ•°å’Œåˆ©ç”¨ç›¸é‚»æ—¶é—´æ­¥çš„å·®å¼‚ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†è„‰å†²ç¥ç»ç½‘ç»œçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è„‰å†²ç¥ç»å…ƒä¸­ç¡¬å¤ä½æœºåˆ¶å¯¹å¤šæ ·åŒ–è†œç”µä½çš„ç»Ÿä¸€å¤„ç†å¯¼è‡´ä¿¡æ¯é€€åŒ–ï¼Œè€Œè¿‡åº¦ç®€åŒ–çš„ç¥ç»å…ƒæ¨¡å‹å¿½ç•¥äº†å¤æ‚çš„ç”Ÿç‰©ç»“æ„ï¼Œé˜»ç¢äº†ç½‘ç»œå‡†ç¡®æ¨¡æ‹Ÿå®é™…ç”µä½ä¼ è¾“è¿‡ç¨‹çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†é‡‡ç”¨è½¯å¤ä½ç­–ç•¥çš„æ ‘çª-èƒä½“-è½´çªï¼ˆDSAï¼‰ç¥ç»å…ƒï¼Œå¹¶ç»“åˆç”µä½å˜åŒ–æ„ŸçŸ¥æœºåˆ¶ï¼Œå½¢æˆäº†å˜åŒ–æ„ŸçŸ¥æ ‘çª-èƒä½“-è½´çªï¼ˆCP-DSAï¼‰ç¥ç»å…ƒæ¨¡å‹ã€‚è¯¥æ¨¡å‹åŒ…å«å¤šä¸ªå¯å­¦ä¹ å‚æ•°ä»¥æ‰©å±•ç¥ç»å…ƒçš„è¡¨ç¤ºç©ºé—´ï¼Œå¹¶é€šè¿‡å˜åŒ–æ„ŸçŸ¥æœºåˆ¶åˆ©ç”¨ç›¸é‚»æ—¶é—´æ­¥çš„å·®å¼‚ä¿¡æ¯åœ¨çŸ­æ—¶é—´æ­¥å†…å®ç°ç«äº‰æ€§æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡ä¸¥æ ¼çš„ç†è®ºåˆ†æè¯æ˜äº†CP-DSAæ¨¡å‹çš„æœ‰æ•ˆæ€§åŠå…¶å†…éƒ¨å‚æ•°çš„åŠŸèƒ½ç‰¹æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯å®äº†CP-DSAæ¨¡å‹ç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> CP-DSAæ¨¡å‹é€šè¿‡è½¯å¤ä½ç­–ç•¥å’Œç”µä½å˜åŒ–æ„ŸçŸ¥æœºåˆ¶æœ‰æ•ˆè§£å†³äº†ç¡¬å¤ä½å¯¼è‡´çš„ä¿¡æ¯é€€åŒ–é—®é¢˜ï¼ŒåŒæ—¶é€šè¿‡æ›´ç²¾ç»†çš„ç”Ÿç‰©ç»“æ„æ¨¡æ‹Ÿæå‡äº†è„‰å†²ç¥ç»ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›å’Œæ€§èƒ½è¡¨ç°ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„SNNæä¾›äº†æ–°çš„ç¥ç»å…ƒæ¨¡å‹è®¾è®¡æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Spiking neurons, the fundamental information processing units of Spiking Neural Networks (SNNs), have the all-or-zero information output form that allows SNNs to be more energy-efficient compared to Artificial Neural Networks (ANNs). However, the hard reset mechanism employed in spiking neurons leads to information degradation due to its uniform handling of diverse membrane potentials. Furthermore, the utilization of overly simplified neuron models that disregard the intricate biological structures inherently impedes the network's capacity to accurately simulate the actual potential transmission process. To address these issues, we propose a dendrite-soma-axon (DSA) neuron employing the soft reset strategy, in conjunction with a potential change-based perception mechanism, culminating in the change-perceptive dendrite-soma-axon (CP-DSA) neuron. Our model contains multiple learnable parameters that expand the representation space of neurons. The change-perceptive (CP) mechanism enables our model to achieve competitive performance in short time steps utilizing the difference information of adjacent time steps. Rigorous theoretical analysis is provided to demonstrate the efficacy of the CP-DSA model and the functional characteristics of its internal parameters. Furthermore, extensive experiments conducted on various datasets substantiate the significant advantages of the CP-DSA model over state-of-the-art approaches.</p>
<h3 id="4-on-the-universal-representation-property-of-spiking-neural-networks">[4] <a href="https://arxiv.org/abs/2512.16872">On the Universal Representation Property of Spiking Neural Networks</a></h3>
<p><em>Shayan Hundrieser, Philipp Tuchel, Insung Kong, Johannes Schmidt-Hieber</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶åˆ†æäº†è„‰å†²ç¥ç»ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›ï¼Œè¯æ˜äº†å…¶ä½œä¸ºè„‰å†²åºåˆ—å¤„ç†å™¨çš„é€šç”¨è¡¨ç¤ºæ€§è´¨ï¼Œå¹¶å»ºç«‹äº†å…³äºæ‰€éœ€æƒé‡å’Œç¥ç»å…ƒæ•°é‡çš„å®šé‡ã€æ„é€ æ€§ä¸”æ¥è¿‘æœ€ä¼˜çš„ç•Œé™ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è„‰å†²ç¥ç»ç½‘ç»œè¡¨ç¤ºèƒ½åŠ›çš„ç†è®ºåŸºç¡€é—®é¢˜ï¼Œé€šè¿‡åˆ†æSNNsä½œä¸ºè„‰å†²åºåˆ—å¤„ç†å™¨çš„èƒ½åŠ›ï¼Œæ¢ç´¢å…¶åœ¨ç¥ç»å½¢æ€è®¡ç®—ç³»ç»Ÿä¸­çš„åŠŸèƒ½å’Œå±€é™æ€§ï¼Œä¸ºç†è§£åŸºäºè„‰å†²çš„ç¥ç»å½¢æ€ç³»ç»Ÿæä¾›ä¸¥è°¨çš„ç†è®ºåŸºç¡€ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å°†è„‰å†²ç¥ç»ç½‘ç»œè§†ä¸ºè„‰å†²åºåˆ—å¤„ç†å™¨ï¼Œå³èƒ½å¤Ÿå°†è¾“å…¥è„‰å†²æµè½¬æ¢ä¸ºè¾“å‡ºè„‰å†²æµçš„ç³»ç»Ÿï¼Œé€šè¿‡åˆ†æè‡ªç„¶ç±»è„‰å†²åºåˆ—å‡½æ•°çš„è¡¨ç¤ºæ€§è´¨ï¼Œé‡‡ç”¨å®šé‡å’Œæ„é€ æ€§çš„æ•°å­¦æ–¹æ³•å»ºç«‹ç†è®ºç•Œé™ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶è¯æ˜äº†è„‰å†²ç¥ç»ç½‘ç»œçš„é€šç”¨è¡¨ç¤ºæ€§è´¨ï¼Œå»ºç«‹äº†å…³äºæ‰€éœ€æƒé‡å’Œç¥ç»å…ƒæ•°é‡çš„å®šé‡ã€æ„é€ æ€§ä¸”æ¥è¿‘æœ€ä¼˜çš„ç•Œé™ï¼Œå‘ç°SNNsç‰¹åˆ«é€‚åˆè¡¨ç¤ºè¾“å…¥è¾ƒå°‘ã€æ—¶é—´å¤æ‚åº¦è¾ƒä½æˆ–æ­¤ç±»å‡½æ•°ç»„åˆçš„å‡½æ•°ï¼Œæ·±å±‚SNNsèƒ½å¤Ÿé€šè¿‡æ¨¡å—åŒ–è®¾è®¡æœ‰æ•ˆæ•è·å¤åˆå‡½æ•°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç†è§£åŸºäºè„‰å†²çš„ç¥ç»å½¢æ€ç³»ç»Ÿçš„èƒ½åŠ›å’Œå±€é™æ€§æä¾›äº†ä¸¥è°¨çš„ç†è®ºåŸºç¡€ï¼Œè¡¨æ˜SNNsåœ¨è¡¨ç¤ºç‰¹å®šç±»å‹å‡½æ•°æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤åˆå‡½æ•°çš„æ¨¡å—åŒ–è¡¨ç¤ºï¼Œä¸ºè„‰å†²åºåˆ—åˆ†ç±»ç­‰åº”ç”¨æä¾›äº†ç†è®ºæ”¯æŒï¼Œæ¨åŠ¨äº†ç¥ç»å½¢æ€è®¡ç®—çš„ç†è®ºå‘å±•ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Inspired by biology, spiking neural networks (SNNs) process information via discrete spikes over time, offering an energy-efficient alternative to the classical computing paradigm and classical artificial neural networks (ANNs). In this work, we analyze the representational power of SNNs by viewing them as sequence-to-sequence processors of spikes, i.e., systems that transform a stream of input spikes into a stream of output spikes. We establish the universal representation property for a natural class of spike train functions. Our results are fully quantitative, constructive, and near-optimal in the number of required weights and neurons. The analysis reveals that SNNs are particularly well-suited to represent functions with few inputs, low temporal complexity, or compositions of such functions. The latter is of particular interest, as it indicates that deep SNNs can efficiently capture composite functions via a modular design. As an application of our results, we discuss spike train classification. Overall, these results contribute to a rigorous foundation for understanding the capabilities and limitations of spike-based neuromorphic systems.</p>
  </article>
</body>
</html>
