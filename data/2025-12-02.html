<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-02.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-uav-mm3d-a-large-scale-synthetic-benchmark-for-3d-perception-of-unmanned-aerial-vehicles-with-multi-modal-data">[1] <a href="https://arxiv.org/abs/2511.22404">UAV-MM3D: A Large-Scale Synthetic Benchmark for 3D Perception of Unmanned Aerial Vehicles with Multi-Modal Data</a></h3>
<p><em>Longkun Zou, Jiale Wang, Rongqin Liang, Hai Wu, Ke Chen, Yaowei Wang</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UAV-MM3Dï¼Œä¸€ä¸ªç”¨äºä½ç©ºæ— äººæœºæ„ŸçŸ¥çš„é«˜ä¿çœŸå¤šæ¨¡æ€åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«40ä¸‡å¸§åŒæ­¥æ•°æ®ã€äº”ç§ä¼ æ„Ÿå™¨æ¨¡æ€å’Œä¸°å¯Œæ ‡æ³¨ï¼Œå¹¶æä¾›äº†LiDARå¼•å¯¼çš„å¤šæ¨¡æ€èåˆåŸºå‡†ç½‘ç»œLGFusionNetå’Œè½¨è¿¹é¢„æµ‹åŸºå‡†ï¼Œä¸ºæ— äººæœº3Dæ„ŸçŸ¥ç ”ç©¶å»ºç«‹äº†å…¬å…±åŸºå‡†ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ— äººæœºåœ¨å¤æ‚ä½ç©ºç¯å¢ƒä¸­çš„ç²¾ç¡®æ„ŸçŸ¥å¯¹ç©ºåŸŸå®‰å…¨å’Œæ™ºèƒ½ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œä½†çœŸå®ä¸–ç•Œæ•°æ®æ”¶é›†é¢ä¸´ç©ºåŸŸç®¡åˆ¶ã€éšç§é—®é¢˜å’Œç¯å¢ƒå˜åŒ–ç­‰å›ºæœ‰çº¦æŸï¼Œè€Œæ‰‹åŠ¨æ ‡æ³¨3Då§¿æ€å’Œè·¨æ¨¡æ€å¯¹åº”å…³ç³»è€—æ—¶ä¸”æ˜‚è´µï¼Œç°æœ‰æ•°æ®é›†éš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡ã€ç²¾ç¡®æ ‡æ³¨å’Œå¤šæ¨¡æ€æ•°æ®çš„éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†UAV-MM3Dé«˜ä¿çœŸå¤šæ¨¡æ€åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«40ä¸‡å¸§åŒæ­¥æ•°æ®ï¼Œè¦†ç›–åŸå¸‚ã€éƒŠåŒºã€æ£®æ—ã€æµ·å²¸ç­‰å¤šæ ·åœºæ™¯å’Œä¸åŒå¤©æ°”æ¡ä»¶ï¼ŒåŒ…å«å¾®å‹ã€å°å‹ã€ä¸­å‹å¤šç§æ— äººæœºæ¨¡å‹ï¼Œæä¾›RGBã€çº¢å¤–ã€LiDARã€é›·è¾¾å’ŒåŠ¨æ€è§†è§‰ä¼ æ„Ÿå™¨äº”ç§æ¨¡æ€æ•°æ®ï¼Œæ¯å¸§åŒ…å«2D/3Dè¾¹ç•Œæ¡†ã€6è‡ªç”±åº¦å§¿æ€å’Œå®ä¾‹çº§æ ‡æ³¨ã€‚åŒæ—¶æå‡ºäº†LiDARå¼•å¯¼çš„å¤šæ¨¡æ€èåˆåŸºå‡†ç½‘ç»œLGFusionNetå’Œä¸“ç”¨çš„æ— äººæœºè½¨è¿¹é¢„æµ‹åŸºå‡†ã€‚</p>
<p><strong>Result:</strong> UAV-MM3Dæ•°æ®é›†æ”¯æŒæ— äººæœº3Dæ£€æµ‹ã€å§¿æ€ä¼°è®¡ã€ç›®æ ‡è·Ÿè¸ªå’ŒçŸ­æœŸè½¨è¿¹é¢„æµ‹ç­‰æ ¸å¿ƒä»»åŠ¡ï¼Œé€šè¿‡å¯æ§ä»¿çœŸç¯å¢ƒå®ç°äº†å…¨é¢åœºæ™¯è¦†ç›–å’Œä¸°å¯Œæ ‡æ³¨ï¼Œä¸ºæ— äººæœº3Dæ„ŸçŸ¥ç ”ç©¶æä¾›äº†é¦–ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€å…¬å…±åŸºå‡†ï¼ŒLGFusionNetå’Œè½¨è¿¹é¢„æµ‹åŸºå‡†ä¸ºç›¸å…³ç ”ç©¶æä¾›äº†å¯æ¯”è¾ƒçš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡åˆæˆæ•°æ®æ–¹æ³•æœ‰æ•ˆè§£å†³äº†çœŸå®ä¸–ç•Œæ— äººæœºæ•°æ®æ”¶é›†çš„å±€é™æ€§ï¼Œä¸ºä½ç©ºæ— äººæœºæ„ŸçŸ¥å’Œè¿åŠ¨ç†è§£æä¾›äº†é«˜è´¨é‡çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œå…¶å¯æ§ä»¿çœŸç¯å¢ƒå’Œå…¨é¢åœºæ™¯è¦†ç›–ä¸ºæ— äººæœº3Dæ„ŸçŸ¥ç®—æ³•çš„å¼€å‘å’Œè¯„ä¼°æä¾›äº†é‡è¦å¹³å°ï¼Œå°†æ¨åŠ¨æ— äººæœºæ„ŸçŸ¥æŠ€æœ¯åœ¨å®é™…å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Accurate perception of UAVs in complex low-altitude environments is critical for airspace security and related intelligent systems. Developing reliable solutions requires large-scale, accurately annotated, and multimodal data. However, real-world UAV data collection faces inherent constraints due to airspace regulations, privacy concerns, and environmental variability, while manual annotation of 3D poses and cross-modal correspondences is time-consuming and costly. To overcome these challenges, we introduce UAV-MM3D, a high-fidelity multimodal synthetic dataset for low-altitude UAV perception and motion understanding. It comprises 400K synchronized frames across diverse scenes (urban areas, suburbs, forests, coastal regions) and weather conditions (clear, cloudy, rainy, foggy), featuring multiple UAV models (micro, small, medium-sized) and five modalities - RGB, IR, LiDAR, Radar, and DVS (Dynamic Vision Sensor). Each frame provides 2D/3D bounding boxes, 6-DoF poses, and instance-level annotations, enabling core tasks related to UAVs such as 3D detection, pose estimation, target tracking, and short-term trajectory forecasting. We further propose LGFusionNet, a LiDAR-guided multimodal fusion baseline, and a dedicated UAV trajectory prediction baseline to facilitate benchmarking. With its controllable simulation environment, comprehensive scenario coverage, and rich annotations, UAV3D offers a public benchmark for advancing 3D perception of UAVs.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="2-privacy-preserving-fall-detection-at-the-edge-using-sony-imx636-event-based-vision-sensor-and-intel-loihi-2-neuromorphic-processor">[2] <a href="https://arxiv.org/abs/2511.22554">Privacy-preserving fall detection at the edge using Sony IMX636 event-based vision sensor and Intel Loihi 2 neuromorphic processor</a></h3>
<p><em>Lyes Khacef, Philipp Weidel, Susumu Hogyoku, Harry Liu, Claire Alexandra BrÃ¤uer, Shunsuke Koshino, Takeshi Oyakawa, Vincent Parret, Yoshitaka Miyatani, Mike Davies, Mathis Richter</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè€å¹´äººæŠ¤ç†çš„ç¥ç»å½¢æ€è·Œå€’æ£€æµ‹ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå°†ç´¢å°¼IMX636äº‹ä»¶è§†è§‰ä¼ æ„Ÿå™¨ä¸è‹±ç‰¹å°”Loihi 2ç¥ç»å½¢æ€å¤„ç†å™¨é›†æˆï¼Œé€šè¿‡ä¸“ç”¨FPGAæ¥å£å®ç°è¾¹ç¼˜è®¡ç®—ï¼Œåœ¨ä¸¥æ ¼ç¡¬ä»¶çº¦æŸä¸‹å®ç°äº†é«˜æ•ˆã€å®æ—¶çš„éšç§ä¿æŠ¤æ£€æµ‹ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºäºéä¾µå…¥å¼è§†è§‰çš„è€å¹´äººè·Œå€’æ£€æµ‹ç³»ç»Ÿé¢ä¸´éšç§ä¿æŠ¤ã€å®æ—¶æ€§è¦æ±‚å’Œè¾¹ç¼˜è®¾å¤‡ç¡¬ä»¶èµ„æºæœ‰é™çš„æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨ä¼ æ„Ÿå™¨è¾¹ç¼˜å®ç°é²æ£’ã€å®æ—¶ä¸”æŒç»­è¿è¡Œçš„æ„ŸçŸ¥ï¼ŒåŒæ—¶æ»¡è¶³ä¸¥æ ¼çš„éšç§è¦æ±‚ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨ç´¢å°¼IMX636äº‹ä»¶è§†è§‰ä¼ æ„Ÿå™¨ä¸è‹±ç‰¹å°”Loihi 2ç¥ç»å½¢æ€å¤„ç†å™¨é›†æˆæ¶æ„ï¼Œé€šè¿‡ä¸“ç”¨FPGAæ¥å£è¿æ¥ï¼Œåˆ©ç”¨äº‹ä»¶æ•°æ®çš„ç¨€ç–æ€§å’Œè¿‘å†…å­˜å¼‚æ­¥å¤„ç†ä¼˜åŠ¿ï¼Œæ¢ç´¢äº†ç¨€ç–ç¥ç»ç½‘ç»œè®¾è®¡ç©ºé—´ï¼ŒåŒ…æ‹¬åŸºäºLIFçš„å·ç§¯è„‰å†²ç¥ç»ç½‘ç»œå’ŒMCUNetç‰¹å¾æå–å™¨ä¸S4DçŠ¶æ€ç©ºé—´æ¨¡å‹çš„ç»„åˆæ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> åŸºäºLIFçš„å·ç§¯SNNé‡‡ç”¨åˆ†çº§è„‰å†²å®ç°äº†æœ€é«˜è®¡ç®—æ•ˆç‡ï¼Œåœ¨F1åˆ†æ•°58%æ—¶è¾¾åˆ°55å€çªè§¦æ“ä½œç¨€ç–åº¦ï¼›åˆ†çº§è„‰å†²ç›¸æ¯”äºŒè¿›åˆ¶è„‰å†²åœ¨F1åˆ†æ•°ä¸Šæå‡6%ä¸”æ“ä½œå‡å°‘5å€ï¼›MCUNetç‰¹å¾æå–å™¨ä¸S4Dæ¨¡å‹ç»„åˆå®ç°äº†84%çš„æœ€é«˜F1åˆ†æ•°ï¼Œçªè§¦æ“ä½œç¨€ç–åº¦ä¸º2å€ï¼ŒLoihi 2ä¸Šçš„æ€»åŠŸè€—ä¸º90mWã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç¥ç»å½¢æ€ä¼ æ„Ÿä¸å¤„ç†é›†æˆçš„å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«é€‚ç”¨äºå»¶è¿Ÿã€èƒ½è€—å’Œéšç§è¦æ±‚ä¸¥æ ¼çš„è¾¹ç¼˜AIåº”ç”¨ï¼Œæ™ºèƒ½å®‰é˜²æ‘„åƒå¤´æ¦‚å¿µéªŒè¯å±•ç¤ºäº†ç¨€ç–ç¥ç»ç½‘ç»œåœ¨èµ„æºå—é™è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Fall detection for elderly care using non-invasive vision-based systems remains an important yet unsolved problem. Driven by strict privacy requirements, inference must run at the edge of the vision sensor, demanding robust, real-time, and always-on perception under tight hardware constraints. To address these challenges, we propose a neuromorphic fall detection system that integrates the Sony IMX636 event-based vision sensor with the Intel Loihi 2 neuromorphic processor via a dedicated FPGA-based interface, leveraging the sparsity of event data together with near-memory asynchronous processing. Using a newly recorded dataset under diverse environmental conditions, we explore the design space of sparse neural networks deployable on a single Loihi 2 chip and analyze the tradeoffs between detection F1 score and computational cost. Notably, on the Pareto front, our LIF-based convolutional SNN with graded spikes achieves the highest computational efficiency, reaching a 55x synaptic operations sparsity for an F1 score of 58%. The LIF with graded spikes shows a gain of 6% in F1 score with 5x less operations compared to binary spikes. Furthermore, our MCUNet feature extractor with patched inference, combined with the S4D state space model, achieves the highest F1 score of 84% with a synaptic operations sparsity of 2x and a total power consumption of 90 mW on Loihi 2. Overall, our smart security camera proof-of-concept highlights the potential of integrating neuromorphic sensing and processing for edge AI applications where latency, energy consumption, and privacy are critical.</p>
  </article>
</body>
</html>
