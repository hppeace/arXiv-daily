<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-30.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 4]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-signal-sgn-topology-enhanced-time-frequency-spiking-graph-network-for-skeleton-based-action-recognition">[1] <a href="https://arxiv.org/abs/2512.22214">Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition</a></h3>
<p><em>Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSignal-SGN++ï¼Œä¸€ç§æ‹“æ‰‘æ„ŸçŸ¥çš„è„‰å†²å›¾æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆç»“æ„è‡ªé€‚åº”æ€§ä¸æ—¶é¢‘è„‰å†²åŠ¨åŠ›å­¦ï¼Œåœ¨ä¿æŒé«˜èƒ½æ•ˆçš„åŒæ—¶æå‡åŸºäºéª¨éª¼çš„åŠ¨ä½œè¯†åˆ«æ€§èƒ½ï¼Œå®ç°äº†ä¼˜äºç°æœ‰SNNæ–¹æ³•çš„ç²¾åº¦-æ•ˆç‡æƒè¡¡ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å›¾å·ç§¯ç½‘ç»œåœ¨éª¨éª¼åŠ¨ä½œè¯†åˆ«ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¯†é›†æµ®ç‚¹è®¡ç®—èƒ½è€—é«˜ï¼›è€Œè„‰å†²ç¥ç»ç½‘ç»œè™½å…·èƒ½æ•ˆä¼˜åŠ¿ï¼Œå´éš¾ä»¥æœ‰æ•ˆæ•æ‰äººä½“è¿åŠ¨çš„è€¦åˆæ—¶é¢‘ä¸æ‹“æ‰‘ä¾èµ–å…³ç³»ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½å…¼é¡¾æ‹“æ‰‘æ„ŸçŸ¥ä¸èƒ½æ•ˆçš„æ–°æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºSignal-SGN++æ¡†æ¶ï¼Œå…¶ä¸»å¹²ç½‘ç»œç”±1Dè„‰å†²å›¾å·ç§¯å’Œé¢‘ç‡è„‰å†²å·ç§¯ç»„æˆï¼Œç”¨äºè”åˆæ—¶ç©ºä¸é¢‘è°±ç‰¹å¾æå–ï¼›åµŒå…¥æ‹“æ‰‘è½¬ç§»è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥è‡ªé€‚åº”åœ°åœ¨å­¦ä¹ åˆ°çš„éª¨éª¼æ‹“æ‰‘é—´è·¯ç”±æ³¨æ„åŠ›ï¼›å¹¶å¼•å…¥è¾…åŠ©çš„å¤šå°ºåº¦å°æ³¢å˜æ¢èåˆåˆ†æ”¯ï¼Œé€šè¿‡æ‹“æ‰‘æ„ŸçŸ¥æ—¶é¢‘èåˆå•å…ƒæ•´åˆç»“æ„å…ˆéªŒä»¥ä¿æŒæ‹“æ‰‘ä¸€è‡´çš„é¢‘è°±èåˆã€‚</p>
<p><strong>Result:</strong> åœ¨å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒSignal-SGN++å®ç°äº†ä¼˜è¶Šçš„ç²¾åº¦-æ•ˆç‡æƒè¡¡ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºäºSNNçš„æ–¹æ³•ï¼Œå¹¶åœ¨æ˜¾è‘—é™ä½èƒ½è€—çš„åŒæ—¶å–å¾—äº†ä¸æœ€å…ˆè¿›GCNsç›¸ç«äº‰çš„ç»“æœã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†æ‹“æ‰‘è‡ªé€‚åº”æœºåˆ¶ä¸è„‰å†²ç¥ç»ç½‘ç»œçš„æ—¶é¢‘åŠ¨åŠ›å­¦ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºé«˜èƒ½æ•ˆçš„éª¨éª¼åŠ¨ä½œè¯†åˆ«æä¾›äº†æ–°èŒƒå¼ï¼Œå±•ç¤ºäº†åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æå‡å›¾çº§æ•æ„Ÿåº¦çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥èŠ‚èƒ½å‹å›¾ç¥ç»ç½‘ç»œè®¾è®¡æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Graph Convolutional Networks (GCNs) demonstrate strong capability in modeling skeletal topology for action recognition, yet their dense floating-point computations incur high energy costs. Spiking Neural Networks (SNNs), characterized by event-driven and sparse activation, offer energy efficiency but remain limited in capturing coupled temporal-frequency and topological dependencies of human motion. To bridge this gap, this article proposes Signal-SGN++, a topology-aware spiking graph framework that integrates structural adaptivity with time-frequency spiking dynamics. The network employs a backbone composed of 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. Within this backbone, a Topology-Shift Self-Attention (TSSA) mechanism is embedded to adaptively route attention across learned skeletal topologies, enhancing graph-level sensitivity without increasing computational complexity. Moreover, an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch decomposes spiking features into multi-resolution temporal-frequency representations, wherein a Topology-Aware Time-Frequency Fusion (TATF) unit incorporates structural priors to preserve topology-consistent spectral fusion. Comprehensive experiments on large-scale benchmarks validate that Signal-SGN++ achieves superior accuracy-efficiency trade-offs, outperforming existing SNN-based methods and achieving competitive results against state-of-the-art GCNs under substantially reduced energy consumption.</p>
<h3 id="2-lecalib-line-based-event-camera-calibration">[2] <a href="https://arxiv.org/abs/2512.22441">LECalib: Line-Based Event Camera Calibration</a></h3>
<p><em>Zibin Liu, Banglei Guana, Yang Shanga, Zhenbao Yu, Yifei Bian, Qifeng Yu</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºçº¿ç‰¹å¾çš„äº‹ä»¶ç›¸æœºæ ‡å®šæ¡†æ¶ï¼Œåˆ©ç”¨äººé€ ç¯å¢ƒä¸­å¸¸è§ç‰©ä½“çš„å‡ ä½•çº¿æ¡ï¼ˆå¦‚é—¨çª—ã€ç®±å­ç­‰ï¼‰è¿›è¡Œæ ‡å®šï¼Œæ— éœ€æ‰‹åŠ¨æ”¾ç½®æ ‡å®šæ¿ï¼Œé€‚ç”¨äºå•ç›®å’Œç«‹ä½“äº‹ä»¶ç›¸æœºã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰äº‹ä»¶ç›¸æœºæ ‡å®šæ–¹æ³•é€šå¸¸éœ€è¦é—ªçƒå›¾æ¡ˆã€é‡å»ºå¼ºåº¦å›¾åƒæˆ–ä»äº‹ä»¶ä¸­æå–ç‰¹å¾ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸è€—æ—¶ä¸”éœ€è¦æ‰‹åŠ¨æ”¾ç½®æ ‡å®šç‰©ä½“ï¼Œæ— æ³•æ»¡è¶³å¿«é€Ÿå˜åŒ–åœºæ™¯çš„éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºåŸºäºçº¿çš„äº‹ä»¶ç›¸æœºæ ‡å®šæ¡†æ¶ï¼Œç›´æ¥ä»äº‹ä»¶æµä¸­æ£€æµ‹çº¿æ¡ï¼Œåˆ©ç”¨äº‹ä»¶-çº¿æ ‡å®šæ¨¡å‹ç”Ÿæˆç›¸æœºå‚æ•°çš„åˆå§‹ä¼°è®¡ï¼Œé€‚ç”¨äºå¹³é¢å’Œéå¹³é¢çº¿æ¡ï¼Œç„¶åé‡‡ç”¨éçº¿æ€§ä¼˜åŒ–æ¥ç»†åŒ–ç›¸æœºå‚æ•°ã€‚</p>
<p><strong>Result:</strong> ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§å’Œå‡†ç¡®æ€§ï¼Œåœ¨å•ç›®å’Œç«‹ä½“äº‹ä»¶ç›¸æœºä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæºä»£ç å·²åœ¨GitHubä¸Šå¼€æºã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨ç¯å¢ƒä¸­è‡ªç„¶å­˜åœ¨çš„å‡ ä½•çº¿æ¡ï¼Œå®ç°äº†æ— éœ€ä¸“é—¨æ ‡å®šç‰©ä½“çš„å¿«é€Ÿäº‹ä»¶ç›¸æœºæ ‡å®šï¼Œä¸ºäººé€ ç¯å¢ƒä¸­çš„äº‹ä»¶è§†è§‰åº”ç”¨æä¾›äº†æ›´å®ç”¨çš„æ ‡å®šè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.</p>
<h3 id="3-event-based-high-temporal-resolution-measurement-of-shock-wave-motion-field">[3] <a href="https://arxiv.org/abs/2512.22474">Event-based high temporal resolution measurement of shock wave motion field</a></h3>
<p><em>Taihang Lei, Banglei Guan, Minzu Liang, Pengju Sun, Jing Tao, Yang Shang, Qifeng Yu</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤šäº‹ä»¶ç›¸æœºçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºé«˜æ—¶ç©ºåˆ†è¾¨ç‡æµ‹é‡å†²å‡»æ³¢è¿åŠ¨å‚æ•°ï¼Œé€šè¿‡å»ºç«‹æåæ ‡ç³»ç¼–ç äº‹ä»¶ã€è‡ªé€‚åº”ROIæå–å’Œè¿­ä»£æ–œç‡åˆ†æç­‰æ–¹æ³•ï¼Œå®ç°äº†å†²å‡»æ³¢ä¸å¯¹ç§°æ€§ä¼°è®¡å’Œè¿åŠ¨åœºé‡å»ºã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å†²å‡»æ³¢åœ¨åŠŸç‡åœºæµ‹è¯•å’ŒæŸä¼¤è¯„ä¼°ç­‰åº”ç”¨ä¸­éœ€è¦é«˜æ—¶ç©ºåˆ†è¾¨ç‡çš„ç²¾ç¡®æµ‹é‡ï¼Œä½†å†²å‡»æ³¢çš„å¿«é€Ÿä¸å‡åŒ€ä¼ æ’­å’Œä¸ç¨³å®šæµ‹è¯•æ¡ä»¶å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ»¡è¶³è¿™äº›è¦æ±‚ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶åˆ©ç”¨å¤šäº‹ä»¶ç›¸æœºçš„é«˜é€Ÿåº¦å’Œé«˜åŠ¨æ€èŒƒå›´èƒ½åŠ›ï¼Œé¦–å…ˆå»ºç«‹æåæ ‡ç³»ç¼–ç äº‹ä»¶ä»¥æ­ç¤ºå†²å‡»æ³¢ä¼ æ’­æ¨¡å¼ï¼Œé€šè¿‡äº‹ä»¶åç§»è®¡ç®—å®ç°è‡ªé€‚åº”æ„Ÿå…´è¶£åŒºåŸŸæå–ï¼›ç„¶åä½¿ç”¨è¿­ä»£æ–œç‡åˆ†ææå–å†²å‡»æ³¢å‰æ²¿äº‹ä»¶ï¼Œåˆ©ç”¨é€Ÿåº¦å˜åŒ–çš„è¿ç»­æ€§ï¼›æœ€åæ ¹æ®åŸºäºäº‹ä»¶çš„å…‰å­¦æˆåƒæ¨¡å‹æ¨å¯¼äº‹ä»¶å‡ ä½•æ¨¡å‹å’Œå†²å‡»æ³¢è¿åŠ¨å‚æ•°ï¼Œç»“åˆä¸‰ç»´é‡å»ºæ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•å®ç°äº†å¤šè§’åº¦å†²å‡»æ³¢æµ‹é‡ã€è¿åŠ¨åœºé‡å»ºå’Œçˆ†ç‚¸å½“é‡åæ¼”ï¼Œé€Ÿåº¦æµ‹é‡ç»“æœä¸å‹åŠ›ä¼ æ„Ÿå™¨å’Œç»éªŒå…¬å¼ç›¸æ¯”ï¼Œæœ€å¤§è¯¯å·®ä¸º5.20%ï¼Œæœ€å°è¯¯å·®ä¸º0.06%ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿä»¥é«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡å®ç°å†²å‡»æ³¢è¿åŠ¨åœºçš„é«˜ç²¾åº¦æµ‹é‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä»£è¡¨äº†å†²å‡»æ³¢æµ‹é‡æŠ€æœ¯çš„æ˜¾è‘—è¿›æ­¥ï¼Œé€šè¿‡äº‹ä»¶ç›¸æœºæ¡†æ¶å®ç°äº†é«˜æ—¶ç©ºåˆ†è¾¨ç‡çš„ç²¾ç¡®æµ‹é‡ï¼Œä¸ºåŠŸç‡åœºæµ‹è¯•å’ŒæŸä¼¤è¯„ä¼°ç­‰åº”ç”¨æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†äº‹ä»¶è§†è§‰åœ¨åŠ¨æ€ç‰©ç†ç°è±¡æµ‹é‡ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.</p>
<h3 id="4-posestreamer-a-multi-modal-framework-for-6dof-pose-estimation-of-unseen-moving-objects">[4] <a href="https://arxiv.org/abs/2512.22979">PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects</a></h3>
<p><em>Huiming Yang, Linglin Liao, Fei Ding, Sibo Wang, Zijian Zeng</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PoseStreamerï¼Œä¸€ä¸ªé’ˆå¯¹é«˜é€Ÿè¿åŠ¨åœºæ™¯è®¾è®¡çš„é²æ£’å¤šæ¨¡æ€6DoFå§¿æ€ä¼°è®¡æ¡†æ¶ï¼Œé€šè¿‡é›†æˆè‡ªé€‚åº”å§¿æ€è®°å¿†é˜Ÿåˆ—ã€ç‰©ä½“ä¸­å¿ƒ2Dè·Ÿè¸ªå™¨å’Œå°„çº¿å§¿æ€æ»¤æ³¢å™¨ï¼Œæ˜¾è‘—æå‡äº†é«˜é€Ÿä½å…‰æ¡ä»¶ä¸‹çš„å§¿æ€ä¼°è®¡æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨é«˜é€Ÿè¿åŠ¨å’Œä½å…‰ç…§åœºæ™¯ä¸­ï¼Œä¼ ç»ŸRGBç›¸æœºå› è¿åŠ¨æ¨¡ç³Šå¯¼è‡´6DoFå§¿æ€ä¼°è®¡æ€§èƒ½ä¸‹é™ï¼Œè€Œäº‹ä»¶ç›¸æœºè™½ç„¶å…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨é«˜é€Ÿç‰©ä½“è¿åŠ¨åœºæ™¯ä¸­ä»è¡¨ç°ä¸ä½³ï¼Œéœ€è¦ä¸“é—¨é’ˆå¯¹é«˜é€Ÿè¿åŠ¨åœºæ™¯è®¾è®¡æ›´é²æ£’çš„å§¿æ€ä¼°è®¡æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„PoseStreameræ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè‡ªé€‚åº”å§¿æ€è®°å¿†é˜Ÿåˆ—åˆ©ç”¨å†å²æ–¹å‘çº¿ç´¢ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ï¼Œç‰©ä½“ä¸­å¿ƒ2Dè·Ÿè¸ªå™¨æä¾›å¼º2Då…ˆéªŒä»¥æå‡3Dä¸­å¿ƒå¬å›ç‡ï¼Œä»¥åŠå°„çº¿å§¿æ€æ»¤æ³¢å™¨æ²¿ç›¸æœºå°„çº¿è¿›è¡Œå‡ ä½•ç»†åŒ–ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†MoCapCube6Då¤šæ¨¡æ€æ•°æ®é›†ç”¨äºå¿«é€Ÿè¿åŠ¨ä¸‹çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPoseStreameråœ¨é«˜é€Ÿè¿åŠ¨åœºæ™¯ä¸­å®ç°äº†å“è¶Šçš„ç²¾åº¦ï¼ŒåŒæ—¶ä½œä¸ºæ— æ¨¡æ¿æ¡†æ¶å¯¹æœªè§è¿‡çš„è¿åŠ¨ç‰©ä½“å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åœ¨ä¸“é—¨æ„å»ºçš„MoCapCube6Dæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒéªŒè¯äº†å…¶åœ¨æŒ‘æˆ˜æ€§é«˜é€Ÿæ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€èåˆå’Œæ—¶é—´ä¸€è‡´æ€§æœºåˆ¶åœ¨é«˜é€Ÿå§¿æ€ä¼°è®¡ä¸­çš„é‡è¦æ€§ï¼Œä¸ºå®æ—¶æœºå™¨äººæ“ä½œå’Œå¢å¼ºç°å®åº”ç”¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚PoseStreamerçš„æ— æ¨¡æ¿ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§ç‰©ä½“ï¼Œä¸ºåŠ¨æ€ç¯å¢ƒä¸­çš„é²æ£’å§¿æ€ä¼°è®¡å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.</p>
  </article>
</body>
</html>
