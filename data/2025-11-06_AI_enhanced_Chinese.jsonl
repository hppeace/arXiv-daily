{"id": "2511.02953", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02953", "abs": "https://arxiv.org/abs/2511.02953", "authors": ["Sadiq Layi Macaulay", "Nimet Kaygusuz", "Simon Hadfield"], "title": "EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation", "comment": null, "summary": "Event cameras, with their high dynamic range (HDR) and low latency, offer a\npromising alternative for robust depth estimation in challenging environments.\nHowever, many event-based depth estimation approaches are constrained by\nsmall-scale annotated datasets, limiting their generalizability to real-world\nscenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event\ncamera dataset curated from publicly available YouTube footage, which contains\nmore than 13B events across various environmental conditions and motions,\nincluding seasonal hiking, flying, scenic driving, and underwater exploration.\nEvtSlowTV is an order of magnitude larger than existing event datasets,\nproviding an unconstrained, naturalistic setting for event-based depth\nlearning. This work shows the suitability of EvtSlowTV for a self-supervised\nlearning framework to capitalise on the HDR potential of raw event streams. We\nfurther demonstrate that training with EvtSlowTV enhances the model's ability\nto generalise to complex scenes and motions. Our approach removes the need for\nframe-based annotations and preserves the asynchronous nature of event data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EvtSlowTV\uff0c\u4e00\u4e2a\u4eceYouTube\u89c6\u9891\u6784\u5efa\u7684\u5927\u89c4\u6a21\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7130\u4ebf\u4e2a\u4e8b\u4ef6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7279\u6027\uff0c\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u5177\u6709\u9c81\u68d2\u6df1\u5ea6\u4f30\u8ba1\u7684\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86EvtSlowTV\u5927\u89c4\u6a21\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4ee5\u5229\u7528\u539f\u59cb\u4e8b\u4ef6\u6d41\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u6f5c\u529b\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u57fa\u4e8e\u5e27\u7684\u6807\u6ce8\u5e76\u4fdd\u6301\u4e86\u4e8b\u4ef6\u6570\u636e\u7684\u5f02\u6b65\u7279\u6027\u3002", "result": "EvtSlowTV\u6570\u636e\u96c6\u6bd4\u73b0\u6709\u4e8b\u4ef6\u6570\u636e\u96c6\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u5305\u542b\u5404\u79cd\u73af\u5883\u6761\u4ef6\u548c\u8fd0\u52a8\u573a\u666f\uff0c\u8bad\u7ec3\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u548c\u8fd0\u52a8\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5927\u89c4\u6a21\u65e0\u7ea6\u675f\u81ea\u7136\u573a\u666f\u4e8b\u4ef6\u6570\u636e\u5bf9\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u7279\u6027\uff0c\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2511.03665", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03665", "abs": "https://arxiv.org/abs/2511.03665", "authors": ["Mehdi Sefidgar Dilmaghani", "Francis Fowley", "Peter Corcoran"], "title": "A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential", "comment": null, "summary": "This paper presents a lightweight three-dimensional convolutional neural\nnetwork (3DCNN) for human activity recognition (HAR) using event-based vision\ndata. Privacy preservation is a key challenge in human monitoring systems, as\nconventional frame-based cameras capture identifiable personal information. In\ncontrast, event cameras record only changes in pixel intensity, providing an\ninherently privacy-preserving sensing modality. The proposed network\neffectively models both spatial and temporal dynamics while maintaining a\ncompact design suitable for edge deployment. To address class imbalance and\nenhance generalization, focal loss with class reweighting and targeted data\naugmentation strategies are employed. The model is trained and evaluated on a\ncomposite dataset derived from the Toyota Smart Home and ETRI datasets.\nExperimental results demonstrate an F1-score of 0.9415 and an overall accuracy\nof 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,\nand MC3_18 by up to 3%. These results highlight the potential of event-based\ndeep learning for developing accurate, efficient, and privacy-aware human\naction recognition systems suitable for real-world edge applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e09\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5229\u7528\u4e8b\u4ef6\u89c6\u89c9\u6570\u636e\u8fdb\u884c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e8694.17%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u67093D-CNN\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5e27\u7684\u6444\u50cf\u5934\u5728\u4eba\u7c7b\u76d1\u63a7\u7cfb\u7edf\u4e2d\u4f1a\u6355\u83b7\u53ef\u8bc6\u522b\u7684\u4e2a\u4eba\u4fe1\u606f\uff0c\u5b58\u5728\u9690\u79c1\u4fdd\u62a4\u6311\u6218\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u4ec5\u8bb0\u5f55\u50cf\u7d20\u5f3a\u5ea6\u53d8\u5316\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u56fa\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u611f\u77e5\u6a21\u5f0f\uff0c\u4f46\u9700\u8981\u6709\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u5b9e\u73b0\u51c6\u786e\u7684\u6d3b\u52a8\u8bc6\u522b\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u4e09\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6709\u6548\u5efa\u6a21\u7a7a\u95f4\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u540c\u65f6\u4fdd\u6301\u7d27\u51d1\u8bbe\u8ba1\u4ee5\u9002\u5e94\u8fb9\u7f18\u90e8\u7f72\uff1b\u4e3a\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u7528\u4e86\u5e26\u7c7b\u522b\u91cd\u65b0\u52a0\u6743\u7684\u7126\u70b9\u635f\u5931\u548c\u9488\u5bf9\u6027\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u5728\u4e30\u7530\u667a\u80fd\u5bb6\u5c45\u548cETRI\u6570\u636e\u96c6\u7ec4\u5408\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578bF1\u5206\u6570\u8fbe\u52300.9415\uff0c\u6574\u4f53\u51c6\u786e\u7387\u4e3a94.17%\uff0c\u6bd4C3D\u3001ResNet3D\u548cMC3_18\u7b49\u57fa\u51c63D-CNN\u67b6\u6784\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe3%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u5f00\u53d1\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u9690\u79c1\u611f\u77e5\u7684\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u7279\u522b\u9002\u5408\u73b0\u5b9e\u4e16\u754c\u7684\u8fb9\u7f18\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u76d1\u63a7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u8def\u5f84\u3002"}}
