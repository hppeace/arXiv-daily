{"id": "2510.12102", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2510.12102", "abs": "https://arxiv.org/abs/2510.12102", "authors": ["Donghyun Lee", "Alex Sima", "Yuhang Li", "Panos Stinis", "Priyadarshini Panda"], "title": "SpikePool: Event-driven Spiking Transformer with Pooling Attention", "comment": null, "summary": "Building on the success of transformers, Spiking Neural Networks (SNNs) have\nincreasingly been integrated with transformer architectures, leading to spiking\ntransformers that demonstrate promising performance on event-based vision\ntasks. However, despite these empirical successes, there remains limited\nunderstanding of how spiking transformers fundamentally process event-based\ndata. Current approaches primarily focus on architectural modifications without\nanalyzing the underlying signal processing characteristics. In this work, we\nanalyze spiking transformers through the frequency spectrum domain and discover\nthat they behave as high-pass filters, contrasting with Vision Transformers\n(ViTs) that act as low-pass filters. This frequency domain analysis reveals why\ncertain designs work well for event-based data, which contains valuable\nhigh-frequency information but is also sparse and noisy. Based on this\nobservation, we propose SpikePool, which replaces spike-based self-attention\nwith max pooling attention, a low-pass filtering operation, to create a\nselective band-pass filtering effect. This design preserves meaningful\nhigh-frequency content while capturing critical features and suppressing noise,\nachieving a better balance for event-based data processing. Our approach\ndemonstrates competitive results on event-based datasets for both\nclassification and object detection tasks while significantly reducing training\nand inference time by up to 42.5% and 32.8%, respectively."}
{"id": "2510.12753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12753", "abs": "https://arxiv.org/abs/2510.12753", "authors": ["Wenpu Li", "Bangyan Liao", "Yi Zhou", "Qi Xu", "Pian Wan", "Peidong Liu"], "title": "E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization", "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems(NeurIPS 2025)", "summary": "The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in\n3D vision, has typically been addressed independently. For neuromorphic vision\n(e.g., event cameras), however, the lack of robust data association makes\nsolving the two problems separately an ill-posed challenge, especially in the\nabsence of supervision via ground truth. Existing works mitigate this\nill-posedness by either enforcing the smoothness of the flow field via an\nexplicit variational regularizer or leveraging explicit structure-and-motion\npriors in the parametrization to improve event alignment. The former notably\nintroduces bias in results and computational overhead, while the latter, which\nparametrizes the optical flow in terms of the scene depth and the camera\nmotion, often converges to suboptimal local minima. To address these issues, we\npropose an unsupervised framework that jointly optimizes egomotion and optical\nflow via implicit spatial-temporal and geometric regularization. First, by\nmodeling camera's egomotion as a continuous spline and optical flow as an\nimplicit neural representation, our method inherently embeds spatial-temporal\ncoherence through inductive biases. Second, we incorporate structure-and-motion\npriors through differential geometric constraints, bypassing explicit depth\nestimation while maintaining rigorous geometric consistency. As a result, our\nframework (called E-MoFlow) unifies egomotion and optical flow estimation via\nimplicit regularization under a fully unsupervised paradigm. Experiments\ndemonstrate its versatility to general 6-DoF motion scenarios, achieving\nstate-of-the-art performance among unsupervised methods and competitive even\nwith supervised approaches."}
