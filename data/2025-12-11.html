<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-11.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-learning-to-remove-lens-flare-in-event-camera">[1] <a href="https://arxiv.org/abs/2512.09016">Learning to Remove Lens Flare in Event Camera</a></h3>
<p><em>Haiqian Han, Lingdong Kong, Jianing Li, Ao Liang, Chengtao Zhu, Jiacheng Lyu, Lai Xing Ng, Xiangyang Ji, Wei Tsang Ooi, Benoit R. Cottereau</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†E-Deflareï¼Œè¿™æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§åœ°ä»äº‹ä»¶ç›¸æœºæ•°æ®ä¸­å»é™¤é•œå¤´å…‰æ™•çš„æ¡†æ¶ï¼Œé€šè¿‡å»ºç«‹ç‰©ç†åŸºç¡€çš„å‰å‘æ¨¡å‹ã€åˆ›å»ºå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†E-Deflare Benchmarkï¼Œå¹¶è®¾è®¡äº†è¾¾åˆ°æœ€å…ˆè¿›æ¢å¤æ€§èƒ½çš„E-DeflareNetã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äº‹ä»¶ç›¸æœºè™½ç„¶å…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡å’ŒåŠ¨æ€èŒƒå›´çš„ä¼˜åŠ¿ï¼Œä½†ä»æ˜“å—é•œå¤´å…‰æ™•è¿™ä¸€åŸºæœ¬å…‰å­¦ä¼ªå½±çš„å½±å“ï¼Œå¯¼è‡´ä¸¥é‡é€€åŒ–ã€‚åœ¨äº‹ä»¶æµä¸­ï¼Œè¿™ç§å…‰å­¦ä¼ªå½±å½¢æˆå¤æ‚æ—¶ç©ºæ‰­æ›²ï¼Œä½†æ­¤å‰ç ”ç©¶å¯¹æ­¤å…³æ³¨ä¸è¶³ï¼Œç¼ºä¹ç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é¦–å…ˆå»ºç«‹äº†ç†è®ºåŸºç¡€ï¼Œæ¨å¯¼äº†éçº¿æ€§æŠ‘åˆ¶æœºåˆ¶çš„ç‰©ç†åŸºç¡€å‰å‘æ¨¡å‹ã€‚åŸºäºæ­¤æ´å¯Ÿåˆ›å»ºäº†E-Deflare Benchmarkï¼ŒåŒ…å«å¤§è§„æ¨¡æ¨¡æ‹Ÿè®­ç»ƒé›†E-Flare-2.7Kå’Œé¦–ä¸ªé…å¯¹çœŸå®ä¸–ç•Œæµ‹è¯•é›†E-Flare-Rã€‚åˆ©ç”¨è¯¥åŸºå‡†ï¼Œè®¾è®¡äº†E-DeflareNetæ¡†æ¶æ¥å®ç°é•œå¤´å…‰æ™•å»é™¤ã€‚</p>
<p><strong>Result:</strong> E-DeflareNetåœ¨é•œå¤´å…‰æ™•å»é™¤ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ¢å¤æ€§èƒ½ã€‚å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æ˜æ˜¾ç›Šå¤„ã€‚æ‰€æå‡ºçš„åŸºå‡†æ•°æ®é›†å’Œä»£ç å‡å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºäº‹ä»¶ç›¸æœºé•œå¤´å…‰æ™•é—®é¢˜æä¾›äº†é¦–ä¸ªç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚é€šè¿‡å»ºç«‹ç‰©ç†æ¨¡å‹å’Œåˆ›å»ºåŸºå‡†æ•°æ®é›†ï¼Œä¸ºåç»­ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡äº‹ä»¶ç›¸æœºè§†è§‰ç³»ç»Ÿçš„é²æ£’æ€§å’Œå®ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available.</p>
<h3 id="2-cs3d-an-efficient-facial-expression-recognition-via-event-vision">[2] <a href="https://arxiv.org/abs/2512.09592">CS3D: An Efficient Facial Expression Recognition via Event Vision</a></h3>
<p><em>Zhe Wang, Qijin Song, Yucen Peng, Weibang Bai</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCS3Dæ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£3Då·ç§¯æ–¹æ³•é™ä½è®¡ç®—å¤æ‚åº¦å’Œèƒ½è€—ï¼Œå¹¶åˆ©ç”¨è½¯è„‰å†²ç¥ç»å…ƒå’Œæ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶å¢å¼ºä¿¡æ¯ä¿ç•™èƒ½åŠ›ï¼Œä»è€Œåœ¨äº‹ä»¶ç›¸æœºé©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä»»åŠ¡ä¸­å®ç°é«˜ç²¾åº¦ä¸ä½èƒ½è€—çš„å¹³è¡¡ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡äº‹ä»¶ç›¸æœºåœ¨æ•æ‰é¢éƒ¨è¡¨æƒ…å˜åŒ–æ–¹é¢å…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡ã€ä½å»¶è¿Ÿå’Œä½å…‰ç…§é²æ£’æ€§ç­‰ä¼˜åŠ¿ï¼Œä½†ä¼ ç»ŸåŸºäºæ·±åº¦å­¦ä¹ çš„é¢éƒ¨è¡¨æƒ…åˆ†ææ–¹æ³•èƒ½è€—é«˜ï¼Œéš¾ä»¥éƒ¨ç½²åœ¨è¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šï¼Œè¿™é™åˆ¶äº†äº‹ä»¶è§†è§‰æ–¹æ³•åœ¨é«˜é¢‘åŠ¨æ€åœºæ™¯ä¸­çš„å®é™…åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºCS3Dæ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£å·ç§¯3Dæ–¹æ³•æ¥é™ä½è®¡ç®—å¤æ‚åº¦å’Œèƒ½è€—ï¼ŒåŒæ—¶é‡‡ç”¨è½¯è„‰å†²ç¥ç»å…ƒå¢å¼ºä¿¡æ¯ä¿ç•™èƒ½åŠ›ï¼Œå¹¶ç»“åˆæ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶è¿›ä¸€æ­¥æå‡é¢éƒ¨è¡¨æƒ…æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒCS3Dæ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šç›¸æ¯”RNNã€Transformerå’ŒC3Dç­‰æ¶æ„è·å¾—äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å…¶èƒ½è€—ä»…ä¸ºåŸå§‹C3Dæ–¹æ³•åœ¨ç›¸åŒè®¾å¤‡ä¸Šæ‰€éœ€èƒ½è€—çš„21.97%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†CS3Dæ¡†æ¶åœ¨äº‹ä»¶ç›¸æœºé©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä»»åŠ¡ä¸­èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡ç²¾åº¦ä¸èƒ½è€—ï¼Œä¸ºè¾¹ç¼˜è®¡ç®—è®¾å¤‡éƒ¨ç½²é«˜æ•ˆçš„äº‹ä»¶è§†è§‰ç³»ç»Ÿæä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œæ¨åŠ¨äº†äººæœºäº¤äº’ä¸­å®æ—¶è¡¨æƒ…è¯†åˆ«æŠ€æœ¯çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\% of the original C3D required on the same device.</p>
  </article>
</body>
</html>
