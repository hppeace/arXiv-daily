<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-22.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-dessert-diffusion-based-event-driven-single-frame-synthesis-via-residual-training">[1] <a href="https://arxiv.org/abs/2512.17323">DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training</a></h3>
<p><em>Jiyun Kong, Jun-Hyuk Kim, Jong-Seok Lee</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DESSERTï¼Œä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„äº‹ä»¶é©±åŠ¨å•å¸§åˆæˆæ¡†æ¶ï¼Œé€šè¿‡æ®‹å·®è®­ç»ƒè§£å†³äº‹ä»¶ç›¸æœºè§†é¢‘å¸§é¢„æµ‹ä¸­çš„ç©ºæ´å’Œæ¨¡ç³Šé—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ï¼Œåœ¨äº‹ä»¶æ•°æ®æ¡ä»¶ä¸‹ç”Ÿæˆæ®‹å·®æ½œåœ¨è¡¨ç¤ºï¼Œå®ç°äº†æ›´æ¸…æ™°ã€æ—¶é—´ä¸€è‡´æ€§çš„å¸§åˆæˆã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿè§†é¢‘å¸§é¢„æµ‹åœ¨åŠ¨æ€åœºæ™¯ä¸­å› ç¼ºä¹ä¸‹ä¸€å¸§ä¿¡æ¯è€Œäº§ç”Ÿé¢„æµ‹è¯¯å·®ï¼Œè€Œäº‹ä»¶ç›¸æœºè™½èƒ½å¼‚æ­¥æ•è·é«˜æ—¶é—´åˆ†è¾¨ç‡çš„äº®åº¦å˜åŒ–ï¼Œä½†ç°æœ‰äº‹ä»¶é©±åŠ¨æ–¹æ³•é€šè¿‡é¢„æµ‹å…‰æµå’Œåƒç´ æ‰­æ›²é‡å»ºå¸§æ—¶ï¼Œä¼šå› åƒç´ ä½ç§»ä¸å‡†ç¡®å¯¼è‡´ç©ºæ´å’Œæ¨¡ç³Šé—®é¢˜ï¼Œéœ€è¦æ›´é²æ£’çš„åˆæˆæ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºDESSERTæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè®¾è®¡äº‹ä»¶åˆ°æ®‹å·®å¯¹é½å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆER-VAEï¼‰ï¼Œå°†é”šç‚¹å’Œç›®æ ‡å¸§ä¹‹é—´çš„äº‹ä»¶å¸§ä¸å¯¹åº”æ®‹å·®å¯¹é½ï¼›ç„¶åè®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨äº‹ä»¶æ•°æ®æ¡ä»¶ä¸‹å¯¹æ®‹å·®æ½œåœ¨è¡¨ç¤ºè¿›è¡Œå»å™ªã€‚æ­¤å¤–å¼•å…¥å¤šæ ·é•¿åº¦æ—¶é—´å¢å¼ºï¼ˆDLTï¼‰ï¼Œé€šè¿‡è®­ç»ƒä¸åŒæ—¶é—´é•¿åº¦çš„å¸§æ®µæå‡é²æ£’æ€§ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„äº‹ä»¶é‡å»ºã€åŸºäºå›¾åƒçš„è§†é¢‘å¸§é¢„æµ‹ã€äº‹ä»¶é©±åŠ¨çš„è§†é¢‘å¸§é¢„æµ‹ä»¥åŠå•è¾¹äº‹ä»¶è§†é¢‘å¸§æ’å€¼æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´æ¸…æ™°ä¸”æ—¶é—´ä¸€è‡´æ€§æ›´å¥½çš„å¸§åˆæˆç»“æœï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºæ‰©æ•£æ¨¡å‹å’Œæ®‹å·®è®­ç»ƒçš„äº‹ä»¶é©±åŠ¨å¸§åˆæˆæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡äº‹ä»¶åˆ°æ®‹å·®çš„å¯¹é½æœºåˆ¶å’Œæ¡ä»¶æ‰©æ•£è¿‡ç¨‹ï¼Œæ˜¾è‘—æ”¹å–„äº†åŠ¨æ€åœºæ™¯ä¸‹çš„å¸§é¢„æµ‹è´¨é‡ã€‚è¯¥æ–¹æ³•ä¸ºäº‹ä»¶ç›¸æœºåœ¨è§†é¢‘å¤„ç†ä¸­çš„åº”ç”¨æä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯æ‰©å±•è‡³æ›´å¤æ‚çš„å¤šå¸§é¢„æµ‹å’Œå®æ—¶åº”ç”¨åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Video frame prediction extrapolates future frames from previous frames, but suffers from prediction errors in dynamic scenes due to the lack of information about the next frame. Event cameras address this limitation by capturing per-pixel brightness changes asynchronously with high temporal resolution. Prior research on event-based video frame prediction has leveraged motion information from event data, often by predicting event-based optical flow and reconstructing frames via pixel warping. However, such approaches introduce holes and blurring when pixel displacement is inaccurate. To overcome this limitation, we propose DESSERT, a diffusion-based event-driven single-frame synthesis framework via residual training. Leveraging a pre-trained Stable Diffusion model, our method is trained on inter-frame residuals to ensure temporal consistency. The training pipeline consists of two stages: (1) an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) that aligns the event frame between anchor and target frames with the corresponding residual, and (2) a diffusion model that denoises the residual latent conditioned on event data. Furthermore, we introduce Diverse-Length Temporal (DLT) augmentation, which improves robustness by training on frame segments of varying temporal lengths. Experimental results demonstrate that our method outperforms existing event-based reconstruction, image-based video frame prediction, event-based video frame prediction, and one-sided event-based video frame interpolation methods, producing sharper and more temporally consistent frame synthesis.</p>
  </article>
</body>
</html>
