{"id": "2601.21503", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.21503", "abs": "https://arxiv.org/abs/2601.21503", "authors": ["Junhong Cai", "Guiqin Wang", "Kejie Zhao", "Jianxiong Tang", "Xiang Wang", "Luziwei Leng", "Ran Cheng", "Yuxin Ma", "Qinghai Guo"], "title": "MAR: Efficient Large Language Models via Module-aware Architecture Refinement", "comment": "Accepted by ICASSP 2026. 5 pages, 5 figures", "summary": "Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs."}
{"id": "2601.20870", "categories": ["cs.NE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20870", "abs": "https://arxiv.org/abs/2601.20870", "authors": ["Matteo Gianferrari", "Omayma Moussadek", "Riccardo Salami", "Cosimo Fiorini", "Lorenzo Tartarini", "Daniela Gandolfi", "Simone Calderara"], "title": "STAER: Temporal Aligned Rehearsal for Continual Spiking Neural Network", "comment": null, "summary": "Spiking Neural Networks (SNNs) are inherently suited for continuous learning due to their event-driven temporal dynamics; however, their application to Class-Incremental Learning (CIL) has been hindered by catastrophic forgetting and the temporal misalignment of spike patterns. In this work, we introduce Spiking Temporal Alignment with Experience Replay (STAER), a novel framework that explicitly preserves temporal structure to bridge the performance gap between SNNs and ANNs. Our approach integrates a differentiable Soft-DTW alignment loss to maintain spike timing fidelity and employs a temporal expansion and contraction mechanism on output logits to enforce robust representation learning. Implemented on a deep ResNet19 spiking backbone, STAER achieves state-of-the-art performance on Sequential-MNIST and Sequential-CIFAR10. Empirical results demonstrate that our method matches or outperforms strong ANN baselines (ER, DER++) while preserving biologically plausible dynamics. Ablation studies further confirm that explicit temporal alignment is critical for representational stability, positioning STAER as a scalable solution for spike-native lifelong learning. Code is available at https://github.com/matteogianferrari/staer."}
{"id": "2601.21503", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.21503", "abs": "https://arxiv.org/abs/2601.21503", "authors": ["Junhong Cai", "Guiqin Wang", "Kejie Zhao", "Jianxiong Tang", "Xiang Wang", "Luziwei Leng", "Ran Cheng", "Yuxin Ma", "Qinghai Guo"], "title": "MAR: Efficient Large Language Models via Module-aware Architecture Refinement", "comment": "Accepted by ICASSP 2026. 5 pages, 5 figures", "summary": "Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs."}
{"id": "2601.21279", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21279", "abs": "https://arxiv.org/abs/2601.21279", "authors": ["Zhengzheng Tang"], "title": "NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with Surrogate-Free Training", "comment": "7 pages, 6 tables, 2 figures. Preprint (January 28, 2026)", "summary": "Spiking Neural Networks (SNNs) promise energy-efficient computing through event-driven sparsity, yet all existing approaches sacrifice accuracy by approximating continuous values with discrete spikes. We propose NEXUS, a framework that achieves bit-exact ANN-to-SNN equivalence -- not approximate, but mathematically identical outputs. Our key insight is constructing all arithmetic operations, both linear and nonlinear, from pure IF neuron logic gates that implement IEEE-754 compliant floating-point arithmetic. Through spatial bit encoding (zero encoding error by construction), hierarchical neuromorphic gate circuits (from basic logic gates to complete transformer layers), and surrogate-free STE training (exact identity mapping rather than heuristic approximation), NEXUS produces outputs identical to standard ANNs up to machine precision. Experiments on models up to LLaMA-2 70B demonstrate identical task accuracy (0.00\\% degradation) with mean ULP error of only 6.19, while achieving 27-168,000$\\times$ energy reduction on neuromorphic hardware. Crucially, spatial bit encoding's single-timestep design renders the framework inherently immune to membrane potential leakage (100\\% accuracy across all decay factors $β\\in[0.1,1.0]$), while tolerating synaptic noise up to $σ=0.2$ with >98\\% gate-level accuracy."}
{"id": "2601.20870", "categories": ["cs.NE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20870", "abs": "https://arxiv.org/abs/2601.20870", "authors": ["Matteo Gianferrari", "Omayma Moussadek", "Riccardo Salami", "Cosimo Fiorini", "Lorenzo Tartarini", "Daniela Gandolfi", "Simone Calderara"], "title": "STAER: Temporal Aligned Rehearsal for Continual Spiking Neural Network", "comment": null, "summary": "Spiking Neural Networks (SNNs) are inherently suited for continuous learning due to their event-driven temporal dynamics; however, their application to Class-Incremental Learning (CIL) has been hindered by catastrophic forgetting and the temporal misalignment of spike patterns. In this work, we introduce Spiking Temporal Alignment with Experience Replay (STAER), a novel framework that explicitly preserves temporal structure to bridge the performance gap between SNNs and ANNs. Our approach integrates a differentiable Soft-DTW alignment loss to maintain spike timing fidelity and employs a temporal expansion and contraction mechanism on output logits to enforce robust representation learning. Implemented on a deep ResNet19 spiking backbone, STAER achieves state-of-the-art performance on Sequential-MNIST and Sequential-CIFAR10. Empirical results demonstrate that our method matches or outperforms strong ANN baselines (ER, DER++) while preserving biologically plausible dynamics. Ablation studies further confirm that explicit temporal alignment is critical for representational stability, positioning STAER as a scalable solution for spike-native lifelong learning. Code is available at https://github.com/matteogianferrari/staer."}
{"id": "2601.21778", "categories": ["cs.NE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21778", "abs": "https://arxiv.org/abs/2601.21778", "authors": ["Zijie Xu", "Zihan Huang", "Yiting Dong", "Kang Chen", "Wenxuan Liu", "Zhaofei Yu"], "title": "Error Amplification Limits ANN-to-SNN Conversion in Continuous Control", "comment": null, "summary": "Spiking Neural Networks (SNNs) can achieve competitive performance by converting already existing well-trained Artificial Neural Networks (ANNs), avoiding further costly training. This property is particularly attractive in Reinforcement Learning (RL), where training through environment interaction is expensive and potentially unsafe. However, existing conversion methods perform poorly in continuous control, where suitable baselines are largely absent. We identify error amplification as the key cause: small action approximation errors become temporally correlated across decision steps, inducing cumulative state distribution shift and severe performance degradation. To address this issue, we propose Cross-Step Residual Potential Initialization (CRPI), a lightweight training-free mechanism that carries over residual membrane potentials across decision steps to suppress temporally correlated errors. Experiments on continuous control benchmarks with both vector and visual observations demonstrate that CRPI can be integrated into existing conversion pipelines and substantially recovers lost performance. Our results highlight continuous control as a critical and challenging benchmark for ANN-to-SNN conversion, where small errors can be strongly amplified and impact performance."}
{"id": "2601.21279", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21279", "abs": "https://arxiv.org/abs/2601.21279", "authors": ["Zhengzheng Tang"], "title": "NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with Surrogate-Free Training", "comment": "7 pages, 6 tables, 2 figures. Preprint (January 28, 2026)", "summary": "Spiking Neural Networks (SNNs) promise energy-efficient computing through event-driven sparsity, yet all existing approaches sacrifice accuracy by approximating continuous values with discrete spikes. We propose NEXUS, a framework that achieves bit-exact ANN-to-SNN equivalence -- not approximate, but mathematically identical outputs. Our key insight is constructing all arithmetic operations, both linear and nonlinear, from pure IF neuron logic gates that implement IEEE-754 compliant floating-point arithmetic. Through spatial bit encoding (zero encoding error by construction), hierarchical neuromorphic gate circuits (from basic logic gates to complete transformer layers), and surrogate-free STE training (exact identity mapping rather than heuristic approximation), NEXUS produces outputs identical to standard ANNs up to machine precision. Experiments on models up to LLaMA-2 70B demonstrate identical task accuracy (0.00\\% degradation) with mean ULP error of only 6.19, while achieving 27-168,000$\\times$ energy reduction on neuromorphic hardware. Crucially, spatial bit encoding's single-timestep design renders the framework inherently immune to membrane potential leakage (100\\% accuracy across all decay factors $β\\in[0.1,1.0]$), while tolerating synaptic noise up to $σ=0.2$ with >98\\% gate-level accuracy."}
{"id": "2601.21823", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2601.21823", "abs": "https://arxiv.org/abs/2601.21823", "authors": ["Zihan Huang", "Zijie Xu", "Yihan Huang", "Shanshan Jia", "Tong Bu", "Yiting Dong", "Wenxuan Liu", "Jianhao Ding", "Zhaofei Yu", "Tiejun Huang"], "title": "General Self-Prediction Enhancement for Spiking Neurons", "comment": null, "summary": "Spiking Neural Networks (SNNs) are highly energy-efficient due to event-driven, sparse computation, but their training is challenged by spike non-differentiability and trade-offs among performance, efficiency, and biological plausibility. Crucially, mainstream SNNs ignore predictive coding, a core cortical mechanism where the brain predicts inputs and encodes errors for efficient perception. Inspired by this, we propose a self-prediction enhanced spiking neuron method that generates an internal prediction current from its input-output history to modulate membrane potential. This design offers dual advantages, it creates a continuous gradient path that alleviates vanishing gradients and boosts training stability and accuracy, while also aligning with biological principles, which resembles distal dendritic modulation and error-driven synaptic plasticity. Experiments show consistent performance gains across diverse architectures, neuron types, time steps, and tasks demonstrating broad applicability for enhancing SNNs."}
{"id": "2601.21503", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.21503", "abs": "https://arxiv.org/abs/2601.21503", "authors": ["Junhong Cai", "Guiqin Wang", "Kejie Zhao", "Jianxiong Tang", "Xiang Wang", "Luziwei Leng", "Ran Cheng", "Yuxin Ma", "Qinghai Guo"], "title": "MAR: Efficient Large Language Models via Module-aware Architecture Refinement", "comment": "Accepted by ICASSP 2026. 5 pages, 5 figures", "summary": "Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs."}
