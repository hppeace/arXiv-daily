<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-02-02.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-fire-on-motion-optimizing-video-pass-bands-for-efficient-spiking-action-recognition">[1] <a href="https://arxiv.org/abs/2601.22675">Fire on Motion: Optimizing Video Pass-bands for Efficient Spiking Action Recognition</a></h3>
<p><em>Shuhan Ye, Yuanbin Qian, Yi Yu, Chong Wang, Yuqi Xie, Jiazhen Xu, Kun Wang, Xudong Jiang</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºé€šå¸¦ä¼˜åŒ–å™¨ï¼ˆPBOï¼‰çš„å³æ’å³ç”¨æ¨¡å—ï¼Œç”¨äºè§£å†³è„‰å†²ç¥ç»ç½‘ç»œåœ¨åŠ¨æ€è§†é¢‘ä»»åŠ¡ä¸­å› æ—¶é—´ä½é€šç‰¹æ€§å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚PBOé€šè¿‡ä¼˜åŒ–æ—¶é—´é€šå¸¦ä»¥èšç„¦äºä»»åŠ¡ç›¸å…³çš„è¿åŠ¨é¢‘å¸¦ï¼Œæ˜¾è‘—æå‡äº†SNNåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è„‰å†²ç¥ç»ç½‘ç»œè™½ç„¶åœ¨é™æ€å›¾åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨æ€è§†é¢‘ä»»åŠ¡ä¸­æ€§èƒ½ä»è½åäºäººå·¥ç¥ç»ç½‘ç»œã€‚ç ”ç©¶å‘ç°è¿™ä¸€é—®é¢˜çš„æ ¹æœ¬åŸå› åœ¨äºæ ‡å‡†è„‰å†²åŠ¨åŠ›å­¦è¡¨ç°ä¸ºæ—¶é—´ä½é€šæ»¤æ³¢å™¨ï¼Œå¼ºè°ƒé™æ€å†…å®¹è€Œè¡°å‡äº†åŒ…å«ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„å…³é”®è¿åŠ¨é¢‘å¸¦ï¼Œå¯¼è‡´SNNåœ¨éœ€è¦ä¸°å¯Œæ—¶é—´ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†é€šå¸¦ä¼˜åŒ–å™¨ï¼ˆPBOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ¨¡å—ï¼Œé€šè¿‡ä¼˜åŒ–æ—¶é—´é€šå¸¦ä½¿å…¶æœå‘ä»»åŠ¡ç›¸å…³çš„è¿åŠ¨é¢‘å¸¦ã€‚PBOä»…å¼•å…¥ä¸¤ä¸ªå¯å­¦ä¹ å‚æ•°å’Œä¸€ä¸ªè½»é‡çº§çš„ä¸€è‡´æ€§çº¦æŸï¼Œä»¥ä¿æŒè¯­ä¹‰å’Œè¾¹ç•Œä¿¡æ¯ï¼Œè®¡ç®—å¼€é”€å¯å¿½ç•¥ä¸”æ— éœ€æ¶æ„ä¿®æ”¹ã€‚è¯¥æ–¹æ³•æœ‰æ„æŠ‘åˆ¶å¯¹åŒºåˆ†è´¡çŒ®è¾ƒå°çš„é™æ€æˆåˆ†ï¼Œæœ‰æ•ˆå®ç°é«˜é€šæ»¤æ³¢ï¼Œä½¿è„‰å†²æ´»åŠ¨é›†ä¸­äºè¿åŠ¨æ‰¿è½½å†…å®¹ã€‚</p>
<p><strong>Result:</strong> åœ¨UCF101æ•°æ®é›†ä¸Šï¼ŒPBOå¸¦æ¥äº†è¶…è¿‡10ä¸ªç™¾åˆ†ç‚¹çš„æ€§èƒ½æå‡ã€‚åœ¨æ›´å¤æ‚çš„å¤šæ¨¡æ€åŠ¨ä½œè¯†åˆ«å’Œå¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒPBOå‡å®ç°äº†æŒç»­ä¸”æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šç§è§†é¢‘å¤„ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºåŸºäºSNNçš„è§†é¢‘å¤„ç†å’Œç†è§£æä¾›äº†æ–°è§†è§’ï¼Œé€šè¿‡è§£å†³é€šå¸¦ä¸åŒ¹é…é—®é¢˜æ˜¾è‘—æå‡äº†SNNåœ¨åŠ¨æ€ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚PBOçš„è½»é‡çº§è®¾è®¡å’Œå³æ’å³ç”¨ç‰¹æ€§ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰è‰¯å¥½å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥SNNåœ¨è§†é¢‘åˆ†æé¢†åŸŸçš„å‘å±•å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Spiking neural networks (SNNs) have gained traction in vision due to their energy efficiency, bio-plausibility, and inherent temporal processing. Yet, despite this temporal capacity, most progress concentrates on static image benchmarks, and SNNs still underperform on dynamic video tasks compared to artificial neural networks (ANNs). In this work, we diagnose a fundamental pass-band mismatch: Standard spiking dynamics behave as a temporal low pass that emphasizes static content while attenuating motion bearing bands, where task relevant information concentrates in dynamic tasks. This phenomenon explains why SNNs can approach ANNs on static tasks yet fall behind on tasks that demand richer temporal understanding.To remedy this, we propose the Pass-Bands Optimizer (PBO), a plug-and-play module that optimizes the temporal pass-band toward task-relevant motion bands. PBO introduces only two learnable parameters, and a lightweight consistency constraint that preserves semantics and boundaries, incurring negligible computational overhead and requires no architectural changes. PBO deliberately suppresses static components that contribute little to discrimination, effectively high passing the stream so that spiking activity concentrates on motion bearing content. On UCF101, PBO yields over ten percentage points improvement. On more complex multi-modal action recognition and weakly supervised video anomaly detection, PBO delivers consistent and significant gains, offering a new perspective for SNN based video processing and understanding.</p>
  </article>
</body>
</html>
