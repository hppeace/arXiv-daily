<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-08.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 3]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-ie2video-adapting-pretrained-diffusion-models-for-event-based-video-reconstruction">[1] <a href="https://arxiv.org/abs/2512.05240">IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction</a></h3>
<p><em>Dmitrii Torbunov, Onur Okuducu, Yi Huang, Odera Dim, Rebecca Coles, Yonggang Cui, Yihui Ren</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ•è·èŒƒå¼ï¼Œé€šè¿‡ç»“åˆç¨€ç–RGBå…³é”®å¸§å’Œè¿ç»­äº‹ä»¶æµæ¥é‡å»ºå®Œæ•´RGBè§†é¢‘ï¼Œä»¥è§£å†³äº‹ä»¶ç›¸æœºä½åŠŸè€—ä½†è¾“å‡ºéæ ‡å‡†è§†é¢‘æ ¼å¼çš„é—®é¢˜ã€‚ç ”ç©¶å¼•å…¥äº†IE2Videoä»»åŠ¡ï¼Œå¹¶æ¢ç´¢äº†è‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹ä¸¤ç§æ¶æ„ç­–ç•¥ï¼Œå…¶ä¸­åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»ŸRGBç›¸æœºåœ¨è¿ç»­è§†é¢‘ç›‘æ§ä¸­é¢ä¸´é«˜åŠŸè€—é™åˆ¶ï¼Œè€Œäº‹ä»¶ç›¸æœºè™½ç„¶åŠŸè€—ä½ä½†äº§ç”Ÿå¼‚æ­¥äº‹ä»¶æµè€Œéæ ‡å‡†RGBè§†é¢‘ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€çŸ›ç›¾ï¼Œæå‡ºæ··åˆæ•è·èŒƒå¼ä»¥åœ¨é™ä½æ•è·åŠŸè€—çš„åŒæ—¶ä¿æŒæ ‡å‡†è§†é¢‘è¾“å‡ºï¼Œæ»¡è¶³ä¸‹æ¸¸åº”ç”¨éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†å›¾åƒå’Œäº‹ä»¶åˆ°è§†é¢‘ï¼ˆIE2Videoï¼‰ä»»åŠ¡ï¼Œæ¢ç´¢äº†ä¸¤ç§æ¶æ„ç­–ç•¥ï¼šä¸ºRGBç”Ÿæˆè°ƒæ•´è‡ªå›å½’æ¨¡å‹ï¼ˆHyperE2VIDï¼‰ï¼Œä»¥åŠé€šè¿‡å­¦ä¹ çš„ç¼–ç å™¨å’Œä½ç§©é€‚åº”å°†äº‹ä»¶è¡¨ç¤ºæ³¨å…¥é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆLTXï¼‰ã€‚è¯¥æ–¹æ³•ç»“åˆç¨€ç–RGBå…³é”®å¸§å’Œè¿ç»­äº‹ä»¶æµè¿›è¡Œç¦»çº¿è§†é¢‘é‡å»ºã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡ä¸Šæ¯”è‡ªå›å½’åŸºçº¿æé«˜äº†33%ï¼ˆLPIPSå¾—åˆ†0.283 vs 0.422ï¼‰ã€‚ç ”ç©¶åœ¨ä¸‰ä¸ªäº‹ä»¶ç›¸æœºæ•°æ®é›†ï¼ˆBS-ERGBã€HS-ERGBè¿œ/è¿‘ï¼‰å’Œä¸åŒåºåˆ—é•¿åº¦ï¼ˆ32-128å¸§ï¼‰ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨æœªè§æ•è·é…ç½®ä¸Šçš„å¼ºå¤§è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ··åˆæ•è·èŒƒå¼åœ¨é™ä½åŠŸè€—åŒæ—¶ä¿æŒè§†é¢‘è´¨é‡çš„å¯è¡Œæ€§ï¼Œæ‰©æ•£æ¨¡å‹åœ¨äº‹ä»¶åˆ°è§†é¢‘è½¬æ¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºä½åŠŸè€—è§†é¢‘ç›‘æ§ç³»ç»Ÿæä¾›äº†æ–°æ–¹å‘ï¼Œå¹¶å±•ç¤ºäº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨è·¨æ¨¡æ€è§†é¢‘é‡å»ºä»»åŠ¡ä¸­çš„é€‚åº”æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="2-neuromorphicrx-from-neural-to-spiking-receiver">[2] <a href="https://arxiv.org/abs/2512.05246">NeuromorphicRx: From Neural to Spiking Receiver</a></h3>
<p><em>Ankit Gupta, Onur Dizdar, Yun Chen, Fehmi Emre Kadan, Ata Sattarzadeh, Stephen Wang</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNeuromorphicRxçš„æ–°å‹èŠ‚èƒ½è„‰å†²ç¥ç»ç½‘ç»œæ¥æ”¶å™¨ï¼Œç”¨äº5G-NR OFDMç³»ç»Ÿï¼Œè¯¥æ¥æ”¶å™¨èƒ½å¤Ÿä»¥æ˜¾è‘—é™ä½çš„èƒ½è€—å®ç°ä¸ä¼ ç»Ÿæ¥æ”¶å™¨ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³5G-NRç³»ç»Ÿä¸­ä¼ ç»Ÿæ¥æ”¶å™¨åœ¨èƒ½æ•ˆæ–¹é¢çš„å±€é™æ€§ï¼Œé€šè¿‡å¼€å‘åŸºäºè„‰å†²ç¥ç»ç½‘ç»œçš„èŠ‚èƒ½æ¥æ”¶å™¨æ¥æ›¿ä»£ä¼ ç»Ÿçš„ä¿¡é“ä¼°è®¡ã€å‡è¡¡å’Œç¬¦å·è§£æ˜ å°„æ¨¡å—ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡ç³»ç»Ÿæ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºäº†ä¸€ç§æ·±åº¦å·ç§¯è„‰å†²ç¥ç»ç½‘ç»œæ¶æ„ï¼Œé‡‡ç”¨è„‰å†²å…ƒç´ çº§æ®‹å·®è¿æ¥ï¼Œå¹¶åˆ©ç”¨é¢†åŸŸçŸ¥è¯†è®¾è®¡å…·æœ‰è„‰å†²ç¼–ç çš„è¾“å…¥è¡¨ç¤ºã€‚ç ”ç©¶é‡‡ç”¨äº†SNNä¸äººå·¥ç¥ç»ç½‘ç»œçš„æ··åˆæ¶æ„ä»¥è·å¾—è½¯è¾“å‡ºï¼Œå¹¶åº”ç”¨ä»£ç†æ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶é€šè¿‡é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå¢å¼ºé²æ£’æ€§å’Œè·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›çš„æ•°å€¼ä»¿çœŸè¡¨æ˜ï¼ŒNeuromorphicRxåœ¨å—é”™è¯¯ç‡æ€§èƒ½ä¸Šç›¸æ¯”ä¼ ç»Ÿ5G-NRæ¥æ”¶å™¨è·å¾—æ˜¾è‘—æå‡ï¼ŒåŒæ—¶ä¸åŸºäºANNçš„å¯¹åº”æ–¹æ¡ˆæ€§èƒ½ç›¸å½“ï¼Œä½†èƒ½è€—é™ä½äº†7.6å€ã€‚ç ”ç©¶è¿˜è¿›è¡Œäº†è¯¦ç»†çš„æ¶ˆèå®éªŒä»¥éªŒè¯å¯¹5G-NRä¿¡å·çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è„‰å†²ç¥ç»ç½‘ç»œåœ¨æ— çº¿é€šä¿¡æ¥æ”¶å™¨è®¾è®¡ä¸­çš„å¯è¡Œæ€§å’Œä¼˜åŠ¿ï¼Œä¸ºå¼€å‘é«˜èƒ½æ•ˆçš„ä¸‹ä¸€ä»£é€šä¿¡ç³»ç»Ÿæä¾›äº†æ–°é€”å¾„ã€‚æ··åˆæ¶æ„å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒçš„ç»“åˆä¸ºå®ç°å®é™…éƒ¨ç½²ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>In this work, we propose a novel energy-efficient spiking neural network (SNN)-based receiver for 5G-NR OFDM system, called neuromorphic receiver (NeuromorphicRx), replacing the channel estimation, equalization and symbol demapping blocks. We leverage domain knowledge to design the input with spiking encoding and propose a deep convolutional SNN with spike-element-wise residual connections. We integrate an SNN with artificial neural network (ANN) hybrid architecture to obtain soft outputs and employ surrogate gradient descent for training. We focus on generalization across diverse scenarios and robustness through quantized aware training. We focus on interpretability of NeuromorphicRx for 5G-NR signals and perform detailed ablation study for 5G-NR signals. Our extensive numerical simulations show that NeuromorphicRx is capable of achieving significant block error rate performance gain compared to 5G-NR receivers and similar performance compared to its ANN-based counterparts with 7.6x less energy consumption.</p>
<h3 id="3-unleashing-temporal-capacity-of-spiking-neural-networks-through-spatiotemporal-separation">[3] <a href="https://arxiv.org/abs/2512.05472">Unleashing Temporal Capacity of Spiking Neural Networks through Spatiotemporal Separation</a></h3>
<p><em>Yiting Dong, Zhaofei Yu, Jianhao Ding, Zijie Xu, Tiejun Huang</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºæ—¶ç©ºå¯åˆ†ç¦»ç½‘ç»œï¼ˆSTSepï¼‰ï¼Œé€šè¿‡è§£è€¦ç©ºé—´å’Œæ—¶é—´åˆ†æ”¯æ¥ä¼˜åŒ–è„‰å†²ç¥ç»ç½‘ç»œåœ¨è§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºå»ºæ¨¡èƒ½åŠ›ï¼Œæ­ç¤ºäº†è†œç”µä½ä¼ æ’­åœ¨å¤æ‚æ—¶åºä»»åŠ¡ä¸­çš„å®é™…è´¡çŒ®æœ‰é™ï¼Œå¹¶æä¾›äº†æ›´æœ‰æ•ˆçš„æ—¶ç©ºè¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç ”ç©¶æ™®éè®¤ä¸ºè„‰å†²ç¥ç»ç½‘ç»œçš„è†œç”µä½ä¼ æ’­æ˜¯å…¶æ ¸å¿ƒæ—¶åºå»ºæ¨¡æœºåˆ¶ï¼Œä½†ç¼ºä¹å¯¹å…¶åœ¨å¤æ‚æ—¶åºä»»åŠ¡ä¸­å®é™…è´¡çŒ®çš„ç³»ç»Ÿåˆ†æï¼Œéœ€è¦é‡åŒ–è†œç”µä½ä¼ æ’­åœ¨æ—¶ç©ºè¡¨ç¤ºå­¦ä¹ ä¸­çš„å…·ä½“ä½œç”¨å¹¶æ¢ç´¢æ›´æœ‰æ•ˆçš„æ—¶ç©ºå»ºæ¨¡æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> é¦–å…ˆè®¾è®¡éçŠ¶æ€æ¨¡å‹é€æ­¥ç§»é™¤è†œç”µä½ä¼ æ’­ä»¥é‡åŒ–å…¶åˆ†é˜¶æ®µä½œç”¨ï¼Œç„¶ååŸºäºæ—¶ç©ºèµ„æºç«äº‰ç†è®ºæå‡ºæ—¶ç©ºå¯åˆ†ç¦»ç½‘ç»œï¼Œå°†æ®‹å·®å—è§£è€¦ä¸ºç‹¬ç«‹çš„ç©ºé—´åˆ†æ”¯å’Œæ—¶é—´åˆ†æ”¯ï¼Œç©ºé—´åˆ†æ”¯ä¸“æ³¨äºè¯­ä¹‰æå–ï¼Œæ—¶é—´åˆ†æ”¯é€šè¿‡æ˜¾å¼æ—¶é—´å·®å¼‚æ•è·è¿åŠ¨ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°åç›´è§‰ç°è±¡ï¼šåœ¨æµ…å±‚æˆ–æ·±å±‚é€‚åº¦ç§»é™¤è†œç”µä½ä¼ æ’­èƒ½æå‡æ€§èƒ½ï¼Œè€Œè¿‡åº¦ç§»é™¤ä¼šå¯¼è‡´å´©æºƒï¼›åœ¨Something-Something V2ã€UCF101å’ŒHMDB51æ•°æ®é›†ä¸Šï¼ŒSTSepå®ç°äº†ä¼˜è¶Šæ€§èƒ½ï¼Œæ£€ç´¢ä»»åŠ¡å’Œæ³¨æ„åŠ›åˆ†æè¯å®å…¶ä¸“æ³¨äºè¿åŠ¨è€Œéé™æ€å¤–è§‚ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†æ—¶ç©ºèµ„æºç«äº‰æœºåˆ¶ï¼Œå³ç¥ç»å…ƒåœ¨æœ‰é™èŒƒå›´å†…åŒæ—¶ç¼–ç è¯­ä¹‰å’ŒåŠ¨æ€ä¿¡æ¯ï¼Œæ—¶é—´çŠ¶æ€æ¶ˆè€—äº†ç©ºé—´å­¦ä¹ å®¹é‡ï¼›è¯¥å·¥ä½œä¸ºSNNçš„æ—¶åºæœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œå¹¶ä¸ºè§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºå»ºæ¨¡æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†æ—¶ç©ºè¡¨ç¤ºå­¦ä¹ çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Spiking Neural Networks (SNNs) are considered naturally suited for temporal processing, with membrane potential propagation widely regarded as the core temporal modeling mechanism. However, existing research lack analysis of its actual contributions in complex temporal tasks. We design Non-Stateful (NS) models progressively removing membrane propagation to quantify its stage-wise role. Experiments reveal a counterintuitive phenomenon: moderate removal in shallow or deep layers improves performance, while excessive removal causes collapse. We attribute this to spatio-temporal resource competition where neurons encode both semantics and dynamics within limited range, with temporal state consuming capacity for spatial learning. Based on this, we propose Spatial-Temporal Separable Network (STSep), decoupling residual blocks into independent spatial and temporal branches. The spatial branch focuses on semantic extraction while the temporal branch captures motion through explicit temporal differences. Experiments on Something-Something V2, UCF101, and HMDB51 show STSep achieves superior performance, with retrieval task and attention analysis confirming focus on motion rather than static appearance. This work provides new perspectives on SNNs' temporal mechanisms and an effective solution for spatiotemporal modeling in video understanding.</p>
<h3 id="4-eventqueues-autodifferentiable-spike-event-queues-for-brain-simulation-on-ai-accelerators">[4] <a href="https://arxiv.org/abs/2512.05906">EventQueues: Autodifferentiable spike event queues for brain simulation on AI accelerators</a></h3>
<p><em>Lennart P. L. Landsmeer, Amirreza Movahedin, Said Hamdioui, Christos Strydis</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºäº‹ä»¶é˜Ÿåˆ—çš„è„‰å†²ç¥ç»ç½‘ç»œæ¢¯åº¦è®¡ç®—æ–¹æ³•ï¼Œå®ç°äº†åŒ…å«å»¶è¿Ÿçš„é«˜æ•ˆå†…å­˜ç®¡ç†ï¼Œå¹¶åœ¨å¤šç§ç¡¬ä»¶å¹³å°ä¸Šè¿›è¡Œäº†æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†ä¸åŒé˜Ÿåˆ—è®¾è®¡å¯¹æ€§èƒ½çš„å…³é”®å½±å“ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è„‰å†²ç¥ç»ç½‘ç»œçš„é«˜æ•ˆæ¨¡æ‹Ÿå’ŒåŸºäºæ¢¯åº¦çš„è®­ç»ƒé¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰AIåŠ é€Ÿå™¨è™½èƒ½æä¾›åŠ é€Ÿï¼Œä½†æ¢¯åº¦è®¡ç®—é€šå¸¸ä½¿ç”¨å†…å­˜å¯†é›†çš„ç¨ å¯†æ•°æ®ç»“æ„å®ç°ç¨€ç–è„‰å†²äº‹ä»¶ï¼Œç°æœ‰ç²¾ç¡®æ¢¯åº¦æ–¹æ³•ç¼ºä¹é€šç”¨æ€§ï¼Œä¸”å½“å‰æ¨¡æ‹Ÿå™¨ç»å¸¸å¿½ç•¥æˆ–ä½æ•ˆå¤„ç†å»¶è¿Ÿè„‰å†²ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é€šè¿‡è„‰å†²äº‹ä»¶é˜Ÿåˆ—æ¨å¯¼æ¢¯åº¦è®¡ç®—ï¼ŒåŒ…å«å»¶è¿Ÿå¤„ç†ï¼Œå¹¶å®ç°äº†å†…å­˜é«˜æ•ˆã€æ”¯æŒæ¢¯åº¦çš„äº‹ä»¶é˜Ÿåˆ—æ•°æ®ç»“æ„ï¼Œåœ¨CPUã€GPUã€TPUå’ŒLPUå¹³å°ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ¢ç´¢äº†é€‰æ‹©æ€§è„‰å†²ä¸¢å¼ƒä½œä¸ºæ€§èƒ½-ç²¾åº¦æƒè¡¡çš„ç®€å•æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> é˜Ÿåˆ—è®¾è®¡å¯¹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼šCPUåœ¨ä¼ ç»Ÿæ ‘ç»“æ„æˆ–FIFOå®ç°ä¸­è¡¨ç°è‰¯å¥½ï¼ŒGPUåœ¨è¾ƒå°æ¨¡æ‹Ÿä¸­ç¯å½¢ç¼“å†²åŒºè¡¨ç°ä¼˜å¼‚ä½†åœ¨é«˜å†…å­˜å‹åŠ›ä¸‹åå¥½ç¨€ç–æ•°æ®ç»“æ„ï¼ŒTPUå€¾å‘äºåŸºäºæ’åºå†…åœ¨å‡½æ•°çš„å®ç°ï¼Œé€‰æ‹©æ€§è„‰å†²ä¸¢å¼ƒæä¾›äº†æœ‰æ•ˆçš„æ€§èƒ½-ç²¾åº¦æƒè¡¡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†ä¸åŒç¡¬ä»¶å¹³å°å¯¹è„‰å†²ç¥ç»ç½‘ç»œäº‹ä»¶é˜Ÿåˆ—æ•°æ®ç»“æ„çš„åå¥½å·®å¼‚ï¼Œä¸ºæœªæ¥è‡ªåŠ¨å¾®åˆ†æ¡†æ¶è®¾è®¡æä¾›äº†é‡è¦å‚è€ƒï¼Œå»ºè®®æ¡†æ¶åº”é€‚åº”ä¸åŒçš„åŸå§‹/åˆ‡å‘æ•°æ®ç»“æ„ä»¥å®ç°æ›´ä¼˜çš„æ€§èƒ½-ç²¾åº¦æƒè¡¡ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Spiking neural networks (SNNs), central to computational neuroscience and neuromorphic machine learning (ML), require efficient simulation and gradient-based training. While AI accelerators offer promising speedups, gradient-based SNNs typically implement sparse spike events using dense, memory-heavy data-structures. Existing exact gradient methods lack generality, and current simulators often omit or inefficiently handle delayed spikes. We address this by deriving gradient computation through spike event queues, including delays, and implementing memory-efficient, gradient-enabled event queue structures. These are benchmarked across CPU, GPU, TPU, and LPU platforms. We find that queue design strongly shapes performance. CPUs, as expected, perform well with traditional tree-based or FIFO implementations, while GPUs excel with ring buffers for smaller simulations, yet under higher memory pressure prefer more sparse data-structures. TPUs seem to favor an implementation based on sorting intrinsics. Selective spike dropping provides a simple performance-accuracy trade-off, which could be enhanced by future autograd frameworks adapting diverging primal/tangent data-structures.</p>
  </article>
</body>
</html>
