<div id=toc></div>

# Table of Contents

- [cs.NE](#cs.NE) [Total: 4]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [1] [STAER: Temporal Aligned Rehearsal for Continual Spiking Neural Network](https://arxiv.org/abs/2601.20870)
*Matteo Gianferrari, Omayma Moussadek, Riccardo Salami, Cosimo Fiorini, Lorenzo Tartarini, Daniela Gandolfi, Simone Calderara*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†STAERæ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼ä¿æŒè„‰å†²æ—¶åºç»“æ„æ¥è§£å†³è„‰å†²ç¥ç»ç½‘ç»œåœ¨ç±»å¢é‡å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå®ç°äº†ä¸äººå·¥ç¥ç»ç½‘ç»œç›¸åª²ç¾çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿç‰©åˆç†çš„åŠ¨æ€ç‰¹æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è„‰å†²ç¥ç»ç½‘ç»œè™½ç„¶å¤©ç”Ÿé€‚åˆè¿ç»­å­¦ä¹ ï¼Œä½†åœ¨ç±»å¢é‡å­¦ä¹ åº”ç”¨ä¸­å—åˆ°ç¾éš¾æ€§é—å¿˜å’Œè„‰å†²æ¨¡å¼æ—¶åºé”™ä½çš„é˜»ç¢ï¼Œå¯¼è‡´å…¶æ€§èƒ½ä¸äººå·¥ç¥ç»ç½‘ç»œå­˜åœ¨æ˜¾è‘—å·®è·ï¼Œéœ€è¦ä¸“é—¨çš„æ–¹æ³•æ¥ä¿æŒæ—¶åºç»“æ„å¹¶æå‡è¡¨ç¤ºç¨³å®šæ€§ã€‚

**Method:** æå‡ºäº†STAERæ¡†æ¶ï¼Œé›†æˆäº†å¯å¾®åˆ†çš„Soft-DTWå¯¹é½æŸå¤±æ¥ä¿æŒè„‰å†²æ—¶åºä¿çœŸåº¦ï¼Œå¹¶é‡‡ç”¨è¾“å‡ºlogitsçš„æ—¶åºæ‰©å±•å’Œæ”¶ç¼©æœºåˆ¶æ¥å¢å¼ºé²æ£’è¡¨ç¤ºå­¦ä¹ ï¼Œè¯¥æ–¹æ³•åŸºäºæ·±åº¦ResNet19è„‰å†²éª¨å¹²ç½‘ç»œå®ç°ã€‚

**Result:** åœ¨Sequential-MNISTå’ŒSequential-CIFAR10åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç»éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åŒ¹é…æˆ–è¶…è¶Šäº†å¼ºäººå·¥ç¥ç»ç½‘ç»œåŸºçº¿ï¼ˆERã€DER++ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿç‰©åˆç†çš„åŠ¨æ€ç‰¹æ€§ï¼Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®æ˜¾å¼æ—¶åºå¯¹é½å¯¹è¡¨ç¤ºç¨³å®šæ€§è‡³å…³é‡è¦ã€‚

**Conclusion:** STAERæ¡†æ¶é€šè¿‡æ˜¾å¼ä¿æŒè„‰å†²æ—¶åºç»“æ„æˆåŠŸè§£å†³äº†è„‰å†²ç¥ç»ç½‘ç»œåœ¨ç±»å¢é‡å­¦ä¹ ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºè„‰å†²åŸç”Ÿç»ˆèº«å­¦ä¹ æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†æ—¶åºå¯¹é½åœ¨ä¿æŒè¡¨ç¤ºç¨³å®šæ€§ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºç”Ÿç‰©åˆç†è®¡ç®—ä¸å®ç”¨æœºå™¨å­¦ä¹ æ€§èƒ½çš„èåˆå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Spiking Neural Networks (SNNs) are inherently suited for continuous learning due to their event-driven temporal dynamics; however, their application to Class-Incremental Learning (CIL) has been hindered by catastrophic forgetting and the temporal misalignment of spike patterns. In this work, we introduce Spiking Temporal Alignment with Experience Replay (STAER), a novel framework that explicitly preserves temporal structure to bridge the performance gap between SNNs and ANNs. Our approach integrates a differentiable Soft-DTW alignment loss to maintain spike timing fidelity and employs a temporal expansion and contraction mechanism on output logits to enforce robust representation learning. Implemented on a deep ResNet19 spiking backbone, STAER achieves state-of-the-art performance on Sequential-MNIST and Sequential-CIFAR10. Empirical results demonstrate that our method matches or outperforms strong ANN baselines (ER, DER++) while preserving biologically plausible dynamics. Ablation studies further confirm that explicit temporal alignment is critical for representational stability, positioning STAER as a scalable solution for spike-native lifelong learning. Code is available at https://github.com/matteogianferrari/staer.


### [2] [NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with Surrogate-Free Training](https://arxiv.org/abs/2601.21279)
*Zhengzheng Tang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†NEXUSæ¡†æ¶ï¼Œé¦–æ¬¡å®ç°äº†ANNåˆ°SNNçš„æ¯”ç‰¹ç²¾ç¡®ç­‰ä»·è½¬æ¢ï¼Œé€šè¿‡çº¯IFç¥ç»å…ƒé€»è¾‘é—¨æ„å»ºIEEE-754å…¼å®¹æµ®ç‚¹è¿ç®—ï¼Œåœ¨ä¿æŒç›¸åŒç²¾åº¦çš„åŒæ—¶å®ç°æ˜¾è‘—çš„èƒ½æ•ˆæå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰SNNæ–¹æ³•é€šè¿‡ç¦»æ•£è„‰å†²è¿‘ä¼¼è¿ç»­å€¼å¯¼è‡´ç²¾åº¦æŸå¤±ï¼Œæ— æ³•å®ç°ä¸ANNçš„ç²¾ç¡®ç­‰ä»·ï¼Œè¿™é™åˆ¶äº†SNNåœ¨èƒ½æ•ˆè®¡ç®—ä¸­çš„å®é™…åº”ç”¨æ½œåŠ›ï¼Œéœ€è¦ä¸€ç§èƒ½ä¿æŒæ•°å­¦ç­‰ä»·æ€§çš„è½¬æ¢æ¡†æ¶ã€‚

**Method:** NEXUSæ¡†æ¶é‡‡ç”¨ç©ºé—´æ¯”ç‰¹ç¼–ç å®ç°é›¶ç¼–ç è¯¯å·®ï¼Œé€šè¿‡çº¯IFç¥ç»å…ƒé€»è¾‘é—¨æ„å»ºIEEE-754å…¼å®¹æµ®ç‚¹ç®—æœ¯è¿ç®—ï¼ŒåŒ…æ‹¬çº¿æ€§å’Œéçº¿æ€§æ“ä½œï¼Œé‡‡ç”¨åˆ†å±‚ç¥ç»å½¢æ€é—¨ç”µè·¯è®¾è®¡å’Œæ— ä»£ç†STEè®­ç»ƒå®ç°ç²¾ç¡®èº«ä»½æ˜ å°„ã€‚

**Result:** åœ¨LLaMA-2 70Bç­‰æ¨¡å‹ä¸Šå®ç°é›¶ç²¾åº¦æŸå¤±ï¼ˆ0.00%é€€åŒ–ï¼‰ï¼Œå¹³å‡ULPè¯¯å·®ä»…ä¸º6.19ï¼Œåœ¨ç¥ç»å½¢æ€ç¡¬ä»¶ä¸Šå®ç°27-168,000å€èƒ½æ•ˆæå‡ï¼Œå•æ—¶é—´æ­¥è®¾è®¡ä½¿å…¶å¯¹è†œç”µä½æ³„æ¼å®Œå…¨å…ç–«ï¼Œåœ¨Ïƒ=0.2çš„çªè§¦å™ªå£°ä¸‹ä¿æŒ>98%é—¨çº§ç²¾åº¦ã€‚

**Conclusion:** è¯¥ç ”ç©¶é¦–æ¬¡è¯æ˜äº†ANNåˆ°SNNçš„æ¯”ç‰¹ç²¾ç¡®ç­‰ä»·è½¬æ¢å¯è¡Œæ€§ï¼Œä¸ºç¥ç»å½¢æ€è®¡ç®—æä¾›äº†ç†è®ºä¿è¯ï¼Œå…¶å•æ—¶é—´æ­¥è®¾è®¡è§£å†³äº†ä¼ ç»ŸSNNçš„æ—¶åºç›¸å…³æ€§é—®é¢˜ï¼Œä¸ºé«˜æ•ˆAIç¡¬ä»¶éƒ¨ç½²å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Spiking Neural Networks (SNNs) promise energy-efficient computing through event-driven sparsity, yet all existing approaches sacrifice accuracy by approximating continuous values with discrete spikes. We propose NEXUS, a framework that achieves bit-exact ANN-to-SNN equivalence -- not approximate, but mathematically identical outputs. Our key insight is constructing all arithmetic operations, both linear and nonlinear, from pure IF neuron logic gates that implement IEEE-754 compliant floating-point arithmetic. Through spatial bit encoding (zero encoding error by construction), hierarchical neuromorphic gate circuits (from basic logic gates to complete transformer layers), and surrogate-free STE training (exact identity mapping rather than heuristic approximation), NEXUS produces outputs identical to standard ANNs up to machine precision. Experiments on models up to LLaMA-2 70B demonstrate identical task accuracy (0.00\% degradation) with mean ULP error of only 6.19, while achieving 27-168,000$\times$ energy reduction on neuromorphic hardware. Crucially, spatial bit encoding's single-timestep design renders the framework inherently immune to membrane potential leakage (100\% accuracy across all decay factors $Î²\in[0.1,1.0]$), while tolerating synaptic noise up to $Ïƒ=0.2$ with >98\% gate-level accuracy.


### [3] [Error Amplification Limits ANN-to-SNN Conversion in Continuous Control](https://arxiv.org/abs/2601.21778)
*Zijie Xu, Zihan Huang, Yiting Dong, Kang Chen, Wenxuan Liu, Zhaofei Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè·¨æ­¥æ®‹å·®ç”µä½åˆå§‹åŒ–ï¼ˆCRPIï¼‰çš„è½»é‡çº§å…è®­ç»ƒæœºåˆ¶ï¼Œç”¨äºè§£å†³è„‰å†²ç¥ç»ç½‘ç»œåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è½¬æ¢æ€§èƒ½å·®çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è·¨å†³ç­–æ­¥ä¼ é€’æ®‹å·®è†œç”µä½æ¥æŠ‘åˆ¶æ—¶é—´ç›¸å…³çš„è¯¯å·®æ”¾å¤§ï¼Œæ˜¾è‘—æå‡äº†ANNåˆ°SNNè½¬æ¢åœ¨è¿ç»­æ§åˆ¶åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„äººå·¥ç¥ç»ç½‘ç»œåˆ°è„‰å†²ç¥ç»ç½‘ç»œè½¬æ¢æ–¹æ³•åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦åŸå› æ˜¯è¯¯å·®æ”¾å¤§é—®é¢˜ï¼šå°çš„åŠ¨ä½œè¿‘ä¼¼è¯¯å·®åœ¨å†³ç­–æ­¥éª¤é—´äº§ç”Ÿæ—¶é—´ç›¸å…³æ€§ï¼Œå¯¼è‡´ç´¯ç§¯çŠ¶æ€åˆ†å¸ƒåç§»å’Œä¸¥é‡çš„æ€§èƒ½ä¸‹é™ã€‚ç”±äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆæœ¬é«˜ä¸”å¯èƒ½ä¸å®‰å…¨ï¼Œéœ€è¦æœ‰æ•ˆçš„å…è®­ç»ƒè½¬æ¢æ–¹æ³•æ¥è§£å†³è¿™ä¸€å…³é”®æŒ‘æˆ˜ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†è·¨æ­¥æ®‹å·®ç”µä½åˆå§‹åŒ–ï¼ˆCRPIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„å…è®­ç»ƒæœºåˆ¶ï¼Œé€šè¿‡è·¨å†³ç­–æ­¥éª¤ä¼ é€’æ®‹å·®è†œç”µä½æ¥æŠ‘åˆ¶æ—¶é—´ç›¸å…³çš„è¯¯å·®ã€‚CRPIå¯ä»¥é›†æˆåˆ°ç°æœ‰çš„è½¬æ¢æµç¨‹ä¸­ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆæœ¬ï¼Œä¸“é—¨é’ˆå¯¹è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¯¯å·®æ”¾å¤§çš„é—®é¢˜è®¾è®¡ï¼Œé€šè¿‡ä¿æŒç”µä½è¿ç»­æ€§æ¥å‡å°‘ç´¯ç§¯è¯¯å·®ã€‚

**Result:** åœ¨åŒ…å«å‘é‡å’Œè§†è§‰è§‚å¯Ÿçš„è¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCRPIæ˜¾è‘—æ¢å¤äº†è½¬æ¢è¿‡ç¨‹ä¸­ä¸¢å¤±çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆé›†æˆåˆ°ç°æœ‰çš„è½¬æ¢æµç¨‹ä¸­ï¼Œå¹¶å¤§å¹…æ”¹å–„è„‰å†²ç¥ç»ç½‘ç»œåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒéªŒè¯äº†CRPIæœºåˆ¶å¯¹æŠ‘åˆ¶æ—¶é—´ç›¸å…³è¯¯å·®çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¿ç»­æ§åˆ¶æ˜¯ANNåˆ°SNNè½¬æ¢çš„å…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œå…¶ä¸­å°è¯¯å·®ä¼šè¢«å¼ºçƒˆæ”¾å¤§å¹¶å½±å“æ€§èƒ½ã€‚CRPIæœºåˆ¶ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†åœ¨å¼ºåŒ–å­¦ä¹ ç­‰åº”ç”¨ä¸­å¼€å‘é²æ£’è½¬æ¢æ–¹æ³•çš„é‡è¦æ€§ï¼Œä¸ºè„‰å†²ç¥ç»ç½‘ç»œåœ¨è¿ç»­æ§åˆ¶é¢†åŸŸçš„å®é™…åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚

---

#### ğŸ“„ Abstract
Spiking Neural Networks (SNNs) can achieve competitive performance by converting already existing well-trained Artificial Neural Networks (ANNs), avoiding further costly training. This property is particularly attractive in Reinforcement Learning (RL), where training through environment interaction is expensive and potentially unsafe. However, existing conversion methods perform poorly in continuous control, where suitable baselines are largely absent. We identify error amplification as the key cause: small action approximation errors become temporally correlated across decision steps, inducing cumulative state distribution shift and severe performance degradation. To address this issue, we propose Cross-Step Residual Potential Initialization (CRPI), a lightweight training-free mechanism that carries over residual membrane potentials across decision steps to suppress temporally correlated errors. Experiments on continuous control benchmarks with both vector and visual observations demonstrate that CRPI can be integrated into existing conversion pipelines and substantially recovers lost performance. Our results highlight continuous control as a critical and challenging benchmark for ANN-to-SNN conversion, where small errors can be strongly amplified and impact performance.


### [4] [General Self-Prediction Enhancement for Spiking Neurons](https://arxiv.org/abs/2601.21823)
*Zihan Huang, Zijie Xu, Yihan Huang, Shanshan Jia, Tong Bu, Yiting Dong, Wenxuan Liu, Jianhao Ding, Zhaofei Yu, Tiejun Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé¢„æµ‹å¢å¼ºçš„è„‰å†²ç¥ç»å…ƒæ–¹æ³•ï¼Œé€šè¿‡ä»è¾“å…¥è¾“å‡ºå†å²ç”Ÿæˆå†…éƒ¨é¢„æµ‹ç”µæµæ¥è°ƒåˆ¶è†œç”µä½ï¼Œæ—¢åˆ›å»ºäº†è¿ç»­æ¢¯åº¦è·¯å¾„ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œåˆç¬¦åˆç”Ÿç‰©é¢„æµ‹ç¼–ç åŸç†ï¼Œä»è€Œæå‡äº†SNNsçš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è„‰å†²ç¥ç»ç½‘ç»œè™½ç„¶å…·æœ‰äº‹ä»¶é©±åŠ¨ç¨€ç–è®¡ç®—çš„é«˜èƒ½æ•ˆä¼˜åŠ¿ï¼Œä½†å…¶è®­ç»ƒé¢ä¸´è„‰å†²ä¸å¯å¾®æ€§ä»¥åŠæ€§èƒ½ã€æ•ˆç‡å’Œç”Ÿç‰©åˆç†æ€§ä¹‹é—´çš„æƒè¡¡æŒ‘æˆ˜ã€‚ä¸»æµSNNså¿½ç•¥äº†é¢„æµ‹ç¼–ç è¿™ä¸€å¤§è„‘æ ¸å¿ƒçš®å±‚æœºåˆ¶ï¼Œå³å¤§è„‘é€šè¿‡é¢„æµ‹è¾“å…¥å¹¶ç¼–ç è¯¯å·®æ¥å®ç°é«˜æ•ˆæ„ŸçŸ¥ï¼Œè¿™æ„æˆäº†ç ”ç©¶çš„å…³é”®åŠ¨æœºã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé¢„æµ‹å¢å¼ºçš„è„‰å†²ç¥ç»å…ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»ç¥ç»å…ƒçš„è¾“å…¥è¾“å‡ºå†å²ä¸­ç”Ÿæˆå†…éƒ¨é¢„æµ‹ç”µæµæ¥è°ƒåˆ¶è†œç”µä½ã€‚è¿™ç§è®¾è®¡åˆ›é€ äº†è¿ç»­çš„æ¢¯åº¦è·¯å¾„ï¼Œç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å¹¶æå‡äº†è®­ç»ƒç¨³å®šæ€§ï¼ŒåŒæ—¶ç¬¦åˆç”Ÿç‰©åŸç†ï¼Œç±»ä¼¼äºè¿œç«¯æ ‘çªè°ƒåˆ¶å’Œè¯¯å·®é©±åŠ¨çš„çªè§¦å¯å¡‘æ€§æœºåˆ¶ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒæ¶æ„ã€ç¥ç»å…ƒç±»å‹ã€æ—¶é—´æ­¥é•¿å’Œä»»åŠ¡ä¸Šå‡èƒ½å¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶å¢å¼ºSNNsçš„å¹¿æ³›é€‚ç”¨æ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†è®­ç»ƒç¨³å®šæ€§å’Œå‡†ç¡®æ€§ï¼Œè¿˜åœ¨ä¿æŒç”Ÿç‰©åˆç†æ€§çš„åŒæ—¶æ˜¾è‘—æ”¹å–„äº†ç½‘ç»œæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡å°†ç”Ÿç‰©é¢„æµ‹ç¼–ç æœºåˆ¶èå…¥SNNsè®¾è®¡ï¼ŒæˆåŠŸè§£å†³äº†è®­ç»ƒä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å¹¶æå‡äº†æ€§èƒ½ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´ç”Ÿç‰©åˆç†çš„è„‰å†²ç¥ç»ç½‘ç»œæä¾›äº†æ–°æ€è·¯ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†å°†ç¥ç»ç§‘å­¦åŸç†ä¸æ·±åº¦å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆçš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæœªæ¥SNNsçš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Spiking Neural Networks (SNNs) are highly energy-efficient due to event-driven, sparse computation, but their training is challenged by spike non-differentiability and trade-offs among performance, efficiency, and biological plausibility. Crucially, mainstream SNNs ignore predictive coding, a core cortical mechanism where the brain predicts inputs and encodes errors for efficient perception. Inspired by this, we propose a self-prediction enhanced spiking neuron method that generates an internal prediction current from its input-output history to modulate membrane potential. This design offers dual advantages, it creates a continuous gradient path that alleviates vanishing gradients and boosts training stability and accuracy, while also aligning with biological principles, which resembles distal dendritic modulation and error-driven synaptic plasticity. Experiments show consistent performance gains across diverse architectures, neuron types, time steps, and tasks demonstrating broad applicability for enhancing SNNs.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [MAR: Efficient Large Language Models via Module-aware Architecture Refinement](https://arxiv.org/abs/2601.21503)
*Junhong Cai, Guiqin Wang, Kejie Zhao, Jianxiong Tang, Xiang Wang, Luziwei Leng, Ran Cheng, Yuxin Ma, Qinghai Guo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºæ¨¡å—æ„ŸçŸ¥æ¶æ„ä¼˜åŒ–ï¼ˆMARï¼‰æ¡†æ¶ï¼Œé€šè¿‡é›†æˆçŠ¶æ€ç©ºé—´æ¨¡å‹å®ç°çº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡ï¼Œå¹¶ç»“åˆæ¿€æ´»ç¨€ç–åŒ–ä¸è‡ªé€‚åº”ä¸‰å…ƒå¤šæ­¥ç¥ç»å…ƒè®¾è®¡ï¼Œæ˜¾è‘—é™ä½å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½è€—åŒæ—¶ä¿æŒæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†é¢ä¸´äºŒæ¬¡æ³¨æ„åŠ›æœºåˆ¶å’Œå¯†é›†å‰é¦ˆç½‘ç»œå¸¦æ¥çš„é«˜èƒ½è€—é—®é¢˜ï¼Œç°æœ‰é«˜æ•ˆæ¨¡å‹åœ¨ä¿¡æ¯å¯†åº¦å’Œæ—¶åºåŒ¹é…æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ–°çš„æ¶æ„ä¼˜åŒ–æ–¹æ³•ã€‚

**Method:** æå‡ºæ¨¡å—æ„ŸçŸ¥æ¶æ„ä¼˜åŒ–ï¼ˆMARï¼‰ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé›†æˆçŠ¶æ€ç©ºé—´æ¨¡å‹å®ç°çº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡ï¼Œåº”ç”¨æ¿€æ´»ç¨€ç–åŒ–é™ä½å‰é¦ˆç½‘ç»œæˆæœ¬ï¼Œè®¾è®¡è‡ªé€‚åº”ä¸‰å…ƒå¤šæ­¥ç¥ç»å…ƒï¼ˆATMNï¼‰å’Œè„‰å†²æ„ŸçŸ¥åŒå‘è’¸é¦ç­–ç•¥ï¼ˆSBDSï¼‰è§£å†³è„‰å†²ç¥ç»ç½‘ç»œä¸çŠ¶æ€ç©ºé—´æ¨¡å‹é›†æˆæ—¶çš„ä¿¡æ¯å¯†åº¦å’Œæ—¶åºå¤±é…é—®é¢˜ã€‚

**Result:** å®éªŒè¡¨æ˜MARåœ¨èµ„æºå—é™æ¡ä»¶ä¸‹æœ‰æ•ˆæ¢å¤äº†å¯†é›†å¯¹åº”æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½æ¨ç†èƒ½è€—ï¼Œåœ¨å¯æ¯”æˆ–æ›´å¤§è§„æ¨¡çš„é«˜æ•ˆæ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶å®é™…åº”ç”¨æ½œåŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡æ¶æ„ä¼˜åŒ–å’Œæ–°å‹ç¥ç»å…ƒè®¾è®¡å®ç°é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä¸ºæ„å»ºå®ç”¨ä¸”èŠ‚èƒ½çš„è¯­è¨€æ¨¡å‹æä¾›äº†æ–°æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨çº¿æ€§æ—¶é—´å»ºæ¨¡ä¸ç¨€ç–æ¿€æ´»çš„ååŒä¼˜åŒ–æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs.
