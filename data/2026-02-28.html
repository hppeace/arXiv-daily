<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-02-28.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 3]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-locally-adaptive-decay-surfaces-for-high-speed-face-and-landmark-detection-with-event-cameras">[1] <a href="https://arxiv.org/abs/2602.23101">Locally Adaptive Decay Surfaces for High-Speed Face and Landmark Detection with Event Cameras</a></h3>
<p><em>Paul Kielty, Timothy Hanley, Peter Corcoran</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å±€éƒ¨è‡ªé€‚åº”è¡°å‡è¡¨é¢ï¼ˆLADSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹äº‹ä»¶ç›¸æœºè¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡æ ¹æ®å±€éƒ¨ä¿¡å·åŠ¨æ€è°ƒåˆ¶æ¯ä¸ªä½ç½®çš„æ—¶é—´è¡°å‡ï¼Œè§£å†³äº†ä¼ ç»Ÿå›ºå®šæ—¶é—´å‚æ•°è¡¨ç¤ºåœ¨é™æ€åŒºåŸŸå’Œå¿«é€Ÿè¿åŠ¨åŒºåŸŸä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äº‹ä»¶ç›¸æœºä»¥å¾®ç§’çº§åˆ†è¾¨ç‡è®°å½•äº®åº¦å˜åŒ–ï¼Œä½†å°†å…¶ç¨€ç–å¼‚æ­¥è¾“å‡ºè½¬æ¢ä¸ºç¥ç»ç½‘ç»œå¯åˆ©ç”¨çš„å¯†é›†å¼ é‡ä»æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¼ ç»Ÿç›´æ–¹å›¾æˆ–å…¨å±€è¡°å‡æ—¶é—´è¡¨é¢è¡¨ç¤ºåœ¨æ•´ä¸ªå›¾åƒå¹³é¢ä¸Šåº”ç”¨å›ºå®šçš„æ—¶é—´å‚æ•°ï¼Œè¿™åœ¨å®é™…ä¸­é€ æˆäº†åœ¨é™æ­¢æœŸé—´ä¿ç•™ç©ºé—´ç»“æ„ä¸åœ¨å¿«é€Ÿè¿åŠ¨æœŸé—´ä¿ç•™æ¸…æ™°è¾¹ç¼˜ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡å¼•å…¥äº†å±€éƒ¨è‡ªé€‚åº”è¡°å‡è¡¨é¢ï¼ˆLADSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§äº‹ä»¶è¡¨ç¤ºæ–¹æ³•å®¶æ—ï¼Œå…¶ä¸­æ¯ä¸ªä½ç½®çš„æ—¶é—´è¡°å‡æ ¹æ®å±€éƒ¨ä¿¡å·åŠ¨æ€è¿›è¡Œè°ƒåˆ¶ã€‚æ¢ç´¢äº†ä¸‰ç§ç­–ç•¥ï¼šåŸºäºäº‹ä»¶ç‡ã€æ‹‰æ™®æ‹‰æ–¯-é«˜æ–¯å“åº”å’Œé«˜é¢‘é¢‘è°±èƒ½é‡ã€‚è¿™äº›è‡ªé€‚åº”æ–¹æ¡ˆåœ¨é™æ­¢åŒºåŸŸä¿ç•™ç»†èŠ‚ï¼ŒåŒæ—¶åœ¨å¯†é›†æ´»åŠ¨åŒºåŸŸå‡å°‘æ¨¡ç³Šã€‚</p>
<p><strong>Result:</strong> åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLADSç›¸æ¯”æ ‡å‡†éè‡ªé€‚åº”è¡¨ç¤ºæŒç»­æ”¹å–„äº†äººè„¸æ£€æµ‹å’Œé¢éƒ¨å…³é”®ç‚¹å‡†ç¡®æ€§ã€‚åœ¨30Hzä¸‹ï¼ŒLADSå®ç°äº†æ¯”åŸºçº¿æ›´é«˜çš„æ£€æµ‹å‡†ç¡®æ€§å’Œæ›´ä½çš„å…³é”®ç‚¹è¯¯å·®ï¼›åœ¨240Hzä¸‹ï¼Œå®ƒç¼“è§£äº†é€šå¸¸åœ¨é«˜é¢‘ä¸‹è§‚å¯Ÿåˆ°çš„å‡†ç¡®æ€§ä¸‹é™ï¼Œç»´æŒäº†2.44%çš„å½’ä¸€åŒ–å¹³å‡è¯¯å·®ç”¨äºå…³é”®ç‚¹å’Œ0.966 mAP50ç”¨äºäººè„¸æ£€æµ‹ã€‚è¿™äº›é«˜é¢‘ç»“æœç”šè‡³è¶…è¿‡äº†å…ˆå‰å·¥ä½œåœ¨30Hzä¸‹æŠ¥å‘Šçš„å‡†ç¡®æ€§ï¼Œä¸ºåŸºäºäº‹ä»¶çš„äººè„¸åˆ†æè®¾å®šäº†æ–°åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡åœ¨è¡¨ç¤ºé˜¶æ®µä¿ç•™ç©ºé—´ç»“æ„ï¼ŒLADSæ”¯æŒä½¿ç”¨æ›´è½»é‡çš„ç½‘ç»œæ¶æ„ï¼ŒåŒæ—¶ä»ä¿æŒå®æ—¶æ€§èƒ½ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ—¶é—´é›†æˆå¯¹äºç¥ç»å½¢æ€è§†è§‰çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‘åˆ©ç”¨äº‹ä»¶ç›¸æœºç‹¬ç‰¹ä¼˜åŠ¿çš„å®æ—¶é«˜é¢‘äººæœºäº¤äº’ç³»ç»Ÿã€‚è¯¥æ–¹æ³•ä¸ºäº‹ä»¶ç›¸æœºè¡¨ç¤ºæä¾›äº†ä¸€ç§è‡ªé€‚åº”æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒè¿åŠ¨æ¡ä»¶ä¸‹ä¼˜åŒ–æ—¶ç©ºä¿¡æ¯ä¿ç•™ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Event cameras record luminance changes with microsecond resolution, but converting their sparse, asynchronous output into dense tensors that neural networks can exploit remains a core challenge. Conventional histograms or globally-decayed time-surface representations apply fixed temporal parameters across the entire image plane, which in practice creates a trade-off between preserving spatial structure during still periods and retaining sharp edges during rapid motion. We introduce Locally Adaptive Decay Surfaces (LADS), a family of event representations in which the temporal decay at each location is modulated according to local signal dynamics. Three strategies are explored, based on event rate, Laplacian-of-Gaussian response, and high-frequency spectral energy. These adaptive schemes preserve detail in quiescent regions while reducing blur in regions of dense activity. Extensive experiments on the public data show that LADS consistently improves both face detection and facial landmark accuracy compared to standard non-adaptive representations. At 30 Hz, LADS achieves higher detection accuracy and lower landmark error than either baseline, and at 240 Hz it mitigates the accuracy decline typically observed at higher frequencies, sustaining 2.44 % normalized mean error for landmarks and 0.966 mAP50 in face detection. These high-frequency results even surpass the accuracy reported in prior works operating at 30 Hz, setting new benchmarks for event-based face analysis. Moreover, by preserving spatial structure at the representation stage, LADS supports the use of much lighter network architectures while still retaining real-time performance. These results highlight the importance of context-aware temporal integration for neuromorphic vision and point toward real-time, high-frequency human-computer interaction systems that exploit the unique advantages of event cameras.</p>
<h3 id="2-motion-aware-event-suppression-for-event-cameras">[2] <a href="https://arxiv.org/abs/2602.23204">Motion-aware Event Suppression for Event Cameras</a></h3>
<p><em>Roberto Pellerito, Nico Messikommer, Giovanni Cioffi, Marco Cannici, Davide Scaramuzza</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªè¿åŠ¨æ„ŸçŸ¥äº‹ä»¶æŠ‘åˆ¶æ¡†æ¶ï¼Œèƒ½å¤Ÿå®æ—¶è¿‡æ»¤ç”±ç‹¬ç«‹è¿åŠ¨ç‰©ä½“å’Œè‡ªèº«è¿åŠ¨è§¦å‘çš„äº‹ä»¶ã€‚è¯¥æ¨¡å‹åŒæ—¶åˆ†å‰²å½“å‰äº‹ä»¶æµä¸­çš„ç‹¬ç«‹è¿åŠ¨ç‰©ä½“å¹¶é¢„æµ‹å…¶æœªæ¥è¿åŠ¨ï¼Œå®ç°äº†å¯¹åŠ¨æ€äº‹ä»¶çš„é¢„è§æ€§æŠ‘åˆ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äº‹ä»¶ç›¸æœºåœ¨åŠ¨æ€åœºæ™¯ä¸­é¢ä¸´ç‹¬ç«‹è¿åŠ¨ç‰©ä½“å’Œè‡ªèº«è¿åŠ¨è§¦å‘çš„å¤§é‡å†—ä½™äº‹ä»¶å¹²æ‰°ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å®æ—¶æœ‰æ•ˆåœ°è¿‡æ»¤è¿™äº›åŠ¨æ€äº‹ä»¶ï¼Œé™åˆ¶äº†äº‹ä»¶ç›¸æœºåœ¨å®æ—¶åº”ç”¨ä¸­çš„æ€§èƒ½è¡¨ç°å’Œä¸‹æ¸¸ä»»åŠ¡æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶é‡‡ç”¨è½»é‡çº§æ¶æ„å®ç°è¿åŠ¨æ„ŸçŸ¥äº‹ä»¶æŠ‘åˆ¶ï¼Œé€šè¿‡è”åˆåˆ†å‰²å½“å‰äº‹ä»¶æµä¸­çš„ç‹¬ç«‹è¿åŠ¨ç‰©ä½“å¹¶é¢„æµ‹å…¶æœªæ¥è¿åŠ¨è½¨è¿¹ï¼Œå®ç°äº†å¯¹åŠ¨æ€äº‹ä»¶çš„é¢„è§æ€§æŠ‘åˆ¶ã€‚æ¨¡å‹è®¾è®¡æ³¨é‡è®¡ç®—æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨æ¶ˆè´¹çº§GPUä¸Šå®ç°å®æ—¶æ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨EVIMOåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²å‡†ç¡®ç‡ä¸Šæ¯”å…ˆå‰æœ€ä¼˜æ–¹æ³•æå‡67%ï¼Œæ¨ç†é€Ÿåº¦æé«˜53%ï¼Œè¾¾åˆ°173Hzã€‚åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­ï¼Œé€šè¿‡ä»¤ç‰Œå‰ªæå°†è§†è§‰Transformeræ¨ç†é€Ÿåº¦æå‡83%ï¼Œå¹¶å°†äº‹ä»¶è§†è§‰é‡Œç¨‹è®¡çš„ç»å¯¹è½¨è¿¹è¯¯å·®é™ä½13%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è¿åŠ¨æ„ŸçŸ¥äº‹ä»¶æŠ‘åˆ¶æ¡†æ¶åœ¨æå‡äº‹ä»¶ç›¸æœºç³»ç»Ÿå®æ—¶æ€§èƒ½å’Œä¸‹æ¸¸ä»»åŠ¡æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä»·å€¼ï¼Œä¸ºäº‹ä»¶ç›¸æœºåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å®é™…åº”ç”¨æä¾›äº†æœ‰æ•ˆçš„é¢„å¤„ç†è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å±•ç¤ºäº†è½»é‡çº§æ¶æ„åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°å®æ—¶æ¨ç†çš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>In this work, we introduce the first framework for Motion-aware Event Suppression, which learns to filter events triggered by IMOs and ego-motion in real time. Our model jointly segments IMOs in the current event stream while predicting their future motion, enabling anticipatory suppression of dynamic events before they occur. Our lightweight architecture achieves 173 Hz inference on consumer-grade GPUs with less than 1 GB of memory usage, outperforming previous state-of-the-art methods on the challenging EVIMO benchmark by 67\% in segmentation accuracy while operating at a 53\% higher inference rate. Moreover, we demonstrate significant benefits for downstream applications: our method accelerates Vision Transformer inference by 83\% via token pruning and improves event-based visual odometry accuracy, reducing Absolute Trajectory Error (ATE) by 13\%.</p>
<h3 id="3-sensor-generalization-for-adaptive-sensing-in-event-based-object-detection-via-joint-distribution-training">[3] <a href="https://arxiv.org/abs/2602.23357">Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training</a></h3>
<p><em>Aheli Saha, RenÃ© Schuster, Didier Stricker</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æ·±å…¥åˆ†æäº†äº‹ä»¶ç›¸æœºå†…åœ¨å‚æ•°å¯¹åŸºäºäº‹ä»¶æ•°æ®çš„ç‰©ä½“æ£€æµ‹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶åˆ©ç”¨è¿™äº›å‘ç°æ‰©å±•äº†ä¸‹æ¸¸æ¨¡å‹å¯¹ä¼ æ„Ÿå™¨æ— å…³é²æ£’æ€§çš„èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡ç”Ÿç‰©å¯å‘çš„äº‹ä»¶ç›¸æœºå› å…¶å¼‚æ­¥å’Œä½å»¶è¿Ÿç‰¹æ€§è€Œå—åˆ°å…³æ³¨ï¼Œä½†ç”±äºå…¶è¾“å‡ºä¿¡å·çš„æ–°é¢–æ€§ï¼Œç°æœ‰æ•°æ®å­˜åœ¨å¯å˜æ€§å·®è·ï¼Œä¸”ç¼ºä¹å¯¹å…¶ä¿¡å·ç‰¹å¾å‚æ•°çš„å¹¿æ³›åˆ†æï¼Œè¿™é™åˆ¶äº†åŸºäºäº‹ä»¶æ•°æ®çš„æ¨¡å‹æ€§èƒ½ä¼˜åŒ–å’Œä¼ æ„Ÿå™¨æ— å…³é²æ£’æ€§çš„å®ç°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é€šè¿‡ç³»ç»Ÿåˆ†æäº‹ä»¶ç›¸æœºçš„å†…åœ¨å‚æ•°å¦‚ä½•å½±å“åŸºäºäº‹ä»¶æ•°æ®çš„ç‰©ä½“æ£€æµ‹æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨è¿™äº›åˆ†æç»“æœæ¥æ‰©å±•ä¸‹æ¸¸æ¨¡å‹çš„ä¼ æ„Ÿå™¨æ— å…³é²æ£’æ€§èƒ½åŠ›ï¼Œå…·ä½“æ–¹æ³•åŒ…æ‹¬å‚æ•°å½±å“åˆ†æå’Œæ¨¡å‹èƒ½åŠ›æ‰©å±•æ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æä¾›äº†å…³äºå†…åœ¨å‚æ•°å¯¹äº‹ä»¶æ•°æ®è®­ç»ƒæ¨¡å‹æ€§èƒ½å½±å“çš„æ·±å…¥ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸Šçš„å…·ä½“å½±å“ï¼Œå¹¶æˆåŠŸæ‰©å±•äº†ä¸‹æ¸¸æ¨¡å‹å¯¹ä¼ æ„Ÿå™¨å˜åŒ–çš„é²æ£’æ€§ï¼Œå®ç°äº†ä¼ æ„Ÿå™¨æ— å…³çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¡«è¡¥äº†äº‹ä»¶ç›¸æœºå‚æ•°åˆ†æé¢†åŸŸçš„ç©ºç™½ï¼Œä¸ºä¼˜åŒ–åŸºäºäº‹ä»¶æ•°æ®çš„æ¨¡å‹æä¾›äº†é‡è¦æŒ‡å¯¼ï¼ŒåŒæ—¶æå‡ºçš„ä¼ æ„Ÿå™¨æ— å…³é²æ£’æ€§æ‰©å±•æ–¹æ³•ä¸ºäº‹ä»¶ç›¸æœºåœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²æä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†äº‹ä»¶è§†è§‰é¢†åŸŸçš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.</p>
  </article>
</body>
</html>
