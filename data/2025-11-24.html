<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-24.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-evdiff-high-quality-video-with-an-event-camera">[1] <a href="https://arxiv.org/abs/2511.17492">EvDiff: High Quality Video with an Event Camera</a></h3>
<p><em>Weilun Li, Lei Sun, Ruixi Gao, Qi Jiang, Yuqin Ma, Kaiwei Wang, Ming-Hsuan Yang, Luc Van Gool, Danda Pani Paudel</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºEvDiffï¼Œä¸€ç§åŸºäºäº‹ä»¶çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ›¿ä»£è®­ç»ƒæ¡†æ¶ä»å•è‰²äº‹ä»¶æµç”Ÿæˆé«˜è´¨é‡å½©è‰²è§†é¢‘ï¼Œåœ¨çœŸå®æ€§å’Œä¿çœŸåº¦ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚è¯¥æ–¹æ³•ä»…éœ€å•æ­¥å‰å‘æ‰©æ•£ï¼Œæ˜¾è‘—é™ä½äº†é«˜å¸§ç‡è§†é¢‘ç”Ÿæˆçš„è®¡ç®—æˆæœ¬ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿäº‹ä»¶ç›¸æœºå›¾åƒé‡å»ºæ–¹æ³•é€šå¸¸é‡‡ç”¨ç«¯åˆ°ç«¯å›å½’èŒƒå¼ï¼Œç›´æ¥å°†äº‹ä»¶æ˜ å°„åˆ°å¼ºåº¦å¸§ï¼Œè¿™äº›æ–¹æ³•è™½ç„¶æœ‰æ•ˆä½†å¾€å¾€äº§ç”Ÿæ„ŸçŸ¥è´¨é‡è¾ƒå·®çš„ç»“æœï¼Œä¸”éš¾ä»¥æ‰©å±•æ¨¡å‹å®¹é‡å’Œè®­ç»ƒæ•°æ®è§„æ¨¡ã€‚äº‹ä»¶ç›¸æœºé‡å»ºä»»åŠ¡ç”±äºç»å¯¹äº®åº¦å›ºæœ‰çš„æ¨¡ç³Šæ€§è€Œé«˜åº¦ä¸é€‚å®šã€‚</p>
<p><strong>Method:</strong> æå‡ºEvDiffäº‹ä»¶æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨æ›¿ä»£è®­ç»ƒæ¡†æ¶æ¶ˆé™¤å¯¹é…å¯¹äº‹ä»¶-å›¾åƒæ•°æ®é›†çš„ä¾èµ–ï¼Œå…è®¸åˆ©ç”¨å¤§è§„æ¨¡å›¾åƒæ•°æ®é›†æå‡æ¨¡å‹å®¹é‡ã€‚è®¾è®¡ä»…æ‰§è¡Œå•æ­¥å‰å‘æ‰©æ•£çš„äº‹ä»¶æ‰©æ•£æ¨¡å‹ï¼Œé…å¤‡æ—¶é—´ä¸€è‡´çš„EvEncoderï¼Œæ˜¾è‘—é™ä½é«˜å¸§ç‡è§†é¢‘ç”Ÿæˆçš„è®¡ç®—è´Ÿæ‹…ã€‚</p>
<p><strong>Result:</strong> åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿çœŸåº¦å’ŒçœŸå®æ€§ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ï¼Œåœ¨åƒç´ çº§å’Œæ„ŸçŸ¥æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚EvDiffèƒ½å¤Ÿä»…ä»å•è‰²äº‹ä»¶æµç”Ÿæˆé«˜è´¨é‡çš„å½©è‰²è§†é¢‘ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨äº‹ä»¶ç›¸æœºå›¾åƒé‡å»ºä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºçš„æ›¿ä»£è®­ç»ƒæ¡†æ¶ä¸ºåˆ©ç”¨å¤§è§„æ¨¡å›¾åƒæ•°æ®æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚å•æ­¥æ‰©æ•£è®¾è®¡ä¸ºé«˜å¸§ç‡è§†é¢‘ç”Ÿæˆæä¾›äº†è®¡ç®—æ•ˆç‡é«˜çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨äº‹ä»¶è§†è§‰é¢†åŸŸå…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>As neuromorphic sensors, event cameras asynchronously record changes in brightness as streams of sparse events with the advantages of high temporal resolution and high dynamic range. Reconstructing intensity images from events is a highly ill-posed task due to the inherent ambiguity of absolute brightness. Early methods generally follow an end-to-end regression paradigm, directly mapping events to intensity frames in a deterministic manner. While effective to some extent, these approaches often yield perceptually inferior results and struggle to scale up in model capacity and training data. In this work, we propose EvDiff, an event-based diffusion model that follows a surrogate training framework to produce high-quality videos. To reduce the heavy computational cost of high-frame-rate video generation, we design an event-based diffusion model that performs only a single forward diffusion step, equipped with a temporally consistent EvEncoder. Furthermore, our novel Surrogate Training Framework eliminates the dependence on paired event-image datasets, allowing the model to leverage large-scale image datasets for higher capacity. The proposed EvDiff is capable of generating high-quality colorful videos solely from monochromatic event streams. Experiments on real-world datasets demonstrate that our method strikes a sweet spot between fidelity and realism, outperforming existing approaches on both pixel-level and perceptual metrics.</p>
  </article>
</body>
</html>
