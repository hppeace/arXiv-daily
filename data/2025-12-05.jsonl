{"id": "2512.04443", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2512.04443", "abs": "https://arxiv.org/abs/2512.04443", "authors": ["Donghyun Lee", "Abhishek Moitra", "Youngeun Kim", "Ruokai Yin", "Priyadarshini Panda"], "title": "MD-SNN: Membrane Potential-aware Distillation on Quantized Spiking Neural Network", "comment": null, "summary": "Spiking Neural Networks (SNNs) offer a promising and energy-efficient alternative to conventional neural networks, thanks to their sparse binary activation. However, they face challenges regarding memory and computation overhead due to complex spatio-temporal dynamics and the necessity for multiple backpropagation computations across timesteps during training. To mitigate this overhead, compression techniques such as quantization are applied to SNNs. Yet, naively applying quantization to SNNs introduces a mismatch in membrane potential, a crucial factor for the firing of spikes, resulting in accuracy degradation. In this paper, we introduce Membrane-aware Distillation on quantized Spiking Neural Network (MD-SNN), which leverages membrane potential to mitigate discrepancies after weight, membrane potential, and batch normalization quantization. To our knowledge, this study represents the first application of membrane potential knowledge distillation in SNNs. We validate our approach on various datasets, including CIFAR10, CIFAR100, N-Caltech101, and TinyImageNet, demonstrating its effectiveness for both static and dynamic data scenarios. Furthermore, for hardware efficiency, we evaluate the MD-SNN with SpikeSim platform, finding that MD-SNNs achieve 14.85X lower energy-delay-area product (EDAP), 2.64X higher TOPS/W, and 6.19X higher TOPS/mm2 compared to floating point SNNs at iso-accuracy on N-Caltech101 dataset."}
{"id": "2512.05015", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2512.05015", "abs": "https://arxiv.org/abs/2512.05015", "authors": ["Rui Chen", "Xingyu Chen", "Yaoqing Hu", "Shihan Kong", "Zhiheng Wu", "Junzhi Yu"], "title": "Plug-and-Play Homeostatic Spark: Zero-Cost Acceleration for SNN Training Across Paradigms", "comment": "12 pages, 4 figures", "summary": "Spiking neural networks offer event driven computation, sparse activation, and hardware efficiency, yet training often converges slowly and lacks stability. We present Adaptive Homeostatic Spiking Activity Regulation (AHSAR), an extremely simple plug in and training paradigm agnostic method that stabilizes optimization and accelerates convergence without changing the model architecture, loss, or gradients. AHSAR introduces no trainable parameters. It maintains a per layer homeostatic state during the forward pass, maps centered firing rate deviations to threshold scales through a bounded nonlinearity, uses lightweight cross layer diffusion to avoid sharp imbalance, and applies a slow across epoch global gain that combines validation progress with activity energy to tune the operating point. The computational cost is negligible. Across diverse training methods, SNN architectures of different depths, widths, and temporal steps, and both RGB and DVS datasets, AHSAR consistently improves strong baselines and enhances out of distribution robustness. These results indicate that keeping layer activity within a moderate band is a simple and effective principle for scalable and efficient SNN training."}
