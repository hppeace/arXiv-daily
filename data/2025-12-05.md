<div id=toc></div>

# Table of Contents

- [cs.NE](#cs.NE) [Total: 2]


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [1] [MD-SNN: Membrane Potential-aware Distillation on Quantized Spiking Neural Network](https://arxiv.org/abs/2512.04443)
*Donghyun Lee, Abhishek Moitra, Youngeun Kim, Ruokai Yin, Priyadarshini Panda*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†è†œç”µä½æ„ŸçŸ¥è’¸é¦é‡åŒ–è„‰å†²ç¥ç»ç½‘ç»œï¼ˆMD-SNNï¼‰ï¼Œé¦–æ¬¡å°†è†œç”µä½çŸ¥è¯†è’¸é¦åº”ç”¨äºSNNé‡åŒ–ï¼Œä»¥è§£å†³é‡åŒ–å¯¼è‡´çš„è†œç”µä½å¤±é…é—®é¢˜ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡ç¡¬ä»¶æ•ˆç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰å› å…¶ç¨€ç–äºŒè¿›åˆ¶æ¿€æ´»è€Œå…·æœ‰é«˜èƒ½æ•ˆä¼˜åŠ¿ï¼Œä½†åœ¨è®­ç»ƒæ—¶é¢ä¸´æ—¶ç©ºåŠ¨æ€å¤æ‚æ€§å’Œå¤šæ—¶é—´æ­¥åå‘ä¼ æ’­è®¡ç®—å¸¦æ¥çš„å†…å­˜ä¸è®¡ç®—å¼€é”€æŒ‘æˆ˜ã€‚é‡åŒ–æŠ€æœ¯è™½å¯ç”¨äºå‹ç¼©SNNsï¼Œä½†ç›´æ¥åº”ç”¨é‡åŒ–ä¼šå¯¼è‡´è†œç”µä½å¤±é…ï¼Œè¿›è€Œå¼•å‘ç²¾åº¦ä¸‹é™ï¼Œéœ€è¦è§£å†³é‡åŒ–è¿‡ç¨‹ä¸­çš„è†œç”µä½ä¸ä¸€è‡´é—®é¢˜ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†è†œç”µä½æ„ŸçŸ¥è’¸é¦é‡åŒ–è„‰å†²ç¥ç»ç½‘ç»œï¼ˆMD-SNNï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è†œç”µä½æ¥ç¼“è§£æƒé‡ã€è†œç”µä½å’Œæ‰¹å½’ä¸€åŒ–é‡åŒ–åçš„å¤±é…é—®é¢˜ã€‚è¿™æ˜¯é¦–æ¬¡å°†è†œç”µä½çŸ¥è¯†è’¸é¦åº”ç”¨äºSNNsçš„ç ”ç©¶ï¼Œé€šè¿‡è†œç”µä½ä¿¡æ¯ä¼ é€’æ¥ä¿æŒé‡åŒ–ç½‘ç»œçš„å‡†ç¡®æ€§ã€‚

**Result:** åœ¨CIFAR10ã€CIFAR100ã€N-Caltech101å’ŒTinyImageNetç­‰å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œé€‚ç”¨äºé™æ€å’ŒåŠ¨æ€æ•°æ®åœºæ™¯ã€‚ç¡¬ä»¶è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨N-Caltech101æ•°æ®é›†ä¸Šï¼ŒMD-SNNç›¸æ¯”æµ®ç‚¹SNNså®ç°äº†14.85å€æ›´ä½çš„èƒ½é‡å»¶è¿Ÿé¢ç§¯ç§¯ï¼ˆEDAPï¼‰ã€2.64å€æ›´é«˜çš„TOPS/Wå’Œ6.19å€æ›´é«˜çš„TOPS/mmÂ²ï¼ŒåŒæ—¶ä¿æŒåŒç­‰ç²¾åº¦ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è†œç”µä½çŸ¥è¯†è’¸é¦åœ¨SNNé‡åŒ–ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé«˜æ•ˆSNNéƒ¨ç½²æä¾›äº†æ–°æ–¹æ³•ã€‚MD-SNNåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡äº†ç¡¬ä»¶æ•ˆç‡ï¼Œä¸ºè¾¹ç¼˜è®¾å¤‡ä¸Šçš„SNNåº”ç”¨å¼€è¾Ÿäº†é“è·¯ï¼Œå±•ç¤ºäº†é‡åŒ–ä¸è’¸é¦ç»“åˆåœ¨è„‰å†²ç¥ç»ç½‘ç»œä¼˜åŒ–ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Spiking Neural Networks (SNNs) offer a promising and energy-efficient alternative to conventional neural networks, thanks to their sparse binary activation. However, they face challenges regarding memory and computation overhead due to complex spatio-temporal dynamics and the necessity for multiple backpropagation computations across timesteps during training. To mitigate this overhead, compression techniques such as quantization are applied to SNNs. Yet, naively applying quantization to SNNs introduces a mismatch in membrane potential, a crucial factor for the firing of spikes, resulting in accuracy degradation. In this paper, we introduce Membrane-aware Distillation on quantized Spiking Neural Network (MD-SNN), which leverages membrane potential to mitigate discrepancies after weight, membrane potential, and batch normalization quantization. To our knowledge, this study represents the first application of membrane potential knowledge distillation in SNNs. We validate our approach on various datasets, including CIFAR10, CIFAR100, N-Caltech101, and TinyImageNet, demonstrating its effectiveness for both static and dynamic data scenarios. Furthermore, for hardware efficiency, we evaluate the MD-SNN with SpikeSim platform, finding that MD-SNNs achieve 14.85X lower energy-delay-area product (EDAP), 2.64X higher TOPS/W, and 6.19X higher TOPS/mm2 compared to floating point SNNs at iso-accuracy on N-Caltech101 dataset.


### [2] [Plug-and-Play Homeostatic Spark: Zero-Cost Acceleration for SNN Training Across Paradigms](https://arxiv.org/abs/2512.05015)
*Rui Chen, Xingyu Chen, Yaoqing Hu, Shihan Kong, Zhiheng Wu, Junzhi Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”ç¨³æ€è„‰å†²æ´»åŠ¨è°ƒèŠ‚ï¼ˆAHSARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æå…¶ç®€å•ã€å³æ’å³ç”¨ä¸”ä¸è®­ç»ƒèŒƒå¼æ— å…³çš„æ–¹æ³•ï¼Œé€šè¿‡ç»´æŒå±‚é—´æ´»åŠ¨åœ¨é€‚åº¦èŒƒå›´å†…æ¥ç¨³å®šè„‰å†²ç¥ç»ç½‘ç»œè®­ç»ƒå¹¶åŠ é€Ÿæ”¶æ•›ï¼Œæ— éœ€æ”¹å˜æ¨¡å‹æ¶æ„ã€æŸå¤±å‡½æ•°æˆ–æ¢¯åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è„‰å†²ç¥ç»ç½‘ç»œè™½ç„¶å…·æœ‰äº‹ä»¶é©±åŠ¨è®¡ç®—ã€ç¨€ç–æ¿€æ´»å’Œç¡¬ä»¶æ•ˆç‡ç­‰ä¼˜åŠ¿ï¼Œä½†å…¶è®­ç»ƒè¿‡ç¨‹å¾€å¾€æ”¶æ•›ç¼“æ…¢ä¸”ç¼ºä¹ç¨³å®šæ€§ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚

**Method:** AHSARæ–¹æ³•åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­ç»´æŒæ¯å±‚çš„ç¨³æ€çŠ¶æ€ï¼Œé€šè¿‡æœ‰ç•Œéçº¿æ€§å°†ä¸­å¿ƒåŒ–è„‰å†²ç‡åå·®æ˜ å°„åˆ°é˜ˆå€¼å°ºåº¦ï¼Œä½¿ç”¨è½»é‡çº§è·¨å±‚æ‰©æ•£é¿å…å°–é”çš„ä¸å¹³è¡¡ï¼Œå¹¶åº”ç”¨è·¨å‘¨æœŸçš„å…¨å±€å¢ç›Šç»“åˆéªŒè¯è¿›åº¦å’Œæ´»åŠ¨èƒ½é‡æ¥è°ƒæ•´æ“ä½œç‚¹ï¼Œè¯¥æ–¹æ³•ä¸å¼•å…¥ä»»ä½•å¯è®­ç»ƒå‚æ•°ä¸”è®¡ç®—æˆæœ¬å¯å¿½ç•¥ã€‚

**Result:** åœ¨å¤šç§è®­ç»ƒæ–¹æ³•ã€ä¸åŒæ·±åº¦ã€å®½åº¦å’Œæ—¶é—´æ­¥é•¿çš„SNNæ¶æ„ä»¥åŠRGBå’ŒDVSæ•°æ®é›†ä¸Šï¼ŒAHSARå§‹ç»ˆèƒ½å¤Ÿæ”¹è¿›å¼ºåŸºçº¿æ€§èƒ½å¹¶å¢å¼ºåˆ†å¸ƒå¤–é²æ£’æ€§ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒè®¾ç½®ä¸‹çš„æ™®é€‚æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†å±‚é—´æ´»åŠ¨ç»´æŒåœ¨é€‚åº¦èŒƒå›´å†…æ˜¯è„‰å†²ç¥ç»ç½‘ç»œå¯æ‰©å±•å’Œé«˜æ•ˆè®­ç»ƒçš„ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„åŸåˆ™ï¼ŒAHSARä½œä¸ºä¸€ç§ä¸è®­ç»ƒèŒƒå¼æ— å…³çš„å³æ’å³ç”¨æ–¹æ³•ï¼Œä¸ºSNNè®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦æä¾›äº†é€šç”¨è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Spiking neural networks offer event driven computation, sparse activation, and hardware efficiency, yet training often converges slowly and lacks stability. We present Adaptive Homeostatic Spiking Activity Regulation (AHSAR), an extremely simple plug in and training paradigm agnostic method that stabilizes optimization and accelerates convergence without changing the model architecture, loss, or gradients. AHSAR introduces no trainable parameters. It maintains a per layer homeostatic state during the forward pass, maps centered firing rate deviations to threshold scales through a bounded nonlinearity, uses lightweight cross layer diffusion to avoid sharp imbalance, and applies a slow across epoch global gain that combines validation progress with activity energy to tune the operating point. The computational cost is negligible. Across diverse training methods, SNN architectures of different depths, widths, and temporal steps, and both RGB and DVS datasets, AHSAR consistently improves strong baselines and enhances out of distribution robustness. These results indicate that keeping layer activity within a moderate band is a simple and effective principle for scalable and efficient SNN training.
