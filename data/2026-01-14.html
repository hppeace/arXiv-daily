<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-14.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.NE">cs.NE</a> [Total: 2]</li>
</ul>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="1-sleep-based-homeostatic-regularization-for-stabilizing-spike-timing-dependent-plasticity-in-recurrent-spiking-neural-networks">[1] <a href="https://arxiv.org/abs/2601.08447">Sleep-Based Homeostatic Regularization for Stabilizing Spike-Timing-Dependent Plasticity in Recurrent Spiking Neural Networks</a></h3>
<p><em>Andreas Massey, Aliaksandr Hubin, Stefano Nichele, Solve SÃ¦bÃ¸</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å—çªè§¦ç¨³æ€å‡è¯´å¯å‘çš„ç¥ç»å½¢æ€æ­£åˆ™åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡å¼•å…¥ç¡çœ -è§‰é†’å‘¨æœŸæ¥ç¨³å®šå…·æœ‰å¾ªç¯è¿æ¥çš„è„‰å†²ç¥ç»ç½‘ç»œçš„STDPå­¦ä¹ ï¼Œé˜²æ­¢ç—…ç†æ€§æƒé‡åŠ¨æ€å¹¶ä¿æŒå­¦ä¹ ç»“æ„ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…·æœ‰å¾ªç¯è¿æ¥çš„è„‰å†²ç¥ç»ç½‘ç»œä¸­ï¼ŒåŸºäºè„‰å†²æ—¶åºä¾èµ–å¯å¡‘æ€§çš„èµ«å¸ƒå¼æƒé‡æ›´æ–°å­˜åœ¨ç—…ç†æ€§æƒé‡åŠ¨æ€é—®é¢˜ï¼ŒåŒ…æ‹¬æ— ç•Œå¢é•¿ã€ç¾éš¾æ€§é—å¿˜å’Œè¡¨å¾å¤šæ ·æ€§ä¸§å¤±ï¼Œè¿™é™åˆ¶äº†ç”Ÿç‰©å¯å¡‘æ€§å­¦ä¹ æœºåˆ¶çš„å®é™…åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§å—çªè§¦ç¨³æ€å‡è¯´å¯å‘çš„ç¥ç»å½¢æ€æ­£åˆ™åŒ–æ–¹æ¡ˆï¼Œå¼•å…¥å‘¨æœŸæ€§çš„ç¦»çº¿ç¡çœ é˜¶æ®µï¼Œåœ¨æ­¤æœŸé—´æŠ‘åˆ¶å¤–éƒ¨è¾“å…¥ï¼Œä½¿çªè§¦æƒé‡éšæœºè¡°å‡åˆ°ç¨³æ€åŸºçº¿ï¼Œå¹¶é€šè¿‡è‡ªå‘æ´»åŠ¨å®ç°è®°å¿†å·©å›ºï¼Œå½¢æˆç¡çœ -è§‰é†’å¾ªç¯çš„å­¦ä¹ æœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œåœ¨STDP-SNNæ¨¡å‹ä¸­ï¼Œä½åˆ°ä¸­ç­‰ç¡çœ æŒç»­æ—¶é—´ï¼ˆè®­ç»ƒæ—¶é—´çš„10-20%ï¼‰æ˜¾è‘—æé«˜äº†MNISTç±»åŸºå‡†æµ‹è¯•çš„ç¨³å®šæ€§ï¼Œæ— éœ€æ•°æ®ç‰¹å®šçš„è¶…å‚æ•°è°ƒæ•´ï¼›ç„¶è€Œï¼Œç›¸åŒçš„ç¡çœ å¹²é¢„å¯¹æ›¿ä»£æ¢¯åº¦è„‰å†²ç¥ç»ç½‘ç»œæ²¡æœ‰å¯æµ‹é‡çš„ç›Šå¤„ã€‚</p>
<p><strong>Conclusion:</strong> å‘¨æœŸæ€§åŸºäºç¡çœ çš„é‡æ–°å½’ä¸€åŒ–å¯èƒ½æ˜¯ç¨³å®šç¥ç»å½¢æ€ç³»ç»Ÿä¸­å±€éƒ¨èµ«å¸ƒå­¦ä¹ çš„åŸºæœ¬æœºåˆ¶ï¼Œä½†å°†å…¶ä¸ç°æœ‰åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•é›†æˆæ—¶éœ€è¦ç‰¹åˆ«è°¨æ…ï¼Œè¿™ä¸ºç”Ÿç‰©å¯å¡‘æ€§å­¦ä¹ ä¸äººå·¥ç¥ç»ç½‘ç»œè®­ç»ƒçš„èåˆæä¾›äº†é‡è¦è§è§£ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Spike-timing-dependent plasticity (STDP) provides a biologically-plausible learning mechanism for spiking neural networks (SNNs); however, Hebbian weight updates in architectures with recurrent connections suffer from pathological weight dynamics: unbounded growth, catastrophic forgetting, and loss of representational diversity. We propose a neuromorphic regularization scheme inspired by the synaptic homeostasis hypothesis: periodic offline phases during which external inputs are suppressed, synaptic weights undergo stochastic decay toward a homeostatic baseline, and spontaneous activity enables memory consolidation. We demonstrate that this sleep-wake cycle prevents weight saturation while preserving learned structure. Empirically, we find that low to intermediate sleep durations (10-20\% of training) improve stability on MNIST-like benchmarks in our STDP-SNN model, without any data-specific hyperparameter tuning. In contrast, the same sleep intervention yields no measurable benefit for the surrogate-gradient spiking neural network (SG-SNN). Taken together, these results suggest that periodic, sleep-based renormalization may represent a fundamental mechanism for stabilizing local Hebbian learning in neuromorphic systems, while also indicating that special care is required when integrating such protocols with existing gradient-based optimization methods.</p>
<h3 id="2-supervised-spike-agreement-dependent-plasticity-for-fast-local-learning-in-spiking-neural-networks">[2] <a href="https://arxiv.org/abs/2601.08526">Supervised Spike Agreement Dependent Plasticity for Fast Local Learning in Spiking Neural Networks</a></h3>
<p><em>Gouri Lakshmi S, Athira Chandrasekharan, Harshit Kumar, Muhammed Sahad E, Bikas C Das, Saptarshi Bej</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç›‘ç£å¼å°–å³°ä¸€è‡´æ€§ä¾èµ–å¯å¡‘æ€§ï¼ˆSADPï¼‰å­¦ä¹ è§„åˆ™ï¼Œä½œä¸ºä¼ ç»ŸSTDPçš„æ‰©å±•ï¼Œé€šè¿‡ç¾¤ä½“ä¸€è‡´æ€§åº¦é‡å–ä»£ç²¾ç¡®çš„å°–å³°æ—¶åºæ¯”è¾ƒï¼Œå®ç°äº†æ— éœ€åå‘ä¼ æ’­ã€æ›¿ä»£æ¢¯åº¦æˆ–æ•™å¸ˆå¼ºåˆ¶çš„å¿«é€Ÿç›‘ç£å­¦ä¹ ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿå°–å³°æ—¶åºä¾èµ–å¯å¡‘æ€§ï¼ˆSTDPï¼‰ä¾èµ–äºç²¾ç¡®çš„å°–å³°æ—¶åºå’Œæˆå¯¹æ›´æ–°ï¼Œé™åˆ¶äº†å°–å³°ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰çš„å¿«é€Ÿæƒé‡å­¦ä¹ ï¼Œéœ€è¦ä¸€ç§æ›´é«˜æ•ˆä¸”ç”Ÿç‰©å­¦åˆç†çš„ç›‘ç£å­¦ä¹ èŒƒå¼ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ç›‘ç£å¼å°–å³°ä¸€è‡´æ€§ä¾èµ–å¯å¡‘æ€§ï¼ˆSADPï¼‰å­¦ä¹ è§„åˆ™ï¼Œä½¿ç”¨ç¾¤ä½“ä¸€è‡´æ€§åº¦é‡ï¼ˆå¦‚Cohen's kappaï¼‰æ›¿ä»£æˆå¯¹å°–å³°æ—¶åºæ¯”è¾ƒï¼Œå¹¶å°†å…¶é›†æˆåˆ°æ··åˆCNN-SNNæ¶æ„ä¸­ï¼Œå…¶ä¸­å·ç§¯ç¼–ç å™¨æä¾›ç´§å‡‘ç‰¹å¾è¡¨ç¤ºï¼Œéšåè½¬æ¢ä¸ºæ³Šæ¾å°–å³°åºåˆ—è¿›è¡Œä¸€è‡´æ€§é©±åŠ¨å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> åœ¨MNISTã€Fashion-MNISTã€CIFAR-10å’Œç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†ç«äº‰æ€§æ€§èƒ½å’Œå¿«é€Ÿæ”¶æ•›ï¼ŒåŒæ—¶åœ¨å¹¿æ³›çš„è¶…å‚æ•°èŒƒå›´å†…ä¿æŒç¨³å®šæ€§èƒ½ï¼Œå¹¶ä¸è®¾å¤‡å¯å‘çš„çªè§¦æ›´æ–°åŠ¨åŠ›å­¦å…¼å®¹ã€‚</p>
<p><strong>Conclusion:</strong> ç›‘ç£å¼SADPè¢«ç¡®ç«‹ä¸ºä¸€ç§å¯æ‰©å±•ã€ç”Ÿç‰©å­¦åŸºç¡€ä¸”ä¸ç¡¬ä»¶å¯¹é½çš„å°–å³°ç¥ç»ç½‘ç»œå­¦ä¹ èŒƒå¼ï¼Œä¸ºæ— éœ€åå‘ä¼ æ’­çš„SNNè®­ç»ƒæä¾›äº†æ–°çš„æ–¹å‘ï¼Œå¹¶å±•ç¤ºäº†åœ¨ç¥ç»å½¢æ€è®¡ç®—ç¡¬ä»¶ä¸­çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Spike-Timing-Dependent Plasticity (STDP) provides a biologically grounded learning rule for spiking neural networks (SNNs), but its reliance on precise spike timing and pairwise updates limits fast learning of weights. We introduce a supervised extension of Spike Agreement-Dependent Plasticity (SADP), which replaces pairwise spike-timing comparisons with population-level agreement metrics such as Cohen's kappa. The proposed learning rule preserves strict synaptic locality, admits linear-time complexity, and enables efficient supervised learning without backpropagation, surrogate gradients, or teacher forcing.
  We integrate supervised SADP within hybrid CNN-SNN architectures, where convolutional encoders provide compact feature representations that are converted into Poisson spike trains for agreement-driven learning in the SNN. Extensive experiments on MNIST, Fashion-MNIST, CIFAR-10, and biomedical image classification tasks demonstrate competitive performance and fast convergence. Additional analyses show stable performance across broad hyperparameter ranges and compatibility with device-inspired synaptic update dynamics. Together, these results establish supervised SADP as a scalable, biologically grounded, and hardware-aligned learning paradigm for spiking neural networks.</p>
  </article>
</body>
</html>
