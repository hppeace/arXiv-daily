<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet](https://arxiv.org/abs/2602.09717)
*Radib Bin Kabir, Tawsif Tashwar Dipto, Mehedi Ahamed, Sabbir Ahmed, Md Hasanul Kabir*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å¯¹è½»é‡çº§è„‰å†²ç¥ç»ç½‘ç»œè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å°†ç´§å‡‘å‹CNNæ¶æ„è½¬æ¢ä¸ºSNNæ¨¡å‹ï¼Œå¹¶åº”ç”¨ç»“æ„åŒ–å‰ªæç­–ç•¥ï¼Œå®ç°äº†é«˜è¾¾15.7å€çš„èƒ½é‡æ•ˆç‡æå‡ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§ç²¾åº¦ï¼Œä¸ºè¾¹ç¼˜æ™ºèƒ½éƒ¨ç½²æä¾›äº†å®ç”¨çš„ä½åŠŸè€—æ›¿ä»£æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å¤§è§„æ¨¡è„‰å†²ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œè€Œè½»é‡çº§CNNåˆ°SNNè½¬æ¢æµç¨‹çš„è®¾è®¡ä¸è¯„ä¼°å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†SNNä½œä¸ºèƒ½é‡é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆåœ¨è¾¹ç¼˜è®¡ç®—åœºæ™¯ä¸­çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç»Ÿä¸€çš„è½¬æ¢æ¡†æ¶ï¼Œå°†ShuffleNetã€SqueezeNetã€MnasNetå’ŒMixNetç­‰ç´§å‡‘å‹CNNæ¶æ„è½¬æ¢ä¸ºè„‰å†²ç¥ç»ç½‘ç»œå˜ä½“ï¼Œä½¿ç”¨Leaky-Integrate-and-Fireç¥ç»å…ƒå»ºæ¨¡æ¿€æ´»ï¼Œé€šè¿‡æ›¿ä»£æ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¯¹æœ€ä½³æ¨¡å‹åº”ç”¨ç»“æ„åŒ–å‰ªæç­–ç•¥ç§»é™¤å†—ä½™æ¨¡å—ï¼Œå¾—åˆ°SNN-SqueezeNet-Pæ¶æ„ã€‚

**Result:** å®éªŒåœ¨CIFAR-10ã€CIFAR-100å’ŒTinyImageNetæ•°æ®é›†ä¸Šè¯„ä¼°äº†å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€å‚æ•°é‡ã€è®¡ç®—å¤æ‚åº¦å’Œèƒ½è€—ç­‰æŒ‡æ ‡ï¼Œç»“æœæ˜¾ç¤ºSNNç›¸æ¯”CNNå¯å®ç°é«˜è¾¾15.7å€çš„èƒ½é‡æ•ˆç‡æå‡ï¼Œå…¶ä¸­SNN-SqueezeNetè¡¨ç°æœ€ä½³ï¼›ç»è¿‡ç»“æ„åŒ–å‰ªæçš„SNN-SqueezeNet-Påœ¨CIFAR-10ä¸Šå‡†ç¡®ç‡æå‡6%ï¼Œå‚æ•°é‡å‡å°‘19%ï¼Œä¸CNN-SqueezeNetçš„ç²¾åº¦å·®è·ä»…1%ï¼Œä½†èƒ½è€—é™ä½88.1%ã€‚

**Conclusion:** è¯¥ç ”ç©¶ç¡®ç«‹äº†è½»é‡çº§SNNä½œä¸ºè¾¹ç¼˜éƒ¨ç½²çš„å®ç”¨ä½åŠŸè€—æ›¿ä»£æ–¹æ¡ˆï¼Œè¯æ˜äº†é€šè¿‡æ¶æ„è½¬æ¢å’Œä¼˜åŒ–ç­–ç•¥å¯ä»¥åœ¨ä¿æŒç«äº‰æ€§ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½èƒ½è€—ï¼Œä¸ºåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ€§èƒ½ã€ä½åŠŸè€—æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [2] [Sparse Axonal and Dendritic Delays Enable Competitive SNNs for Keyword Classification](https://arxiv.org/abs/2602.09746)
*Younes Bouhadjar, Emre Neftci*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºåœ¨è„‰å†²ç¥ç»ç½‘ç»œä¸­å­¦ä¹ è½´çªæˆ–æ ‘çªå»¶è¿Ÿï¼Œç›¸æ¯”ä¼ ç»Ÿçš„çªè§¦å»¶è¿Ÿå­¦ä¹ æ–¹æ³•ï¼Œèƒ½ä»¥æ›´ä½çš„å†…å­˜å’Œè®¡ç®—å¼€é”€å®ç°ç›¸å½“çš„æ—¶åºä»»åŠ¡æ€§èƒ½ï¼Œä¸ºSNNæä¾›äº†ä¸€ç§èµ„æºé«˜æ•ˆçš„æ—¶åºè¡¨ç¤ºæœºåˆ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è„‰å†²ç¥ç»ç½‘ç»œä¸­çš„å¯å­¦ä¹ ä¼ è¾“å»¶è¿Ÿæ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯çªè§¦å»¶è¿Ÿï¼‰å­˜åœ¨è¾ƒé«˜çš„å†…å­˜å’Œè®¡ç®—å¼€é”€ï¼Œéœ€è¦æ¢ç´¢æ›´é«˜æ•ˆçš„å»¶è¿Ÿå­¦ä¹ æœºåˆ¶æ¥æå‡SNNåœ¨å¤æ‚æ—¶åºä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½èµ„æºæ¶ˆè€—ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç”±æ³„æ¼ç§¯åˆ†å‘æ”¾ç¥ç»å…ƒç»„æˆçš„æ·±åº¦å‰é¦ˆSNNæ¶æ„ï¼Œåˆ†åˆ«å®ç°äº†è½´çªå»¶è¿Ÿå’Œæ ‘çªå»¶è¿Ÿçš„å­¦ä¹ æœºåˆ¶ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„çªè§¦å»¶è¿Ÿå­¦ä¹ æ–¹æ³•è¿›è¡Œå¯¹æ¯”ï¼Œé€šè¿‡è°ƒæ•´å»¶è¿Ÿå‚æ•°ä¼˜åŒ–åŸºçº¿æ¨¡å‹æ€§èƒ½ã€‚

**Result:** åœ¨Google Speech Commandå’ŒSpiking Speech Commandæ•°æ®é›†ä¸Šï¼Œè½´çªå’Œæ ‘çªå»¶è¿Ÿæ¨¡å‹åˆ†åˆ«è¾¾åˆ°95.58%å’Œ80.97%çš„å‡†ç¡®ç‡ï¼ŒåŒ¹é…æˆ–è¶…è¶Šäº†åŸºäºçªè§¦å»¶è¿Ÿæˆ–æ›´å¤æ‚ç¥ç»å…ƒæ¨¡å‹çš„å…ˆå‰æ–¹æ³•ï¼Œä¸”è½´çªå»¶è¿Ÿåœ¨ç¼“å†²éœ€æ±‚å’Œå‡†ç¡®æ€§ä¹‹é—´æä¾›äº†æœ€ä½³æƒè¡¡ï¼Œå³ä½¿åœ¨20%å»¶è¿Ÿç¨€ç–æ€§ä¸‹æ€§èƒ½ä»åŸºæœ¬ä¿æŒã€‚

**Conclusion:** å¯å­¦ä¹ çš„è½´çªå’Œæ ‘çªå»¶è¿Ÿä¸ºSNNæä¾›äº†ä¸€ç§èµ„æºé«˜æ•ˆä¸”æœ‰æ•ˆçš„æ—¶åºè¡¨ç¤ºæœºåˆ¶ï¼Œè½´çªå»¶è¿Ÿå°¤å…¶å…·æœ‰ä¼˜åŠ¿ï¼Œå…¶è¾ƒä½çš„ç¼“å†²éœ€æ±‚å’Œè‰¯å¥½çš„æ€§èƒ½ä¿æŒèƒ½åŠ›ä½¿å…¶æ›´é€‚åˆå®é™…éƒ¨ç½²ï¼Œå»¶è¿Ÿç¨€ç–æ€§è¿›ä¸€æ­¥é™ä½äº†èµ„æºéœ€æ±‚ã€‚

---

#### ğŸ“„ Abstract
Training transmission delays in spiking neural networks (SNNs) has been shown to substantially improve their performance on complex temporal tasks. In this work, we show that learning either axonal or dendritic delays enables deep feedforward SNNs composed of leaky integrate-and-fire (LIF) neurons to reach accuracy comparable to existing synaptic delay learning approaches, while significantly reducing memory and computational overhead. SNN models with either axonal or dendritic delays achieve up to $95.58\%$ on the Google Speech Command (GSC) and $80.97\%$ on the Spiking Speech Command (SSC) datasets, matching or exceeding prior methods based on synaptic delays or more complex neuron models. By adjusting the delay parameters, we obtain improved performance for synaptic delay learning baselines, strengthening the comparison. We find that axonal delays offer the most favorable trade-off, combining lower buffering requirements with slightly higher accuracy than dendritic delays. We further show that the performance of axonal and dendritic delay models is largely preserved under strong delay sparsity, with as few as $20\%$ of delays remaining active, further reducing buffering requirements. Overall, our results indicate that learnable axonal and dendritic delays provide a resource-efficient and effective mechanism for temporal representation in SNNs. Code will be made available publicly upon acceptance. Code is available at https://github.com/YounesBouhadjar/AxDenSynDelaySNN
