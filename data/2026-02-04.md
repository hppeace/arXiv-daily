<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images](https://arxiv.org/abs/2602.03669)
*Sandeep Patil, Yongqi Dong, Haneen Farah, Hans Hellendoorn*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…·æœ‰æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºè½¦é“çº¿æ£€æµ‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿèšç„¦è½¦é“çº¿å…³é”®ç‰¹å¾å¹¶åˆ©ç”¨è¿ç»­å›¾åƒå¸§é—´çš„æ—¶ç©ºç›¸å…³æ€§ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°äº†å“è¶Šçš„æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è½¦é“çº¿æ£€æµ‹æ–¹æ³•åœ¨æ··åˆäº¤é€šç¯å¢ƒä¸‹ç¼ºä¹å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œå®æ—¶æ€§ï¼Œç‰¹åˆ«æ˜¯åŸºäºè§†è§‰çš„æ–¹æ³•å¾€å¾€å¿½ç•¥å›¾åƒå…³é”®åŒºåŸŸåŠå…¶æ—¶ç©ºæ˜¾è‘—æ€§ï¼Œå¯¼è‡´åœ¨ä¸¥é‡é®æŒ¡å’Œçœ©å…‰ç­‰å›°éš¾åœºæ™¯ä¸‹æ€§èƒ½ä¸ä½³ã€‚

**Method:** æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„åºåˆ—ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œé‡‡ç”¨æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶æ¥èšç„¦è½¦é“çº¿å…³é”®ç‰¹å¾å¹¶åˆ©ç”¨è¿ç»­å›¾åƒå¸§é—´çš„æ—¶ç©ºç›¸å…³æ€§ï¼Œæ¨¡å‹åŸºäºæ ‡å‡†çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„å’Œå¸¸è§çš„ç¥ç»ç½‘ç»œéª¨å¹²ç½‘ç»œæ„å»ºã€‚

**Result:** åœ¨ä¸‰ä¸ªå¤§è§„æ¨¡å¼€æºæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å„ç§æµ‹è¯•åœºæ™¯ä¸­å‡ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼›åŒæ—¶ï¼Œæ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹å‚æ•°æ›´å°‘ã€ä¹˜åŠ è¿ç®—é‡æ›´ä½ï¼Œè®¡ç®—æ•ˆç‡æ˜¾è‘—æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶åœ¨è½¦é“çº¿æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ä»…æå‡äº†æ£€æµ‹ç²¾åº¦å’Œé²æ£’æ€§ï¼Œè¿˜é€šè¿‡å‡å°‘å‚æ•°å’Œè®¡ç®—é‡å®ç°äº†æ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.


### [2] [EventFlash: Towards Efficient MLLMs for Event-Based Vision](https://arxiv.org/abs/2602.03230)
*Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang, Ming Li, Xiangyang Ji*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºEventFlashï¼Œä¸€ç§é«˜æ•ˆçš„äº‹ä»¶é©±åŠ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ—¶ç©ºä»¤ç‰Œç¨€ç–åŒ–æŠ€æœ¯å‡å°‘æ•°æ®å†—ä½™å¹¶åŠ é€Ÿæ¨ç†ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°12.4å€çš„ååé‡æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºäº‹ä»¶çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€šå¸¸é‡‡ç”¨å¯†é›†çš„å›¾åƒå¼å¤„ç†èŒƒå¼ï¼Œå¿½è§†äº†äº‹ä»¶æµçš„æ—¶ç©ºç¨€ç–ç‰¹æ€§ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦æ¢ç´¢æ›´é«˜æ•ˆçš„æ—¶ç©ºä»¤ç‰Œç¨€ç–åŒ–æ–¹æ³•æ¥å‡å°‘æ•°æ®å†—ä½™å¹¶åŠ é€Ÿæ¨ç†ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†åŒ…å«50ä¸‡æŒ‡ä»¤é›†çš„å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®é›†EventMindï¼Œé‡‡ç”¨è¯¾ç¨‹è®­ç»ƒç­–ç•¥ï¼›è®¾è®¡äº†è‡ªé€‚åº”æ—¶é—´çª—å£èšåˆæ¨¡å—ä»¥é«˜æ•ˆå‹ç¼©æ—¶é—´ä»¤ç‰Œå¹¶ä¿ç•™å…³é”®æ—¶é—´çº¿ç´¢ï¼›å¼€å‘äº†ç¨€ç–å¯†åº¦å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ï¼Œé€šè¿‡é€‰æ‹©ä¿¡æ¯ä¸°å¯ŒåŒºåŸŸå¹¶æŠ‘åˆ¶ç©ºç¨€ç–åŒºåŸŸæ¥æé«˜ç©ºé—´ä»¤ç‰Œæ•ˆç‡ã€‚

**Result:** EventFlashç›¸æ¯”åŸºçº¿æ¨¡å‹EventFlash-Zeroå®ç°äº†12.4å€çš„ååé‡æå‡ï¼ŒåŒæ—¶ä¿æŒå¯æ¯”æ€§èƒ½ï¼›æ”¯æŒå¤„ç†é•¿è¾¾1000ä¸ªæ—¶é—´ä»“çš„é•¿èŒƒå›´äº‹ä»¶æµï¼Œæ˜¾è‘—ä¼˜äºEventGPTçš„5ä»“é™åˆ¶ï¼Œå±•ç¤ºäº†é«˜æ•ˆçš„äº‹ä»¶è§†è§‰åŸºç¡€æ¨¡å‹èƒ½åŠ›ã€‚

**Conclusion:** EventFlashé€šè¿‡æ¢ç´¢æ—¶ç©ºä»¤ç‰Œç¨€ç–åŒ–ï¼Œä¸ºåŸºäºäº‹ä»¶çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿå¯†é›†å¤„ç†èŒƒå¼çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œä¸ºé«˜é€Ÿå’Œä½å…‰åœºæ™¯ä¸‹çš„é²æ£’æ„ŸçŸ¥å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.


### [3] [EventNeuS: 3D Mesh Reconstruction from a Single Event Camera](https://arxiv.org/abs/2602.03847)
*Shreyas Sachan, Viktor Rudnev, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†EventNeuSï¼Œä¸€ç§ç”¨äºä»å•ç›®å½©è‰²äº‹ä»¶æµä¸­å­¦ä¹ 3Dè¡¨ç¤ºçš„è‡ªç›‘ç£ç¥ç»æ¨¡å‹ï¼Œé¦–æ¬¡å°†3Dç¬¦å·è·ç¦»å‡½æ•°å’Œå¯†åº¦åœºå­¦ä¹ ä¸äº‹ä»¶ç›‘ç£ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†äº‹ä»¶ç›¸æœºåœ¨3Dç½‘æ ¼é‡å»ºæ–¹é¢çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äº‹ä»¶ç›¸æœºåœ¨è®¸å¤šåœºæ™¯ä¸­æä¾›äº†RGBç›¸æœºçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„äº‹ä»¶åŸºæ–°é¢–è§†è§’åˆæˆæ–¹æ³•åœ¨å¯†é›†3Dç½‘æ ¼é‡å»ºæ–¹é¢æ¢ç´¢ä¸è¶³ï¼Œä¸”ç°æœ‰äº‹ä»¶åŸºæŠ€æœ¯åœ¨3Dé‡å»ºç²¾åº¦ä¸Šå­˜åœ¨ä¸¥é‡é™åˆ¶ï¼Œéœ€è¦è§£å†³è¿™ä¸€æ€§èƒ½å·®è·ã€‚

**Method:** è¯¥æ–¹æ³•é¦–æ¬¡å°†3Dç¬¦å·è·ç¦»å‡½æ•°å’Œå¯†åº¦åœºå­¦ä¹ ä¸äº‹ä»¶åŸºç›‘ç£ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥çƒè°ç¼–ç ä»¥å¢å¼ºå¯¹è§†è§’ç›¸å…³æ•ˆåº”çš„å¤„ç†èƒ½åŠ›ï¼Œæ„å»ºäº†ä¸€ä¸ªè‡ªç›‘ç£ç¥ç»æ¨¡å‹ç”¨äºä»å•ç›®å½©è‰²äº‹ä»¶æµä¸­å­¦ä¹ 3Dè¡¨ç¤ºã€‚

**Result:** EventNeuSåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æ¯”å…ˆå‰æœ€ä½³æ–¹æ³•é™ä½äº†34%çš„Chamferè·ç¦»å’Œ31%çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼Œåœ¨3Dé‡å»ºç²¾åº¦æ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å°†ç¥ç»éšå¼è¡¨ç¤ºä¸äº‹ä»¶åŸºç›‘ç£ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºäº‹ä»¶ç›¸æœºåœ¨3Dé‡å»ºä»»åŠ¡ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå±•ç¤ºäº†çƒè°ç¼–ç åœ¨å¤„ç†è§†è§’ç›¸å…³æ•ˆåº”æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥äº‹ä»¶åŸº3Dæ„ŸçŸ¥ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Event cameras offer a considerable alternative to RGB cameras in many scenarios. While there are recent works on event-based novel-view synthesis, dense 3D mesh reconstruction remains scarcely explored and existing event-based techniques are severely limited in their 3D reconstruction accuracy. To address this limitation, we present EventNeuS, a self-supervised neural model for learning 3D representations from monocular colour event streams. Our approach, for the first time, combines 3D signed distance function and density field learning with event-based supervision. Furthermore, we introduce spherical harmonics encodings into our model for enhanced handling of view-dependent effects. EventNeuS outperforms existing approaches by a significant margin, achieving 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.
