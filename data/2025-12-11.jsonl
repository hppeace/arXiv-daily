{"id": "2512.09016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09016", "abs": "https://arxiv.org/abs/2512.09016", "authors": ["Haiqian Han", "Lingdong Kong", "Jianing Li", "Ao Liang", "Chengtao Zhu", "Jiacheng Lyu", "Lai Xing Ng", "Xiangyang Ji", "Wei Tsang Ooi", "Benoit R. Cottereau"], "title": "Learning to Remove Lens Flare in Event Camera", "comment": "Preprint; 29 pages, 14 figures, 4 tables; Project Page at https://e-flare.github.io/", "summary": "Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available."}
{"id": "2512.09592", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09592", "abs": "https://arxiv.org/abs/2512.09592", "authors": ["Zhe Wang", "Qijin Song", "Yucen Peng", "Weibang Bai"], "title": "CS3D: An Efficient Facial Expression Recognition via Event Vision", "comment": null, "summary": "Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\\% of the original C3D required on the same device."}
