<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating](https://arxiv.org/abs/2602.04182)
*Weidong Hao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºHoloEv-Netï¼Œä¸€ç§é«˜æ•ˆçš„äº‹ä»¶é©±åŠ¨åŠ¨ä½œè¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡ç´§å‡‘å…¨æ¯æ—¶ç©ºè¡¨ç¤ºå’Œå…¨å±€è°±é—¨æ§æ¨¡å—ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—å†—ä½™ã€ç»“æ„å†—ä½™å’Œè°±ä¿¡æ¯åˆ©ç”¨ä¸è¶³æ–¹é¢çš„æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰äº‹ä»¶é©±åŠ¨åŠ¨ä½œè¯†åˆ«æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šå¯†é›†ä½“ç´ è¡¨ç¤ºå¯¼è‡´è®¡ç®—å†—ä½™ï¼Œå¤šåˆ†æ”¯æ¶æ„å­˜åœ¨ç»“æ„å†—ä½™ï¼Œä»¥åŠè°±ä¿¡æ¯åœ¨æ•æ‰å…¨å±€è¿åŠ¨æ¨¡å¼æ—¶åˆ©ç”¨ä¸è¶³ï¼Œé™åˆ¶äº†æ–¹æ³•çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**Method:** æå‡ºç´§å‡‘å…¨æ¯æ—¶ç©ºè¡¨ç¤ºå°†æ°´å¹³ç©ºé—´çº¿ç´¢éšå¼åµŒå…¥æ—¶é—´-é«˜åº¦è§†å›¾ï¼Œåœ¨2Dè¡¨ç¤ºä¸­ä¿ç•™3Dæ—¶ç©ºä¸Šä¸‹æ–‡ï¼›è®¾è®¡å…¨å±€è°±é—¨æ§æ¨¡å—åˆ©ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢åœ¨é¢‘åŸŸè¿›è¡Œå…¨å±€ä»¤ç‰Œæ··åˆï¼Œä»¥æå°çš„å‚æ•°å¼€é”€å¢å¼ºè¡¨ç¤ºèƒ½åŠ›ã€‚

**Result:** HoloEv-Net-Baseåœ¨THU-EACT-50-CHLã€HARDVSå’ŒDailyDVS-200æ•°æ®é›†ä¸Šåˆ†åˆ«ä»¥10.29%ã€1.71%å’Œ6.25%çš„ä¼˜åŠ¿è¶…è¶Šç°æœ‰æ–¹æ³•ï¼›è½»é‡çº§å˜ä½“HoloEv-Net-Smallåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶ï¼Œå‚æ•°é‡å‡å°‘5.4å€ï¼Œè®¡ç®—é‡å‡å°‘300å€ï¼Œå»¶è¿Ÿé™ä½2.4å€ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç´§å‡‘è¡¨ç¤ºå’Œè°±åŸŸå¤„ç†å¯ä»¥æ˜¾è‘—æå‡äº‹ä»¶é©±åŠ¨åŠ¨ä½œè¯†åˆ«çš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œè½»é‡çº§å˜ä½“å±•ç¤ºäº†è¾¹ç¼˜éƒ¨ç½²çš„æ½œåŠ›ï¼Œä¸ºé«˜æ•ˆäº‹ä»¶è§†è§‰ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent in multi-branch architectures, and (iii) the under-utilization of spectral information in capturing global motion patterns. To address these challenges, we propose an efficient EAR framework named HoloEv-Net. First, to simultaneously tackle representation and structural redundancies, we introduce a Compact Holographic Spatiotemporal Representation (CHSR). Departing from computationally expensive voxel grids, CHSR implicitly embeds horizontal spatial cues into the Time-Height (T-H) view, effectively preserving 3D spatiotemporal contexts within a 2D representation. Second, to exploit the neglected spectral cues, we design a Global Spectral Gating (GSG) module. By leveraging the Fast Fourier Transform (FFT) for global token mixing in the frequency domain, GSG enhances the representation capability with negligible parameter overhead. Extensive experiments demonstrate the scalability and effectiveness of our framework. Specifically, HoloEv-Net-Base achieves state-of-the-art performance on THU-EACT-50-CHL, HARDVS and DailyDVS-200, outperforming existing methods by 10.29%, 1.71% and 6.25%, respectively. Furthermore, our lightweight variant, HoloEv-Net-Small, delivers highly competitive accuracy while offering extreme efficiency, reducing parameters by 5.4 times, FLOPs by 300times, and latency by 2.4times compared to heavy baselines, demonstrating its potential for edge deployment.


### [2] [PEPR: Privileged Event-based Predictive Regularization for Domain Generalization](https://arxiv.org/abs/2602.04583)
*Gabriele Magrini, Federico Becattini, NiccolÃ² Biondi, Pietro Pala*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç‰¹æƒäº‹ä»¶é¢„æµ‹æ­£åˆ™åŒ–ï¼ˆPEPRï¼‰çš„è·¨æ¨¡æ€æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒé²æ£’çš„å•ä¸€æ¨¡æ€RGBæ¨¡å‹ï¼Œé€šè¿‡åˆ©ç”¨ä»…åœ¨è®­ç»ƒæ—¶å¯ç”¨çš„äº‹ä»¶ç›¸æœºä½œä¸ºç‰¹æƒä¿¡æ¯ï¼Œè§£å†³è§†è§‰æ„ŸçŸ¥æ¨¡å‹åœ¨é¢†åŸŸæ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ·±åº¦è§†è§‰æ„ŸçŸ¥ç½‘ç»œå¯¹é¢†åŸŸåç§»é«˜åº¦æ•æ„Ÿï¼Œè¿™åœ¨å®é™…éƒ¨ç½²ä¸­æ„æˆå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºç°å®æ¡ä»¶å¾€å¾€ä¸è®­ç»ƒæ•°æ®å­˜åœ¨å·®å¼‚ã€‚è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³é¢†åŸŸæ³›åŒ–é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨å­¦ä¹ ä½¿ç”¨ç‰¹æƒä¿¡æ¯ï¼ˆLUPIï¼‰èŒƒå¼ï¼Œåœ¨è®­ç»ƒæœŸé—´åˆ©ç”¨äº‹ä»¶ç›¸æœºä½œä¸ºç‰¹æƒä¿¡æ¯æºï¼Œä»¥å¢å¼ºå•ä¸€æ¨¡æ€RGBæ¨¡å‹çš„é²æ£’æ€§ã€‚

**Method:** è¯¥ç ”ç©¶æå‡ºç‰¹æƒäº‹ä»¶é¢„æµ‹æ­£åˆ™åŒ–ï¼ˆPEPRï¼‰æ¡†æ¶ï¼Œå°†LUPIé‡æ–°æ„å»ºä¸ºå…±äº«æ½œåœ¨ç©ºé—´ä¸­çš„é¢„æµ‹é—®é¢˜ã€‚è¯¥æ–¹æ³•é¿å…ç›´æ¥è·¨æ¨¡æ€ç‰¹å¾å¯¹é½ï¼Œè€Œæ˜¯è®­ç»ƒRGBç¼–ç å™¨é¢„æµ‹åŸºäºäº‹ä»¶çš„æ½œåœ¨ç‰¹å¾ï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²è¯­ä¹‰ä¸°å¯Œæ€§çš„æƒ…å†µä¸‹æå–é²æ£’æ€§ã€‚ä¸¤ç§æ¨¡æ€å…·æœ‰äº’è¡¥ç‰¹æ€§ï¼šRGBæµè¯­ä¹‰å¯†é›†ä½†é¢†åŸŸä¾èµ–ï¼Œäº‹ä»¶æµç¨€ç–ä½†æ›´é¢†åŸŸä¸å˜ã€‚

**Result:** æ‰€æå‡ºçš„ç‹¬ç«‹RGBæ¨¡å‹åœ¨æ˜¼å¤œè½¬æ¢åŠå…¶ä»–é¢†åŸŸåç§»æ¡ä»¶ä¸‹å±•ç°å‡ºæŒç»­æ”¹è¿›çš„é²æ£’æ€§ã€‚åœ¨ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å‡ä¼˜äºåŸºäºå¯¹é½çš„åŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†PEPRæ¡†æ¶åœ¨é¢†åŸŸæ³›åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å°†ç‰¹æƒä¿¡æ¯å­¦ä¹ é‡æ–°æ„å»ºä¸ºé¢„æµ‹é—®é¢˜è€Œéç›´æ¥ç‰¹å¾å¯¹é½ï¼Œå¯ä»¥åœ¨ä¿æŒè¯­ä¹‰ä¸°å¯Œæ€§çš„åŒæ—¶æœ‰æ•ˆæå–é¢†åŸŸä¸å˜æ€§ã€‚è¿™ä¸€æ¡†æ¶ä¸ºåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®å¢å¼ºå•ä¸€æ¨¡æ€æ¨¡å‹çš„é²æ£’æ€§æä¾›äº†æ–°æ€è·¯ï¼Œå¯¹å®é™…éƒ¨ç½²ä¸­åº”å¯¹é¢†åŸŸåç§»å…·æœ‰é‡è¦ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Deep neural networks for visual perception are highly susceptible to domain shift, which poses a critical challenge for real-world deployment under conditions that differ from the training data. To address this domain generalization challenge, we propose a cross-modal framework under the learning using privileged information (LUPI) paradigm for training a robust, single-modality RGB model. We leverage event cameras as a source of privileged information, available only during training. The two modalities exhibit complementary characteristics: the RGB stream is semantically dense but domain-dependent, whereas the event stream is sparse yet more domain-invariant. Direct feature alignment between them is therefore suboptimal, as it forces the RGB encoder to mimic the sparse event representation, thereby losing semantic detail. To overcome this, we introduce Privileged Event-based Predictive Regularization (PEPR), which reframes LUPI as a predictive problem in a shared latent space. Instead of enforcing direct cross-modal alignment, we train the RGB encoder with PEPR to predict event-based latent features, distilling robustness without sacrificing semantic richness. The resulting standalone RGB model consistently improves robustness to day-to-night and other domain shifts, outperforming alignment-based baselines across object detection and semantic segmentation.


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [3] [Real-time processing of analog signals on accelerated neuromorphic hardware](https://arxiv.org/abs/2602.04582)
*Yannik Stradmann, Johannes Schemmel, Mihai A. Petrovici, Laura Kriener*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç›´æ¥æ¨¡æ‹Ÿä¿¡å·æ³¨å…¥æ–¹æ³•ï¼Œç”¨äºç¥ç»å½¢æ€ç³»ç»Ÿï¼Œæ¶ˆé™¤äº†ä¼ ç»Ÿä¿¡å·è½¬æ¢ä¸­çš„åŠŸç‡å¯†é›†å‹æ¨¡æ•°/æ•°æ¨¡è½¬æ¢ï¼Œå®ç°äº†ä»ä¼ æ„Ÿå™¨è¾“å…¥åˆ°ç‰©ç†åŠ¨ä½œçš„å®Œå…¨ç‰‡ä¸Šå¤„ç†ç®¡é“ï¼Œå¹¶åœ¨BrainScaleS-2å¹³å°ä¸Šå±•ç¤ºäº†å®æ—¶å£°æºå®šä½ä¸ä¼ºæœç”µæœºå¯¹å‡†çš„åº”ç”¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿç¥ç»å½¢æ€ç³»ç»Ÿé€šå¸¸ä½¿ç”¨äº‹ä»¶ä¼ æ„Ÿå™¨æˆ–å°†è¾“å…¥ä¿¡å·è½¬æ¢ä¸ºè„‰å†²ï¼Œè¿™æ¶‰åŠåŠŸç‡å¯†é›†å‹çš„æ¨¡æ•°/æ•°æ¨¡è½¬æ¢è¿‡ç¨‹ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€æ•ˆç‡é—®é¢˜ï¼Œé€šè¿‡ç›´æ¥æ¨¡æ‹Ÿä¿¡å·æ³¨å…¥æ–¹æ³•ï¼Œä¸ºè¿‘ä¼ æ„Ÿå™¨å¤„ç†æä¾›æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦å®æ—¶å¤„ç†çš„åµŒå…¥å¼åº”ç”¨åœºæ™¯ã€‚

**Method:** æœ¬ç ”ç©¶é‡‡ç”¨BrainScaleS-2æ··åˆä¿¡å·ç¥ç»å½¢æ€ç ”ç©¶å¹³å°ï¼Œå®ç°äº†ç›´æ¥æ¨¡æ‹Ÿä¿¡å·æ³¨å…¥æŠ€æœ¯ï¼Œå°†éº¦å…‹é£ä¿¡å·ç›´æ¥è¾“å…¥åˆ°å¹³å°çš„æ¨¡æ‹Ÿè®¡ç®—å•å…ƒä¸­ã€‚åˆ©ç”¨è„‰å†²ç¥ç»ç½‘ç»œå°†åŒè€³æ—¶é—´å·®è½¬æ¢ä¸ºç©ºé—´ç¼–ç ï¼Œå¹¶é€šè¿‡åµŒå…¥å¼å¾®å¤„ç†å™¨æ§åˆ¶ä¼ºæœç”µæœºæ‰§è¡Œå™¨ï¼Œæ„å»ºäº†ä»ä¼ æ„Ÿå™¨è¾“å…¥åˆ°ç‰©ç†åŠ¨ä½œçš„å®Œæ•´ç‰‡ä¸Šå¤„ç†ç®¡é“ã€‚

**Result:** ç ”ç©¶æˆåŠŸå±•ç¤ºäº†ç›´æ¥è¿ç»­å€¼ä¼ æ„Ÿå™¨æ•°æ®æ³¨å…¥BrainScaleS-2 ASICæ¨¡æ‹Ÿè®¡ç®—å•å…ƒçš„é¦–ä¸ªæ¼”ç¤ºï¼Œä»¥åŠä½¿ç”¨å…¶åµŒå…¥å¼å¾®å¤„ç†å™¨è¿›è¡Œæ‰§è¡Œå™¨æ§åˆ¶ã€‚ç³»ç»Ÿèƒ½å¤Ÿå®æ—¶å®šä½ç¬æ€å™ªå£°å³°å€¼çš„ç©ºé—´æ–¹å‘ï¼Œå¹¶æ§åˆ¶ä¼ºæœç”µæœºå¯¹å‡†å£°æºæ–¹å‘ï¼Œå……åˆ†åˆ©ç”¨äº†å¹³å°1000å€çš„åŠ é€Ÿå› å­ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç›´æ¥æ¨¡æ‹Ÿä¿¡å·æ³¨å…¥æ–¹æ³•çš„å¯è¡Œæ€§å’Œæ•ˆç‡ä¼˜åŠ¿ï¼Œä¸ºç¥ç»å½¢æ€è®¡ç®—ç³»ç»Ÿæä¾›äº†æ›´èŠ‚èƒ½çš„ä¼ æ„Ÿå™¨æ¥å£æ–¹æ¡ˆã€‚å®Œå…¨ç‰‡ä¸Šå¤„ç†ç®¡é“çš„å®ç°å±•ç¤ºäº†ç¥ç»å½¢æ€ç¡¬ä»¶åœ¨å®æ—¶åµŒå…¥å¼åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥é«˜æ•ˆè¿‘ä¼ æ„Ÿå™¨å¤„ç†ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Sensory processing with neuromorphic systems is typically done by using either event-based sensors or translating input signals to spikes before presenting them to the neuromorphic processor. Here, we offer an alternative approach: direct analog signal injection eliminates superfluous and power-intensive analog-to-digital and digital-to-analog conversions, making it particularly suitable for efficient near-sensor processing. We demonstrate this by using the accelerated BrainScaleS-2 mixed-signal neuromorphic research platform and interfacing it directly to microphones and a servo-motor-driven actuator. Utilizing BrainScaleS-2's 1000-fold acceleration factor, we employ a spiking neural network to transform interaural time differences into a spatial code and thereby predict the location of sound sources. Our primary contributions are the first demonstrations of direct, continuous-valued sensor data injection into the analog compute units of the BrainScaleS-2 ASIC, and actuator control using its embedded microprocessors. This enables a fully on-chip processing pipeline$\unicode{x2014}$from sensory input handling, via spiking neural network processing to physical action. We showcase this by programming the system to localize and align a servo motor with the spatial direction of transient noise peaks in real-time.
