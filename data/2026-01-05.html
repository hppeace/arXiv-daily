<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-05.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.NE">cs.NE</a> [Total: 3]</li>
</ul>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="1-personalized-spiking-neural-networks-with-ferroelectric-synapses-for-eeg-signal-processing">[1] <a href="https://arxiv.org/abs/2601.00020">Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing</a></h3>
<p><em>Nikhil Garg, Anxiong Song, Niklas Plessnig, Nathan Savoia, Laura BÃ©gon-Lours</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶å±•ç¤ºäº†å°–å³°ç¥ç»ç½‘ç»œåœ¨é“ç”µå¿†é˜»çªè§¦å™¨ä»¶ä¸Šå®ç°è‡ªé€‚åº”è„‘ç”µè¿åŠ¨æƒ³è±¡è§£ç çš„å¯è¡Œæ€§ï¼Œæå‡ºäº†ä¸€ç§è®¾å¤‡æ„ŸçŸ¥çš„æƒé‡æ›´æ–°ç­–ç•¥ï¼Œåœ¨çœŸå®å™¨ä»¶çº¦æŸä¸‹å®ç°äº†ä¸è½¯ä»¶SNNç›¸å½“çš„åˆ†ç±»æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºäºè„‘ç”µå›¾çš„è„‘æœºæ¥å£å—åˆ°éå¹³ç¨³ç¥ç»ä¿¡å·çš„å½±å“ï¼Œè¿™äº›ä¿¡å·åœ¨ä¸åŒä¼šè¯å’Œä¸ªä½“é—´å­˜åœ¨å·®å¼‚ï¼Œé™åˆ¶äº†ä¸»ä½“æ— å…³æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦åœ¨èµ„æºå—é™å¹³å°ä¸Šå®ç°è‡ªé€‚åº”å’Œä¸ªæ€§åŒ–å­¦ä¹ ï¼Œè€Œå¯ç¼–ç¨‹å¿†é˜»ç¡¬ä»¶ä¸ºæ­¤æä¾›äº†æœ‰å‰æ™¯çš„åŸºåº•ï¼Œä½†å®é™…å®ç°å—åˆ°æƒé‡åˆ†è¾¨ç‡æœ‰é™ã€å™¨ä»¶å˜å¼‚æ€§ã€éçº¿æ€§ç¼–ç¨‹åŠ¨æ€å’Œæœ‰é™å™¨ä»¶è€ä¹…æ€§ç­‰æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶åˆ¶é€ ã€è¡¨å¾å¹¶å»ºæ¨¡äº†é“ç”µçªè§¦å™¨ä»¶ï¼Œè¯„ä¼°äº†å·ç§¯-å¾ªç¯å°–å³°ç¥ç»ç½‘ç»œæ¶æ„åœ¨ä¸¤ç§äº’è¡¥éƒ¨ç½²ç­–ç•¥ä¸‹çš„è¡¨ç°ï¼šä½¿ç”¨é“ç”µçªè§¦æ¨¡å‹è¿›è¡Œè®¾å¤‡æ„ŸçŸ¥è®­ç»ƒï¼Œä»¥åŠè½¯ä»¶è®­ç»ƒæƒé‡è½¬ç§»åè¿›è¡Œä½å¼€é”€ç‰‡ä¸Šé‡è°ƒä¼˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§è®¾å¤‡æ„ŸçŸ¥æƒé‡æ›´æ–°ç­–ç•¥ï¼Œå…¶ä¸­åŸºäºæ¢¯åº¦çš„æ›´æ–°åœ¨æ•°å­—ä¸Šç´¯ç§¯ï¼Œä»…åœ¨è¶…è¿‡é˜ˆå€¼æ—¶è½¬æ¢ä¸ºç¦»æ•£ç¼–ç¨‹äº‹ä»¶ï¼Œä»¥æ¨¡æ‹Ÿéçº¿æ€§çŠ¶æ€ä¾èµ–ç¼–ç¨‹åŠ¨æ€åŒæ—¶å‡å°‘ç¼–ç¨‹é¢‘ç‡ã€‚</p>
<p><strong>Result:</strong> ä¸¤ç§éƒ¨ç½²ç­–ç•¥å‡å®ç°äº†ä¸æœ€å…ˆè¿›è½¯ä»¶å°–å³°ç¥ç»ç½‘ç»œç›¸å½“çš„åˆ†ç±»æ€§èƒ½ï¼Œé€šè¿‡ä»…é‡è®­ç»ƒæœ€ç»ˆç½‘ç»œå±‚å®ç°çš„ä¸»ä½“ç‰¹å®šè¿ç§»å­¦ä¹ è¿›ä¸€æ­¥æé«˜äº†åˆ†ç±»å‡†ç¡®ç‡ï¼Œè¯æ˜äº†åœ¨çœŸå®å™¨ä»¶çº¦æŸä¸‹é“ç”µç¡¬ä»¶èƒ½å¤Ÿæ”¯æŒç¨³å¥çš„ä½å¼€é”€è‡ªé€‚åº”ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å¯ç¼–ç¨‹é“ç”µç¡¬ä»¶èƒ½å¤Ÿæ”¯æŒå°–å³°ç¥ç»ç½‘ç»œä¸­çš„ç¨³å¥ä½å¼€é”€è‡ªé€‚åº”ï¼Œä¸ºç¥ç»ä¿¡å·çš„ä¸ªæ€§åŒ–ç¥ç»å½¢æ€å¤„ç†å¼€è¾Ÿäº†å®ç”¨è·¯å¾„ï¼Œè®¾å¤‡æ„ŸçŸ¥è®­ç»ƒå’Œæƒé‡è½¬ç§»ç­–ç•¥ä¸ºåœ¨èµ„æºå—é™å¹³å°ä¸Šå®ç°è‡ªé€‚åº”è„‘æœºæ¥å£æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Electroencephalography (EEG)-based brain-computer interfaces (BCIs) are strongly affected by non-stationary neural signals that vary across sessions and individuals, limiting the generalization of subject-agnostic models and motivating adaptive and personalized learning on resource-constrained platforms. Programmable memristive hardware offers a promising substrate for such post-deployment adaptation; however, practical realization is challenged by limited weight resolution, device variability, nonlinear programming dynamics, and finite device endurance. In this work, we show that spiking neural networks (SNNs) can be deployed on ferroelectric memristive synaptic devices for adaptive EEG-based motor imagery decoding under realistic device constraints. We fabricate, characterize, and model ferroelectric synapses. We evaluate a convolutional-recurrent SNN architecture under two complementary deployment strategies: (i) device-aware training using a ferroelectric synapse model, and (ii) transfer of software-trained weights followed by low-overhead on-device re-tuning. To enable efficient adaptation, we introduce a device-aware weight-update strategy in which gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded, emulating nonlinear, state-dependent programming dynamics while reducing programming frequency. Both deployment strategies achieve classification performance comparable to state-of-the-art software-based SNNs. Furthermore, subject-specific transfer learning achieved by retraining only the final network layers improves classification accuracy. These results demonstrate that programmable ferroelectric hardware can support robust, low-overhead adaptation in spiking neural networks, opening a practical path toward personalized neuromorphic processing of neural signals.</p>
<h3 id="2-modern-neuromorphic-ai-from-intra-token-to-inter-token-processing">[2] <a href="https://arxiv.org/abs/2601.00245">Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing</a></h3>
<p><em>Osvaldo Simeone</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç³»ç»Ÿé˜è¿°äº†ç°ä»£ç¥ç»å½¢æ€AIæ¨¡å‹é€šè¿‡"ä»¤ç‰Œå†…å¤„ç†"å’Œ"ä»¤ç‰Œé—´å¤„ç†"çš„è§†è§’ï¼Œæ­ç¤ºäº†ç¥ç»å½¢æ€è®¡ç®—åŸç†ã€çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸Transformeræ¶æ„ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œå¹¶ç»¼è¿°äº†ç›¸åº”çš„è®­ç»ƒæ–¹æ³•å­¦ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†å¼ºå¤§çš„æ•°æ®å¤„ç†å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½†åŒæ—¶ä¹Ÿä¼´éšç€æ€¥å‰§å¢é•¿çš„èƒ½è€—éœ€æ±‚ï¼Œè¿™ä¿ƒä½¿ç ”ç©¶è€…é‡æ–°å…³æ³¨ç¥ç»å½¢æ€è®¡ç®—åŸç†ï¼Œè¯¥åŸç†é€šè¿‡ç¦»æ•£ç¨€ç–æ¿€æ´»ã€å¾ªç¯åŠ¨åŠ›å­¦å’Œéçº¿æ€§åé¦ˆæœºåˆ¶ï¼Œæœ‰æœ›å®ç°ç±»ä¼¼å¤§è„‘çš„é«˜èƒ½æ•ˆè®¡ç®—ã€‚</p>
<p><strong>Method:</strong> è®ºæ–‡é€šè¿‡"ä»¤ç‰Œå†…å¤„ç†"å’Œ"ä»¤ç‰Œé—´å¤„ç†"çš„åŒºåˆ†æ¡†æ¶ç³»ç»Ÿåˆ†æç°ä»£ç¥ç»å½¢æ€AIæ¨¡å‹ï¼Œå…¶ä¸­ä»¤ç‰Œå†…å¤„ç†ä¸»è¦åŸºäºè„‰å†²ç¥ç»ç½‘ç»œå¯¹åŒä¸€å‘é‡è¾“å…¥çš„å¤šé€šé“ç‰¹å¾è¿›è¡Œå˜æ¢ï¼Œè€Œä»¤ç‰Œé—´å¤„ç†åˆ™åˆ©ç”¨çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦æˆ–ç¨€ç–è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡ç›¸å…³æ€§é€‰æ‹©æ€§åœ°ç»„åˆä¸åŒä¿¡æ¯å…ƒç´ ï¼Œå®ç°å…³è”è®°å¿†æœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å»ºç«‹äº†ç¥ç»å½¢æ€æ¨¡å‹ã€çŠ¶æ€ç©ºé—´æ¨¡å‹å’ŒTransformeræ¶æ„ä¹‹é—´çš„ç³»ç»Ÿæ€§è”ç³»ï¼Œå±•ç¤ºäº†ç°ä»£AIæ¶æ„å¦‚ä½•é€šè¿‡é‡åº¦é‡åŒ–æ¿€æ´»ã€çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦å’Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä½“ç°ç¥ç»å½¢æ€åŸç†ï¼Œå¹¶ç»¼è¿°äº†ä»åˆ©ç”¨å¹¶è¡Œå·ç§¯å¤„ç†çš„ä»£ç†æ¢¯åº¦åˆ°åŸºäºå¼ºåŒ–å­¦ä¹ æœºåˆ¶çš„å±€éƒ¨å­¦ä¹ è§„åˆ™ç­‰å¤šç§è®­ç»ƒæ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç†è§£ç¥ç»å½¢æ€è®¡ç®—åŸç†åœ¨ç°ä»£AIæ¶æ„ä¸­çš„ä½“ç°æä¾›äº†ç»Ÿä¸€æ¡†æ¶ï¼Œæ­ç¤ºäº†é€šè¿‡ä»¤ç‰Œé—´å¤„ç†å®ç°é«˜æ•ˆä¿¡æ¯æ•´åˆçš„æ½œåŠ›ï¼Œä¸ºè®¾è®¡ä¸‹ä¸€ä»£é«˜èƒ½æ•ˆAIç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ï¼ŒåŒæ—¶ç³»ç»ŸåŒ–çš„è®­ç»ƒæ–¹æ³•ç»¼è¿°ä¸ºå®é™…åº”ç”¨ç¥ç»å½¢æ€AIæ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.</p>
<h3 id="3-three-factor-delay-learning-rules-for-spiking-neural-networks">[3] <a href="https://arxiv.org/abs/2601.00668">Three factor delay learning rules for spiking neural networks</a></h3>
<p><em>Luke Vassallo, Nima Taherinejad</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨çº¿å­¦ä¹ è„‰å†²ç¥ç»ç½‘ç»œä¸­çªè§¦å’Œè½´çªå»¶è¿Ÿå‚æ•°çš„ä¸‰å› å­å­¦ä¹ è§„åˆ™ï¼Œé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„å»¶è¿Ÿå‚æ•°æ˜¾è‘—æå‡äº†SNNåœ¨æ—¶åºä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†æ¨¡å‹è§„æ¨¡å’Œæ¨ç†å»¶è¿Ÿã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è„‰å†²ç¥ç»ç½‘ç»œé€šå¸¸ä»…å­¦ä¹ çªè§¦æƒé‡å‚æ•°ï¼Œå¯¹æ—¶åºæ¨¡å¼è¯†åˆ«è´¡çŒ®æœ‰é™ï¼Œè€Œç°æœ‰çš„å»¶è¿Ÿå­¦ä¹ æ–¹æ³•ä¾èµ–å¤§å‹ç½‘ç»œå’Œç¦»çº¿å­¦ä¹ ï¼Œæ— æ³•åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å®ç°å®æ—¶æ“ä½œï¼Œå› æ­¤éœ€è¦å¼€å‘é€‚ç”¨äºåœ¨çº¿å­¦ä¹ çš„å»¶è¿Ÿå‚æ•°ä¼˜åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶åœ¨åŸºäºLIFçš„å‰é¦ˆå’Œå¾ªç¯SNNä¸­å¼•å…¥çªè§¦å’Œè½´çªå»¶è¿Ÿå‚æ•°ï¼Œæå‡ºä¸‰å› å­å­¦ä¹ è§„åˆ™åœ¨çº¿åŒæ—¶å­¦ä¹ å»¶è¿Ÿå‚æ•°ï¼Œé‡‡ç”¨å¹³æ»‘é«˜æ–¯ä»£ç†å‡½æ•°è¿‘ä¼¼è„‰å†²å¯¼æ•°è®¡ç®—èµ„æ ¼è¿¹ï¼Œç»“åˆè‡ªä¸Šè€Œä¸‹çš„è¯¯å·®ä¿¡å·ç¡®å®šå‚æ•°æ›´æ–°ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜å¼•å…¥å»¶è¿Ÿå‚æ•°ç›¸æ¯”ä»…å­¦ä¹ æƒé‡çš„åŸºçº¿å‡†ç¡®ç‡æå‡é«˜è¾¾20%ï¼Œåœ¨ç›¸ä¼¼å‚æ•°æ•°é‡ä¸‹è”åˆå­¦ä¹ æƒé‡å’Œå»¶è¿Ÿå‡†ç¡®ç‡æå‡è¾¾14%ï¼›åœ¨SHDè¯­éŸ³è¯†åˆ«æ•°æ®é›†ä¸Šè¾¾åˆ°ä¸ç¦»çº¿åå‘ä¼ æ’­æ–¹æ³•ç›¸å½“çš„å‡†ç¡®ç‡ï¼Œç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•æ¨¡å‹è§„æ¨¡å‡å°‘6.6å€ï¼Œæ¨ç†å»¶è¿Ÿé™ä½67%ï¼Œä»…æŸå¤±2.4%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•é€šè¿‡åœ¨çº¿å­¦ä¹ å»¶è¿Ÿå‚æ•°æ˜¾è‘—æå‡äº†SNNçš„æ—¶åºå¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†æ¨¡å‹å¤æ‚åº¦å’Œè®¡ç®—éœ€æ±‚ï¼Œæœ‰åˆ©äºè®¾è®¡åŠŸè€—å’Œé¢ç§¯å—é™çš„ç¥ç»å½¢æ€å¤„ç†å™¨ï¼Œæ”¯æŒè®¾å¤‡ç«¯å­¦ä¹ å¹¶é™ä½å†…å­˜éœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Spiking Neural Networks (SNNs) are dynamical systems that operate on spatiotemporal data, yet their learnable parameters are often limited to synaptic weights, contributing little to temporal pattern recognition. Learnable parameters that delay spike times can improve classification performance in temporal tasks, but existing methods rely on large networks and offline learning, making them unsuitable for real-time operation in resource-constrained environments. In this paper, we introduce synaptic and axonal delays to leaky integrate and fire (LIF)-based feedforward and recurrent SNNs, and propose three-factor learning rules to simultaneously learn delay parameters online. We employ a smooth Gaussian surrogate to approximate spike derivatives exclusively for the eligibility trace calculation, and together with a top-down error signal determine parameter updates. Our experiments show that incorporating delays improves accuracy by up to 20% over a weights-only baseline, and for networks with similar parameter counts, jointly learning weights and delays yields up to 14% higher accuracy. On the SHD speech recognition dataset, our method achieves similar accuracy to offline backpropagation-based approaches. Compared to state-of-the-art methods, it reduces model size by 6.6x and inference latency by 67%, with only a 2.4% drop in classification accuracy. Our findings benefit the design of power and area-constrained neuromorphic processors by enabling on-device learning and lowering memory requirements.</p>
  </article>
</body>
</html>
