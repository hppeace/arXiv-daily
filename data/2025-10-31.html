<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-31.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-spiking-patches-asynchronous-sparse-and-efficient-tokens-for-event-cameras">[1] <a href="https://arxiv.org/abs/2510.26614">Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras</a></h3>
<p><em>Christoffer Koo Ã˜hrstrÃ¸m, Ronja GÃ¼ldenring, Lazaros Nalpantidis</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Spiking Patchesäº‹ä»¶æ ‡è®°åŒ–æ–¹æ³•ï¼Œä¸“é—¨ä¸ºäº‹ä»¶ç›¸æœºè®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒäº‹ä»¶å¼‚æ­¥æ€§å’Œç©ºé—´ç¨€ç–æ€§çš„åŒæ—¶å®ç°é«˜æ•ˆæ¨ç†ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒç²¾åº¦çš„å‰æä¸‹ï¼Œæ¨ç†é€Ÿåº¦æ¯”ä½“ç´ æ ‡è®°å¿«3.4å€ï¼Œæ¯”å¸§è¡¨ç¤ºå¿«10.4å€ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„äº‹ä»¶ç›¸æœºè¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚å¸§è¡¨ç¤ºå’Œä½“ç´ è¡¨ç¤ºï¼‰è™½ç„¶èƒ½è·å¾—é«˜ç²¾åº¦ï¼Œä½†ç ´åäº†äº‹ä»¶çš„å¼‚æ­¥ç‰¹æ€§å’Œç©ºé—´ç¨€ç–æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§èƒ½å¤Ÿä¿æŒäº‹ä»¶ç›¸æœºç‹¬ç‰¹å±æ€§çš„äº‹ä»¶è¡¨ç¤ºæ–¹æ³•ï¼ŒåŒæ—¶ä¸ç‰ºç‰²è¯†åˆ«ç²¾åº¦ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†Spiking Patchesæ ‡è®°åŒ–æ–¹æ³•ï¼Œå°†å¼‚æ­¥ç¨€ç–çš„äº‹ä»¶æµè½¬æ¢ä¸ºæ ‡è®°è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒäº‹ä»¶å¼‚æ­¥æ€§å’Œç©ºé—´ç¨€ç–æ€§çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨GNNã€PCNå’ŒTransformerç­‰æ¶æ„åœ¨å§¿æ€è¯†åˆ«å’Œç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpiking Patchesæ ‡è®°åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ¯”ä½“ç´ æ ‡è®°å¿«3.4å€ï¼Œæ¯”å¸§è¡¨ç¤ºå¿«10.4å€ã€‚åœ¨ç²¾åº¦æ–¹é¢ä¸ç°æœ‰æ–¹æ³•ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼Œå§¿æ€è¯†åˆ«ç»å¯¹æå‡è¾¾3.8%ï¼Œç‰©ä½“æ£€æµ‹æå‡è¾¾1.4%ã€‚</p>
<p><strong>Conclusion:</strong> æ ‡è®°åŒ–ä¸ºäº‹ä»¶è§†è§‰ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ï¼Œæ ‡å¿—ç€å‘ä¿æŒäº‹ä»¶ç›¸æœºç‰¹æ€§çš„æ–¹æ³•è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚è¯¥æ–¹æ³•è¯æ˜äº†åœ¨ä¿æŒäº‹ä»¶ç›¸æœºå›ºæœ‰å±æ€§çš„åŒæ—¶å®ç°é«˜æ•ˆå‡†ç¡®æ¨ç†çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥äº‹ä»¶è§†è§‰ç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="2-unsupervised-local-learning-based-on-voltage-dependent-synaptic-plasticity-for-resistive-and-ferroelectric-synapses">[2] <a href="https://arxiv.org/abs/2510.25787">Unsupervised local learning based on voltage-dependent synaptic plasticity for resistive and ferroelectric synapses</a></h3>
<p><em>Nikhil Garg, Ismael Balafrej, Joao Henrique Quintino Palhares, Laura BÃ©gon-Lours, Davide Florini, Donato Francesco Falcone, Tommaso Stecconi, Valeria Bragaglia, Bert Jan Offrein, Jean-Michel Portal, Damien Querlioz, Yann Beilliard, Dominique Drouin, Fabien Alibart</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºç”µå‹ä¾èµ–æ€§çªè§¦å¯å¡‘æ€§ï¼ˆVDSPï¼‰ä½œä¸ºåŸºäºå¿†é˜»çªè§¦çš„é«˜æ•ˆæ— ç›‘ç£å±€éƒ¨å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€å¤æ‚çš„è„‰å†²æ•´å½¢ç”µè·¯å³å¯å®ç°åœ¨çº¿å­¦ä¹ ï¼Œå¹¶åœ¨ä¸‰ç§ä¸åŒç±»å‹çš„å¿†é˜»å™¨ä»¶ä¸ŠéªŒè¯äº†å…¶åœ¨MNISTæ¨¡å¼è¯†åˆ«ä»»åŠ¡ä¸­çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸ŠAIéƒ¨ç½²é¢ä¸´èƒ½è€—å’ŒåŠŸèƒ½æ€§çš„é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦è„‘å¯å‘çš„å­¦ä¹ æœºåˆ¶æ¥å®ç°ä½åŠŸè€—å®æ—¶è‡ªé€‚åº”ï¼Œè€ŒåŸºäºçº³ç±³çº§å¿†é˜»å™¨çš„å†…å­˜è®¡ç®—åœ¨æ”¯æŒè¾¹ç¼˜è®¾å¤‡æ‰§è¡ŒAIå·¥ä½œè´Ÿè½½æ–¹é¢å¯èƒ½å‘æŒ¥å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†ç”µå‹ä¾èµ–æ€§çªè§¦å¯å¡‘æ€§ï¼ˆVDSPï¼‰ä½œä¸ºåŸºäºHebbianåŸç†çš„é«˜æ•ˆæ— ç›‘ç£å±€éƒ¨å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¿å…äº†ä¼ ç»Ÿè„‰å†²æ—¶åºä¾èµ–æ€§å¯å¡‘æ€§æ‰€éœ€çš„å¤æ‚è„‰å†²æ•´å½¢ç”µè·¯ï¼Œå¹¶åœ¨ä¸‰ç§å¿†é˜»å™¨ä»¶ï¼ˆTiOâ‚‚ã€HfOâ‚‚åŸºé‡‘å±æ°§åŒ–ç‰©ä¸çŠ¶çªè§¦å’ŒHfZrOâ‚„åŸºé“ç”µéš§é“ç»“ï¼‰ä¸Šè¿›è¡Œäº†é€‚åº”æ€§éªŒè¯ã€‚</p>
<p><strong>Result:</strong> ç³»ç»Ÿçº§å°–å³°ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿåœ¨MNISTæ¨¡å¼è¯†åˆ«ä»»åŠ¡ä¸­éªŒè¯äº†æ— ç›‘ç£å­¦ä¹ æ€§èƒ½ï¼Œæ‰€æœ‰å™¨ä»¶ä½¿ç”¨200ä¸ªç¥ç»å…ƒå‡å®ç°äº†è¶…è¿‡83%çš„å‡†ç¡®ç‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼ŒåŒæ—¶è¯„ä¼°äº†å™¨ä»¶å˜å¼‚æ€§å½±å“å¹¶æå‡ºäº†å¢å¼ºé²æ£’æ€§çš„ç¼“è§£ç­–ç•¥ã€‚</p>
<p><strong>Conclusion:</strong> VDSPæ–¹æ³•ä¸ºè¾¹ç¼˜AIåº”ç”¨æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ— ç›‘ç£å­¦ä¹ è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡é€‚åº”ä¸åŒç±»å‹çš„å¿†é˜»å™¨ä»¶ç‰¹æ€§å¹¶è§£å†³å˜å¼‚æ€§é—®é¢˜ï¼Œä¸ºå®ç°ä½åŠŸè€—å®æ—¶è‡ªé€‚åº”ç³»ç»Ÿé“ºå¹³äº†é“è·¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>The deployment of AI on edge computing devices faces significant challenges
related to energy consumption and functionality. These devices could greatly
benefit from brain-inspired learning mechanisms, allowing for real-time
adaptation while using low-power. In-memory computing with nanoscale resistive
memories may play a crucial role in enabling the execution of AI workloads on
these edge devices. In this study, we introduce voltage-dependent synaptic
plasticity (VDSP) as an efficient approach for unsupervised and local learning
in memristive synapses based on Hebbian principles. This method enables online
learning without requiring complex pulse-shaping circuits typically necessary
for spike-timing-dependent plasticity (STDP). We show how VDSP can be
advantageously adapted to three types of memristive devices (TiO$_2$,
HfO$_2$-based metal-oxide filamentary synapses, and HfZrO$_4$-based
ferroelectric tunnel junctions (FTJ)) with disctinctive switching
characteristics. System-level simulations of spiking neural networks
incorporating these devices were conducted to validate unsupervised learning on
MNIST-based pattern recognition tasks, achieving state-of-the-art performance.
The results demonstrated over 83% accuracy across all devices using 200
neurons. Additionally, we assessed the impact of device variability, such as
switching thresholds and ratios between high and low resistance state levels,
and proposed mitigation strategies to enhance robustness.</p>
  </article>
</body>
</html>
