<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-02-13.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="1-energy-aware-spike-budgeting-for-continual-learning-in-spiking-neural-networks-for-neuromorphic-vision">[1] <a href="https://arxiv.org/abs/2602.12236">Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision</a></h3>
<p><em>Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘è„‰å†²ç¥ç»ç½‘ç»œæŒç»­å­¦ä¹ çš„èƒ½é‡æ„ŸçŸ¥è„‰å†²é¢„ç®—æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ•´åˆç»éªŒå›æ”¾ã€å¯å­¦ä¹ çš„æ³„æ¼ç§¯åˆ†å‘æ”¾ç¥ç»å…ƒå‚æ•°å’Œè‡ªé€‚åº”è„‰å†²è°ƒåº¦å™¨ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œæ•°æ®é›†ç‰¹å®šçš„èƒ½é‡çº¦æŸï¼Œä»è€Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½èƒ½è€—ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºäºè„‰å†²ç¥ç»ç½‘ç»œçš„ç¥ç»å½¢æ€è§†è§‰ç³»ç»Ÿè™½ç„¶ä¸ºäº‹ä»¶å‹å’Œå¸§å‹ç›¸æœºæä¾›äº†è¶…ä½åŠŸè€—æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†ç¾éš¾æ€§é—å¿˜ä»ç„¶æ˜¯å…¶åœ¨æŒç»­æ¼”åŒ–ç¯å¢ƒä¸­éƒ¨ç½²çš„å…³é”®éšœç¢ã€‚ç°æœ‰çš„æŒç»­å­¦ä¹ æ–¹æ³•ä¸»è¦é’ˆå¯¹äººå·¥ç¥ç»ç½‘ç»œå¼€å‘ï¼Œå¾ˆå°‘åŒæ—¶ä¼˜åŒ–å‡†ç¡®æ€§å’Œèƒ½é‡æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨äº‹ä»¶å‹æ•°æ®é›†ä¸Šçš„æ¢ç´¢å°¤ä¸ºæœ‰é™ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºäº†ä¸€ä¸ªèƒ½é‡æ„ŸçŸ¥çš„è„‰å†²é¢„ç®—æ¡†æ¶ï¼Œæ•´åˆäº†ç»éªŒå›æ”¾ã€å¯å­¦ä¹ çš„æ³„æ¼ç§¯åˆ†å‘æ”¾ç¥ç»å…ƒå‚æ•°ä»¥åŠè‡ªé€‚åº”è„‰å†²è°ƒåº¦å™¨ã€‚è¯¥æ¡†æ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œæ•°æ®é›†ç‰¹å®šçš„èƒ½é‡çº¦æŸï¼Œé€šè¿‡æ§åˆ¶è„‰å†²é¢„ç®—æ¥å®ç°èƒ½é‡æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<p><strong>Result:</strong> åœ¨å¸§å‹æ•°æ®é›†ï¼ˆMNISTã€CIFAR-10ï¼‰ä¸Šï¼Œè„‰å†²é¢„ç®—ä½œä¸ºç¨€ç–æ€§è¯±å¯¼æ­£åˆ™åŒ–å™¨ï¼Œåœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶å°†è„‰å†²ç‡é™ä½é«˜è¾¾47%ï¼›åœ¨äº‹ä»¶å‹æ•°æ®é›†ï¼ˆDVS-Gestureã€N-MNISTã€CIFAR-10-DVSï¼‰ä¸Šï¼Œå—æ§çš„é¢„ç®—æ¾å¼›ä½¿å‡†ç¡®ç‡æå‡é«˜è¾¾17.45ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”è®¡ç®—å¼€é”€æœ€å°ã€‚åœ¨æ¶µç›–ä¸¤ç§æ¨¡æ€çš„äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æœ€å°åŒ–åŠ¨æ€åŠŸè€—çš„åŒæ—¶è¡¨ç°å‡ºæŒç»­çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†è„‰å†²é¢„ç®—åœ¨ç¥ç»å½¢æ€è§†è§‰ç³»ç»Ÿä¸­å®ç°æŒç»­å­¦ä¹ çš„åŒé‡ä½œç”¨ï¼šåœ¨å¸§å‹æ•°æ®ä¸Šä½œä¸ºæ­£åˆ™åŒ–å™¨æé«˜ç¨€ç–æ€§ï¼Œåœ¨äº‹ä»¶å‹æ•°æ®ä¸Šé€šè¿‡é¢„ç®—æ¾å¼›æå‡å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½èƒ½è€—ï¼Œæ¨åŠ¨äº†ç¥ç»å½¢æ€è§†è§‰ç³»ç»Ÿä¸­æŒç»­å­¦ä¹ çš„å®é™…å¯è¡Œæ€§ï¼Œä¸ºèƒ½é‡å—é™ç¯å¢ƒä¸‹çš„è‡ªé€‚åº”æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.</p>
  </article>
</body>
</html>
