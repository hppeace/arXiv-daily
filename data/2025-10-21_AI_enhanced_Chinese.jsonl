{"id": "2510.17392", "categories": ["cs.NE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.17392", "abs": "https://arxiv.org/abs/2510.17392", "authors": ["Sonu Kumar", "Arjun S. Nair", "Bhawna Chaudhary", "Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine", "comment": null, "summary": "We present a Cortical Neural Pool (CNP) architecture featuring a high-speed,\nresource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike\nshared CORDIC-based DNN approaches, the proposed neuron leverages modular and\nperformance-optimised CORDIC stages with a latency-area trade-off. The FPGA\nimplementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved\nspeed, compared to SoTA designs, with 70% better normalised root mean square\nerror (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69\nGOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only\na 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The\noverall results indicate that the design shows biologically accurate,\nlow-resource spiking neural network implementations for resource-constrained\nedge AI applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCORDIC\u7684Hodgkin Huxley\u795e\u7ecf\u5143\u6a21\u578b\u548c\u76ae\u8d28\u795e\u7ecf\u6c60\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u751f\u7269\u7cbe\u786e\u3001\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u5728FPGA\u4e0a\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8d44\u6e90\u4f18\u5316\u548c\u6027\u80fd\u63d0\u5347\u3002\u8be5\u8bbe\u8ba1\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18AI\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684SNN\u5b9e\u73b0\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eCORDIC\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5b58\u5728\u8d44\u6e90\u5171\u4eab\u548c\u6027\u80fd\u9650\u5236\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18AI\u5e94\u7528\u5bf9\u751f\u7269\u7cbe\u786e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u9700\u6c42\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u8d44\u6e90\u7684SNN\u5b9e\u73b0\u65b9\u6848\uff0c\u89e3\u51b3\u73b0\u6709\u8bbe\u8ba1\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76ae\u8d28\u795e\u7ecf\u6c60\u67b6\u6784\uff0c\u91c7\u7528\u6a21\u5757\u5316\u548c\u6027\u80fd\u4f18\u5316\u7684CORDIC\u7ea7\u8054\u7ed3\u6784\uff0c\u8bbe\u8ba1\u4e86\u9ad8\u901f\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u57fa\u4e8eCORDIC\u7684Hodgkin Huxley\u795e\u7ecf\u5143\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5ef6\u8fdf-\u9762\u79ef\u6743\u8861\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u6bd4\u5171\u4eabCORDIC\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "FPGA\u5b9e\u73b0\u663e\u793a\uff0cRCHH\u795e\u7ecf\u5143\u76f8\u6bd4\u6700\u5148\u8fdb\u8bbe\u8ba1\u51cf\u5c11\u4e8624.5%\u7684LUT\u4f7f\u7528\uff0c\u901f\u5ea6\u63d0\u534735.2%\uff0c\u5f52\u4e00\u5316\u5747\u65b9\u6839\u8bef\u5dee\u6539\u558470%\u3002\u76ae\u8d28\u795e\u7ecf\u6c60\u5728MNIST\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e862.85\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u8fbe\u523012.69 GOPS\uff0c\u4e0e\u7b49\u6548DNN\u76f8\u6bd4\u7cbe\u5ea6\u4ec5\u4e0b\u964d0.35%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8eCORDIC\u7684Hodgkin Huxley\u795e\u7ecf\u5143\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u751f\u7269\u7cbe\u786e\u4e14\u8d44\u6e90\u9ad8\u6548\u7684SNN\u5b9e\u73b0\uff0c\u4e3a\u8fb9\u7f18AI\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u67b6\u6784\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6027\u80fd\u4f18\u5316\u7b56\u7565\u4e3a\u672a\u6765\u4f4e\u529f\u8017\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.17745", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17745", "abs": "https://arxiv.org/abs/2510.17745", "authors": ["Lars Niedermeier", "Vyom Shah", "Jeffrey L. Krichmar"], "title": "A Multi-Threading Kernel for Enabling Neuromorphic Edge Applications", "comment": "Submitted to ISCAS 2026", "summary": "Spiking Neural Networks (SNNs) have sparse, event driven processing that can\nleverage neuromorphic applications. In this work, we introduce a\nmulti-threading kernel that enables neuromorphic applications running at the\nedge, meaning they process sensory input directly and without any up-link to or\ndependency on a cloud service. The kernel shows speed-up gains over single\nthread processing by a factor of four on moderately sized SNNs and 1.7X on a\nSynfire network. Furthermore, it load-balances all cores available on\nmulti-core processors, such as ARM, which run today's mobile devices and is up\nto 70% more energy efficient compared to statical core assignment. The present\nwork can enable the development of edge applications that have low Size,\nWeight, and Power (SWaP), and can prototype the integration of neuromorphic\nchips.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ebf\u7a0b\u5185\u6838\uff0c\u4f7f\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u5b9e\u73b0\u4e864\u500d\u7684\u52a0\u901f\u6bd4\u548c70%\u7684\u80fd\u6548\u63d0\u5347\uff0c\u652f\u6301\u4f4eSWaP\u7684\u795e\u7ecf\u5f62\u6001\u5e94\u7528\u5f00\u53d1\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u5f62\u6001\u5e94\u7528\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u9762\u4e34\u5904\u7406\u6548\u7387\u4f4e\u548c\u80fd\u8017\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u5728\u65e0\u4e91\u670d\u52a1\u4f9d\u8d56\u6761\u4ef6\u4e0b\u76f4\u63a5\u5904\u7406\u611f\u5b98\u8f93\u5165\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u591a\u6838\u5904\u7406\u5668\u7684\u8d1f\u8f7d\u5747\u8861\u4f18\u5316\u9700\u6c42\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u7ebf\u7a0b\u5185\u6838\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u7a00\u758f\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u6838ARM\u5904\u7406\u5668\u4e0a\u7684\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\uff0c\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684\u9759\u6001\u6838\u5fc3\u5206\u914d\u65b9\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u5185\u6838\u5728\u4e2d\u7b49\u89c4\u6a21SNN\u4e0a\u5b9e\u73b0\u4e864\u500d\u7684\u52a0\u901f\u6bd4\uff0c\u5728Synfire\u7f51\u7edc\u4e0a\u8fbe\u52301.7\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u9759\u6001\u6838\u5fc3\u5206\u914d\u80fd\u8017\u964d\u4f4e70%\uff0c\u6709\u6548\u5e73\u8861\u4e86\u591a\u6838\u5904\u7406\u5668\u7684\u6240\u6709\u53ef\u7528\u6838\u5fc3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f00\u53d1\u4f4e\u5c3a\u5bf8\u3001\u91cd\u91cf\u548c\u529f\u8017\u7684\u8fb9\u7f18\u795e\u7ecf\u5f62\u6001\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u6280\u672f\u652f\u6491\uff0c\u540c\u65f6\u4e3a\u795e\u7ecf\u5f62\u6001\u82af\u7247\u7684\u96c6\u6210\u539f\u578b\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u8fb9\u7f18AI\u8ba1\u7b97\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.17347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17347", "abs": "https://arxiv.org/abs/2510.17347", "authors": ["Jingqian Wu", "Shengpeng Xu", "Yunbo Jia", "Edmund Y. Lam"], "title": "Exploring The Missing Semantics In Event Modality", "comment": null, "summary": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSemantic-E2VID\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\uff0c\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578bSAM\u7684\u8bed\u4e49\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e8b\u4ef6\u76f8\u673a\u89c6\u9891\u91cd\u5efa\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u7684\u8bed\u4e49\u4fe1\u606f\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u867d\u7136\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u7b49\u4f18\u52bf\uff0c\u4f46\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u4efb\u52a1\u9762\u4e34\u8bed\u4e49\u4fe1\u606f\u7f3a\u5931\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u4e8b\u4ef6\u76f8\u673a\u4ec5\u6355\u83b7\u5f3a\u5ea6\u53d8\u5316\u800c\u5ffd\u7565\u9759\u6001\u5bf9\u8c61\u548c\u80cc\u666f\uff0c\u5bfc\u81f4\u73b0\u6709E2V\u65b9\u6cd5\u96be\u4ee5\u6062\u590d\u8bed\u4e49\u5185\u5bb9\u3002", "method": "\u63d0\u51faSemantic-E2VID\u6846\u67b6\uff0c\u5305\u542b\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u5c06SAM\u7684\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e8b\u4ef6\u7f16\u7801\u5668\uff0c\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\u5757\u6574\u5408\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u7279\u5f81\u5f62\u6210\u4e30\u5bcc\u8bed\u4e49\u7684\u4e8b\u4ef6\u8868\u793a\uff0c\u4ee5\u53ca\u8bed\u4e49\u611f\u77e5E2V\u76d1\u7763\u673a\u5236\u5229\u7528SAM\u751f\u6210\u7684\u7c7b\u522b\u6807\u7b7e\u6307\u5bfc\u8bed\u4e49\u7ec6\u8282\u91cd\u5efa\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSemantic-E2VID\u663e\u8457\u63d0\u5347\u4e86\u5e27\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684E2V\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u4fe1\u606f\u5bf9\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e8b\u4ef6\u6a21\u6001\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8bed\u4e49\u589e\u5f3a\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u4e8b\u4ef6\u89c6\u89c9\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\u3002"}}
