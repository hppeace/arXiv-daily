<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UniE2F: A Unified Diffusion Framework for Event-to-Frame Reconstruction with Video Foundation Models](https://arxiv.org/abs/2602.19202)
*Gang Xu, Zhiyu Zhu, Junhui Hou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒï¼Œä»ç¨€ç–äº‹ä»¶æ•°æ®é‡å»ºé«˜ä¿çœŸè§†é¢‘å¸§çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡äº‹ä»¶å¼•å¯¼å’Œå¸§é—´æ®‹å·®å¢å¼ºï¼Œæ˜¾è‘—æå‡äº†äº‹ä»¶ç›¸æœºè§†é¢‘é‡å»ºçš„è´¨é‡ï¼Œå¹¶å®ç°äº†é›¶æ ·æœ¬çš„è§†é¢‘å¸§æ’å€¼å’Œé¢„æµ‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äº‹ä»¶ç›¸æœºè™½ç„¶åœ¨é«˜é€Ÿåº¦ã€ä½åŠŸè€—å’Œé«˜åŠ¨æ€èŒƒå›´åœºæ™¯æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä»…è®°å½•ç›¸å¯¹å¼ºåº¦å˜åŒ–è€Œéç»å¯¹å¼ºåº¦çš„ç‰¹æ€§å¯¼è‡´æ•°æ®æµå­˜åœ¨æ˜¾è‘—çš„ç©ºé—´ä¿¡æ¯å’Œé™æ€çº¹ç†ç»†èŠ‚æŸå¤±ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€é™åˆ¶ï¼Œé€šè¿‡ä»ç¨€ç–äº‹ä»¶æ•°æ®é‡å»ºé«˜è´¨é‡è§†é¢‘å¸§æ¥å¼¥è¡¥äº‹ä»¶ç›¸æœºçš„å›ºæœ‰ç¼ºé™·ã€‚

**Method:** è¯¥æ–¹æ³•é¦–å…ˆå»ºç«‹åŸºçº¿æ¨¡å‹ï¼Œç›´æ¥å°†äº‹ä»¶æ•°æ®ä½œä¸ºæ¡ä»¶è¾“å…¥æ¥åˆæˆè§†é¢‘ã€‚åŸºäºäº‹ä»¶æµä¸è§†é¢‘å¸§ä¹‹é—´çš„ç‰©ç†ç›¸å…³æ€§ï¼Œè¿›ä¸€æ­¥å¼•å…¥åŸºäºäº‹ä»¶çš„å¸§é—´æ®‹å·®å¼•å¯¼ä»¥å¢å¼ºè§†é¢‘å¸§é‡å»ºçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡è°ƒåˆ¶åå‘æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•ä»¥é›¶æ ·æœ¬æ–¹å¼æ‰©å±•åˆ°è§†é¢‘å¸§æ’å€¼å’Œé¢„æµ‹ï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„äº‹ä»¶åˆ°å¸§é‡å»ºæ¡†æ¶ã€‚

**Result:** åœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šå‡æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘å¸§é‡å»ºè´¨é‡æ–¹é¢å–å¾—äº†æ˜æ˜¾æå‡ï¼ŒåŒæ—¶èƒ½å¤Ÿä»¥é›¶æ ·æœ¬æ–¹å¼å®ç°è§†é¢‘å¸§æ’å€¼å’Œé¢„æµ‹ä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** æœ¬ç ”ç©¶è¯æ˜äº†åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒå¯ä»¥æœ‰æ•ˆè§£å†³äº‹ä»¶ç›¸æœºæ•°æ®çš„ä¿¡æ¯æŸå¤±é—®é¢˜ï¼Œä¸ºäº‹ä»¶è§†è§‰ç³»ç»Ÿæä¾›äº†é«˜è´¨é‡çš„è§†é¢‘é‡å»ºèƒ½åŠ›ã€‚è¯¥æ–¹æ³•æ„å»ºçš„ç»Ÿä¸€æ¡†æ¶ä¸ä»…æå‡äº†é‡å»ºç²¾åº¦ï¼Œè¿˜æ‰©å±•äº†äº‹ä»¶ç›¸æœºçš„åº”ç”¨èŒƒå›´ï¼Œä¸ºæœªæ¥äº‹ä»¶è§†è§‰ä¸ç”Ÿæˆæ¨¡å‹çš„ç»“åˆå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Event cameras excel at high-speed, low-power, and high-dynamic-range scene perception. However, as they fundamentally record only relative intensity changes rather than absolute intensity, the resulting data streams suffer from a significant loss of spatial information and static texture details. In this paper, we address this limitation by leveraging the generative prior of a pre-trained video diffusion model to reconstruct high-fidelity video frames from sparse event data. Specifically, we first establish a baseline model by directly applying event data as a condition to synthesize videos. Then, based on the physical correlation between the event stream and video frames, we further introduce the event-based inter-frame residual guidance to enhance the accuracy of video frame reconstruction. Furthermore, we extend our method to video frame interpolation and prediction in a zero-shot manner by modulating the reverse diffusion sampling process, thereby creating a unified event-to-frame reconstruction framework. Experimental results on real-world and synthetic datasets demonstrate that our method significantly outperforms previous approaches both quantitatively and qualitatively. We also refer the reviewers to the video demo contained in the supplementary material for video results. The code will be publicly available at https://github.com/CS-GangXu/UniE2F.


### [2] [Pay Attention to CTC: Fast and Robust Pseudo-Labelling for Unified Speech Recognition](https://arxiv.org/abs/2602.19316)
*Alexandros Haliassos, Rodrigo Mira, Stavros Petridis*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†USR 2.0ï¼Œä¸€ç§æ”¹è¿›çš„ç»Ÿä¸€è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡CTCé©±åŠ¨çš„æ•™å¸ˆå¼ºåˆ¶å’Œæ··åˆé‡‡æ ·æŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬å¹¶æå‡äº†æ¨¡å‹åœ¨åˆ†å¸ƒåç§»ä¸‹çš„é²æ£’æ€§ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç»Ÿä¸€è¯­éŸ³è¯†åˆ«æ¡†æ¶ä¾èµ–è‡ªå›å½’ä¼ªæ ‡ç­¾å¯¼è‡´è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œä¸”CTCä¸æ³¨æ„åŠ›åˆ†æ”¯çš„è§£è€¦ç›‘ç£ä½¿å…¶åœ¨é¢ä¸´é•¿åºåˆ—ã€å™ªå£°æˆ–æœªè§åŸŸç­‰åˆ†å¸ƒåç§»æ—¶å®¹æ˜“äº§ç”Ÿè‡ªæˆ‘å¼ºåŒ–é”™è¯¯ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ•ˆç‡å’Œé²æ£’æ€§ã€‚

**Method:** æå‡ºäº†CTCé©±åŠ¨çš„æ•™å¸ˆå¼ºåˆ¶æ–¹æ³•ï¼Œå°†è´ªå©ªè§£ç çš„CTCä¼ªæ ‡ç­¾ç›´æ¥é¦ˆå…¥è§£ç å™¨ä»¥åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­ç”Ÿæˆæ³¨æ„åŠ›ç›®æ ‡ï¼ŒåŒæ—¶å¼•å…¥æ··åˆé‡‡æ ·æŠ€æœ¯æ¥ç¼“è§£è§£ç å™¨ä»…ä¾èµ–CTCè¾“å…¥å¸¦æ¥çš„æ›å…‰åå·®é—®é¢˜ã€‚

**Result:** USR 2.0å°†è®­ç»ƒæ—¶é—´å‡åŠï¼Œæ˜¾è‘—æå‡äº†å¯¹åˆ†å¸ƒå¤–è¾“å…¥çš„é²æ£’æ€§ï¼Œåœ¨LRS3ã€LRS2å’ŒWildVSRåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åŸå§‹USRæ¡†æ¶å’Œæ¨¡æ€ç‰¹å®šçš„è‡ªç›‘ç£åŸºçº¿æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜CTCé©±åŠ¨çš„ä¼ªæ ‡ç­¾æ–¹æ³•èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„çŸ¥è¯†è¿ç§»ï¼Œä½¿æ¨¡å‹åŒæ—¶å—ç›ŠäºCTCçš„é²æ£’æ€§å’Œæ³¨æ„åŠ›çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¸ºåŠç›‘ç£è¯­éŸ³è¯†åˆ«æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆä¸”é²æ£’çš„è®­ç»ƒèŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Unified Speech Recognition (USR) has emerged as a semi-supervised framework for training a single model for audio, visual, and audiovisual speech recognition, achieving state-of-the-art results on in-distribution benchmarks. However, its reliance on autoregressive pseudo-labelling makes training expensive, while its decoupled supervision of CTC and attention branches increases susceptibility to self-reinforcing errors, particularly under distribution shifts involving longer sequences, noise, or unseen domains. We propose CTC-driven teacher forcing, where greedily decoded CTC pseudo-labels are fed into the decoder to generate attention targets in a single forward pass. Although these can be globally incoherent, in the pseudo-labelling setting they enable efficient and effective knowledge transfer. Because CTC and CTC-driven attention pseudo-labels have the same length, the decoder can predict both simultaneously, benefiting from the robustness of CTC and the expressiveness of attention without costly beam search. We further propose mixed sampling to mitigate the exposure bias of the decoder relying solely on CTC inputs. The resulting method, USR 2.0, halves training time, improves robustness to out-of-distribution inputs, and achieves state-of-the-art results on LRS3, LRS2, and WildVSR, surpassing USR and modality-specific self-supervised baselines.
