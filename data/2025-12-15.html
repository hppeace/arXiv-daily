<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-15.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-reconstruction-as-a-bridge-for-event-based-visual-question-answering">[1] <a href="https://arxiv.org/abs/2512.11510">Reconstruction as a Bridge for Event-Based Visual Question Answering</a></h3>
<p><em>Hanyue Lou, Jiayi Zhou, Yang Zhang, Boyu Li, Yi Wang, Guangnan Ye, Boxin Shi</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†äº‹ä»¶ç›¸æœºä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é›†æˆçš„æ–¹æ³•ï¼Œé€šè¿‡é‡å»ºä½œä¸ºæ¡¥æ¢ï¼Œè®¾è®¡äº†FRTå’ŒARTä¸¤ç§æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªå®¢è§‚çš„åŸºäºäº‹ä»¶çš„MLLMåŸºå‡†EvQAï¼Œåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ç†è§£ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°†äº‹ä»¶ç›¸æœºä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é›†æˆæœ‰æœ›åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¡ä»¶ä¸‹å®ç°é€šç”¨åœºæ™¯ç†è§£ï¼Œä½†éœ€è¦åœ¨ä¿ç•™äº‹ä»¶æ•°æ®ç‹¬ç‰¹ä¼˜åŠ¿ä¸ç¡®ä¿ä¸åŸºäºå¸§çš„æ¨¡å‹å…¼å®¹æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œè¿™æ˜¯å½“å‰ç ”ç©¶é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼šåŸºäºå¸§çš„é‡å»ºä¸æ ‡è®°åŒ–æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç›´æ¥çš„é‡å»ºæ¡¥æ¢æ–¹æ³•ï¼›ä»¥åŠè‡ªé€‚åº”é‡å»ºä¸æ ‡è®°åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº‹ä»¶æ•°æ®çš„ç¨€ç–æ€§å®ç°é«˜æ•ˆå¤„ç†ã€‚åŒæ—¶ï¼Œä½œè€…è¿˜è®¾è®¡äº†EvQAåŸºå‡†ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºäº‹ä»¶çš„MLLMå®¢è§‚çœŸå®ä¸–ç•ŒåŸºå‡†ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨EvQAåŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè¯¥åŸºå‡†åŒ…å«æ¥è‡ª22ä¸ªå…¬å…±æ•°æ®é›†çš„1000ä¸ªäº‹ä»¶é—®ç­”å¯¹ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨äº‹ä»¶è§†è§‰åœºæ™¯ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨äº‹ä»¶è§†è§‰é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡åˆ›æ–°çš„é‡å»ºæ¡¥æ¢æ–¹æ³•å’Œå…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼Œä¸ºäº‹ä»¶ç›¸æœºä¸å…ˆè¿›AIæ¨¡å‹çš„é›†æˆæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†åœ¨æŒ‘æˆ˜æ€§è§†è§‰æ¡ä»¶ä¸‹çš„é€šç”¨åœºæ™¯ç†è§£èƒ½åŠ›å‘å±•ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&amp;A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="2-cognisnn-enabling-neuron-expandability-pathway-reusability-and-dynamic-configurability-with-random-graph-architectures-in-spiking-neural-networks">[2] <a href="https://arxiv.org/abs/2512.11743">CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks</a></h3>
<p><em>Yongsheng Huang, Peibo Duan, Yujie Wu, Kai Sun, Zhipeng Liu, Changsheng Zhang, Bin Zhang, Mingkun Xu</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCogniSNNçš„æ–°å‹è„‰å†²ç¥ç»ç½‘ç»œèŒƒå¼ï¼Œé€šè¿‡å¼•å…¥éšæœºå›¾æ¶æ„æ¥æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç½‘ç»œçš„éšæœºè¿æ¥ç‰¹æ€§ï¼Œå¹¶ç»“åˆæ”¹è¿›çš„æ®‹å·®æœºåˆ¶ã€è‡ªé€‚åº”æ± åŒ–ç­–ç•¥ã€å…³é”®è·¯å¾„å­¦ä¹ ä¸åŠ¨æ€å¢é•¿ç®—æ³•ï¼Œæ˜¾è‘—æå‡äº†SNNçš„æ€§èƒ½å’Œè¿ç»­å­¦ä¹ èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ä¸»æµè„‰å†²ç¥ç»ç½‘ç»œç ”ç©¶å¤§å¤šç›´æ¥é‡‡ç”¨ä¼ ç»Ÿäººå·¥ç¥ç»ç½‘ç»œçš„åˆšæ€§é“¾å¼å±‚æ¬¡æ¶æ„ï¼Œå¿½è§†äº†å¤§è„‘çš„å…³é”®ç»“æ„ç‰¹å¾ï¼Œå¦‚ç¥ç»å…ƒçš„éšæœºäº’è¿ã€ç¥ç»é€šè·¯çš„å¤æ‚ç½‘ç»œç»“æ„ä»¥åŠç¥ç»å…ƒå¯æ‰©å±•æ€§ã€é€šè·¯å¯é‡ç”¨æ€§å’ŒåŠ¨æ€å¯é…ç½®æ€§ç­‰ç‰¹æ€§ï¼Œè¿™é™åˆ¶äº†SNNåœ¨è„‘å¯å‘æ™ºèƒ½æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†CogniSNNèŒƒå¼ï¼Œæ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼šå¼•å…¥éšæœºå›¾æ¶æ„æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç½‘ç»œçš„éšæœºè¿æ¥ï¼›è®¾è®¡æ”¹è¿›çš„çº¯è„‰å†²æ®‹å·®æœºåˆ¶å’Œè‡ªé€‚åº”æ± åŒ–ç­–ç•¥è§£å†³æ·±åº¦é€šè·¯ä¸­çš„ç½‘ç»œé€€åŒ–å’Œç»´åº¦ä¸åŒ¹é…é—®é¢˜ï¼›å¼€å‘åŸºäºå…³é”®è·¯å¾„çš„å­¦ä¹ ä¸é—å¿˜æ–¹æ³•ï¼Œé€‰æ‹©æ€§é‡ç”¨å…³é”®ç¥ç»é€šè·¯å¹¶ä¿ç•™å†å²çŸ¥è¯†ï¼›æå‡ºåŠ¨æ€å¢é•¿å­¦ä¹ ç®—æ³•ï¼Œä½¿ç¥ç»å…ƒå’Œçªè§¦èƒ½å¤Ÿæ²¿å†…éƒ¨æ—¶é—´ç»´åº¦åŠ¨æ€å¢é•¿ã€‚</p>
<p><strong>Result:</strong> åœ¨ç¥ç»å½¢æ€æ•°æ®é›†å’ŒTiny-ImageNetä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCogniSNNå®ç°äº†ä¸å½“å‰æœ€å…ˆè¿›SNNç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚é€šè·¯å¯é‡ç”¨æ€§å¢å¼ºäº†ç½‘ç»œåœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¿ç»­å­¦ä¹ èƒ½åŠ›ï¼Œè€ŒåŠ¨æ€å¢é•¿ç®—æ³•æé«˜äº†å¯¹æŠ—å¹²æ‰°çš„é²æ£’æ€§ï¼Œå¹¶ç¼“è§£äº†ç¥ç»å½¢æ€èŠ¯ç‰‡éƒ¨ç½²ä¸­çš„å›ºå®šæ—¶é—´æ­¥çº¦æŸã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å…·æœ‰éšæœºå›¾ç»“æ„çš„SNNåœ¨æ¨è¿›è„‘å¯å‘æ™ºèƒ½æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºå…¶åœ¨ç¥ç»å½¢æ€ç¡¬ä»¶ä¸Šçš„å®é™…åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚CogniSNNèŒƒå¼é€šè¿‡æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç½‘ç»œçš„å…³é”®ç»“æ„ç‰¹å¾ï¼Œä¸ºæ„å»ºæ›´æ¥è¿‘å¤§è„‘è®¡ç®—åŸç†çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.</p>
  </article>
</body>
</html>
