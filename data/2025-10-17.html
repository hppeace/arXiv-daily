<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-17.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 3]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-event-interval-modulation-a-novel-scheme-for-event-based-optical-camera-communication">[1] <a href="https://arxiv.org/abs/2510.14245">Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication</a></h3>
<p><em>Miu Sumino, Mayu Ishii, Shun Kaizu, Daisuke Hisano, Yu Nakayama</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“ä¸ºäº‹ä»¶ç›¸æœºå…‰é€šä¿¡è®¾è®¡çš„æ–°å‹è°ƒåˆ¶æ–¹æ¡ˆâ€”â€”äº‹ä»¶é—´éš”è°ƒåˆ¶ï¼Œé€šè¿‡åˆ©ç”¨äº‹ä»¶ä¹‹é—´çš„æ—¶é—´é—´éš”æ¥è°ƒåˆ¶ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†ä¼ è¾“é€Ÿåº¦ï¼Œåœ¨å®¤å†…ç¯å¢ƒä¸­å®ç°äº†28 kbpsï¼ˆ10ç±³ï¼‰å’Œ8.4 kbpsï¼ˆ50ç±³ï¼‰çš„ä¼ è¾“é€Ÿç‡ï¼Œåˆ›ä¸‹äº†äº‹ä»¶ç›¸æœºå…‰é€šä¿¡ç³»ç»Ÿçš„æ¯”ç‰¹ç‡æ–°çºªå½•ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»ŸåŸºäºå¸§ç›¸æœºçš„å…‰é€šä¿¡ç³»ç»Ÿå­˜åœ¨æ¯”ç‰¹ç‡ä½å’Œå¤„ç†è´Ÿè½½é«˜ç­‰é™åˆ¶ï¼Œè€Œç°æœ‰äº‹ä»¶ç›¸æœºå…‰é€šä¿¡ç³»ç»Ÿè™½ç„¶é‡‡ç”¨å¼‚æ­¥æ“ä½œå’Œé«˜åŠ¨æ€èŒƒå›´ç‰¹æ€§æå‡äº†æ€§èƒ½ï¼Œä½†ä½¿ç”¨çš„ä¼ ç»Ÿè°ƒåˆ¶æ–¹æ¡ˆå¦‚OOKå’Œè„‰å†²ä½ç½®è°ƒåˆ¶æœªèƒ½å……åˆ†åˆ©ç”¨äº‹ä»¶ç›¸æœºçš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œå› æ­¤éœ€è¦å¼€å‘ä¸“é—¨é’ˆå¯¹äº‹ä»¶ç›¸æœºç‰¹æ€§çš„è°ƒåˆ¶æ–¹æ³•æ¥è¿›ä¸€æ­¥æå‡é€šä¿¡æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†äº‹ä»¶é—´éš”è°ƒåˆ¶æ–¹æ¡ˆï¼Œé€šè¿‡è°ƒåˆ¶äº‹ä»¶ä¹‹é—´çš„æ—¶é—´é—´éš”æ¥ç¼–ç ä¿¡æ¯ï¼Œå»ºç«‹äº†EIMçš„ç†è®ºæ¨¡å‹å¹¶è¿›è¡Œäº†æ¦‚å¿µéªŒè¯å®éªŒï¼Œé¦–å…ˆå¯¹äº‹ä»¶ç›¸æœºçš„å‚æ•°è¿›è¡Œè°ƒä¼˜å’Œå®šåˆ¶ä»¥ä¼˜åŒ–é’ˆå¯¹EIMçš„é¢‘ç‡å“åº”ï¼Œç„¶åé€šè¿‡å®éªŒç¡®å®šäº†EIMå¯ç”¨çš„æœ€å¤§è°ƒåˆ¶é˜¶æ•°ï¼Œæœ€ååŸºäºè·å¾—çš„å‚æ•°è¿›è¡Œäº†ä¼ è¾“å®éªŒéªŒè¯ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å®¤å†…ç¯å¢ƒä¸­æˆåŠŸå®ç°äº†28 kbpsï¼ˆ10ç±³è·ç¦»ï¼‰å’Œ8.4 kbpsï¼ˆ50ç±³è·ç¦»ï¼‰çš„ä¼ è¾“é€Ÿç‡ï¼Œè¿™ä¸€æ€§èƒ½åˆ›ä¸‹äº†äº‹ä»¶ç›¸æœºå…‰é€šä¿¡ç³»ç»Ÿçš„æ¯”ç‰¹ç‡æ–°çºªå½•ï¼Œè¯æ˜äº†EIMæ–¹æ¡ˆåœ¨æå‡ä¼ è¾“é€Ÿåº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> äº‹ä»¶é—´éš”è°ƒåˆ¶æ–¹æ¡ˆå……åˆ†åˆ©ç”¨äº†äº‹ä»¶ç›¸æœºçš„å¼‚æ­¥ç‰¹æ€§å’Œé«˜æ—¶é—´åˆ†è¾¨ç‡ï¼Œä¸ºäº‹ä»¶ç›¸æœºå…‰é€šä¿¡ç³»ç»Ÿæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è°ƒåˆ¶æ–¹æ³•ï¼Œè¯¥ç ”ç©¶ä¸ä»…å»ºç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ï¼Œè¿˜ä¸ºæœªæ¥é«˜é€Ÿã€ä½å»¶è¿Ÿçš„å¯è§å…‰é€šä¿¡ç³»ç»Ÿå¼€å‘æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„å’Œç†è®ºæ”¯æ’‘ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.</p>
<h3 id="2-experimental-demonstration-of-event-based-optical-camera-communication-in-long-range-outdoor-environment">[2] <a href="https://arxiv.org/abs/2510.14266">Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment</a></h3>
<p><em>Miu Sumino, Mayu Ishii, Shun Kaizu, Daisuke Hisano, Yu Nakayama</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºäº‹ä»¶è§†è§‰ä¼ æ„Ÿå™¨çš„å…‰å­¦ç›¸æœºé€šä¿¡é²æ£’è§£è°ƒæ–¹æ¡ˆï¼Œé¦–æ¬¡åœ¨å®¤å¤–å®éªŒä¸­å®ç°äº†200ç±³-60kbpså’Œ400ç±³-30kbpsæ¡ä»¶ä¸‹è¯¯ç ç‡ä½äº10^-3çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…‰å­¦ç›¸æœºé€šä¿¡ç³»ç»Ÿåœ¨é•¿è·ç¦»ä¼ è¾“ä¸­é¢ä¸´ä¿¡å·è¡°å‡å’Œå™ªå£°å¹²æ‰°çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¼€å‘æ›´é²æ£’çš„è§£è°ƒæ–¹æ¡ˆæ¥æé«˜é€šä¿¡å¯é æ€§å’Œä¼ è¾“è·ç¦»ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨äº‹ä»¶è§†è§‰ä¼ æ„Ÿå™¨ç»“åˆOOKè°ƒåˆ¶ä¸åˆ‡æ¢è§£è°ƒæŠ€æœ¯ï¼Œå¹¶é›†æˆæ•°å­—é”ç›¸ç¯æ¥å®ç°ç²¾ç¡®çš„ä¿¡å·åŒæ­¥å’Œç¨³å®šçš„è§£è°ƒæ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> åœ¨å®¤å¤–å®éªŒä¸­å®ç°äº†çªç ´æ€§çš„æ€§èƒ½æŒ‡æ ‡ï¼šåœ¨200ç±³è·ç¦»ä¸‹è¾¾åˆ°60kbpsä¼ è¾“é€Ÿç‡æ—¶è¯¯ç ç‡ä½äº10^-3ï¼Œåœ¨400ç±³è·ç¦»ä¸‹è¾¾åˆ°30kbpsä¼ è¾“é€Ÿç‡æ—¶åŒæ ·ä¿æŒè¯¯ç ç‡ä½äº10^-3ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†äº‹ä»¶è§†è§‰ä¼ æ„Ÿå™¨åœ¨å…‰å­¦ç›¸æœºé€šä¿¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé•¿è·ç¦»ã€é«˜å¯é æ€§çš„å¯è§å…‰é€šä¿¡ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} &lt; 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.</p>
<h3 id="3-mocom-motion-based-inter-mav-visual-communication-using-event-vision-and-spiking-neural-networks">[3] <a href="https://arxiv.org/abs/2510.14770">MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks</a></h3>
<p><em>Zhang Nengbo, Hann Woei Ho, Ye Zhou</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¿åŠ¨ä¿¡å·çš„å¾®å‹é£è¡Œå™¨ç¾¤é€šä¿¡æ¡†æ¶ï¼Œçµæ„Ÿæ¥æºäºèœœèœ‚æ‘‡æ‘†èˆï¼Œé€šè¿‡äº‹ä»¶ç›¸æœºæ•è·MAVçš„é£è¡Œæ¨¡å¼å¹¶ä½¿ç”¨è„‰å†²ç¥ç»ç½‘ç»œè¿›è¡Œè§£ç ï¼Œå®ç°äº†åœ¨å—é™ç¯å¢ƒä¸­çš„å¯é ä½åŠŸè€—é€šä¿¡ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»ŸåŸºäºæ— çº¿ç”µçš„å¾®å‹é£è¡Œå™¨ç¾¤é€šä¿¡åœ¨é¢‘è°±æ‹¥å¡ã€å¹²æ‰°å’Œé«˜åŠŸè€—ç¯å¢ƒä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å¯»æ‰¾åœ¨å—é™ç¯å¢ƒä¸‹å¯é ä¸”èŠ‚èƒ½çš„æ›¿ä»£é€šä¿¡æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºè¿åŠ¨ä¿¡å·çš„è§†è§‰é€šä¿¡æ¡†æ¶ï¼Œä½¿ç”¨å››ç§è¿åŠ¨åŸºå…ƒï¼ˆå‚ç›´ã€æ°´å¹³ã€å·¦-ä¸Š-å³ã€å·¦-ä¸‹-å³ï¼‰è¡¨ç¤ºæ§åˆ¶ç¬¦å·ï¼Œè®¾è®¡äº‹ä»¶å¸§åˆ†å‰²æ¨¡å‹å’Œè½»é‡çº§è„‰å†²ç¥ç»ç½‘ç»œè¿›è¡ŒåŠ¨ä½œè¯†åˆ«ï¼Œç»“åˆé›†æˆè§£ç ç®—æ³•è§£é‡ŠMAVè¿åŠ¨åºåˆ—ã€‚</p>
<p><strong>Result:</strong> å®éªŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†ç²¾ç¡®çš„è§£ç æ€§èƒ½å’Œä½åŠŸè€—ç‰¹æ€§ï¼Œåœ¨å—é™ç¯å¢ƒä¸­å®ç°äº†å¯é çš„é€šä¿¡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥è§†è§‰é€šä¿¡æ¡†æ¶ä¸ºå¾®å‹é£è¡Œå™¨ç¾¤æä¾›äº†ä¸€ç§èŠ‚èƒ½é«˜æ•ˆçš„æ›¿ä»£é€šä¿¡æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºæ— çº¿ç”µå—é™ç¯å¢ƒï¼Œå±•ç¤ºäº†ç”Ÿç‰©å¯å‘æ–¹æ³•åœ¨æœºå™¨äººé€šä¿¡ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in
environments, where conventional radio-based methods suffer from spectrum
congestion, jamming, and high power consumption. Inspired by the waggle dance
of honeybees, which efficiently communicate the location of food sources
without sound or contact, we propose a novel visual communication framework for
MAV swarms using motion-based signaling. In this framework, MAVs convey
information, such as heading and distance, through deliberate flight patterns,
which are passively captured by event cameras and interpreted using a
predefined visual codebook of four motion primitives: vertical (up/down),
horizontal (left/right), left-to-up-to-right, and left-to-down-to-right,
representing control symbols (<code>start'',</code>end'', <code>1'',</code>0''). To decode
these signals, we design an event frame-based segmentation model and a
lightweight Spiking Neural Network (SNN) for action recognition. An integrated
decoding algorithm then combines segmentation and classification to robustly
interpret MAV motion sequences. Experimental results validate the framework's
effectiveness, which demonstrates accurate decoding and low power consumption,
and highlights its potential as an energy-efficient alternative for MAV
communication in constrained environments.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="4-spiking-neural-network-architecture-search-a-survey">[4] <a href="https://arxiv.org/abs/2510.14235">Spiking Neural Network Architecture Search: A Survey</a></h3>
<p><em>Kama Svoboda, Tosiron Adegbija</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ä»ç¡¬ä»¶/è½¯ä»¶ååŒè®¾è®¡çš„ç‹¬ç‰¹è§†è§’å¯¹è„‰å†²ç¥ç»ç½‘ç»œæ¶æ„æœç´¢è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œå¼ºè°ƒååŒè®¾è®¡æ–¹æ³•å¯¹äºé‡Šæ”¾ç¥ç»å½¢æ€è®¡ç®—å…¨éƒ¨æ½œåŠ›çš„é‡è¦æ€§ã€‚è¯¥è°ƒæŸ¥æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›SNNæ¶æ„æœç´¢çš„æ•´ä½“è§†å›¾ï¼Œå¹¶ä½œä¸ºè¯¥é¢†åŸŸçš„å®è´µèµ„æºã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è„‰å†²ç¥ç»ç½‘ç»œå› å…¶åœ¨èƒ½æ•ˆå’Œå®æ—¶èµ„æºå—é™å¤„ç†æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œæˆä¸ºç¥ç»å½¢æ€è®¡ç®—çš„æœ‰å‰æ™¯æ–¹æ³•ï¼Œç‰¹åˆ«é€‚åˆè¾¹ç¼˜è®¡ç®—å’Œç‰©è”ç½‘åº”ç”¨ã€‚ç„¶è€Œï¼Œè®¾è®¡æœ€ä¼˜SNNæ¶æ„é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å…¶å›ºæœ‰çš„å¤æ‚æ€§ä»¥åŠç¡¬ä»¶çº¦æŸä¸SNNæ¨¡å‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œè¿™éœ€è¦ä¸“é—¨çš„æ¶æ„æœç´¢æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é‡‡ç”¨è°ƒæŸ¥åˆ†ææ–¹æ³•ï¼Œé¦–å…ˆæ¦‚è¿°SNNçš„æ“ä½œåŸç†åŠå…¶ä¸ä¼ ç»Ÿäººå·¥ç¥ç»ç½‘ç»œçš„å…³é”®åŒºåˆ«ï¼Œç„¶åç®€è¦å›é¡¾ANNä¸­ç¥ç»æ¶æ„æœç´¢çš„æœ€æ–°æŠ€æœ¯ï¼Œé‡ç‚¹åˆ†æå°†è¿™äº›æ–¹æ³•ç›´æ¥åº”ç”¨äºSNNæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æœ€åç³»ç»Ÿè°ƒæŸ¥SNNç‰¹å®šçš„NASæ–¹æ³•ï¼Œä»ç¡¬ä»¶/è½¯ä»¶ååŒè®¾è®¡è§’åº¦è¿›è¡Œåˆ†æã€‚</p>
<p><strong>Result:</strong> è°ƒæŸ¥æ­ç¤ºäº†ç¡¬ä»¶/è½¯ä»¶ååŒè®¾è®¡åœ¨SNNæ¶æ„æœç´¢ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¼ºè°ƒäº†ä¼ ç»ŸANN NASæ–¹æ³•ä¸èƒ½ç›´æ¥åº”ç”¨äºSNNçš„å±€é™æ€§ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æç°æœ‰SNNç‰¹å®šNASæ–¹æ³•ï¼Œè¯†åˆ«äº†å½“å‰ç ”ç©¶ä¸­çš„ç©ºç™½å’ŒæŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> ç¡¬ä»¶/è½¯ä»¶ååŒè®¾è®¡å¯¹äºå……åˆ†å‘æŒ¥SNNæ½œåŠ›è‡³å…³é‡è¦ï¼Œæœªæ¥ç ”ç©¶åº”æ›´åŠ æ³¨é‡è¿™ç§ååŒæ–¹æ³•ã€‚è¯¥è°ƒæŸ¥ä¸ºSNNç ”ç©¶æä¾›äº†é‡è¦è§è§£ï¼Œå¼ºè°ƒäº†ååŒè®¾è®¡åœ¨è§£é”ç¥ç»å½¢æ€è®¡ç®—å…¨éƒ¨èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>This survey paper presents a comprehensive examination of Spiking Neural
Network (SNN) architecture search (SNNaS) from a unique hardware/software
co-design perspective. SNNs, inspired by biological neurons, have emerged as a
promising approach to neuromorphic computing. They offer significant advantages
in terms of power efficiency and real-time resource-constrained processing,
making them ideal for edge computing and IoT applications. However, designing
optimal SNN architectures poses significant challenges, due to their inherent
complexity (e.g., with respect to training) and the interplay between hardware
constraints and SNN models. We begin by providing an overview of SNNs,
emphasizing their operational principles and key distinctions from traditional
artificial neural networks (ANNs). We then provide a brief overview of the
state of the art in NAS for ANNs, highlighting the challenges of directly
applying these approaches to SNNs. We then survey the state-of-the-art in
SNN-specific NAS approaches. Finally, we conclude with insights into future
research directions for SNN research, emphasizing the potential of
hardware/software co-design in unlocking the full capabilities of SNNs. This
survey aims to serve as a valuable resource for researchers and practitioners
in the field, offering a holistic view of SNNaS and underscoring the importance
of a co-design approach to harness the true potential of neuromorphic
computing.</p>
  </article>
</body>
</html>
