{"id": "2512.02459", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2512.02459", "abs": "https://arxiv.org/abs/2512.02459", "authors": ["Qianhui Liu", "Jing Yang", "Miao Yu", "Trevor E. Carlson", "Gang Pan", "Haizhou Li", "Zhumin Chen"], "title": "Efficient Eye-based Emotion Recognition via Neural Architecture Search of Time-to-First-Spike-Coded Spiking Neural Networks", "comment": null, "summary": "Eye-based emotion recognition enables eyewear devices to perceive users' emotional states and support emotion-aware interaction, yet deploying such functionality on their resource-limited embedded hardware remains challenging. Time-to-first-spike (TTFS)-coded spiking neural networks (SNNs) offer a promising solution, as each neuron emits at most one binary spike, resulting in extremely sparse and energy-efficient computation. While prior works have primarily focused on improving TTFS SNN training algorithms, the impact of network architecture has been largely overlooked. In this paper, we propose TNAS-ER, the first neural architecture search (NAS) framework tailored to TTFS SNNs for eye-based emotion recognition. TNAS-ER presents a novel ANN-assisted search strategy that leverages a ReLU-based ANN counterpart sharing an identity mapping with the TTFS SNN to guide architecture optimization. TNAS-ER employs an evolutionary algorithm, with weighted and unweighted average recall jointly defined as fitness objectives for emotion recognition. Extensive experiments demonstrate that TNAS-ER achieves high recognition performance with significantly improved efficiency. Furthermore, when deployed on neuromorphic hardware, TNAS-ER attains a low latency of 48 ms and an energy consumption of 0.05 J, confirming its superior energy efficiency and strong potential for practical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TNAS-ER\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u773c\u57fa\u60c5\u611f\u8bc6\u522b\u7684TTFS\u7f16\u7801\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7ANN\u8f85\u52a9\u641c\u7d22\u7b56\u7565\u548c\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u3002", "motivation": "\u773c\u57fa\u60c5\u611f\u8bc6\u522b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u867d\u7136TTFS\u7f16\u7801\u7684SNNs\u63d0\u4f9b\u4e86\u80fd\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bad\u7ec3\u7b97\u6cd5\u6539\u8fdb\uff0c\u800c\u7f51\u7edc\u67b6\u6784\u7684\u5f71\u54cd\u88ab\u5ffd\u89c6\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9TTFS SNNs\u7684\u67b6\u6784\u4f18\u5316\u65b9\u6cd5\u3002", "method": "TNAS-ER\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684ANN\u8f85\u52a9\u641c\u7d22\u7b56\u7565\uff0c\u5229\u7528\u4e0eTTFS SNN\u5171\u4eab\u6052\u7b49\u6620\u5c04\u7684ReLU\u57faANN\u5bf9\u5e94\u7269\u6765\u6307\u5bfc\u67b6\u6784\u4f18\u5316\uff0c\u91c7\u7528\u8fdb\u5316\u7b97\u6cd5\u5e76\u4ee5\u52a0\u6743\u548c\u975e\u52a0\u6743\u5e73\u5747\u53ec\u56de\u7387\u4f5c\u4e3a\u60c5\u611f\u8bc6\u522b\u7684\u9002\u5e94\u5ea6\u76ee\u6807\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTNAS-ER\u5728\u4fdd\u6301\u9ad8\u8bc6\u522b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u90e8\u7f72\u65f6\u5b9e\u73b0\u4e8648\u6beb\u79d2\u7684\u4f4e\u5ef6\u8fdf\u548c0.05\u7126\u8033\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u8bc1\u5b9e\u4e86\u5176\u4f18\u8d8a\u7684\u80fd\u6548\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u9488\u5bf9\u7279\u5b9aSNN\u7f16\u7801\u65b9\u6848\u8fdb\u884c\u67b6\u6784\u641c\u7d22\u7684\u91cd\u8981\u6027\uff0cTNAS-ER\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u60c5\u611f\u8bc6\u522b\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.02258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02258", "abs": "https://arxiv.org/abs/2512.02258", "authors": ["Shuang Chen", "Tomas Krajnik", "Farshad Arvin", "Amir Atapour-Abarghouei"], "title": "Exploring the Potentials of Spiking Neural Networks for Image Deraining", "comment": "Accepted By AAAI2026", "summary": "Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u53bb\u96e8\u4efb\u52a1\u7684\u89c6\u89c9LIF\uff08VLIF\uff09\u795e\u7ecf\u5143\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8109\u51b2\u5206\u89e3\u589e\u5f3a\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u5355\u5143\uff0c\u5728\u663e\u8457\u964d\u4f4e\u80fd\u8017\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709SNN\u65b9\u6cd5\u7684\u53bb\u96e8\u6027\u80fd\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u53bb\u96e8\u7b49\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edf\u8109\u51b2\u795e\u7ecf\u5143\u7f3a\u4e4f\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u4e14\u5b58\u5728\u9891\u57df\u9971\u548c\u9650\u5236\uff0c\u8fd9\u963b\u788d\u4e86SNNs\u5728\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u89c6\u89c9LIF\uff08VLIF\uff09\u795e\u7ecf\u5143\u6765\u514b\u670d\u4f20\u7edf\u8109\u51b2\u795e\u7ecf\u5143\u5728\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8109\u51b2\u5206\u89e3\u589e\u5f3a\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u8109\u51b2\u591a\u5c3a\u5ea6\u5355\u5143\uff0c\u901a\u8fc7\u5206\u5c42\u591a\u5c3a\u5ea6\u8868\u793a\u5b66\u4e60\u6765\u89e3\u51b3\u9891\u57df\u9971\u548c\u95ee\u9898\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u53bb\u96e8\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eSNN\u7684\u53bb\u96e8\u65b9\u6cd5\uff0c\u540c\u65f6\u4ec5\u6d88\u8017\u517613%\u7684\u80fd\u8017\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e0e\u4f4e\u80fd\u8017\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u9ad8\u6027\u80fd\u3001\u4f4e\u80fd\u8017\u7684\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u4e2d\u90e8\u7f72SNNs\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u751f\u7269\u542f\u53d1\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u5728\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2512.02447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02447", "abs": "https://arxiv.org/abs/2512.02447", "authors": ["Fan Luo", "Zeyu Gao", "Xinhao Luo", "Kai Zhao", "Yanfeng Lu"], "title": "Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors", "comment": null, "summary": "Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTemporal Dynamics Enhancer\uff08TDE\uff09\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u65f6\u5e8f\u4fe1\u606f\u5efa\u6a21\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7Spiking Encoder\u751f\u6210\u591a\u6837\u5316\u7684\u65f6\u95f4\u6b65\u8f93\u5165\u523a\u6fc0\uff0c\u5e76\u91c7\u7528Spike-Driven Attention\u964d\u4f4e\u6ce8\u610f\u529b\u673a\u5236\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u663e\u8457\u63d0\u5347\u4e86SNN\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u901a\u5e38\u76f4\u63a5\u590d\u5236\u8f93\u5165\u6216\u5728\u56fa\u5b9a\u95f4\u9694\u5185\u805a\u5408\u8f93\u5165\u5e27\uff0c\u5bfc\u81f4\u795e\u7ecf\u5143\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u63a5\u6536\u51e0\u4e4e\u76f8\u540c\u7684\u523a\u6fc0\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\uff09\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u9700\u8981\u589e\u5f3aSNNs\u7684\u65f6\u5e8f\u4fe1\u606f\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51faTemporal Dynamics Enhancer\uff08TDE\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aSpiking Encoder\uff08SE\uff09\u7528\u4e8e\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u751f\u6210\u591a\u6837\u5316\u7684\u8f93\u5165\u523a\u6fc0\uff0c\u4ee5\u53caAttention Gating Module\uff08AGM\uff09\u57fa\u4e8e\u65f6\u95f4\u95f4\u4f9d\u8d56\u6027\u6307\u5bfcSE\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u6d88\u9664AGM\u5f15\u5165\u7684\u9ad8\u80fd\u8017\u4e58\u6cd5\u64cd\u4f5c\uff0c\u63d0\u51fa\u4e86Spike-Driven Attention\uff08SDA\uff09\u6765\u663e\u8457\u964d\u4f4e\u6ce8\u610f\u529b\u76f8\u5173\u7684\u80fd\u91cf\u6d88\u8017\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTDE\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684SNN\u68c0\u6d4b\u5668\u4e2d\uff0c\u5728\u9759\u6001PASCAL VOC\u6570\u636e\u96c6\u4e0a\u8fbe\u523057.7%\u7684mAP50-95\u5206\u6570\uff0c\u5728\u795e\u7ecf\u5f62\u6001EvDET200K\u6570\u636e\u96c6\u4e0a\u8fbe\u523047.6%\u7684mAP50-95\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5728\u80fd\u8017\u65b9\u9762\uff0cSDA\u4ec5\u6d88\u8017\u4f20\u7edf\u6ce8\u610f\u529b\u6a21\u57570.240\u500d\u7684\u80fd\u91cf\u3002", "conclusion": "TDE\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86SNNs\u7684\u65f6\u5e8f\u5efa\u6a21\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u8f93\u5165\u523a\u6fc0\u5355\u4e00\u5316\u7684\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7Spike-Driven Attention\u5b9e\u73b0\u4e86\u9ad8\u6548\u80fd\u8ba1\u7b97\u3002\u8be5\u7814\u7a76\u4e3aSNNs\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u80fd\u8017\uff0c\u63a8\u52a8\u4e86\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u5316\u53d1\u5c55\u3002"}}
