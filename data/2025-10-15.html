<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-15.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-e-moflow-learning-egomotion-and-optical-flow-from-event-data-via-implicit-regularization">[1] <a href="https://arxiv.org/abs/2510.12753">E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization</a></h3>
<p><em>Wenpu Li, Bangyan Liao, Yi Zhou, Qi Xu, Pian Wan, Peidong Liu</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£æ¡†æ¶E-MoFlowï¼Œé€šè¿‡éšå¼æ—¶ç©ºå’Œå‡ ä½•æ­£åˆ™åŒ–è”åˆä¼˜åŒ–äº‹ä»¶ç›¸æœºä¸­çš„è‡ªè¿åŠ¨å’Œå…‰æµä¼°è®¡ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å› ç‹¬ç«‹å¤„ç†è¿™ä¸¤ä¸ªé—®é¢˜è€Œå¯¼è‡´çš„ç—…æ€æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿæ–¹æ³•å°†å…‰æµä¼°è®¡å’Œ6è‡ªç”±åº¦è‡ªè¿åŠ¨ä¼°è®¡ä½œä¸ºç‹¬ç«‹ä»»åŠ¡å¤„ç†ï¼Œä½†åœ¨ç¥ç»å½¢æ€è§†è§‰ï¼ˆå¦‚äº‹ä»¶ç›¸æœºï¼‰ä¸­ï¼Œç”±äºç¼ºä¹é²æ£’çš„æ•°æ®å…³è”ï¼Œè¿™ç§åˆ†ç¦»å¤„ç†æ–¹å¼åœ¨æ²¡æœ‰åœ°é¢çœŸå€¼ç›‘ç£çš„æƒ…å†µä¸‹å˜å¾—ç—…æ€ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æ˜¾å¼å˜åˆ†æ­£åˆ™åŒ–å¼ºåˆ¶å…‰æµåœºå¹³æ»‘æˆ–åˆ©ç”¨æ˜¾å¼ç»“æ„è¿åŠ¨å…ˆéªŒæ¥æ”¹å–„äº‹ä»¶å¯¹é½ï¼Œä½†å‰è€…ä¼šå¼•å…¥ç»“æœåå·®å’Œè®¡ç®—å¼€é”€ï¼Œåè€…åˆ™å®¹æ˜“æ”¶æ•›åˆ°æ¬¡ä¼˜å±€éƒ¨æå°å€¼ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é€šè¿‡å°†ç›¸æœºè‡ªè¿åŠ¨å»ºæ¨¡ä¸ºè¿ç»­æ ·æ¡ã€å…‰æµå»ºæ¨¡ä¸ºéšå¼ç¥ç»è¡¨ç¤ºï¼Œåœ¨å½’çº³åç½®ä¸­åµŒå…¥æ—¶ç©ºä¸€è‡´æ€§ã€‚åŒæ—¶å¼•å…¥å¾®åˆ†å‡ ä½•çº¦æŸæ¥æ•´åˆç»“æ„è¿åŠ¨å…ˆéªŒï¼Œç»•è¿‡æ˜¾å¼æ·±åº¦ä¼°è®¡åŒæ—¶ä¿æŒä¸¥æ ¼çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œä»è€Œåœ¨å®Œå…¨æ— ç›‘ç£èŒƒå¼ä¸‹é€šè¿‡éšå¼æ­£åˆ™åŒ–ç»Ÿä¸€è‡ªè¿åŠ¨å’Œå…‰æµä¼°è®¡ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ¡†æ¶å¯¹ä¸€èˆ¬6è‡ªç”±åº¦è¿åŠ¨åœºæ™¯å…·æœ‰è‰¯å¥½é€‚åº”æ€§ï¼Œåœ¨æ— ç›‘ç£æ–¹æ³•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œç”šè‡³ä¸æœ‰ç›‘ç£æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡éšå¼æ­£åˆ™åŒ–è”åˆä¼˜åŒ–è‡ªè¿åŠ¨å’Œå…‰æµä¼°è®¡çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç¥ç»å½¢æ€è§†è§‰ä¸­çš„è¿åŠ¨ä¼°è®¡é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†éšå¼è¡¨ç¤ºå’Œå‡ ä½•çº¦æŸåœ¨è§£å†³æ— ç›‘ç£è§†è§‰ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in
3D vision, has typically been addressed independently. For neuromorphic vision
(e.g., event cameras), however, the lack of robust data association makes
solving the two problems separately an ill-posed challenge, especially in the
absence of supervision via ground truth. Existing works mitigate this
ill-posedness by either enforcing the smoothness of the flow field via an
explicit variational regularizer or leveraging explicit structure-and-motion
priors in the parametrization to improve event alignment. The former notably
introduces bias in results and computational overhead, while the latter, which
parametrizes the optical flow in terms of the scene depth and the camera
motion, often converges to suboptimal local minima. To address these issues, we
propose an unsupervised framework that jointly optimizes egomotion and optical
flow via implicit spatial-temporal and geometric regularization. First, by
modeling camera's egomotion as a continuous spline and optical flow as an
implicit neural representation, our method inherently embeds spatial-temporal
coherence through inductive biases. Second, we incorporate structure-and-motion
priors through differential geometric constraints, bypassing explicit depth
estimation while maintaining rigorous geometric consistency. As a result, our
framework (called E-MoFlow) unifies egomotion and optical flow estimation via
implicit regularization under a fully unsupervised paradigm. Experiments
demonstrate its versatility to general 6-DoF motion scenarios, achieving
state-of-the-art performance among unsupervised methods and competitive even
with supervised approaches.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="2-spikepool-event-driven-spiking-transformer-with-pooling-attention">[2] <a href="https://arxiv.org/abs/2510.12102">SpikePool: Event-driven Spiking Transformer with Pooling Attention</a></h3>
<p><em>Donghyun Lee, Alex Sima, Yuhang Li, Panos Stinis, Priyadarshini Panda</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é€šè¿‡é¢‘åŸŸåˆ†æå‘ç°è„‰å†²å˜å‹å™¨å…·æœ‰é«˜é€šæ»¤æ³¢ç‰¹æ€§ï¼Œå¹¶æå‡ºSpikePoolæ–¹æ³•å°†è„‰å†²è‡ªæ³¨æ„åŠ›æ›¿æ¢ä¸ºæœ€å¤§æ± åŒ–æ³¨æ„åŠ›ï¼Œå®ç°é€‰æ‹©æ€§å¸¦é€šæ»¤æ³¢ï¼Œåœ¨äº‹ä»¶è§†è§‰ä»»åŠ¡ä¸­å–å¾—ç«äº‰æ€§æ€§èƒ½å¹¶æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è„‰å†²å˜å‹å™¨åœ¨äº‹ä»¶è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œä½†å¯¹å…¶å¤„ç†äº‹ä»¶æ•°æ®çš„åŸºæœ¬æœºåˆ¶ç¼ºä¹æ·±å…¥ç†è§£ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ¶æ„ä¿®æ”¹è€Œå¿½ç•¥åº•å±‚ä¿¡å·å¤„ç†ç‰¹æ€§åˆ†æï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„ä¼˜åŒ–å’Œè®¾è®¡ã€‚</p>
<p><strong>Method:</strong> é€šè¿‡é¢‘åŸŸåˆ†ææ­ç¤ºè„‰å†²å˜å‹å™¨çš„ä¿¡å·å¤„ç†ç‰¹æ€§ï¼Œå‘ç°å…¶è¡¨ç°ä¸ºé«˜é€šæ»¤æ³¢å™¨ï¼Œå¹¶æå‡ºSpikePoolæ–¹æ³•ç”¨æœ€å¤§æ± åŒ–æ³¨æ„åŠ›æ›¿ä»£è„‰å†²è‡ªæ³¨æ„åŠ›ï¼Œæœ€å¤§æ± åŒ–ä½œä¸ºä½é€šæ»¤æ³¢æ“ä½œä¸è„‰å†²çš„é«˜é€šç‰¹æ€§ç»“åˆå½¢æˆé€‰æ‹©æ€§å¸¦é€šæ»¤æ³¢æ•ˆæœã€‚</p>
<p><strong>Result:</strong> åœ¨äº‹ä»¶åˆ†ç±»å’Œç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—ç«äº‰æ€§ç»“æœï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´å‡å°‘é«˜è¾¾42.5%ï¼Œæ¨ç†æ—¶é—´å‡å°‘32.8%ï¼Œæœ‰æ•ˆå¹³è¡¡äº†é«˜é¢‘ä¿¡æ¯ä¿ç•™ä¸å™ªå£°æŠ‘åˆ¶çš„éœ€æ±‚ã€‚</p>
<p><strong>Conclusion:</strong> é¢‘åŸŸåˆ†æä¸ºç†è§£è„‰å†²å˜å‹å™¨æä¾›äº†æ–°è§†è§’ï¼Œæ­ç¤ºäº†å…¶é«˜é€šæ»¤æ³¢ç‰¹æ€§ä¸äº‹ä»¶æ•°æ®å¤„ç†çš„é€‚é…æ€§ï¼ŒSpikePoolçš„è®¾è®¡ç†å¿µè¡¨æ˜ç»“åˆé«˜é€šå’Œä½é€šæ»¤æ³¢æ“ä½œå¯ä»¥æ›´å¥½åœ°å¤„ç†äº‹ä»¶æ•°æ®çš„ç¨€ç–æ€§å’Œå™ªå£°é—®é¢˜ï¼Œä¸ºæœªæ¥è„‰å†²ç¥ç»ç½‘ç»œè®¾è®¡æä¾›äº†ç†è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Building on the success of transformers, Spiking Neural Networks (SNNs) have
increasingly been integrated with transformer architectures, leading to spiking
transformers that demonstrate promising performance on event-based vision
tasks. However, despite these empirical successes, there remains limited
understanding of how spiking transformers fundamentally process event-based
data. Current approaches primarily focus on architectural modifications without
analyzing the underlying signal processing characteristics. In this work, we
analyze spiking transformers through the frequency spectrum domain and discover
that they behave as high-pass filters, contrasting with Vision Transformers
(ViTs) that act as low-pass filters. This frequency domain analysis reveals why
certain designs work well for event-based data, which contains valuable
high-frequency information but is also sparse and noisy. Based on this
observation, we propose SpikePool, which replaces spike-based self-attention
with max pooling attention, a low-pass filtering operation, to create a
selective band-pass filtering effect. This design preserves meaningful
high-frequency content while capturing critical features and suppressing noise,
achieving a better balance for event-based data processing. Our approach
demonstrates competitive results on event-based datasets for both
classification and object detection tasks while significantly reducing training
and inference time by up to 42.5% and 32.8%, respectively.</p>
  </article>
</body>
</html>
