<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-06.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-evtslowtv-a-large-and-diverse-dataset-for-event-based-depth-estimation">[1] <a href="https://arxiv.org/abs/2511.02953">EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation</a></h3>
<p><em>Sadiq Layi Macaulay, Nimet Kaygusuz, Simon Hadfield</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†EvtSlowTVï¼Œä¸€ä¸ªä»YouTubeè§†é¢‘æ„å»ºçš„å¤§è§„æ¨¡äº‹ä»¶ç›¸æœºæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡130äº¿ä¸ªäº‹ä»¶ï¼Œç”¨äºè§£å†³äº‹ä»¶ç›¸æœºæ·±åº¦ä¼°è®¡ä¸­æ ‡æ³¨æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶å±•ç¤ºäº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äº‹ä»¶ç›¸æœºå› å…¶é«˜åŠ¨æ€èŒƒå›´å’Œä½å»¶è¿Ÿç‰¹æ€§ï¼Œåœ¨æŒ‘æˆ˜æ€§ç¯å¢ƒä¸­å…·æœ‰é²æ£’æ·±åº¦ä¼°è®¡çš„æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å—é™äºå°è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†EvtSlowTVå¤§è§„æ¨¡äº‹ä»¶ç›¸æœºæ•°æ®é›†ï¼Œæ„å»ºäº†è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ä»¥åˆ©ç”¨åŸå§‹äº‹ä»¶æµçš„é«˜åŠ¨æ€èŒƒå›´æ½œåŠ›ï¼Œè¯¥æ–¹æ³•æ— éœ€åŸºäºå¸§çš„æ ‡æ³¨å¹¶ä¿æŒäº†äº‹ä»¶æ•°æ®çš„å¼‚æ­¥ç‰¹æ€§ã€‚</p>
<p><strong>Result:</strong> EvtSlowTVæ•°æ®é›†æ¯”ç°æœ‰äº‹ä»¶æ•°æ®é›†å¤§ä¸€ä¸ªæ•°é‡çº§ï¼ŒåŒ…å«å„ç§ç¯å¢ƒæ¡ä»¶å’Œè¿åŠ¨åœºæ™¯ï¼Œè®­ç»ƒä½¿ç”¨è¯¥æ•°æ®é›†èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤æ‚åœºæ™¯å’Œè¿åŠ¨ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤§è§„æ¨¡æ— çº¦æŸè‡ªç„¶åœºæ™¯äº‹ä»¶æ•°æ®å¯¹äºæ·±åº¦ä¼°è®¡çš„é‡è¦æ€§ï¼Œè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨äº‹ä»¶ç›¸æœºçš„é«˜åŠ¨æ€èŒƒå›´ç‰¹æ€§ï¼Œä¸ºäº‹ä»¶è§†è§‰åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„å‘å±•æä¾›äº†é‡è¦åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Event cameras, with their high dynamic range (HDR) and low latency, offer a
promising alternative for robust depth estimation in challenging environments.
However, many event-based depth estimation approaches are constrained by
small-scale annotated datasets, limiting their generalizability to real-world
scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event
camera dataset curated from publicly available YouTube footage, which contains
more than 13B events across various environmental conditions and motions,
including seasonal hiking, flying, scenic driving, and underwater exploration.
EvtSlowTV is an order of magnitude larger than existing event datasets,
providing an unconstrained, naturalistic setting for event-based depth
learning. This work shows the suitability of EvtSlowTV for a self-supervised
learning framework to capitalise on the HDR potential of raw event streams. We
further demonstrate that training with EvtSlowTV enhances the model's ability
to generalise to complex scenes and motions. Our approach removes the need for
frame-based annotations and preserves the asynchronous nature of event data.</p>
<h3 id="2-a-lightweight-3d-cnn-for-event-based-human-action-recognition-with-privacy-preserving-potential">[2] <a href="https://arxiv.org/abs/2511.03665">A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential</a></h3>
<p><em>Mehdi Sefidgar Dilmaghani, Francis Fowley, Peter Corcoran</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ä¸‰ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼Œåˆ©ç”¨äº‹ä»¶è§†è§‰æ•°æ®è¿›è¡Œäººç±»æ´»åŠ¨è¯†åˆ«ï¼Œåœ¨ä¿æŠ¤éšç§çš„åŒæ—¶å®ç°äº†94.17%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºç°æœ‰3D-CNNåŸºå‡†æ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»ŸåŸºäºå¸§çš„æ‘„åƒå¤´åœ¨äººç±»ç›‘æ§ç³»ç»Ÿä¸­ä¼šæ•è·å¯è¯†åˆ«çš„ä¸ªäººä¿¡æ¯ï¼Œå­˜åœ¨éšç§ä¿æŠ¤æŒ‘æˆ˜ï¼Œè€Œäº‹ä»¶ç›¸æœºä»…è®°å½•åƒç´ å¼ºåº¦å˜åŒ–ï¼Œæä¾›äº†ä¸€ç§å›ºæœ‰çš„éšç§ä¿æŠ¤æ„ŸçŸ¥æ¨¡å¼ï¼Œä½†éœ€è¦æœ‰æ•ˆçš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ¥å®ç°å‡†ç¡®çš„æ´»åŠ¨è¯†åˆ«ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨è½»é‡çº§ä¸‰ç»´å·ç§¯ç¥ç»ç½‘ç»œæœ‰æ•ˆå»ºæ¨¡ç©ºé—´å’Œæ—¶é—´åŠ¨æ€ï¼ŒåŒæ—¶ä¿æŒç´§å‡‘è®¾è®¡ä»¥é€‚åº”è¾¹ç¼˜éƒ¨ç½²ï¼›ä¸ºè§£å†³ç±»åˆ«ä¸å¹³è¡¡å’Œå¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä½¿ç”¨äº†å¸¦ç±»åˆ«é‡æ–°åŠ æƒçš„ç„¦ç‚¹æŸå¤±å’Œé’ˆå¯¹æ€§æ•°æ®å¢å¼ºç­–ç•¥ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸°ç”°æ™ºèƒ½å®¶å±…å’ŒETRIæ•°æ®é›†ç»„åˆä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹F1åˆ†æ•°è¾¾åˆ°0.9415ï¼Œæ•´ä½“å‡†ç¡®ç‡ä¸º94.17%ï¼Œæ¯”C3Dã€ResNet3Då’ŒMC3_18ç­‰åŸºå‡†3D-CNNæ¶æ„æ€§èƒ½æå‡é«˜è¾¾3%ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜åŸºäºäº‹ä»¶çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å…·æœ‰å¼€å‘å‡†ç¡®ã€é«˜æ•ˆä¸”éšç§æ„ŸçŸ¥çš„äººç±»åŠ¨ä½œè¯†åˆ«ç³»ç»Ÿçš„æ½œåŠ›ï¼Œç‰¹åˆ«é€‚åˆç°å®ä¸–ç•Œçš„è¾¹ç¼˜åº”ç”¨åœºæ™¯ï¼Œä¸ºéšç§ä¿æŠ¤ç›‘æ§ç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>This paper presents a lightweight three-dimensional convolutional neural
network (3DCNN) for human activity recognition (HAR) using event-based vision
data. Privacy preservation is a key challenge in human monitoring systems, as
conventional frame-based cameras capture identifiable personal information. In
contrast, event cameras record only changes in pixel intensity, providing an
inherently privacy-preserving sensing modality. The proposed network
effectively models both spatial and temporal dynamics while maintaining a
compact design suitable for edge deployment. To address class imbalance and
enhance generalization, focal loss with class reweighting and targeted data
augmentation strategies are employed. The model is trained and evaluated on a
composite dataset derived from the Toyota Smart Home and ETRI datasets.
Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy
of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,
and MC3_18 by up to 3%. These results highlight the potential of event-based
deep learning for developing accurate, efficient, and privacy-aware human
action recognition systems suitable for real-world edge applications.</p>
  </article>
</body>
</html>
