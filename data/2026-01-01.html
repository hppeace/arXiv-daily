<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-01.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-u-net-like-spiking-neural-networks-for-single-image-dehazing">[1] <a href="https://arxiv.org/abs/2512.23950">U-Net-Like Spiking Neural Networks for Single Image Dehazing</a></h3>
<p><em>Huibin Li, Haoran Liu, Mingzhe Liu, Yulong Xiao, Peng Li, Guibin Zan</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDehazeSNNï¼Œä¸€ç§å°†U-Netæ¶æ„ä¸è„‰å†²ç¥ç»ç½‘ç»œ(SNN)ç›¸ç»“åˆçš„æ–°å‹å›¾åƒå»é›¾æ–¹æ³•ï¼Œé€šè¿‡æ­£äº¤æ³„æ¼ç§¯åˆ†å‘æ”¾å—(OLIFBlock)å¢å¼ºè·¨é€šé“é€šä¿¡ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°é«˜æ•ˆå»é›¾ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å›¾åƒå»é›¾æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤§æ°”æ•£å°„æ¨¡å‹ï¼Œè€Œæ·±åº¦å­¦ä¹ æ–¹æ³•å¦‚CNNå’ŒTransformerè™½æå‡äº†æ€§èƒ½ä½†å­˜åœ¨å±€é™ï¼šCNNéš¾ä»¥å¤„ç†é•¿ç¨‹ä¾èµ–ï¼ŒTransformeråˆ™éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œå› æ­¤éœ€è¦ä¸€ç§æ—¢èƒ½æœ‰æ•ˆæ•è·å¤šå°ºåº¦ç‰¹å¾åˆèƒ½é™ä½è®¡ç®—è´Ÿæ‹…çš„æ–°æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºDehazeSNNæ¶æ„ï¼Œé‡‡ç”¨U-Netå¼è®¾è®¡é›†æˆè„‰å†²ç¥ç»ç½‘ç»œ(SNN)ï¼Œæœ‰æ•ˆæ•è·å¤šå°ºåº¦å›¾åƒç‰¹å¾å¹¶ç®¡ç†å±€éƒ¨ä¸é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œé€šè¿‡å¼•å…¥æ­£äº¤æ³„æ¼ç§¯åˆ†å‘æ”¾å—(OLIFBlock)å¢å¼ºè·¨é€šé“é€šä¿¡ï¼Œä»è€Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ã€‚</p>
<p><strong>Result:</strong> åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDehazeSNNä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”å…·æœ‰é«˜åº¦ç«äº‰åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡æ— é›¾å›¾åƒï¼ŒåŒæ—¶æ¨¡å‹å°ºå¯¸æ›´å°ä¸”ä¹˜ç§¯ç´¯åŠ è¿ç®—(MAC)æ›´å°‘ï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡ä¸å»é›¾æ€§èƒ½çš„è‰¯å¥½å¹³è¡¡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†SNNåœ¨å›¾åƒå»é›¾ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œæä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºæœªæ¥ä½åŠŸè€—è§†è§‰åº”ç”¨å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼ŒåŒæ—¶å…¬å¼€çš„ä»£ç åº“ä¿ƒè¿›äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ä¸å¤ç°ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Image dehazing is a critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with long-range dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an innovative architecture that integrates a U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with a smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN.</p>
<h3 id="2-mambaseg-harnessing-mamba-for-accurate-and-efficient-image-event-semantic-segmentation">[2] <a href="https://arxiv.org/abs/2512.24243">MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation</a></h3>
<p><em>Fuqiang Gu, Yuanke Li, Xianlei Long, Kangping Ji, Chao Chen, Qingyi Gu, Zhenliang Ni</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MambaSegï¼Œä¸€ç§æ–°é¢–çš„åŒåˆ†æ”¯è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨å¹¶è¡ŒMambaç¼–ç å™¨é«˜æ•ˆå»ºæ¨¡RGBå›¾åƒå’Œäº‹ä»¶æµï¼Œå¹¶é€šè¿‡åŒç»´äº¤äº’æ¨¡å—å®ç°ç©ºé—´å’Œæ—¶é—´ç»´åº¦çš„ç»†ç²’åº¦èåˆï¼Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> RGBç›¸æœºåœ¨å¿«é€Ÿè¿åŠ¨ã€ä½å…‰ç…§æˆ–é«˜åŠ¨æ€èŒƒå›´æ¡ä»¶ä¸‹æ€§èƒ½ä¸‹é™ï¼Œè€Œäº‹ä»¶ç›¸æœºè™½ç„¶å…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡å’Œä½å»¶è¿Ÿä¼˜åŠ¿ï¼Œä½†ç¼ºä¹é¢œè‰²å’Œçº¹ç†ä¿¡æ¯ã€‚ç°æœ‰RGBä¸äº‹ä»¶æ•°æ®èåˆæ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”ä¸»è¦å…³æ³¨ç©ºé—´èåˆï¼Œå¿½ç•¥äº†äº‹ä»¶æµå›ºæœ‰çš„æ—¶é—´åŠ¨æ€ç‰¹æ€§ï¼Œéœ€è¦æ›´é«˜æ•ˆã€å…¨é¢çš„è·¨æ¨¡æ€èåˆæ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> MambaSegé‡‡ç”¨å¹¶è¡ŒMambaç¼–ç å™¨åˆ†åˆ«å¤„ç†RGBå›¾åƒå’Œäº‹ä»¶æµï¼Œå¹¶å¼•å…¥åŒç»´äº¤äº’æ¨¡å—ï¼ˆDDIMï¼‰åŒ…å«è·¨ç©ºé—´äº¤äº’æ¨¡å—ï¼ˆCSIMï¼‰å’Œè·¨æ—¶é—´äº¤äº’æ¨¡å—ï¼ˆCTIMï¼‰ï¼Œåœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šè”åˆæ‰§è¡Œç»†ç²’åº¦èåˆï¼Œæ”¹å–„è·¨æ¨¡æ€å¯¹é½å¹¶å‡å°‘æ¨¡ç³Šæ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨DDD17å’ŒDSECæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMambaSegå®ç°äº†æœ€å…ˆè¿›çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå±•ç¤ºäº†å…¶åœ¨é«˜æ•ˆã€å¯æ‰©å±•å’Œé²æ£’çš„å¤šæ¨¡æ€æ„ŸçŸ¥æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¹¶è¡ŒMambaç¼–ç å™¨å’ŒåŒç»´äº¤äº’æ¨¡å—å¯ä»¥æœ‰æ•ˆèåˆRGBå’Œäº‹ä»¶æ•°æ®çš„äº’è¡¥ç‰¹æ€§ï¼Œä¸ºé«˜æ•ˆçš„å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—èµ„æºå—é™çš„å®æ—¶åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.</p>
  </article>
</body>
</html>
