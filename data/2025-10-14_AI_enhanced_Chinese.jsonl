{"id": "2510.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09854", "abs": "https://arxiv.org/abs/2510.09854", "authors": ["Kaiwen Shi", "Zheyuan Zhang", "Zhengqing Yuan", "Keerthiram Murugesan", "Vincent Galass", "Chuxu Zhang", "Yanfang Ye"], "title": "NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering", "comment": null, "summary": "Diet plays a central role in human health, and Nutrition Question Answering\n(QA) offers a promising path toward personalized dietary guidance and the\nprevention of diet-related chronic diseases. However, existing methods face two\nfundamental challenges: the limited reasoning capacity of single-agent systems\nand the complexity of designing effective multi-agent architectures, as well as\ncontextual overload that hinders accurate decision-making. We introduce\nNutritional-Graph Router (NG-Router), a novel framework that formulates\nnutritional QA as a supervised, knowledge-graph-guided multi-agent\ncollaboration problem. NG-Router integrates agent nodes into heterogeneous\nknowledge graphs and employs a graph neural network to learn task-aware routing\ndistributions over agents, leveraging soft supervision derived from empirical\nagent performance. To further address contextual overload, we propose a\ngradient-based subgraph retrieval mechanism that identifies salient evidence\nduring training, thereby enhancing multi-hop and relational reasoning.\nExtensive experiments across multiple benchmarks and backbone models\ndemonstrate that NG-Router consistently outperforms both single-agent and\nensemble baselines, offering a principled approach to domain-aware multi-agent\nreasoning for complex nutritional health tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faNG-Router\u6846\u67b6\uff0c\u5c06\u8425\u517b\u95ee\u7b54\u5efa\u6a21\u4e3a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u7684\u8def\u7531\u5206\u5e03\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u6709\u9650\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1\u590d\u6742\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u8425\u517b\u95ee\u7b54\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u57fa\u672c\u6311\u6218\uff1a\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u6709\u6548\u591a\u667a\u80fd\u4f53\u67b6\u6784\u7684\u590d\u6742\u6027\uff0c\u540c\u65f6\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u95ee\u9898\u963b\u788d\u4e86\u51c6\u786e\u51b3\u7b56\u5236\u5b9a\u3002", "method": "NG-Router\u6846\u67b6\u5c06\u667a\u80fd\u4f53\u8282\u70b9\u96c6\u6210\u5230\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u57fa\u4e8e\u7ecf\u9a8c\u667a\u80fd\u4f53\u6027\u80fd\u7684\u8f6f\u76d1\u7763\u4efb\u52a1\u611f\u77e5\u8def\u7531\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u7684\u5b50\u56fe\u68c0\u7d22\u673a\u5236\u6765\u8bc6\u522b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u8bc1\u636e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cNG-Router\u59cb\u7ec8\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u96c6\u6210\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u590d\u6742\u8425\u517b\u5065\u5eb7\u4efb\u52a1\u63d0\u4f9b\u4e86\u9886\u57df\u611f\u77e5\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u8425\u517b\u5065\u5eb7\u4efb\u52a1\u63d0\u4f9b\u4e86\u9886\u57df\u611f\u77e5\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5f15\u5bfc\u7684\u534f\u4f5c\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u8425\u517b\u95ee\u7b54\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u548c\u51b3\u7b56\u51c6\u786e\u6027\u3002"}}
{"id": "2510.09869", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09869", "abs": "https://arxiv.org/abs/2510.09869", "authors": ["Sil Hamilton", "Matthew Wilkens", "Andrew Piper"], "title": "NarraBench: A Comprehensive Framework for Narrative Benchmarking", "comment": null, "summary": "We present NarraBench, a theory-informed taxonomy of narrative-understanding\ntasks, as well as an associated survey of 78 existing benchmarks in the area.\nWe find significant need for new evaluations covering aspects of narrative\nunderstanding that are either overlooked in current work or are poorly aligned\nwith existing metrics. Specifically, we estimate that only 27% of narrative\ntasks are well captured by existing benchmarks, and we note that some areas --\nincluding narrative events, style, perspective, and revelation -- are nearly\nabsent from current evaluations. We also note the need for increased\ndevelopment of benchmarks capable of assessing constitutively subjective and\nperspectival aspects of narrative, that is, aspects for which there is\ngenerally no single correct answer. Our taxonomy, survey, and methodology are\nof value to NLP researchers seeking to test LLM narrative understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NarraBench\uff0c\u4e00\u4e2a\u7406\u8bba\u6307\u5bfc\u7684\u53d9\u4e8b\u7406\u89e3\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u5e76\u5bf9\u8be5\u9886\u57df\u768478\u4e2a\u73b0\u6709\u57fa\u51c6\u8fdb\u884c\u4e86\u7cfb\u7edf\u8c03\u67e5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bc4\u4f30\u5728\u53d9\u4e8b\u7406\u89e3\u5173\u952e\u7ef4\u5ea6\u4e0a\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u53d9\u4e8b\u7406\u89e3\u9886\u57df\u7684\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u7a7a\u767d\uff0c\u8bb8\u591a\u5173\u952e\u53d9\u4e8b\u7ef4\u5ea6\u5728\u73b0\u6709\u57fa\u51c6\u4e2d\u8981\u4e48\u88ab\u5ffd\u89c6\uff0c\u8981\u4e48\u4e0e\u73b0\u6709\u6307\u6807\u4e0d\u5339\u914d\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u8fd9\u4e9b\u8bc4\u4f30\u7f3a\u53e3\u5e76\u6307\u5bfc\u672a\u6765\u57fa\u51c6\u5f00\u53d1\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u7406\u8bba\u6307\u5bfc\u7684\u53d9\u4e8b\u7406\u89e3\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u5e76\u5bf978\u4e2a\u73b0\u6709\u57fa\u51c6\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\uff0c\u91c7\u7528\u7cfb\u7edf\u5316\u65b9\u6cd5\u8bc4\u4f30\u5f53\u524d\u57fa\u51c6\u5bf9\u53d9\u4e8b\u4efb\u52a1\u7684\u8986\u76d6\u7a0b\u5ea6\u548c\u8bc4\u4f30\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ec5\u670927%\u7684\u53d9\u4e8b\u4efb\u52a1\u88ab\u73b0\u6709\u57fa\u51c6\u5145\u5206\u8986\u76d6\uff0c\u53d9\u4e8b\u4e8b\u4ef6\u3001\u98ce\u683c\u3001\u89c6\u89d2\u548c\u542f\u793a\u7b49\u5173\u952e\u7ef4\u5ea6\u5728\u73b0\u6709\u8bc4\u4f30\u4e2d\u51e0\u4e4e\u7f3a\u5931\uff0c\u6784\u6210\u6027\u4e3b\u89c2\u548c\u89c6\u89d2\u6027\u65b9\u9762\u7684\u8bc4\u4f30\u80fd\u529b\u5c24\u5176\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aNLP\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u8bc6\u522b\u53d9\u4e8b\u7406\u89e3\u8bc4\u4f30\u7f3a\u53e3\u7684\u91cd\u8981\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u80fd\u591f\u8bc4\u4f30\u4e3b\u89c2\u548c\u89c6\u89d2\u6027\u53d9\u4e8b\u65b9\u9762\u7684\u57fa\u51c6\u7684\u8feb\u5207\u9700\u6c42\uff0c\u5bf9\u6307\u5bfc\u672a\u6765LLM\u53d9\u4e8b\u7406\u89e3\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.10082", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10082", "abs": "https://arxiv.org/abs/2510.10082", "authors": ["Parthiv Chatterjee", "Shivam Sonawane", "Amey Hengle", "Aditya Tanna", "Sourish Dasgupta", "Tanmoy Chakraborty"], "title": "Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers", "comment": null, "summary": "Document summarization enables efficient extraction of user-relevant content\nbut is inherently shaped by individual subjectivity, making it challenging to\nidentify subjective salient information in multifaceted documents. This\ncomplexity underscores the necessity for personalized summarization. However,\ntraining models for personalized summarization has so far been challenging,\nparticularly because diverse training data containing both user preference\nhistory (i.e., click-skip trajectory) and expected (gold-reference) summaries\nare scarce. The MS/CAS PENS dataset is a valuable resource but includes only\npreference history without target summaries, preventing end-to-end supervised\nlearning, and its limited topic-transition diversity further restricts\ngeneralization. To address this, we propose $\\mathrm{PerAugy}$, a novel\ncross-trajectory shuffling and summary-content perturbation based data\naugmentation technique that significantly boosts the accuracy of four\nstate-of-the-art baseline (SOTA) user-encoders commonly used in personalized\nsummarization frameworks (best result: $\\text{0.132}$$\\uparrow$ w.r.t AUC). We\nselect two such SOTA summarizer frameworks as baselines and observe that when\naugmented with their corresponding improved user-encoders, they consistently\nshow an increase in personalization (avg. boost: $\\text{61.2\\%}\\uparrow$ w.r.t.\nPSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the\naugmented dataset by \\peraugy, we introduce three dataset diversity metrics --\n$\\mathrm{TP}$, $\\mathrm{RTC}$, and \\degreed\\ to quantify the induced diversity.\nWe find that $\\mathrm{TP}$ and $\\mathrm{DegreeD}$ strongly correlate with\nuser-encoder performance on the PerAugy-generated dataset across all accuracy\nmetrics, indicating that increased dataset diversity is a key factor driving\nperformance gains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faPerAugy\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u8de8\u8f68\u8ff9\u6df7\u6d17\u548c\u6458\u8981\u5185\u5bb9\u6270\u52a8\u89e3\u51b3\u4e2a\u6027\u5316\u6458\u8981\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7f16\u7801\u5668\u6027\u80fd\u548c\u4e2a\u6027\u5316\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u4e2a\u6027\u5316\u6458\u8981\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709MS/CAS PENS\u6570\u636e\u96c6\u4ec5\u5305\u542b\u7528\u6237\u504f\u597d\u5386\u53f2\u800c\u7f3a\u4e4f\u76ee\u6807\u6458\u8981\uff0c\u65e0\u6cd5\u652f\u6301\u7aef\u5230\u7aef\u76d1\u7763\u5b66\u4e60\uff0c\u4e14\u4e3b\u9898\u8f6c\u6362\u591a\u6837\u6027\u6709\u9650\u9650\u5236\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faPerAugy\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u91c7\u7528\u8de8\u8f68\u8ff9\u6df7\u6d17\u548c\u6458\u8981\u5185\u5bb9\u6270\u52a8\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56db\u79cd\u6700\u5148\u8fdb\u7528\u6237\u7f16\u7801\u5668\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165TP\u3001RTC\u548cDegreeD\u4e09\u4e2a\u6570\u636e\u96c6\u591a\u6837\u6027\u6307\u6807\u91cf\u5316\u589e\u5f3a\u6548\u679c\u3002", "result": "PerAugy\u4f7f\u56db\u79cdSOTA\u7528\u6237\u7f16\u7801\u5668\u7684AUC\u6307\u6807\u6700\u4f73\u63d0\u53470.132\uff0c\u589e\u5f3a\u540e\u7684\u4e2a\u6027\u5316\u6458\u8981\u6846\u67b6\u5728PSE-SU4\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u534761.2%\uff0cTP\u548cDegreeD\u591a\u6837\u6027\u6307\u6807\u4e0e\u7528\u6237\u7f16\u7801\u5668\u6027\u80fd\u5448\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u6570\u636e\u96c6\u591a\u6837\u6027\u662f\u9a71\u52a8\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\uff0cPerAugy\u901a\u8fc7\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u4e86\u4e2a\u6027\u5316\u6458\u8981\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316\u4fe1\u606f\u63d0\u53d6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10658", "abs": "https://arxiv.org/abs/2510.10658", "authors": ["Guy Mor-Lan", "Tamir Sheafer", "Shaul R. Shenhav"], "title": "You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News", "comment": null, "summary": "While media bias is widely studied, the epistemic strategies behind factual\nreporting remain computationally underexplored. This paper analyzes these\nstrategies through a large-scale comparison of CNN and Fox News. To isolate\nreporting style from topic selection, we employ an article matching strategy to\ncompare reports on the same events and apply the FactAppeal framework to a\ncorpus of over 470K articles covering two highly politicized periods: the\nCOVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting\ncontains more factual statements and is more likely to ground them in external\nsources. The outlets also exhibit sharply divergent sourcing patterns: CNN\nbuilds credibility by citing Experts} and Expert Documents, constructing an\nappeal to formal authority, whereas Fox News favors News Reports and direct\nquotations. This work quantifies how partisan outlets use systematically\ndifferent epistemic strategies to construct reality, adding a new dimension to\nthe study of media bias.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u6bd4\u8f83CNN\u548c\u798f\u514b\u65af\u65b0\u95fb\u7684\u62a5\u9053\u98ce\u683c\uff0c\u91cf\u5316\u4e86\u515a\u6d3e\u5a92\u4f53\u5728\u6784\u5efa\u73b0\u5b9e\u65f6\u91c7\u7528\u7684\u4e0d\u540c\u8ba4\u77e5\u7b56\u7565\uff0c\u4e3a\u5a92\u4f53\u504f\u89c1\u7814\u7a76\u589e\u6dfb\u4e86\u65b0\u7684\u7ef4\u5ea6\u3002\u7814\u7a76\u53d1\u73b0CNN\u66f4\u503e\u5411\u4e8e\u4f7f\u7528\u4e8b\u5b9e\u9648\u8ff0\u5e76\u5f15\u7528\u5916\u90e8\u6765\u6e90\uff0c\u800c\u798f\u514b\u65af\u65b0\u95fb\u5219\u504f\u597d\u65b0\u95fb\u62a5\u9053\u548c\u76f4\u63a5\u5f15\u8bed\u3002", "motivation": "\u867d\u7136\u5a92\u4f53\u504f\u89c1\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u4e8b\u5b9e\u62a5\u9053\u80cc\u540e\u7684\u8ba4\u77e5\u7b56\u7565\u5728\u8ba1\u7b97\u5c42\u9762\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5a92\u4f53\u5728\u62a5\u9053\u76f8\u540c\u4e8b\u4ef6\u65f6\u91c7\u7528\u7684\u8ba4\u77e5\u7b56\u7565\u5dee\u5f02\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6587\u7ae0\u5339\u914d\u7b56\u7565\u6765\u9694\u79bb\u62a5\u9053\u98ce\u683c\u4e0e\u4e3b\u9898\u9009\u62e9\u7684\u5f71\u54cd\uff0c\u5c06FactAppeal\u6846\u67b6\u5e94\u7528\u4e8e\u5305\u542b47\u4e07\u7bc7\u6587\u7ae0\u7684\u8bed\u6599\u5e93\uff0c\u6db5\u76d6COVID-19\u5927\u6d41\u884c\u548c\u4ee5\u8272\u5217-\u54c8\u9a6c\u65af\u6218\u4e89\u4e24\u4e2a\u9ad8\u5ea6\u653f\u6cbb\u5316\u65f6\u671f\uff0c\u6bd4\u8f83CNN\u548c\u798f\u514b\u65af\u65b0\u95fb\u5bf9\u76f8\u540c\u4e8b\u4ef6\u7684\u62a5\u9053\u65b9\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0CNN\u7684\u62a5\u9053\u5305\u542b\u66f4\u591a\u4e8b\u5b9e\u9648\u8ff0\u4e14\u66f4\u503e\u5411\u4e8e\u5c06\u5176\u5efa\u7acb\u5728\u5916\u90e8\u6765\u6e90\u57fa\u7840\u4e0a\uff0c\u4e24\u5bb6\u5a92\u4f53\u5c55\u73b0\u51fa\u660e\u663e\u4e0d\u540c\u7684\u6765\u6e90\u6a21\u5f0f\uff1aCNN\u901a\u8fc7\u5f15\u7528\u4e13\u5bb6\u548c\u4e13\u5bb6\u6587\u4ef6\u6784\u5efa\u6b63\u5f0f\u6743\u5a01\u8bc9\u6c42\uff0c\u800c\u798f\u514b\u65af\u65b0\u95fb\u5219\u504f\u7231\u65b0\u95fb\u62a5\u9053\u548c\u76f4\u63a5\u5f15\u8bed\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u91cf\u5316\u4e86\u515a\u6d3e\u5a92\u4f53\u5982\u4f55\u7cfb\u7edf\u6027\u5730\u4f7f\u7528\u4e0d\u540c\u7684\u8ba4\u77e5\u7b56\u7565\u6765\u6784\u5efa\u73b0\u5b9e\uff0c\u63ed\u793a\u4e86\u5a92\u4f53\u62a5\u9053\u4e0d\u4ec5\u5728\u9009\u62e9\u4e3b\u9898\u4e0a\u5b58\u5728\u504f\u89c1\uff0c\u5728\u4e8b\u5b9e\u5448\u73b0\u7684\u8ba4\u77e5\u7b56\u7565\u4e0a\u4e5f\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u4e3a\u7406\u89e3\u5a92\u4f53\u504f\u89c1\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u7ef4\u5ea6\u3002"}}
{"id": "2510.09867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09867", "abs": "https://arxiv.org/abs/2510.09867", "authors": ["Zhi Chen", "Xin Yu", "Xiaohui Tao", "Yan Li", "Zi Huang"], "title": "Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation", "comment": "Accepted to the journal Pattern Recognition in 2025", "summary": "Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across\nvarious tasks by pre-training on numerous image-text pairs. These models often\nbenefit from using an ensemble of context prompts to represent a class. Despite\nbeing effective, conventional prompt ensembling that averages textual features\nof context prompts often yields suboptimal results. This is because feature\naveraging shifts the class centroids away from the true class distribution. To\naddress this issue, we propose the Cluster-Aware Prompt Ensemble Learning\n(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL\nclassifies images into one of several class clusters, each represented by a\ndistinct prompt. Instead of ensembling prompts in the feature space, we perform\nensembling in the classification logits space, aligning better with the visual\nfeature distribution. To further optimize prompt fine-tuning while maintaining\ncluster-specific discriminative power, we introduce a cluster-preserving\nregularization term. This ensures that prompts remain distinct and specialized\nfor different clusters, preventing collapse into a uniform direction.\nAdditionally, we integrate an adaptive prompt weighting technique to\ndynamically adjust the attention weights for flawed or ambiguous prompts,\nensuring robust performance across diverse datasets and tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u805a\u7c7b\u611f\u77e5\u63d0\u793a\u96c6\u6210\u5b66\u4e60\u6846\u67b6CAPEL\uff0c\u901a\u8fc7\u5c06\u63d0\u793a\u96c6\u6210\u4ece\u7279\u5f81\u7a7a\u95f4\u8f6c\u79fb\u5230\u5206\u7c7blogits\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u805a\u7c7b\u4fdd\u6301\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\u4e2d\u7279\u5f81\u5e73\u5747\u5bfc\u81f4\u7c7b\u4e2d\u5fc3\u504f\u79fb\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\u901a\u8fc7\u5e73\u5747\u591a\u4e2a\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u6587\u672c\u7279\u5f81\u6765\u5b9e\u73b0\u96f6\u6837\u672c\u5206\u7c7b\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\uff0c\u56e0\u4e3a\u7279\u5f81\u5e73\u5747\u4f1a\u4f7f\u7c7b\u4e2d\u5fc3\u504f\u79bb\u771f\u5b9e\u7684\u7c7b\u5206\u5e03\uff0c\u65e0\u6cd5\u6709\u6548\u4fdd\u7559\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u805a\u7c7b\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u805a\u7c7b\u611f\u77e5\u63d0\u793a\u96c6\u6210\u5b66\u4e60\u6846\u67b6CAPEL\uff0c\u8be5\u65b9\u6cd5\u5c06\u56fe\u50cf\u5206\u7c7b\u5230\u7531\u4e0d\u540c\u63d0\u793a\u8868\u793a\u7684\u591a\u4e2a\u7c7b\u7c07\u4e2d\uff0c\u5728\u5206\u7c7blogits\u7a7a\u95f4\u800c\u975e\u7279\u5f81\u7a7a\u95f4\u8fdb\u884c\u63d0\u793a\u96c6\u6210\uff0c\u66f4\u597d\u5730\u4e0e\u89c6\u89c9\u7279\u5f81\u5206\u5e03\u5bf9\u9f50\uff1b\u540c\u65f6\u5f15\u5165\u805a\u7c7b\u4fdd\u6301\u6b63\u5219\u5316\u9879\u6765\u4f18\u5316\u63d0\u793a\u5fae\u8c03\uff0c\u4fdd\u6301\u4e0d\u540c\u7c07\u95f4\u63d0\u793a\u7684\u533a\u5206\u6027\uff1b\u8fd8\u96c6\u6210\u4e86\u81ea\u9002\u5e94\u63d0\u793a\u52a0\u6743\u6280\u672f\uff0c\u52a8\u6001\u8c03\u6574\u6709\u7f3a\u9677\u6216\u6a21\u7cca\u63d0\u793a\u7684\u6ce8\u610f\u529b\u6743\u91cd\u3002", "result": "CAPEL\u6846\u67b6\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u4fdd\u7559\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u805a\u7c7b\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u7279\u5f81\u5e73\u5747\u65b9\u6cd5\u5bfc\u81f4\u7684\u7c7b\u4e2d\u5fc3\u504f\u79fb\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u5728\u5206\u7c7blogits\u7a7a\u95f4\u8fdb\u884c\u63d0\u793a\u96c6\u6210\u6bd4\u5728\u7279\u5f81\u7a7a\u95f4\u66f4\u6709\u6548\uff0c\u805a\u7c7b\u4fdd\u6301\u673a\u5236\u5bf9\u4e8e\u7ef4\u6301\u63d0\u793a\u7684\u533a\u5206\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2510.10035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10035", "abs": "https://arxiv.org/abs/2510.10035", "authors": ["Jusheng Zhang", "Kaitong Cai", "Qinglin Zeng", "Ningyuan Liu", "Stephen Fan", "Ziliang Chen", "Keze Wang"], "title": "Failure-Driven Workflow Refinement", "comment": null, "summary": "Optimizing LLM-based workflows is typically formulated as a global search,\nwhere candidate workflows are evaluated based on a scalar metric. This\nparadigm, however, suffers from a critical flaw: information collapse. By\nreducing rich, multi-step execution traces to simple success/failure signals,\nexisting methods are rendered blind to the underlying structure of failures,\nfundamentally preventing them from modeling the workflow's failure\ndistribution. We reconceptualize this challenge as a distributional problem. We\npropose a new paradigm where the optimization goal is not to maximize a scalar\nscore, but to directly minimize a workflow's Expected Failure Mass, i.e., the\nintegral of its failure probability density function defined over a\nhigh-dimensional Failure Signature Space (FSS). This distributional lens allows\nus to move from inefficient, zero-order optimization to a principled,\ngradient-like descent on the failure landscape itself. We introduce CE-Graph, a\nframework that operationalizes this paradigm through a novel, failure-driven\nrefinement process. CE-Graph approximates the failure distribution from a pool\nof counterexamples, identifies its densest regions as recurring failure modes,\nand applies targeted, operator-constrained graph edits via a Propose-and-Verify\nmechanism to greedily reduce the failure mass. On math, code, and QA\nbenchmarks, our CE-Graph achieves higher robustness at a significantly lower\ncost than strong baselines. This suggests that a system's reliability emerges\nnot from avoiding failures, but from systematically learning and reshaping the\ngeometric structure of its failure distributions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u5de5\u4f5c\u6d41\u4f18\u5316\u8303\u5f0f\uff0c\u5c06\u95ee\u9898\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u5206\u5e03\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u671f\u671b\u5931\u8d25\u8d28\u91cf\u800c\u975e\u6700\u5927\u5316\u6807\u91cf\u5206\u6570\u6765\u7cfb\u7edf\u6027\u5730\u5b66\u4e60\u5e76\u91cd\u5851\u5931\u8d25\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709LLM\u5de5\u4f5c\u6d41\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u574d\u7f29\u95ee\u9898\uff0c\u5c06\u4e30\u5bcc\u7684\u591a\u6b65\u9aa4\u6267\u884c\u8f68\u8ff9\u7b80\u5316\u4e3a\u7b80\u5355\u7684\u6210\u529f/\u5931\u8d25\u4fe1\u53f7\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5efa\u6a21\u5de5\u4f5c\u6d41\u7684\u5931\u8d25\u5206\u5e03\u7ed3\u6784\uff0c\u4ece\u6839\u672c\u4e0a\u9650\u5236\u4e86\u4f18\u5316\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86CE-Graph\u6846\u67b6\uff0c\u901a\u8fc7\u5931\u8d25\u9a71\u52a8\u7684\u7ec6\u5316\u8fc7\u7a0b\u6765\u64cd\u4f5c\u5316\u8fd9\u4e00\u8303\u5f0f\uff1a\u4ece\u53cd\u4f8b\u6c60\u4e2d\u8fd1\u4f3c\u5931\u8d25\u5206\u5e03\uff0c\u8bc6\u522b\u6700\u5bc6\u96c6\u533a\u57df\u4f5c\u4e3a\u91cd\u590d\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa-\u9a8c\u8bc1\u673a\u5236\u5e94\u7528\u6709\u9488\u5bf9\u6027\u7684\u64cd\u4f5c\u7b26\u7ea6\u675f\u56fe\u7f16\u8f91\u6765\u8d2a\u5a6a\u5730\u51cf\u5c11\u5931\u8d25\u8d28\u91cf\u3002", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548c\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCE-Graph\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u6210\u672c\u5b9e\u73b0\u4e86\u6bd4\u5f3a\u57fa\u7ebf\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u6548\u7387\u548c\u6548\u679c\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u5e76\u975e\u6765\u81ea\u907f\u514d\u5931\u8d25\uff0c\u800c\u662f\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u5b66\u4e60\u548c\u91cd\u5851\u5176\u5931\u8d25\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u5b9e\u73b0\uff0c\u8fd9\u4e3aLLM\u5de5\u4f5c\u6d41\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u8bba\u6846\u67b6\u3002"}}
{"id": "2510.11277", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11277", "abs": "https://arxiv.org/abs/2510.11277", "authors": ["Guangyu Wei", "Ke Han", "Yueming Lyu", "Yu Luo", "Yue Jiang", "Caifeng Shan", "Nicu Sebe"], "title": "Towards Real-Time Fake News Detection under Evidence Scarcity", "comment": null, "summary": "Fake news detection becomes particularly challenging in real-time scenarios,\nwhere emerging events often lack sufficient supporting evidence. Existing\napproaches often rely heavily on external evidence and therefore struggle to\ngeneralize under evidence scarcity. To address this issue, we propose\nEvaluation-Aware Selection of Experts (EASE), a novel framework for real-time\nfake news detection that dynamically adapts its decision-making process\naccording to the assessed sufficiency of available evidence. EASE introduces a\nsequential evaluation mechanism comprising three independent perspectives: (1)\nEvidence-based evaluation, which assesses evidence and incorporates it into\ndecision-making only when the evidence is sufficiently supportive; (2)\nReasoning-based evaluation, which leverages the world knowledge of large\nlanguage models (LLMs) and applies them only when their reliability is\nadequately established; and (3) Sentiment-based fallback, which integrates\nsentiment cues when neither evidence nor reasoning is reliable. To enhance the\naccuracy of evaluation processes, EASE employs instruction tuning with pseudo\nlabels to guide each evaluator in justifying its perspective-specific knowledge\nthrough interpretable reasoning. Furthermore, the expert modules integrate the\nevaluators' justified assessments with the news content to enable\nevaluation-aware decision-making, thereby enhancing overall detection accuracy.\nMoreover, we introduce RealTimeNews-25, a new benchmark comprising recent news\nfor evaluating model generalization on emerging news with limited evidence.\nExtensive experiments demonstrate that EASE not only achieves state-of-the-art\nperformance across multiple benchmarks, but also significantly improves\ngeneralization to real-time news. The code and dataset are available:\nhttps://github.com/wgyhhhh/EASE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EASE\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u8bc1\u636e\u5145\u5206\u6027\u6765\u6539\u8fdb\u5b9e\u65f6\u5047\u65b0\u95fb\u68c0\u6d4b\uff0c\u5728\u8bc1\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e09\u9636\u6bb5\u8bc4\u4f30\u673a\u5236\uff0c\u7ed3\u5408\u6307\u4ee4\u8c03\u4f18\u548c\u4f2a\u6807\u7b7e\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u5916\u90e8\u8bc1\u636e\uff0c\u5728\u5b9e\u65f6\u573a\u666f\u4e0b\u65b0\u5174\u4e8b\u4ef6\u5f80\u5f80\u7f3a\u4e4f\u8db3\u591f\u8bc1\u636e\u652f\u6301\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u8bc1\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u52a8\u6001\u9002\u5e94\u8bc1\u636e\u53ef\u7528\u6027\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u63d0\u51faEASE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u72ec\u7acb\u89c6\u89d2\u7684\u5e8f\u5217\u8bc4\u4f30\u673a\u5236\uff1a\u57fa\u4e8e\u8bc1\u636e\u7684\u8bc4\u4f30\u4ec5\u5728\u8bc1\u636e\u5145\u5206\u652f\u6301\u65f6\u6574\u5408\u8bc1\u636e\uff1b\u57fa\u4e8e\u63a8\u7406\u7684\u8bc4\u4f30\u5728\u53ef\u9760\u6027\u8db3\u591f\u65f6\u5229\u7528LLM\u7684\u4e16\u754c\u77e5\u8bc6\uff1b\u60c5\u611f\u56de\u9000\u673a\u5236\u5728\u8bc1\u636e\u548c\u63a8\u7406\u5747\u4e0d\u53ef\u9760\u65f6\u6574\u5408\u60c5\u611f\u7ebf\u7d22\u3002\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u548c\u4f2a\u6807\u7b7e\u8bad\u7ec3\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "result": "EASE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u5b9e\u65f6\u65b0\u95fb\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6784\u5efa\u4e86RealTimeNews-25\u65b0\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u65b0\u5174\u65b0\u95fb\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u8bc1\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "EASE\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u8bc1\u636e\u5145\u5206\u6027\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\uff0c\u8bc1\u660e\u4e86\u591a\u89c6\u89d2\u8bc4\u4f30\u673a\u5236\u5728\u8bc1\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u5b9e\u65f6\u5047\u65b0\u95fb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u611f\u77e5\u51b3\u7b56\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.10257", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10257", "abs": "https://arxiv.org/abs/2510.10257", "authors": ["Abdelrhman Elrawy", "Emad A. Mohammed"], "title": "Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its\nstandard adaptive density control (ADC) can lead to overfitting and bloated\nreconstructions. While state-of-the-art methods like FSGS improve quality, they\noften do so by significantly increasing the primitive count. This paper\npresents a framework that revises the core 3DGS optimization to prioritize\nefficiency. We replace the standard positional gradient heuristic with a novel\ndensification trigger that uses the opacity gradient as a lightweight proxy for\nrendering error. We find this aggressive densification is only effective when\npaired with a more conservative pruning schedule, which prevents destructive\noptimization cycles. Combined with a standard depth-correlation loss for\ngeometric guidance, our framework demonstrates a fundamental improvement in\nefficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k\nvs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a\nreduction of approximately 70%. This dramatic gain in compactness is achieved\nwith a modest trade-off in reconstruction metrics, establishing a new\nstate-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view\nsynthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u6548\u7387\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0d\u900f\u660e\u5ea6\u68af\u5ea6\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u6e32\u67d3\u8bef\u5dee\u4ee3\u7406\u6765\u66ff\u4ee3\u6807\u51c6\u4f4d\u7f6e\u68af\u5ea6\u542f\u53d1\u5f0f\uff0c\u7ed3\u5408\u4fdd\u5b88\u526a\u679d\u7b56\u7565\u548c\u6df1\u5ea6\u76f8\u5173\u635f\u5931\uff0c\u5728\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u57fa\u5143\u6570\u91cf\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5c11\u6837\u672c\u573a\u666f\u4e2d\u9762\u4e34\u8fc7\u62df\u5408\u548c\u91cd\u5efa\u81a8\u80c0\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5982FSGS\u867d\u7136\u63d0\u9ad8\u4e86\u8d28\u91cf\u4f46\u663e\u8457\u589e\u52a0\u4e86\u57fa\u5143\u6570\u91cf\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u6838\u5fc33DGS\u7b97\u6cd5\u6765\u4f18\u5148\u8003\u8651\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4e0d\u900f\u660e\u5ea6\u68af\u5ea6\u7684\u65b0\u578b\u81f4\u5bc6\u5316\u89e6\u53d1\u673a\u5236\u66ff\u4ee3\u6807\u51c6\u4f4d\u7f6e\u68af\u5ea6\u542f\u53d1\u5f0f\uff0c\u7ed3\u5408\u66f4\u4fdd\u5b88\u7684\u526a\u679d\u7b56\u7565\u6765\u9632\u6b62\u7834\u574f\u6027\u4f18\u5316\u5faa\u73af\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u6df1\u5ea6\u76f8\u5173\u635f\u5931\u63d0\u4f9b\u51e0\u4f55\u6307\u5bfc\u3002", "result": "\u57283\u89c6\u56feLLFF\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u6bd4FSGS\u7d27\u51d140%\u4ee5\u4e0a\uff0832k vs 57k\u57fa\u5143\uff09\uff0c\u5728Mip-NeRF 360\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7ea670%\u7684\u57fa\u5143\u51cf\u5c11\uff0c\u4ec5\u4ee5\u9002\u5ea6\u7684\u91cd\u5efa\u6307\u6807\u635f\u5931\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5c11\u6837\u672c\u89c6\u56fe\u5408\u6210\u7684\u8d28\u91cf-\u6548\u7387\u5e15\u7d2f\u6258\u8fb9\u754c\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u6838\u5fc3\u4f18\u5316\u7b56\u7565\u53ef\u4ee5\u5b9e\u73b0\u6548\u7387\u7684\u6839\u672c\u6027\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2510.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10047", "abs": "https://arxiv.org/abs/2510.10047", "authors": ["Ruohao Li", "Hongjun Liu", "Leyi Zhao", "Zisu Li", "Jiawei Li", "Jiajun Jiang", "Linning Xu", "Chen Zhao", "Mingming Fan", "Chen Liang"], "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning", "comment": "14 pages, 7 figures", "summary": "Large language model (LLM) agents have shown remarkable reasoning abilities.\nHowever, existing multi-agent frameworks often rely on fixed roles or\ncentralized control, limiting scalability and adaptability in long-horizon\nreasoning. We introduce SwarmSys, a closed-loop framework for distributed\nmulti-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys\nemerges through iterative interactions among three specialized roles,\nExplorers, Workers, and Validators, that continuously cycle through\nexploration, exploitation, and validation. To enable scalable and adaptive\ncollaboration, we integrate adaptive agent and event profiles, embedding-based\nprobabilistic matching, and a pheromone-inspired reinforcement mechanism,\nsupporting dynamic task allocation and self-organizing convergence without\nglobal supervision. Across symbolic reasoning, research synthesis, and\nscientific programming tasks, SwarmSys consistently outperforms baselines,\nimproving both accuracy and reasoning stability. These findings highlight\nswarm-inspired coordination as a promising paradigm for scalable, robust, and\nadaptive multi-agent reasoning, suggesting that coordination scaling may rival\nmodel scaling in advancing LLM intelligence.", "AI": {"tldr": "SwarmSys\u662f\u4e00\u4e2a\u53d7\u7fa4\u4f53\u667a\u80fd\u542f\u53d1\u7684\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u7d22\u8005\u3001\u5de5\u4f5c\u8005\u548c\u9a8c\u8bc1\u8005\u4e09\u79cd\u89d2\u8272\u7684\u8fed\u4ee3\u4ea4\u4e92\u5b9e\u73b0\u95ed\u73af\u63a8\u7406\uff0c\u5728\u7b26\u53f7\u63a8\u7406\u3001\u7814\u7a76\u7efc\u5408\u548c\u79d1\u5b66\u7f16\u7a0b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u8be5\u7814\u7a76\u8868\u660e\u534f\u8c03\u6269\u5c55\u53ef\u80fd\u6210\u4e3a\u4e0e\u6a21\u578b\u6269\u5c55\u540c\u7b49\u91cd\u8981\u7684LLM\u667a\u80fd\u53d1\u5c55\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709LLM\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u89d2\u8272\u6216\u96c6\u4e2d\u63a7\u5236\uff0c\u5728\u957f\u89c6\u91ce\u63a8\u7406\u4e2d\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5b9e\u73b0\u5206\u5e03\u5f0f\u3001\u81ea\u9002\u5e94\u534f\u4f5c\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "SwarmSys\u6846\u67b6\u901a\u8fc7\u4e09\u79cd\u4e13\u95e8\u89d2\u8272\u7684\u8fed\u4ee3\u4ea4\u4e92\u5b9e\u73b0\u534f\u8c03\uff1a\u63a2\u7d22\u8005\u3001\u5de5\u4f5c\u8005\u548c\u9a8c\u8bc1\u8005\uff0c\u5b83\u4eec\u6301\u7eed\u5faa\u73af\u6267\u884c\u63a2\u7d22\u3001\u5229\u7528\u548c\u9a8c\u8bc1\u8fc7\u7a0b\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u548c\u4e8b\u4ef6\u914d\u7f6e\u6587\u4ef6\u3001\u57fa\u4e8e\u5d4c\u5165\u7684\u6982\u7387\u5339\u914d\u4ee5\u53ca\u53d7\u4fe1\u606f\u7d20\u542f\u53d1\u7684\u5f3a\u5316\u673a\u5236\uff0c\u652f\u6301\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u65e0\u5168\u5c40\u76d1\u7763\u7684\u81ea\u7ec4\u7ec7\u6536\u655b\u3002", "result": "\u5728\u7b26\u53f7\u63a8\u7406\u3001\u7814\u7a76\u7efc\u5408\u548c\u79d1\u5b66\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSwarmSys\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u3002\u8be5\u6846\u67b6\u5728\u5404\u79cd\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7fa4\u4f53\u542f\u53d1\u7684\u534f\u8c03\u673a\u5236\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u6709\u524d\u666f\u8303\u5f0f\u3002\u534f\u8c03\u6269\u5c55\u53ef\u80fd\u4e0e\u6a21\u578b\u6269\u5c55\u5728\u63a8\u8fdbLLM\u667a\u80fd\u65b9\u9762\u5177\u6709\u540c\u7b49\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.11328", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11328", "abs": "https://arxiv.org/abs/2510.11328", "authors": ["Chenxi Wang", "Yixuan Zhang", "Ruiji Yu", "Yufei Zheng", "Lang Gao", "Zirui Song", "Zixiang Xu", "Gus Xia", "Huishuai Zhang", "Dongyan Zhao", "Xiuying Chen"], "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control", "comment": "19 pages, 8 figures, 8 tables. Code and dataset available at\n  https://github.com/Aurora-cx/EmotionCircuits-LLM", "summary": "As the demand for emotional intelligence in large language models (LLMs)\ngrows, a key challenge lies in understanding the internal mechanisms that give\nrise to emotional expression and in controlling emotions in generated text.\nThis study addresses three core questions: (1) Do LLMs contain context-agnostic\nmechanisms shaping emotional expression? (2) What form do these mechanisms\ntake? (3) Can they be harnessed for universal emotion control? We first\nconstruct a controlled dataset, SEV (Scenario-Event with Valence), to elicit\ncomparable internal states across emotions. Subsequently, we extract\ncontext-agnostic emotion directions that reveal consistent, cross-context\nencoding of emotion (Q1). We identify neurons and attention heads that locally\nimplement emotional computation through analytical decomposition and causal\nanalysis, and validate their causal roles via ablation and enhancement\ninterventions. Next, we quantify each sublayer's causal influence on the\nmodel's final emotion representation and integrate the identified local\ncomponents into coherent global emotion circuits that drive emotional\nexpression (Q2). Directly modulating these circuits achieves 99.65%\nemotion-expression accuracy on the test set, surpassing prompting- and\nsteering-based methods (Q3). To our knowledge, this is the first systematic\nstudy to uncover and validate emotion circuits in LLMs, offering new insights\ninto interpretability and controllable emotional intelligence.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u63ed\u793a\u5e76\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u60c5\u611f\u7535\u8def\u673a\u5236\uff0c\u901a\u8fc7\u6784\u5efa\u53d7\u63a7\u6570\u636e\u96c6\u548c\u56e0\u679c\u5206\u6790\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e86\u8de8\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u60c5\u611f\u7f16\u7801\u6a21\u5f0f\uff0c\u5e76\u5b9e\u73b0\u4e8699.65%\u7684\u60c5\u611f\u8868\u8fbe\u63a7\u5236\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u60c5\u611f\u667a\u80fd\u53d1\u5c55\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u7406\u89e3\u60c5\u611f\u8868\u8fbe\u7684\u5185\u90e8\u673a\u5236\u5e76\u5b9e\u73b0\u5bf9\u751f\u6210\u6587\u672c\u4e2d\u60c5\u611f\u7684\u63a7\u5236\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1aLLMs\u662f\u5426\u5305\u542b\u5851\u9020\u60c5\u611f\u8868\u8fbe\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u673a\u5236\u3001\u8fd9\u4e9b\u673a\u5236\u7684\u5177\u4f53\u5f62\u5f0f\u4ee5\u53ca\u80fd\u5426\u7528\u4e8e\u901a\u7528\u60c5\u611f\u63a7\u5236\u3002", "method": "\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86\u53d7\u63a7\u6570\u636e\u96c6SEV\u6765\u5f15\u53d1\u8de8\u60c5\u611f\u7684\u53ef\u6bd4\u5185\u90e8\u72b6\u6001\uff0c\u968f\u540e\u63d0\u53d6\u4e86\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u60c5\u611f\u65b9\u5411\uff0c\u901a\u8fc7\u5206\u6790\u6027\u5206\u89e3\u548c\u56e0\u679c\u5206\u6790\u8bc6\u522b\u4e86\u5c40\u90e8\u6267\u884c\u60c5\u611f\u8ba1\u7b97\u7684\u795e\u7ecf\u5143\u548c\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u548c\u589e\u5f3a\u5e72\u9884\u9a8c\u8bc1\u5176\u56e0\u679c\u4f5c\u7528\uff0c\u6700\u540e\u91cf\u5316\u5404\u5b50\u5c42\u5bf9\u6700\u7ec8\u60c5\u611f\u8868\u793a\u7684\u56e0\u679c\u5f71\u54cd\u5e76\u6574\u5408\u5c40\u90e8\u7ec4\u4ef6\u4e3a\u9a71\u52a8\u60c5\u611f\u8868\u8fbe\u7684\u5168\u5c40\u60c5\u611f\u7535\u8def\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u8de8\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u60c5\u611f\u7f16\u7801\u673a\u5236\uff0c\u8bc6\u522b\u51fa\u5c40\u90e8\u60c5\u611f\u8ba1\u7b97\u7ec4\u4ef6\u5e76\u9a8c\u8bc1\u5176\u56e0\u679c\u4f5c\u7528\uff0c\u76f4\u63a5\u8c03\u5236\u8fd9\u4e9b\u60c5\u611f\u7535\u8def\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8699.65%\u7684\u60c5\u611f\u8868\u8fbe\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u63d0\u793a\u548c\u5bfc\u5411\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u63ed\u793a\u548c\u9a8c\u8bc1LLMs\u4e2d\u60c5\u611f\u7535\u8def\u7684\u7814\u7a76\uff0c\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u60c5\u611f\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u901a\u8fc7\u7406\u89e3\u5185\u90e8\u60c5\u611f\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u60c5\u611f\u63a7\u5236\uff0c\u4e3a\u5f00\u53d1\u66f4\u7cbe\u51c6\u7684\u60c5\u611f\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.10395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10395", "abs": "https://arxiv.org/abs/2510.10395", "authors": ["Xinlong Chen", "Yue Ding", "Weihong Lin", "Jingyun Hua", "Linli Yao", "Yang Shi", "Bozhou Li", "Yuanxing Zhang", "Qiang Liu", "Pengfei Wan", "Liang Wang", "Tieniu Tan"], "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration", "comment": "Project webpage: https://avocado-captioner.github.io/", "summary": "Audiovisual video captioning aims to generate semantically rich descriptions\nwith temporal alignment between visual and auditory events, thereby benefiting\nboth video understanding and generation. In this paper, we present AVoCaDO, a\npowerful audiovisual video captioner driven by the temporal orchestration\nbetween audio and visual modalities. We propose a two-stage post-training\npipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated\ndataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)\nAVoCaDO GRPO, which leverages tailored reward functions to further enhance\ntemporal coherence and dialogue accuracy while regularizing caption length and\nreducing collapse. Experimental results demonstrate that AVoCaDO significantly\noutperforms existing open-source models across four audiovisual video\ncaptioning benchmarks, and also achieves competitive performance on the VDC and\nDREAM-1K benchmark under visual-only settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AVoCaDO\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u542c\u65f6\u5e8f\u7f16\u6392\u7684\u5f3a\u5927\u591a\u6a21\u6001\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u89c6\u542c\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u65e8\u5728\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u4e14\u5177\u6709\u89c6\u89c9\u548c\u542c\u89c9\u4e8b\u4ef6\u65f6\u5e8f\u5bf9\u9f50\u7684\u63cf\u8ff0\uff0c\u8fd9\u5bf9\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u5e8f\u5bf9\u9f50\u548c\u6a21\u6001\u534f\u8c03\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6d41\u7a0b\uff1aAVoCaDO SFT\u9636\u6bb5\u5728\u65b0\u6784\u5efa\u7684107K\u9ad8\u8d28\u91cf\u65f6\u5e8f\u5bf9\u9f50\u89c6\u542c\u63cf\u8ff0\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1bAVoCaDO GRPO\u9636\u6bb5\u5229\u7528\u5b9a\u5236\u7684\u5956\u52b1\u51fd\u6570\u8fdb\u4e00\u6b65\u589e\u5f3a\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u5bf9\u8bdd\u51c6\u786e\u6027\uff0c\u540c\u65f6\u89c4\u8303\u5316\u63cf\u8ff0\u957f\u5ea6\u5e76\u51cf\u5c11\u6a21\u578b\u574d\u584c\u3002", "result": "AVoCaDO\u5728\u56db\u4e2a\u89c6\u542c\u89c6\u9891\u63cf\u8ff0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5728\u4ec5\u89c6\u89c9\u8bbe\u7f6e\u7684VDC\u548cDREAM-1K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u65f6\u5e8f\u7f16\u6392\u7684\u89c6\u542c\u878d\u5408\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u89c6\u9891\u63cf\u8ff0\u8d28\u91cf\uff0c\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u3002"}}
{"id": "2510.10454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10454", "abs": "https://arxiv.org/abs/2510.10454", "authors": ["Sihang Zeng", "Yujuan Fu", "Sitong Zhou", "Zixuan Yu", "Lucas Jing Liu", "Jun Wen", "Matthew Thompson", "Ruth Etzioni", "Meliha Yetisgen"], "title": "Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction", "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) offer a generalizable approach for modeling\npatient trajectories, but suffer from the long and noisy nature of electronic\nhealth records (EHR) data in temporal reasoning. To address these challenges,\nwe introduce Traj-CoA, a multi-agent system involving chain-of-agents for\npatient trajectory modeling. Traj-CoA employs a chain of worker agents to\nprocess EHR data in manageable chunks sequentially, distilling critical events\ninto a shared long-term memory module, EHRMem, to reduce noise and preserve a\ncomprehensive timeline. A final manager agent synthesizes the worker agents'\nsummary and the extracted timeline in EHRMem to make predictions. In a\nzero-shot one-year lung cancer risk prediction task based on five-year EHR\ndata, Traj-CoA outperforms baselines of four categories. Analysis reveals that\nTraj-CoA exhibits clinically aligned temporal reasoning, establishing it as a\npromisingly robust and generalizable approach for modeling complex patient\ntrajectories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Traj-CoA\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u60a3\u8005\u8f68\u8ff9\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u94fe\u5f0f\u667a\u80fd\u4f53\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u957f\u5e8f\u5217\u6570\u636e\uff0c\u5728\u96f6\u6837\u672c\u80ba\u764c\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u4e3a\u60a3\u8005\u8f68\u8ff9\u5efa\u6a21\u63d0\u4f9b\u4e86\u901a\u7528\u65b9\u6cd5\uff0c\u4f46\u5728\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u957f\u5e8f\u5217\u548c\u566a\u58f0\u6570\u636e\u65f6\u9762\u4e34\u65f6\u5e8f\u63a8\u7406\u56f0\u96be\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u957f\u5ea6\u548c\u566a\u58f0\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u9650\u5236\u95ee\u9898\u3002", "method": "Traj-CoA\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u542b\u591a\u4e2a\u5de5\u4f5c\u667a\u80fd\u4f53\u6309\u987a\u5e8f\u5904\u7406\u53ef\u7ba1\u7406\u7684\u6570\u636e\u5757\uff0c\u5c06\u5173\u952e\u4e8b\u4ef6\u63d0\u70bc\u5230\u5171\u4eab\u957f\u671f\u8bb0\u5fc6\u6a21\u5757EHRMem\u4e2d\uff0c\u6700\u540e\u7531\u7ba1\u7406\u667a\u80fd\u4f53\u7efc\u5408\u5de5\u4f5c\u667a\u80fd\u4f53\u7684\u603b\u7ed3\u548cEHRMem\u4e2d\u7684\u65f6\u95f4\u7ebf\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u57fa\u4e8e\u4e94\u5e74\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u96f6\u6837\u672c\u4e00\u5e74\u671f\u80ba\u764c\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cTraj-CoA\u5728\u56db\u7c7b\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5206\u6790\u663e\u793a\u8be5\u65b9\u6cd5\u5c55\u73b0\u51fa\u4e0e\u4e34\u5e8a\u5bf9\u9f50\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Traj-CoA\u4e3a\u590d\u6742\u60a3\u8005\u8f68\u8ff9\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\uff0c\u5176\u591a\u667a\u80fd\u4f53\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u5904\u7406\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5728\u533b\u7597\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.11370", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11370", "abs": "https://arxiv.org/abs/2510.11370", "authors": ["Wenhan Ma", "Hailin Zhang", "Liang Zhao", "Yifan Song", "Yudong Wang", "Zhifang Sui", "Fuli Luo"], "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRollout Routing Replay (R3)\u65b9\u6cd5\u6765\u89e3\u51b3\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8def\u7531\u673a\u5236\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u8bb0\u5f55\u63a8\u7406\u9636\u6bb5\u7684\u8def\u7531\u5206\u5e03\u5e76\u5728\u8bad\u7ec3\u9636\u6bb5\u91cd\u653e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3-\u63a8\u7406\u7b56\u7565KL\u6563\u5ea6\uff0c\u6709\u6548\u9632\u6b62\u4e86RL\u8bad\u7ec3\u5d29\u6e83\u3002", "motivation": "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u4e2d\u7684\u8def\u7531\u673a\u5236\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u5f15\u5165\u4e0d\u7a33\u5b9a\u6027\uff0c\u751a\u81f3\u5bfc\u81f4\u707e\u96be\u6027\u7684\u8bad\u7ec3\u5d29\u6e83\u3002\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5b58\u5728\u663e\u8457\u7684\u8def\u7531\u884c\u4e3a\u4e0d\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\uff0c\u8def\u7531\u6846\u67b6\u4e5f\u4f1a\u5728\u91cd\u590d\u524d\u5411\u4f20\u64ad\u4e2d\u4ea7\u751f\u4e0d\u540c\u7684\u4e13\u5bb6\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u4e86Rollout Routing Replay (R3)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8bb0\u5f55\u63a8\u7406\u5f15\u64ce\u4e2d\u7684\u8def\u7531\u5206\u5e03\u5e76\u5728\u8bad\u7ec3\u9636\u6bb5\u8fdb\u884c\u91cd\u653e\u3002R3\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3-\u63a8\u7406\u7b56\u7565KL\u6563\u5ea6\uff0c\u7f13\u89e3\u4e86\u6781\u7aef\u5dee\u5f02\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u8bad\u7ec3\u901f\u5ea6\u3002", "result": "\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9e\uff0cR3\u6210\u529f\u7a33\u5b9a\u4e86RL\u8bad\u7ec3\uff0c\u9632\u6b62\u4e86\u5d29\u6e83\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86GSPO\u548cTIS\u7b49\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8def\u7531\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7a33\u5b9a\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u786e\u4fdd\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u8def\u7531\u4e00\u81f4\u6027\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86\u8def\u7531\u673a\u5236\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3aMoE\u6a21\u578b\u7684RL\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.10546", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10546", "abs": "https://arxiv.org/abs/2510.10546", "authors": ["Zuha Fatima", "Muhammad Anser Sohaib", "Muhammad Talha", "Sidra Sultana", "Ayesha Kanwal", "Nazia Perwaiz"], "title": "GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction", "comment": null, "summary": "Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high\nmountain regions, yet predictive research is hindered by fragmented and\nunimodal data. Most prior efforts emphasize post-event mapping, whereas\nforecasting requires harmonized datasets that combine visual indicators with\nphysical precursors. We present GLOFNet, a multimodal dataset for GLOF\nmonitoring and prediction, focused on the Shisper Glacier in the Karakoram. It\nintegrates three complementary sources: Sentinel-2 multispectral imagery for\nspatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and\nMODIS Land Surface Temperature records spanning over two decades. Preprocessing\nincluded cloud masking, quality filtering, normalization, temporal\ninterpolation, augmentation, and cyclical encoding, followed by harmonization\nacross modalities. Exploratory analysis reveals seasonal glacier velocity\ncycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in\ncryospheric conditions. The resulting dataset, GLOFNet, is publicly available\nto support future research in glacial hazard prediction. By addressing\nchallenges such as class imbalance, cloud contamination, and coarse resolution,\nGLOFNet provides a structured foundation for benchmarking multimodal deep\nlearning approaches to rare hazard prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GLOFNet\uff0c\u4e00\u4e2a\u7528\u4e8e\u51b0\u5ddd\u6e56\u6e83\u51b3\u6d2a\u6c34\u76d1\u6d4b\u548c\u9884\u6d4b\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u591a\u5149\u8c31\u5f71\u50cf\u3001\u51b0\u5ddd\u8fd0\u52a8\u901f\u5ea6\u548c\u5730\u8868\u6e29\u5ea6\u6570\u636e\uff0c\u4e3a\u7f55\u89c1\u707e\u5bb3\u9884\u6d4b\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u51c6\u3002", "motivation": "\u51b0\u5ddd\u6e56\u6e83\u51b3\u6d2a\u6c34\u662f\u9ad8\u5c71\u5730\u533a\u7f55\u89c1\u4f46\u5177\u6709\u7834\u574f\u6027\u7684\u707e\u5bb3\uff0c\u73b0\u6709\u7814\u7a76\u53d7\u9650\u4e8e\u788e\u7247\u5316\u548c\u5355\u6a21\u6001\u6570\u636e\uff0c\u4e3b\u8981\u5173\u6ce8\u4e8b\u540e\u5236\u56fe\uff0c\u800c\u9884\u6d4b\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u6307\u6807\u4e0e\u7269\u7406\u524d\u5146\u7684\u534f\u8c03\u6570\u636e\u96c6\u3002", "method": "\u7814\u7a76\u6574\u5408\u4e86\u4e09\u79cd\u4e92\u8865\u6570\u636e\u6e90\uff1aSentinel-2\u591a\u5149\u8c31\u5f71\u50cf\u7528\u4e8e\u7a7a\u95f4\u76d1\u6d4b\u3001NASA ITS_LIVE\u901f\u5ea6\u4ea7\u54c1\u7528\u4e8e\u51b0\u5ddd\u8fd0\u52a8\u5b66\u3001MODIS\u5730\u8868\u6e29\u5ea6\u8bb0\u5f55\u7528\u4e8e\u957f\u671f\u8d8b\u52bf\u5206\u6790\uff0c\u5e76\u8fdb\u884c\u4e86\u4e91\u63a9\u819c\u3001\u8d28\u91cf\u8fc7\u6ee4\u3001\u5f52\u4e00\u5316\u3001\u65f6\u95f4\u63d2\u503c\u3001\u6570\u636e\u589e\u5f3a\u548c\u5faa\u73af\u7f16\u7801\u7b49\u9884\u5904\u7406\u3002", "result": "\u63a2\u7d22\u6027\u5206\u6790\u63ed\u793a\u4e86\u51b0\u5ddd\u901f\u5ea6\u7684\u5b63\u8282\u6027\u5468\u671f\u3001\u6bcf\u5341\u5e74\u7ea60.8K\u7684\u957f\u671f\u5347\u6e29\u8d8b\u52bf\u4ee5\u53ca\u51b0\u51bb\u5708\u6761\u4ef6\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\uff0c\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\u4ee5\u652f\u6301\u672a\u6765\u51b0\u5ddd\u707e\u5bb3\u9884\u6d4b\u7814\u7a76\u3002", "conclusion": "GLOFNet\u901a\u8fc7\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u4e91\u6c61\u67d3\u548c\u7c97\u5206\u8fa8\u7387\u7b49\u6311\u6218\uff0c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u7f55\u89c1\u707e\u5bb3\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u51b0\u5ddd\u707e\u5bb3\u9884\u6d4b\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.11604", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11604", "abs": "https://arxiv.org/abs/2510.11604", "authors": ["Sanjula De Alwis", "Indrajith Ekanayake"], "title": "Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce", "comment": null, "summary": "In online retail, customer acquisition typically incurs higher costs than\ncustomer retention, motivating firms to invest in churn analytics. However,\nmany contemporary churn models operate as opaque black boxes, limiting insight\ninto the determinants of attrition, the timing of retention opportunities, and\nthe identification of high-risk customer segments. Accordingly, the emphasis\nshould shift from prediction alone to the design of personalized retention\nstrategies grounded in interpretable evidence. This study advances a\nthree-component framework that integrates explainable AI to quantify feature\ncontributions, survival analysis to model time-to-event churn risk, and RFM\nprofiling to segment customers by transactional behaviour. In combination,\nthese methods enable the attribution of churn drivers, estimation of\nintervention windows, and prioritization of segments for targeted actions,\nthereby supporting strategies that reduce attrition and strengthen customer\nloyalty.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u7ec4\u4ef6\u6846\u67b6\uff0c\u5c06\u53ef\u89e3\u91caAI\u3001\u751f\u5b58\u5206\u6790\u548cRFM\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5ba2\u6237\u6d41\u5931\u9884\u6d4b\u548c\u4fdd\u7559\u7b56\u7565\u5236\u5b9a\u3002\u8be5\u6846\u67b6\u80fd\u591f\u91cf\u5316\u7279\u5f81\u8d21\u732e\u3001\u5efa\u6a21\u6d41\u5931\u98ce\u9669\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u8bc6\u522b\u9ad8\u4ef7\u503c\u5ba2\u6237\u7ec6\u5206\uff0c\u4ece\u800c\u652f\u6301\u4e2a\u6027\u5316\u4fdd\u7559\u5e72\u9884\u3002", "motivation": "\u5f53\u524d\u5ba2\u6237\u6d41\u5931\u6a21\u578b\u591a\u4e3a\u9ed1\u76d2\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u4f01\u4e1a\u5bf9\u6d41\u5931\u9a71\u52a8\u56e0\u7d20\u3001\u5e72\u9884\u65f6\u673a\u548c\u9ad8\u98ce\u9669\u5ba2\u6237\u7ec6\u5206\u7684\u6df1\u5165\u7406\u89e3\u3002\u7814\u7a76\u65e8\u5728\u4ece\u5355\u7eaf\u9884\u6d4b\u8f6c\u5411\u57fa\u4e8e\u53ef\u89e3\u91ca\u8bc1\u636e\u7684\u4e2a\u6027\u5316\u4fdd\u7559\u7b56\u7565\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u6d1e\u5bdf\u529b\u548c\u53ef\u64cd\u4f5c\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u7ec4\u4ef6\u6846\u67b6\uff1a\u4f7f\u7528\u53ef\u89e3\u91caAI\u91cf\u5316\u7279\u5f81\u5bf9\u6d41\u5931\u7684\u8d21\u732e\u5ea6\uff0c\u5e94\u7528\u751f\u5b58\u5206\u6790\u5efa\u6a21\u65f6\u95f4\u5230\u4e8b\u4ef6\u7684\u6d41\u5931\u98ce\u9669\u52a8\u6001\uff0c\u7ed3\u5408RFM\u5206\u6790\u6839\u636e\u4ea4\u6613\u884c\u4e3a\u5bf9\u5ba2\u6237\u8fdb\u884c\u7ec6\u5206\u3002\u8fd9\u4e9b\u65b9\u6cd5\u534f\u540c\u5de5\u4f5c\u5b9e\u73b0\u6d41\u5931\u9a71\u52a8\u56e0\u7d20\u5f52\u56e0\u3001\u5e72\u9884\u7a97\u53e3\u4f30\u8ba1\u548c\u76ee\u6807\u7ec6\u5206\u4f18\u5148\u7ea7\u6392\u5e8f\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u6d41\u5931\u9a71\u52a8\u56e0\u7d20\u7684\u53ef\u89e3\u91ca\u5f52\u56e0\uff0c\u51c6\u786e\u4f30\u8ba1\u4fdd\u7559\u5e72\u9884\u7684\u6700\u4f73\u65f6\u95f4\u7a97\u53e3\uff0c\u5e76\u6709\u6548\u8bc6\u522b\u9700\u8981\u4f18\u5148\u5173\u6ce8\u7684\u9ad8\u98ce\u9669\u5ba2\u6237\u7ec6\u5206\u3002\u7efc\u5408\u65b9\u6cd5\u652f\u6301\u5236\u5b9a\u51cf\u5c11\u5ba2\u6237\u6d41\u5931\u548c\u589e\u5f3a\u5ba2\u6237\u5fe0\u8bda\u5ea6\u7684\u9488\u5bf9\u6027\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u3001\u65f6\u95f4\u52a8\u6001\u5206\u6790\u548c\u884c\u4e3a\u7ec6\u5206\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5ba2\u6237\u6d41\u5931\u7ba1\u7406\u7684\u6218\u7565\u4ef7\u503c\u3002\u8be5\u6846\u67b6\u4e3a\u4ece\u9884\u6d4b\u6027\u5206\u6790\u8f6c\u5411\u53ef\u64cd\u4f5c\u7684\u4fdd\u7559\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u5f3a\u8c03\u4e86\u89e3\u91ca\u6027\u5728\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.11618", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11618", "abs": "https://arxiv.org/abs/2510.11618", "authors": ["Zehao Chen", "Rong Pan", "Haoran Li"], "title": "StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models", "comment": "Project: https://storyboxproject.github.io", "summary": "Human writers often begin their stories with an overarching mental scene,\nwhere they envision the interactions between characters and their environment.\nInspired by this creative process, we propose a novel approach to long-form\nstory generation, termed hybrid bottom-up long-form story generation, using\nmulti-agent simulations. In our method, agents interact within a dynamic\nsandbox environment, where their behaviors and interactions with one another\nand the environment generate emergent events. These events form the foundation\nfor the story, enabling organic character development and plot progression.\nUnlike traditional top-down approaches that impose rigid structures, our hybrid\nbottom-up approach allows for the natural unfolding of events, fostering more\nspontaneous and engaging storytelling. The system is capable of generating\nstories exceeding 10,000 words while maintaining coherence and consistency,\naddressing some of the key challenges faced by current story generation models.\nWe achieve state-of-the-art performance across several metrics. This approach\noffers a scalable and innovative solution for creating dynamic, immersive\nlong-form stories that evolve organically from agent-driven interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u81ea\u5e95\u5411\u4e0a\u7684\u957f\u6587\u672c\u6545\u4e8b\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6a21\u62df\u5b9e\u73b0\u6709\u673a\u7684\u6545\u4e8b\u6f14\u8fdb\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u667a\u80fd\u4f53\u5728\u52a8\u6001\u6c99\u76d2\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u4ea7\u751f\u6d8c\u73b0\u4e8b\u4ef6\uff0c\u80fd\u591f\u751f\u6210\u8d85\u8fc710,000\u5b57\u7684\u957f\u7bc7\u6545\u4e8b\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u81ea\u4e0a\u800c\u4e0b\u7684\u6545\u4e8b\u751f\u6210\u65b9\u6cd5\u5f3a\u52a0\u521a\u6027\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u6545\u4e8b\u7684\u6709\u673a\u53d1\u5c55\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u6545\u4e8b\u751f\u6210\u6a21\u578b\u5728\u957f\u7bc7\u53d9\u4e8b\u4e2d\u9762\u4e34\u7684\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027\u6311\u6218\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u4f5c\u5bb6\u4ece\u6574\u4f53\u5fc3\u7406\u573a\u666f\u51fa\u53d1\u7684\u521b\u4f5c\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u6545\u4e8b\u6f14\u8fdb\u3002", "method": "\u91c7\u7528\u6df7\u5408\u81ea\u5e95\u5411\u4e0a\u7684\u957f\u6587\u672c\u6545\u4e8b\u751f\u6210\u6846\u67b6\uff0c\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6280\u672f\u3002\u667a\u80fd\u4f53\u5728\u52a8\u6001\u6c99\u76d2\u73af\u5883\u4e2d\u8fdb\u884c\u4ea4\u4e92\uff0c\u5176\u884c\u4e3a\u548c\u4e0e\u73af\u5883\u53ca\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u4e92\u52a8\u4ea7\u751f\u6d8c\u73b0\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u6784\u6210\u6545\u4e8b\u7684\u57fa\u7840\uff0c\u652f\u6301\u6709\u673a\u7684\u89d2\u8272\u53d1\u5c55\u548c\u60c5\u8282\u63a8\u8fdb\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u8d85\u8fc710,000\u5b57\u7684\u957f\u7bc7\u6545\u4e8b\uff0c\u540c\u65f6\u4fdd\u6301\u6545\u4e8b\u7684\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027\u3002\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u957f\u6587\u672c\u6545\u4e8b\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6df7\u5408\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u6cd5\u4e3a\u521b\u5efa\u52a8\u6001\u3001\u6c89\u6d78\u5f0f\u7684\u957f\u7bc7\u6545\u4e8b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u4ea4\u4e92\u5b9e\u73b0\u6545\u4e8b\u7684\u81ea\u7136\u6f14\u8fdb\uff0c\u4e3a\u6545\u4e8b\u751f\u6210\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u6a21\u62df\u5728\u521b\u9020\u6027\u5199\u4f5c\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.10577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10577", "abs": "https://arxiv.org/abs/2510.10577", "authors": ["Haonan Wang", "Hanyu Zhou", "Haoyue Liu", "Luxin Yan"], "title": "Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes", "comment": null, "summary": "Optical flow estimation has achieved promising results in conventional scenes\nbut faces challenges in high-speed and low-light scenes, which suffer from\nmotion blur and insufficient illumination. These conditions lead to weakened\ntexture and amplified noise and deteriorate the appearance saturation and\nboundary completeness of frame cameras, which are necessary for motion feature\nmatching. In degraded scenes, the frame camera provides dense appearance\nsaturation but sparse boundary completeness due to its long imaging time and\nlow dynamic range. In contrast, the event camera offers sparse appearance\nsaturation, while its short imaging time and high dynamic range gives rise to\ndense boundary completeness. Traditionally, existing methods utilize feature\nfusion or domain adaptation to introduce event to improve boundary\ncompleteness. However, the appearance features are still deteriorated, which\nseverely affects the mostly adopted discriminative models that learn the\nmapping from visual features to motion fields and generative models that\ngenerate motion fields based on given visual features. So we introduce\ndiffusion models that learn the mapping from noising flow to clear flow, which\nis not affected by the deteriorated visual features. Therefore, we propose a\nnovel optical flow estimation framework Diff-ABFlow based on diffusion models\nwith frame-event appearance-boundary fusion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5e27-\u4e8b\u4ef6\u5916\u89c2-\u8fb9\u754c\u878d\u5408\u5149\u6d41\u4f30\u8ba1\u6846\u67b6Diff-ABFlow\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4ece\u566a\u58f0\u6d41\u5230\u6e05\u6670\u6d41\u7684\u6620\u5c04\uff0c\u89e3\u51b3\u4e86\u9ad8\u901f\u548c\u4f4e\u5149\u573a\u666f\u4e0b\u4f20\u7edf\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\u56e0\u8fd0\u52a8\u6a21\u7cca\u548c\u5149\u7167\u4e0d\u8db3\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\u5728\u9ad8\u901f\u548c\u4f4e\u5149\u573a\u666f\u4e0b\u9762\u4e34\u8fd0\u52a8\u6a21\u7cca\u548c\u5149\u7167\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u7eb9\u7406\u51cf\u5f31\u3001\u566a\u58f0\u653e\u5927\uff0c\u5e76\u5f71\u54cd\u5e27\u76f8\u673a\u7684\u5916\u89c2\u9971\u548c\u5ea6\u548c\u8fb9\u754c\u5b8c\u6574\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u7279\u5f81\u878d\u5408\u6216\u57df\u9002\u5e94\u5f15\u5165\u4e8b\u4ef6\u76f8\u673a\u6765\u6539\u5584\u8fb9\u754c\u5b8c\u6574\u6027\uff0c\u4f46\u5916\u89c2\u7279\u5f81\u4ecd\u7136\u9000\u5316\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u57fa\u4e8e\u89c6\u89c9\u7279\u5f81\u5230\u8fd0\u52a8\u573a\u6620\u5c04\u7684\u5224\u522b\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faDiff-ABFlow\u6846\u67b6\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u6784\u5efa\u5e27-\u4e8b\u4ef6\u5916\u89c2-\u8fb9\u754c\u878d\u5408\u7684\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u5229\u7528\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4ece\u566a\u58f0\u6d41\u5230\u6e05\u6670\u6d41\u7684\u6620\u5c04\u8fc7\u7a0b\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u9000\u5316\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u7ed3\u5408\u5e27\u76f8\u673a\u63d0\u4f9b\u5bc6\u96c6\u5916\u89c2\u9971\u548c\u5ea6\u548c\u4e8b\u4ef6\u76f8\u673a\u63d0\u4f9b\u5bc6\u96c6\u8fb9\u754c\u5b8c\u6574\u6027\u7684\u4e92\u8865\u4f18\u52bf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9ad8\u901f\u548c\u4f4e\u5149\u9000\u5316\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5149\u6d41\u4f30\u8ba1\u6027\u80fd\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u8fd0\u52a8\u573a\u4f30\u8ba1\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6269\u6563\u6a21\u578b\u4e3a\u5149\u6d41\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4e8e\u9000\u5316\u89c6\u89c9\u7279\u5f81\u7684\u65b0\u8303\u5f0f\uff0c\u5e27-\u4e8b\u4ef6\u4f20\u611f\u5668\u7684\u4e92\u8865\u878d\u5408\u7b56\u7565\u4e3a\u89e3\u51b3\u6781\u7aef\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u8fd0\u52a8\u4f30\u8ba1\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.10653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10653", "abs": "https://arxiv.org/abs/2510.10653", "authors": ["Sebastian Schmidt", "Julius K\u00f6rner", "Stephan G\u00fcnnemann"], "title": "A Machine Learning Perspective on Automated Driving Corner Cases", "comment": null, "summary": "For high-stakes applications, like autonomous driving, a safe operation is\nnecessary to prevent harm, accidents, and failures. Traditionally, difficult\nscenarios have been categorized into corner cases and addressed individually.\nHowever, this example-based categorization is not scalable and lacks a data\ncoverage perspective, neglecting the generalization to training data of machine\nlearning models. In our work, we propose a novel machine learning approach that\ntakes the underlying data distribution into account. Based on our novel\nperspective, we present a framework for effective corner case recognition for\nperception on individual samples. In our evaluation, we show that our approach\n(i) unifies existing scenario-based corner case taxonomies under a\ndistributional perspective, (ii) achieves strong performance on corner case\ndetection tasks across standard benchmarks for which we extend established\nout-of-distribution detection benchmarks, and (iii) enables analysis of\ncombined corner cases via a newly introduced fog-augmented Lost & Found\ndataset. These results provide a principled basis for corner case recognition,\nunderlining our manual specification-free definition.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u5206\u5e03\u89c6\u89d2\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u89d2\u70b9\u6848\u4f8b\u8bc6\u522b\u3002\u8be5\u65b9\u6cd5\u7edf\u4e00\u4e86\u73b0\u6709\u573a\u666f\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u89d2\u70b9\u6848\u4f8b\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u793a\u4f8b\u5206\u7c7b\u5904\u7406\u89d2\u70b9\u6848\u4f8b\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u4e14\u5ffd\u89c6\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6570\u636e\u8986\u76d6\u89c6\u89d2\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u5e95\u5c42\u6570\u636e\u5206\u5e03\u7684\u65b0\u578b\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u5206\u5e03\u89c6\u89d2\u7684\u89d2\u70b9\u6848\u4f8b\u8bc6\u522b\u6846\u67b6\uff0c\u80fd\u591f\u5bf9\u5355\u4e2a\u6837\u672c\u8fdb\u884c\u6709\u6548\u7684\u611f\u77e5\u8bc6\u522b\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u7edf\u4e00\u4e86\u73b0\u6709\u57fa\u4e8e\u573a\u666f\u7684\u89d2\u70b9\u6848\u4f8b\u5206\u7c7b\u6cd5\uff0c\u5728\u6269\u5c55\u7684\u6807\u51c6\u5206\u5e03\u5916\u68c0\u6d4b\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u89d2\u70b9\u6848\u4f8b\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u65b0\u5f15\u5165\u7684\u96fe\u589e\u5f3aLost & Found\u6570\u636e\u96c6\u652f\u6301\u7ec4\u5408\u89d2\u70b9\u6848\u4f8b\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u4e3a\u89d2\u70b9\u6848\u4f8b\u8bc6\u522b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u65e0\u9700\u4eba\u5de5\u89c4\u8303\u7684\u5b9a\u4e49\u65b9\u6cd5\uff0c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u7684\u5b89\u5168\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5206\u5e03\u89c6\u89d2\u7684\u7406\u8bba\u652f\u6491\u548c\u5b9e\u8df5\u6846\u67b6\u3002"}}
{"id": "2510.10750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10750", "abs": "https://arxiv.org/abs/2510.10750", "authors": ["Laura Weihl", "Nejc Novak", "Stefan H. Bengtson", "Malte Pedersen"], "title": "Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection", "comment": null, "summary": "Underwater video monitoring is a promising strategy for assessing marine\nbiodiversity, but the vast volume of uneventful footage makes manual inspection\nhighly impractical. In this work, we explore the use of visual anomaly\ndetection (VAD) based on deep neural networks to automatically identify\ninteresting or anomalous events. We introduce AURA, the first multi-annotator\nbenchmark dataset for underwater VAD, and evaluate four VAD models across two\nmarine scenes. We demonstrate the importance of robust frame selection\nstrategies to extract meaningful video segments. Our comparison against\nmultiple annotators reveals that VAD performance of current models varies\ndramatically and is highly sensitive to both the amount of training data and\nthe variability in visual content that defines \"normal\" scenes. Our results\nhighlight the value of soft and consensus labels and offer a practical approach\nfor supporting scientific exploration and scalable biodiversity monitoring.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86AURA\u2014\u2014\u9996\u4e2a\u591a\u6807\u6ce8\u8005\u6c34\u4e0b\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u56db\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u4e24\u79cd\u6d77\u6d0b\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u6027\u80fd\u5bf9\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u89c6\u89c9\u5185\u5bb9\u53d8\u5f02\u6027\u7684\u9ad8\u5ea6\u654f\u611f\u6027\u3002", "motivation": "\u6c34\u4e0b\u89c6\u9891\u76d1\u6d4b\u662f\u8bc4\u4f30\u6d77\u6d0b\u751f\u7269\u591a\u6837\u6027\u7684\u6709\u6548\u7b56\u7565\uff0c\u4f46\u6d77\u91cf\u65e0\u4e8b\u4ef6\u89c6\u9891\u4f7f\u5f97\u4eba\u5de5\u68c0\u67e5\u6781\u4e0d\u5b9e\u7528\uff0c\u9700\u8981\u81ea\u52a8\u8bc6\u522b\u6709\u8da3\u6216\u5f02\u5e38\u4e8b\u4ef6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6c34\u4e0b\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u9996\u4e2a\u591a\u6807\u6ce8\u8005\u57fa\u51c6\u6570\u636e\u96c6AURA\uff0c\u5e76\u8bc4\u4f30\u4e86\u56db\u79cdVAD\u6a21\u578b\u5728\u4e24\u79cd\u6d77\u6d0b\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u9c81\u68d2\u5e27\u9009\u62e9\u7b56\u7565\u5bf9\u63d0\u53d6\u6709\u610f\u4e49\u89c6\u9891\u7247\u6bb5\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5f53\u524dVAD\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5bf9\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u5b9a\u4e49\"\u6b63\u5e38\"\u573a\u666f\u7684\u89c6\u89c9\u5185\u5bb9\u53d8\u5f02\u6027\u9ad8\u5ea6\u654f\u611f\uff0c\u4e0e\u591a\u6807\u6ce8\u8005\u7684\u6bd4\u8f83\u63ed\u793a\u4e86\u8f6f\u6807\u7b7e\u548c\u5171\u8bc6\u6807\u7b7e\u7684\u4ef7\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u652f\u6301\u79d1\u5b66\u63a2\u7d22\u548c\u53ef\u6269\u5c55\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u591a\u6807\u6ce8\u8005\u57fa\u51c6\u548c\u9c81\u68d2\u5e27\u9009\u62e9\u7b56\u7565\u5728\u6c34\u4e0b\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.11232", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11232", "abs": "https://arxiv.org/abs/2510.11232", "authors": ["Neilansh Chauhan", "Piyush Kumar Gupta", "Faraz Doja"], "title": "LightPneumoNet: Lightweight Pneumonia Classifier", "comment": "13 pages (including references), 5 figures", "summary": "Effective pneumonia diagnosis is often challenged by the difficulty of\ndeploying large, computationally expensive deep learning models in\nresource-limited settings. This study introduces LightPneumoNet, an efficient,\nlightweight convolutional neural network (CNN) built from scratch to provide an\naccessible and accurate diagnostic solution for pneumonia detection from chest\nX-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.\nPreprocessing included image resizing to 224x224, grayscale conversion, and\npixel normalization, with data augmentation (rotation, zoom, shear) to prevent\noverfitting. The custom architecture features four blocks of stacked\nconvolutional layers and contains only 388,082 trainable parameters, resulting\nin a minimal 1.48 MB memory footprint. On the independent test set, our model\ndelivered exceptional performance, achieving an overall accuracy of 0.942,\nprecision of 0.92, and an F1-Score of 0.96. Critically, it obtained a\nsensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify\ntrue pneumonia cases and minimize clinically significant false negatives.\nNotably, LightPneumoNet achieves this high recall on the same dataset where\nexisting approaches typically require significantly heavier architectures or\nfail to reach comparable sensitivity levels. The model's efficiency enables\ndeployment on low-cost hardware, making advanced computer-aided diagnosis\naccessible in underserved clinics and serving as a reliable second-opinion tool\nto improve patient outcomes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86LightPneumoNet\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4e13\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u80ba\u708eX\u5149\u8bca\u65ad\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u80ba\u708e\u8bca\u65ad\u9762\u4e34\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u56f0\u96be\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u80fd\u529b\u548c\u5b58\u50a8\u7a7a\u95f4\u6709\u9650\u7684\u533b\u7597\u573a\u666f\u4e0b\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4ece\u5934\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\uff0c\u5305\u542b\u56db\u4e2a\u5377\u79ef\u5757\u5806\u53e0\uff0c\u4ec5388,082\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u9884\u5904\u7406\u5305\u62ec\u56fe\u50cf\u7f29\u653e\u81f3224x224\u3001\u7070\u5ea6\u8f6c\u6362\u548c\u50cf\u7d20\u5f52\u4e00\u5316\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u9632\u6b62\u8fc7\u62df\u5408\u3002", "result": "\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e860.942\u7684\u603b\u4f53\u51c6\u786e\u7387\u30010.92\u7684\u7cbe\u786e\u7387\u548c0.96\u7684F1\u5206\u6570\uff0c\u7279\u522b\u662f\u8fbe\u5230\u4e860.99\u7684\u654f\u611f\u5ea6\uff0c\u51e0\u4e4e\u5b8c\u7f8e\u8bc6\u522b\u771f\u5b9e\u80ba\u708e\u75c5\u4f8b\u5e76\u6700\u5c0f\u5316\u4e34\u5e8a\u91cd\u8981\u7684\u5047\u9634\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u7684\u9ad8\u6548\u6027\u4f7f\u5176\u80fd\u591f\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u90e8\u7f72\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u8bca\u6240\u63d0\u4f9b\u5148\u8fdb\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\uff0c\u4f5c\u4e3a\u53ef\u9760\u7684\u7b2c\u4e8c\u610f\u89c1\u5de5\u5177\u6539\u5584\u60a3\u8005\u9884\u540e\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u67b6\u6784\u5728\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.11005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11005", "abs": "https://arxiv.org/abs/2510.11005", "authors": ["Kai Han", "Siqi Ma", "Chengxuan Qian", "Jun Chen", "Chongwen Lyu", "Yuqing Song", "Zhe Liu"], "title": "Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation", "comment": null, "summary": "Accurate segmentation of tumors and adjacent normal tissues in medical images\nis essential for surgical planning and tumor staging. Although foundation\nmodels generally perform well in segmentation tasks, they often struggle to\nfocus on foreground areas in complex, low-contrast backgrounds, where some\nmalignant tumors closely resemble normal organs, complicating contextual\ndifferentiation. To address these challenges, we propose the Foreground-Aware\nSpectrum Segmentation (FASS) framework. First, we introduce a foreground-aware\nmodule to amplify the distinction between background and the entire volume\nspace, allowing the model to concentrate more effectively on target areas.\nNext, a feature-level frequency enhancement module, based on wavelet transform,\nextracts discriminative high-frequency features to enhance boundary recognition\nand detail perception. Eventually, we introduce an edge constraint module to\npreserve geometric continuity in segmentation boundaries. Extensive experiments\non multiple medical datasets demonstrate superior performance across all\nmetrics, validating the effectiveness of our framework, particularly in\nrobustness under complex conditions and fine structure recognition. Our\nframework significantly enhances segmentation of low-contrast images, paving\nthe way for applications in more diverse and complex medical imaging scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u524d\u666f\u611f\u77e5\u9891\u8c31\u5206\u5272\uff08FASS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u666f\u611f\u77e5\u6a21\u5757\u3001\u5c0f\u6ce2\u53d8\u6362\u7279\u5f81\u589e\u5f3a\u548c\u8fb9\u7f18\u7ea6\u675f\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u4e2d\u4f4e\u5bf9\u6bd4\u5ea6\u80bf\u7624\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4f4e\u5bf9\u6bd4\u5ea6\u80cc\u666f\u4e0b\u96be\u4ee5\u805a\u7126\u524d\u666f\u533a\u57df\uff0c\u7279\u522b\u662f\u5f53\u6076\u6027\u80bf\u7624\u4e0e\u6b63\u5e38\u5668\u5b98\u5f62\u6001\u76f8\u4f3c\u65f6\uff0c\u4e0a\u4e0b\u6587\u533a\u5206\u53d8\u5f97\u56f0\u96be\uff0c\u8fd9\u9650\u5236\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u624b\u672f\u89c4\u5212\u548c\u80bf\u7624\u5206\u671f\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faFASS\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u524d\u666f\u611f\u77e5\u6a21\u5757\u589e\u5f3a\u80cc\u666f\u4e0e\u6574\u4f53\u4f53\u79ef\u7a7a\u95f4\u7684\u533a\u5206\u5ea6\uff1b\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u7279\u5f81\u7ea7\u9891\u7387\u589e\u5f3a\u6a21\u5757\u63d0\u53d6\u5224\u522b\u6027\u9ad8\u9891\u7279\u5f81\u4ee5\u6539\u5584\u8fb9\u754c\u8bc6\u522b\uff1b\u8fb9\u7f18\u7ea6\u675f\u6a21\u5757\u4fdd\u6301\u5206\u5272\u8fb9\u754c\u7684\u51e0\u4f55\u8fde\u7eed\u6027\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u7ec6\u7ed3\u6784\u8bc6\u522b\u65b9\u9762\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5bf9\u6bd4\u5ea6\u56fe\u50cf\u7684\u5206\u5272\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u4f4e\u5bf9\u6bd4\u5ea6\u533b\u5b66\u56fe\u50cf\u7684\u5206\u5272\u80fd\u529b\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u548c\u590d\u6742\u7684\u533b\u5b66\u6210\u50cf\u573a\u666f\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5728\u624b\u672f\u89c4\u5212\u548c\u80bf\u7624\u5206\u671f\u7b49\u4e34\u5e8a\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.11063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11063", "abs": "https://arxiv.org/abs/2510.11063", "authors": ["Chang Liu", "Henghui Ding", "Kaining Ying", "Lingyi Hong", "Ning Xu", "Linjie Yang", "Yuchen Fan", "Mingqi Gao", "Jingkun Chen", "Yunqi Miao", "Gengshen Wu", "Zhijin Qin", "Jungong Han", "Zhixiong Zhang", "Shuangrui Ding", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Chang Soo Lim", "Joonyoung Moon", "Donghyeon Cho", "Tingmin Li", "Yixuan Li", "Yang Yang", "An Yan", "Leilei Cao", "Feng Lu", "Ran Hong", "Youhai Jiang", "Fengjie Zhu", "Yujie Xie", "Hongyang Zhang", "Zhihui Liu", "Shihai Ruan", "Quanzhu Niu", "Dengxian Gong", "Shihao Chen", "Tao Zhang", "Yikang Zhou", "Haobo Yuan", "Lu Qi", "Xiangtai Li", "Shunping Ji", "Ran Hong", "Feng Lu", "Leilei Cao", "An Yan", "Alexey Nekrasov", "Ali Athar", "Daan de Geus", "Alexander Hermans", "Bastian Leibe"], "title": "LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation", "comment": "16 pages, 9 figures", "summary": "This report presents an overview of the 7th Large-scale Video Object\nSegmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the\ntwo traditional tracks of LSVOS that jointly target robustness in realistic\nvideo scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition\nfeatures a newly introduced track, Complex VOS (MOSEv2). Building upon prior\ninsights, MOSEv2 substantially increases difficulty, introducing more\nchallenging but realistic scenarios including denser small objects, frequent\ndisappear/reappear events, severe occlusions, adverse weather and lighting,\netc., pushing long-term consistency and generalization beyond curated\nbenchmarks. The challenge retains standard ${J}$, $F$, and ${J\\&F}$ metrics for\nVOS and RVOS, while MOSEv2 adopts ${J\\&\\dot{F}}$ as the primary ranking metric\nto better evaluate objects across scales and disappearance cases. We summarize\ndatasets and protocols, highlight top-performing solutions, and distill\nemerging trends, such as the growing role of LLM/MLLM components and\nmemory-aware propagation, aiming to chart future directions for resilient,\nlanguage-aware video segmentation in the wild.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ICCV 2025\u7b2c\u4e03\u5c4a\u5927\u89c4\u6a21\u89c6\u9891\u76ee\u6807\u5206\u5272\u6311\u6218\u8d5b\uff0c\u5728\u4f20\u7edfVOS\u548cRVOS\u8d5b\u9053\u57fa\u7840\u4e0a\u65b0\u589e\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684MOSEv2\u8d5b\u9053\uff0c\u901a\u8fc7\u5f15\u5165\u66f4\u590d\u6742\u7684\u771f\u5b9e\u573a\u666f\u63a8\u52a8\u957f\u671f\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u53d1\u5c55\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u89c6\u9891\u76ee\u6807\u5206\u5272\u5728\u771f\u5b9e\u590d\u6742\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5bc6\u96c6\u5c0f\u76ee\u6807\u3001\u9891\u7e41\u6d88\u5931\u91cd\u73b0\u3001\u4e25\u91cd\u906e\u6321\u3001\u6076\u52a3\u5929\u6c14\u548c\u5149\u7167\u7b49\u66f4\u5177\u6311\u6218\u6027\u4f46\u66f4\u73b0\u5b9e\u7684\u573a\u666f\uff0c\u63a8\u52a8\u6a21\u578b\u5728\u975e\u7cbe\u5fc3\u7b56\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e4b\u5916\u7684\u957f\u671f\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6311\u6218\u8d5b\u4fdd\u7559\u4e86VOS\u548cRVOS\u4e24\u4e2a\u4f20\u7edf\u8d5b\u9053\uff0c\u5e76\u65b0\u589e\u4e86MOSEv2\u8d5b\u9053\uff0c\u8be5\u8d5b\u9053\u91c7\u7528J&F\u4f5c\u4e3a\u4e3b\u8981\u6392\u540d\u6307\u6807\u4ee5\u66f4\u597d\u5730\u8bc4\u4f30\u8de8\u5c3a\u5ea6\u548c\u6d88\u5931\u60c5\u51b5\u4e0b\u7684\u76ee\u6807\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86LLM/MLLM\u7ec4\u4ef6\u548c\u5185\u5b58\u611f\u77e5\u4f20\u64ad\u7b49\u65b0\u5174\u6280\u672f\u8d8b\u52bf\u3002", "result": "\u6311\u6218\u8d5b\u91c7\u7528\u4e86\u6807\u51c6J\u3001F\u548cJ&F\u6307\u6807\u8bc4\u4f30VOS\u548cRVOS\u8d5b\u9053\u6027\u80fd\uff0c\u800cMOSEv2\u8d5b\u9053\u91c7\u7528J&F\u4f5c\u4e3a\u4e3b\u8981\u6392\u540d\u6307\u6807\uff0c\u901a\u8fc7\u603b\u7ed3\u9876\u7ea7\u89e3\u51b3\u65b9\u6848\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e0b\u89c6\u9891\u5206\u5272\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9c81\u68d2\u3001\u8bed\u8a00\u611f\u77e5\u7684\u89c6\u9891\u5206\u5272\u6280\u672f\u6307\u660e\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5728\u771f\u5b9e\u590d\u6742\u573a\u666f\u4e2d\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u8bc4\u4f30\u957f\u671f\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u65b0\u57fa\u51c6\u548c\u534f\u8bae\u3002"}}
{"id": "2510.11302", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11302", "abs": "https://arxiv.org/abs/2510.11302", "authors": ["Samer Al-Hamadani"], "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models", "comment": "23 pages, 4 figures, 4 tables", "summary": "Object detection systems have traditionally relied on supervised learning\nwith manually annotated bounding boxes, achieving high accuracy at the cost of\nsubstantial annotation investment. The emergence of Vision-Language Models\n(VLMs) offers an alternative paradigm enabling zero-shot detection through\nnatural language queries, eliminating annotation requirements but operating\nwith reduced accuracy. This paper presents the first comprehensive\ncost-effectiveness analysis comparing supervised detection (YOLO) with\nzero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on\n1,000 stratified COCO images and 200 diverse product images spanning consumer\nelectronics and rare categories, combined with detailed Total Cost of Ownership\nmodeling, we establish quantitative break-even thresholds governing\narchitecture selection. Our findings reveal that supervised YOLO achieves 91.2%\naccuracy versus 68.5% for zero-shot Gemini on standard categories, representing\na 22.7 percentage point advantage that costs $10,800 in annotation for\n100-category systems. However, this advantage justifies investment only beyond\n55 million inferences, equivalent to 151,000 images daily for one year.\nZero-shot Gemini demonstrates 52.3% accuracy on diverse product categories\n(ranging from highly web-prevalent consumer electronics at 75-85% to rare\nspecialized equipment at 25-40%) where supervised YOLO achieves 0% due to\narchitectural constraints preventing detection of untrained classes. Cost per\nCorrect Detection analysis reveals substantially lower per-detection costs for\nGemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We\ndevelop decision frameworks demonstrating that optimal architecture selection\ndepends critically on deployment volume, category stability, budget\nconstraints, and accuracy requirements rather than purely technical performance\nmetrics.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5bf9\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\uff08YOLO\uff09\u4e0e\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\uff08Gemini Flash 2.5\uff09\u8fdb\u884c\u6210\u672c\u6548\u76ca\u7efc\u5408\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u67b6\u6784\u9009\u62e9\u7684\u5b9a\u91cf\u5e73\u8861\u70b9\u9608\u503c\uff0c\u63ed\u793a\u4e86\u90e8\u7f72\u89c4\u6a21\u3001\u7c7b\u522b\u7a33\u5b9a\u6027\u548c\u9884\u7b97\u7ea6\u675f\u5bf9\u6700\u4f18\u67b6\u6784\u9009\u62e9\u7684\u5173\u952e\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u8fb9\u754c\u6846\uff0c\u867d\u7136\u7cbe\u5ea6\u9ad8\u4f46\u6807\u6ce8\u6210\u672c\u5de8\u5927\uff0c\u800c\u65b0\u5174\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u652f\u6301\u96f6\u6837\u672c\u68c0\u6d4b\u4f46\u7cbe\u5ea6\u8f83\u4f4e\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u6210\u672c\u6548\u76ca\u6bd4\u8f83\u5206\u6790\u6765\u6307\u5bfc\u5b9e\u9645\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u57281,000\u5f20\u5206\u5c42COCO\u56fe\u50cf\u548c200\u5f20\u591a\u6837\u5316\u4ea7\u54c1\u56fe\u50cf\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u5408\u8be6\u7ec6\u7684\u603b\u62e5\u6709\u6210\u672c\u5efa\u6a21\uff0c\u6bd4\u8f83\u76d1\u7763YOLO\u68c0\u6d4b\u4e0e\u96f6\u6837\u672cGemini Flash 2.5\u63a8\u7406\u7684\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u3002", "result": "\u76d1\u7763YOLO\u5728\u6807\u51c6\u7c7b\u522b\u4e0a\u8fbe\u523091.2%\u51c6\u786e\u7387\uff0c\u663e\u8457\u9ad8\u4e8eGemini\u768468.5%\uff0c\u4f46\u9700\u898110,800\u7f8e\u5143\u6807\u6ce8\u6210\u672c\uff1b\u53ea\u6709\u5728\u8d85\u8fc75,500\u4e07\u6b21\u63a8\u7406\u65f6\u6295\u8d44\u624d\u5408\u7406\uff1bGemini\u5728\u591a\u6837\u5316\u4ea7\u54c1\u7c7b\u522b\u4e0a\u5b9e\u73b052.3%\u51c6\u786e\u7387\uff0c\u800cYOLO\u56e0\u67b6\u6784\u9650\u5236\u5bf9\u672a\u8bad\u7ec3\u7c7b\u522b\u51c6\u786e\u7387\u4e3a0%\uff1b\u6bcf\u6b63\u786e\u68c0\u6d4b\u6210\u672c\u5206\u6790\u663e\u793aGemini\u572810\u4e07\u6b21\u63a8\u7406\u65f6\u6210\u672c\u663e\u8457\u66f4\u4f4e\uff080.00050\u7f8e\u5143 vs 0.143\u7f8e\u5143\uff09\u3002", "conclusion": "\u6700\u4f18\u68c0\u6d4b\u67b6\u6784\u9009\u62e9\u5173\u952e\u53d6\u51b3\u4e8e\u90e8\u7f72\u89c4\u6a21\u3001\u7c7b\u522b\u7a33\u5b9a\u6027\u3001\u9884\u7b97\u7ea6\u675f\u548c\u7cbe\u5ea6\u8981\u6c42\uff0c\u800c\u975e\u7eaf\u7cb9\u6280\u672f\u6027\u80fd\u6307\u6807\uff1b\u7814\u7a76\u5f00\u53d1\u4e86\u51b3\u7b56\u6846\u67b6\uff0c\u8bc1\u660e\u5728\u4f4e\u63a8\u7406\u91cf\u3001\u52a8\u6001\u7c7b\u522b\u6216\u9884\u7b97\u53d7\u9650\u573a\u666f\u4e0b\uff0c\u96f6\u6837\u672cVLM\u66f4\u5177\u6210\u672c\u6548\u76ca\uff0c\u800c\u5728\u9ad8\u63a8\u7406\u91cf\u3001\u7a33\u5b9a\u7c7b\u522b\u573a\u666f\u4e0b\u76d1\u7763\u5b66\u4e60\u66f4\u4f18\u3002"}}
{"id": "2510.11305", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.11305", "abs": "https://arxiv.org/abs/2510.11305", "authors": ["Jean-Paul Travert", "C\u00e9dric Goeury", "S\u00e9bastien Boyaval", "Vito Bacchi", "Fabrice Zaoui"], "title": "Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation", "comment": null, "summary": "Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)\nimagery are crucial for calibrating and validating hydraulic models. This study\nuses SAR imagery to evaluate various preprocessing (especially speckle noise\nreduction), flood mapping, and water depth estimation methods. The impact of\nthe choice of method at different steps and its hyperparameters is studied by\nconsidering an ensemble of preprocessed images, flood maps, and water depth\nfields. The evaluation is conducted for two flood events on the Garonne River\n(France) in 2019 and 2021, using hydrodynamic simulations and in-situ\nobservations as reference data. Results show that the choice of speckle filter\nalters flood extent estimations with variations of several square kilometers.\nFurthermore, the selection and tuning of flood mapping methods also affect\nperformance. While supervised methods outperformed unsupervised ones, tuned\nunsupervised approaches (such as local thresholding or change detection) can\nachieve comparable results. The compounded uncertainty from preprocessing and\nflood mapping steps also introduces high variability in the water depth field\nestimates. This study highlights the importance of considering the entire\nprocessing pipeline, encompassing preprocessing, flood mapping, and water depth\nestimation methods and their associated hyperparameters. Rather than relying on\na single configuration, adopting an ensemble approach and accounting for\nmethodological uncertainty should be privileged. For flood mapping, the method\nchoice has the most influence. For water depth estimation, the most influential\nprocessing step was the flood map input resulting from the flood mapping step\nand the hyperparameters of the methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86SAR\u56fe\u50cf\u6d2a\u6c34\u5236\u56fe\u548c\u6c34\u6df1\u4f30\u8ba1\u7684\u5b8c\u6574\u5904\u7406\u6d41\u7a0b\uff0c\u53d1\u73b0\u65b9\u6cd5\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u7ed3\u679c\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5efa\u8bae\u91c7\u7528\u96c6\u6210\u65b9\u6cd5\u800c\u975e\u5355\u4e00\u914d\u7f6e\u6765\u5904\u7406\u6d2a\u6c34\u76d1\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u6d2a\u6c34\u5236\u56fe\u548c\u6c34\u6df1\u4f30\u8ba1\u5bf9\u4e8e\u6821\u51c6\u548c\u9a8c\u8bc1\u6c34\u529b\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406SAR\u56fe\u50cf\u65f6\u5b58\u5728\u9884\u5904\u7406\u3001\u6d2a\u6c34\u5236\u56fe\u548c\u6c34\u6df1\u4f30\u8ba1\u5404\u6b65\u9aa4\u65b9\u6cd5\u9009\u62e9\u53ca\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u56e0\u7d20\u5bf9\u6700\u7ec8\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u96c6\u6210\u65b9\u6cd5\u8bc4\u4f30\u591a\u79cd\u9884\u5904\u7406\u6280\u672f\uff08\u7279\u522b\u662f\u6591\u70b9\u566a\u58f0\u6291\u5236\uff09\u3001\u6d2a\u6c34\u5236\u56fe\u65b9\u6cd5\uff08\u5305\u62ec\u76d1\u7763\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff09\u548c\u6c34\u6df1\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u9884\u5904\u7406\u56fe\u50cf\u3001\u6d2a\u6c34\u56fe\u548c\u6c34\u6df1\u573a\u7684\u96c6\u5408\u6765\u5206\u6790\u4e0d\u540c\u6b65\u9aa4\u65b9\u6cd5\u9009\u62e9\u53ca\u5176\u8d85\u53c2\u6570\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\u6591\u70b9\u6ee4\u6ce2\u5668\u7684\u9009\u62e9\u4f1a\u6539\u53d8\u6570\u5e73\u65b9\u516c\u91cc\u7684\u6d2a\u6c34\u8303\u56f4\u4f30\u8ba1\uff0c\u76d1\u7763\u65b9\u6cd5\u4f18\u4e8e\u65e0\u76d1\u7763\u65b9\u6cd5\u4f46\u8c03\u4f18\u540e\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff08\u5982\u5c40\u90e8\u9608\u503c\u6216\u53d8\u5316\u68c0\u6d4b\uff09\u53ef\u8fbe\u5230\u76f8\u5f53\u6027\u80fd\uff0c\u9884\u5904\u7406\u548c\u6d2a\u6c34\u5236\u56fe\u6b65\u9aa4\u7684\u7d2f\u79ef\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u6c34\u6df1\u573a\u4f30\u8ba1\u5b58\u5728\u9ad8\u5ea6\u53d8\u5f02\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u5fc5\u987b\u8003\u8651\u5305\u542b\u9884\u5904\u7406\u3001\u6d2a\u6c34\u5236\u56fe\u548c\u6c34\u6df1\u4f30\u8ba1\u65b9\u6cd5\u53ca\u5176\u76f8\u5173\u8d85\u53c2\u6570\u7684\u5b8c\u6574\u5904\u7406\u6d41\u7a0b\uff0c\u5e94\u91c7\u7528\u96c6\u6210\u65b9\u6cd5\u5e76\u8003\u8651\u65b9\u6cd5\u5b66\u4e0d\u786e\u5b9a\u6027\u800c\u975e\u4f9d\u8d56\u5355\u4e00\u914d\u7f6e\uff0c\u6d2a\u6c34\u5236\u56fe\u4e2d\u65b9\u6cd5\u9009\u62e9\u5f71\u54cd\u6700\u5927\uff0c\u6c34\u6df1\u4f30\u8ba1\u4e2d\u6700\u5177\u5f71\u54cd\u529b\u7684\u5904\u7406\u6b65\u9aa4\u662f\u6d2a\u6c34\u5236\u56fe\u6b65\u9aa4\u4ea7\u751f\u7684\u6d2a\u6c34\u56fe\u8f93\u5165\u548c\u65b9\u6cd5\u8d85\u53c2\u6570\u3002"}}
{"id": "2510.11565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11565", "abs": "https://arxiv.org/abs/2510.11565", "authors": ["Aniket Gupta", "Hanhui Wang", "Charles Saunders", "Aruni RoyChowdhury", "Hanumant Singh", "Huaizu Jiang"], "title": "SNAP: Towards Segmenting Anything in Any Point Cloud", "comment": "Project Page, https://neu-vi.github.io/SNAP/", "summary": "Interactive 3D point cloud segmentation enables efficient annotation of\ncomplex 3D scenes through user-guided prompts. However, current approaches are\ntypically restricted in scope to a single domain (indoor or outdoor), and to a\nsingle form of user interaction (either spatial clicks or textual prompts).\nMoreover, training on multiple datasets often leads to negative transfer,\nresulting in domain-specific tools that lack generalizability. To address these\nlimitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in\n\\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D\nsegmentation that supports both point-based and text-based prompts across\ndiverse domains. Our approach achieves cross-domain generalizability by\ntraining on 7 datasets spanning indoor, outdoor, and aerial environments, while\nemploying domain-adaptive normalization to prevent negative transfer. For\ntext-prompted segmentation, we automatically generate mask proposals without\nhuman intervention and match them against CLIP embeddings of textual queries,\nenabling both panoptic and open-vocabulary segmentation. Extensive experiments\ndemonstrate that SNAP consistently delivers high-quality segmentation results.\nWe achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for\nspatial-prompted segmentation and demonstrate competitive results on all 5\ntext-prompted benchmarks. These results show that a unified model can match or\nexceed specialized domain-specific approaches, providing a practical tool for\nscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/", "AI": {"tldr": "SNAP\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4ea4\u4e92\u5f0f3D\u70b9\u4e91\u5206\u5272\u6a21\u578b\uff0c\u652f\u6301\u8de8\u57df\u7684\u70b9\u57fa\u548c\u6587\u672c\u57fa\u63d0\u793a\uff0c\u901a\u8fc7\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bad\u7ec3\u548c\u57df\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4ea4\u4e92\u5f0f3D\u70b9\u4e91\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u9886\u57df\u9650\u5236\uff08\u4ec5\u5ba4\u5185\u6216\u5ba4\u5916\uff09\u548c\u4ea4\u4e92\u65b9\u5f0f\u5355\u4e00\uff08\u4ec5\u7a7a\u95f4\u70b9\u51fb\u6216\u6587\u672c\u63d0\u793a\uff09\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5728\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u65f6\u5bb9\u6613\u4ea7\u751f\u8d1f\u8fc1\u79fb\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u901a\u7528\u6027\u7684\u9886\u57df\u7279\u5b9a\u5de5\u5177\u3002", "method": "\u63d0\u51faSNAP\u7edf\u4e00\u6a21\u578b\uff0c\u91c7\u7528\u57df\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u9632\u6b62\u8d1f\u8fc1\u79fb\uff0c\u57287\u4e2a\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff1b\u5bf9\u4e8e\u6587\u672c\u63d0\u793a\u5206\u5272\uff0c\u81ea\u52a8\u751f\u6210\u63a9\u7801\u63d0\u6848\u5e76\u4e0e\u6587\u672c\u67e5\u8be2\u7684CLIP\u5d4c\u5165\u8fdb\u884c\u5339\u914d\uff0c\u5b9e\u73b0\u5168\u666f\u548c\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u3002", "result": "\u5728\u7a7a\u95f4\u63d0\u793a\u5206\u5272\u76849\u4e2a\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c8\u4e2a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5728\u6587\u672c\u63d0\u793a\u5206\u5272\u76845\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u6a21\u578b\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8d8a\u4e13\u95e8\u7684\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7edf\u4e00\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u8de8\u57df\u901a\u7528\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u76843D\u6807\u6ce8\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u9886\u57df\u548c\u4ea4\u4e92\u65b9\u5f0f\u4e0a\u7684\u9650\u5236\u3002"}}
{"id": "2510.11579", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11579", "abs": "https://arxiv.org/abs/2510.11579", "authors": ["Hongyu Zhu", "Lin Chen", "Mounim A. El-Yacoubi", "Mingsheng Shang"], "title": "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis", "comment": "Under Review", "summary": "Multimodal Sentiment Analysis (MSA) aims to identify and interpret human\nemotions by integrating information from heterogeneous data sources such as\ntext, video, and audio. While deep learning models have advanced in network\narchitecture design, they remain heavily limited by scarce multimodal annotated\ndata. Although Mixup-based augmentation improves generalization in unimodal\ntasks, its direct application to MSA introduces critical challenges: random\nmixing often amplifies label ambiguity and semantic inconsistency due to the\nlack of emotion-aware mixing mechanisms. To overcome these issues, we propose\nMS-Mix, an adaptive, emotion-sensitive augmentation framework that\nautomatically optimizes sample mixing in multimodal settings. The key\ncomponents of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)\nstrategy that effectively prevents semantic confusion caused by mixing samples\nwith contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module\nusing multi-head self-attention to compute modality-specific mixing ratios\ndynamically based on their respective emotional intensities. (3) a Sentiment\nAlignment Loss (SAL) that aligns the prediction distributions across\nmodalities, and incorporates the Kullback-Leibler-based loss as an additional\nregularization term to train the emotion intensity predictor and the backbone\nnetwork jointly. Extensive experiments on three benchmark datasets with six\nstate-of-the-art backbones confirm that MS-Mix consistently outperforms\nexisting methods, establishing a new standard for robust multimodal sentiment\naugmentation. The source code is available at:\nhttps://github.com/HongyuZhu-s/MS-Mix.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MS-Mix\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u81ea\u9002\u5e94\u60c5\u611f\u654f\u611f\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u611f\u77e5\u6837\u672c\u9009\u62e9\u3001\u60c5\u611f\u5f3a\u5ea6\u5f15\u5bfc\u7684\u6df7\u5408\u6bd4\u4f8b\u8ba1\u7b97\u548c\u60c5\u611f\u5bf9\u9f50\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfMixup\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\u5bfc\u81f4\u7684\u6807\u7b7e\u6a21\u7cca\u548c\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u800c\u4f20\u7edf\u7684Mixup\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u573a\u666f\u65f6\u4f1a\u5f15\u5165\u6807\u7b7e\u6a21\u7cca\u548c\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u60c5\u611f\u611f\u77e5\u7684\u6df7\u5408\u673a\u5236\u5bfc\u81f4\u968f\u673a\u6df7\u5408\u53ef\u80fd\u653e\u5927\u60c5\u611f\u77db\u76fe\u6837\u672c\u95f4\u7684\u8bed\u4e49\u6df7\u6dc6\u3002", "method": "MS-Mix\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u60c5\u611f\u611f\u77e5\u6837\u672c\u9009\u62e9\u7b56\u7565\u9632\u6b62\u60c5\u611f\u77db\u76fe\u6837\u672c\u6df7\u5408\u5bfc\u81f4\u7684\u8bed\u4e49\u6df7\u6dc6\uff1b\u60c5\u611f\u5f3a\u5ea6\u5f15\u5bfc\u6a21\u5757\u4f7f\u7528\u591a\u5934\u81ea\u6ce8\u610f\u529b\u52a8\u6001\u8ba1\u7b97\u6a21\u6001\u7279\u5b9a\u7684\u6df7\u5408\u6bd4\u4f8b\uff1b\u60c5\u611f\u5bf9\u9f50\u635f\u5931\u901a\u8fc7KL\u6563\u5ea6\u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\u8054\u5408\u8bad\u7ec3\u60c5\u611f\u5f3a\u5ea6\u9884\u6d4b\u5668\u548c\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u516d\u4e2a\u6700\u5148\u8fdb\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMS-Mix\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u9c81\u68d2\u7684\u591a\u6a21\u6001\u60c5\u611f\u589e\u5f3a\u5efa\u7acb\u4e86\u65b0\u7684\u6807\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u60c5\u611f\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u91cd\u8981\u6027\uff0cMS-Mix\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u6df7\u5408\u673a\u5236\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u589e\u5f3a\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u5411\u3002"}}
{"id": "2510.11717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11717", "abs": "https://arxiv.org/abs/2510.11717", "authors": ["Takuya Nakabayashi", "Navami Kairanda", "Hideo Saito", "Vladislav Golyanik"], "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams", "comment": null, "summary": "Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ev4DGS\uff0c\u8fd9\u662f\u9996\u4e2a\u4ec5\u4ece\u5355\u76ee\u4e8b\u4ef6\u6d41\u4e2d\u5b9e\u73b0\u975e\u521a\u6027\u53d8\u5f62\u7269\u4f53\u65b0\u89c6\u89d2\u6e32\u67d3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u53d8\u5f623D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u548c\u57fa\u4e8e\u4e8b\u4ef6\u7684\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981RGB\u8f93\u5165\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u65b0\u89c6\u89d2\u6e32\u67d3\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u521a\u6027\u573a\u666f\uff0c\u5bf9\u4e8e\u975e\u521a\u6027\u7269\u4f53\u5219\u9700\u8981\u989d\u5916\u7684\u7a00\u758fRGB\u8f93\u5165\uff0c\u8fd9\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u663e\u8457\u9650\u5236\uff1b\u672c\u6587\u65e8\u5728\u63a2\u7d22\u662f\u5426\u80fd\u591f\u4ec5\u4ece\u4e8b\u4ef6\u6d41\u4e2d\u5b66\u4e60\u7c7b\u4f3c\u6a21\u578b\uff0c\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u56de\u5f52\u53ef\u53d8\u5f623D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff0c\u91c7\u7528\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a1\uff09\u5c06\u4f30\u8ba1\u6a21\u578b\u8f93\u51fa\u4e0e2D\u4e8b\u4ef6\u89c2\u6d4b\u7a7a\u95f4\u76f8\u5173\u8054\u7684\u635f\u5931\u51fd\u6570\uff1b2\uff09\u4ece\u4e8b\u4ef6\u751f\u6210\u7684\u4e8c\u503c\u63a9\u7801\u4e2d\u8bad\u7ec3\u7684\u7c97\u7cd93D\u53d8\u5f62\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4ec5\u57fa\u4e8e\u4e8b\u4ef6\u6d41\u7684\u975e\u521a\u6027\u573a\u666f\u5efa\u6a21\u3002", "result": "\u5728\u73b0\u6709\u7684\u5408\u6210\u6570\u636e\u96c6\u548c\u65b0\u5f55\u5236\u7684\u771f\u5b9e\u975e\u521a\u6027\u7269\u4f53\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEv4DGS\u65b9\u6cd5\u6709\u6548\u53ef\u884c\uff0c\u5e76\u4e14\u5728\u6211\u4eec\u7684\u8bbe\u5b9a\u4e2d\u76f8\u6bd4\u591a\u4e2a\u6734\u7d20\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4ec5\u4f7f\u7528\u4e8b\u4ef6\u6d41\u8fdb\u884c\u975e\u521a\u6027\u65b0\u89c6\u89d2\u6e32\u67d3\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8bc1\u660e\u4e86\u4ec5\u4ece\u4e8b\u4ef6\u6d41\u4e2d\u5b66\u4e60\u975e\u521a\u6027\u53d8\u5f62\u7269\u4f53\u65b0\u89c6\u89d2\u6e32\u67d3\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u540c\u65f6\u53d1\u5e03\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u53d1\u5c55\u3002"}}
