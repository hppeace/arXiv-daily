{"id": "2510.10577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10577", "abs": "https://arxiv.org/abs/2510.10577", "authors": ["Haonan Wang", "Hanyu Zhou", "Haoyue Liu", "Luxin Yan"], "title": "Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes", "comment": null, "summary": "Optical flow estimation has achieved promising results in conventional scenes\nbut faces challenges in high-speed and low-light scenes, which suffer from\nmotion blur and insufficient illumination. These conditions lead to weakened\ntexture and amplified noise and deteriorate the appearance saturation and\nboundary completeness of frame cameras, which are necessary for motion feature\nmatching. In degraded scenes, the frame camera provides dense appearance\nsaturation but sparse boundary completeness due to its long imaging time and\nlow dynamic range. In contrast, the event camera offers sparse appearance\nsaturation, while its short imaging time and high dynamic range gives rise to\ndense boundary completeness. Traditionally, existing methods utilize feature\nfusion or domain adaptation to introduce event to improve boundary\ncompleteness. However, the appearance features are still deteriorated, which\nseverely affects the mostly adopted discriminative models that learn the\nmapping from visual features to motion fields and generative models that\ngenerate motion fields based on given visual features. So we introduce\ndiffusion models that learn the mapping from noising flow to clear flow, which\nis not affected by the deteriorated visual features. Therefore, we propose a\nnovel optical flow estimation framework Diff-ABFlow based on diffusion models\nwith frame-event appearance-boundary fusion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5e27-\u4e8b\u4ef6\u5916\u89c2\u8fb9\u754c\u878d\u5408\u5149\u6d41\u4f30\u8ba1\u6846\u67b6Diff-ABFlow\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4ece\u566a\u58f0\u6d41\u5230\u6e05\u6670\u6d41\u7684\u6620\u5c04\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u901f\u548c\u4f4e\u5149\u573a\u666f\u4e0b\u4f20\u7edf\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\u56e0\u8fd0\u52a8\u6a21\u7cca\u548c\u5149\u7167\u4e0d\u8db3\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\u5728\u9ad8\u901f\u548c\u4f4e\u5149\u573a\u666f\u4e0b\u9762\u4e34\u4e25\u91cd\u6311\u6218\uff0c\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u8fd0\u52a8\u6a21\u7cca\u548c\u5149\u7167\u4e0d\u8db3\u5bfc\u81f4\u7eb9\u7406\u7279\u5f81\u51cf\u5f31\u3001\u566a\u58f0\u653e\u5927\uff0c\u5e76\u7834\u574f\u4e86\u5e27\u76f8\u673a\u7684\u5916\u89c2\u9971\u548c\u5ea6\u548c\u8fb9\u754c\u5b8c\u6574\u6027\u3002\u867d\u7136\u4e8b\u4ef6\u76f8\u673a\u80fd\u591f\u63d0\u4f9b\u5bc6\u96c6\u7684\u8fb9\u754c\u5b8c\u6574\u6027\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u878d\u5408\u6216\u57df\u9002\u5e94\u5f15\u5165\u4e8b\u4ef6\u6570\u636e\u540e\uff0c\u5916\u89c2\u7279\u5f81\u4ecd\u7136\u4e25\u91cd\u9000\u5316\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86\u57fa\u4e8e\u89c6\u89c9\u7279\u5f81\u5230\u8fd0\u52a8\u573a\u6620\u5c04\u7684\u5224\u522b\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684Diff-ABFlow\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u5e27-\u4e8b\u4ef6\u5916\u89c2\u8fb9\u754c\u878d\u5408\u7b56\u7565\u3002\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4ece\u566a\u58f0\u6d41\u5230\u6e05\u6670\u6d41\u7684\u6620\u5c04\u8fc7\u7a0b\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u9000\u5316\u7684\u89c6\u89c9\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u5145\u5206\u5229\u7528\u5e27\u76f8\u673a\u63d0\u4f9b\u7684\u5916\u89c2\u9971\u548c\u5ea6\u548c\u4e8b\u4ef6\u76f8\u673a\u63d0\u4f9b\u7684\u8fb9\u754c\u5b8c\u6574\u6027\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u7684\u751f\u6210\u80fd\u529b\u6765\u6062\u590d\u9ad8\u8d28\u91cf\u7684\u5149\u6d41\u573a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684Diff-ABFlow\u6846\u67b6\u5728\u9ad8\u901f\u548c\u4f4e\u5149\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5149\u6d41\u4f30\u8ba1\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u9000\u5316\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u5b9e\u73b0\u4e86\u5bf9\u8fd0\u52a8\u6a21\u7cca\u548c\u5149\u7167\u4e0d\u8db3\u6761\u4ef6\u4e0b\u5149\u6d41\u573a\u7684\u51c6\u786e\u6062\u590d\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6269\u6563\u6a21\u578b\u5728\u5149\u6d41\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u7279\u5f81\u9000\u5316\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u3002\u901a\u8fc7\u5c06\u5e27\u76f8\u673a\u548c\u4e8b\u4ef6\u76f8\u673a\u7684\u4e92\u8865\u4f18\u52bf\u4e0e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u548c\u751f\u6210\u6a21\u578b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.11717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11717", "abs": "https://arxiv.org/abs/2510.11717", "authors": ["Takuya Nakabayashi", "Navami Kairanda", "Hideo Saito", "Vladislav Golyanik"], "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams", "comment": null, "summary": "Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ev4DGS\uff0c\u8fd9\u662f\u9996\u4e2a\u4ece\u5355\u76ee\u4e8b\u4ef6\u6d41\u4e2d\u5b9e\u73b0\u975e\u521a\u6027\u53d8\u5f62\u7269\u4f53\u65b0\u89c6\u89d2\u6e32\u67d3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56de\u5f52\u53ef\u53d8\u5f623D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff0c\u4ec5\u4f7f\u7528\u4e8b\u4ef6\u6570\u636e\u5373\u53ef\u751f\u6210RGB\u6216\u7070\u5ea6\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u65b9\u6cd5\u5728\u5904\u7406\u975e\u521a\u6027\u7269\u4f53\u65f6\u9700\u8981\u989d\u5916\u7684\u7a00\u758fRGB\u8f93\u5165\uff0c\u8fd9\u6784\u6210\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u663e\u8457\u9650\u5236\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u662f\u5426\u80fd\u591f\u4ec5\u4ece\u4e8b\u4ef6\u6d41\u4e2d\u5b66\u4e60\u7c7b\u4f3c\u6a21\u578b\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e24\u79cd\u673a\u5236\u56de\u5f52\u53ef\u53d8\u5f623D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff1a\u4e00\u662f\u5c06\u4f30\u8ba1\u6a21\u578b\u8f93\u51fa\u4e0e2D\u4e8b\u4ef6\u89c2\u6d4b\u7a7a\u95f4\u5173\u8054\u7684\u635f\u5931\u51fd\u6570\uff0c\u4e8c\u662f\u4ece\u4e8b\u4ef6\u751f\u6210\u7684\u4e8c\u503c\u63a9\u7801\u4e2d\u8bad\u7ec3\u7684\u7c97\u7cd93D\u53d8\u5f62\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4ec5\u57fa\u4e8e\u4e8b\u4ef6\u6570\u636e\u7684\u975e\u521a\u6027\u573a\u666f\u5efa\u6a21\u3002", "result": "\u5728\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\u548c\u65b0\u8bb0\u5f55\u7684\u5305\u542b\u975e\u521a\u6027\u7269\u4f53\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u6bd4\u8f83\u8868\u660e\uff0cEv4DGS\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53ca\u5176\u76f8\u5bf9\u4e8e\u591a\u79cd\u53ef\u5e94\u7528\u4e8e\u8be5\u8bbe\u7f6e\u7684\u6734\u7d20\u57fa\u7ebf\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u4ec5\u4ece\u4e8b\u4ef6\u6d41\u5b66\u4e60\u975e\u521a\u6027\u7269\u4f53\u65b0\u89c6\u89d2\u6e32\u67d3\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u540c\u65f6\u5c06\u53d1\u5e03\u8bc4\u4f30\u4e2d\u4f7f\u7528\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
