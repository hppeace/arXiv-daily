<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-02-09.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-a-neuromorphic-model-of-the-insect-visual-system-for-natural-image-processing">[1] <a href="https://arxiv.org/abs/2602.06405">A neuromorphic model of the insect visual system for natural image processing</a></h3>
<p><em>Adam D. Hines, Karin NordstrÃ¶m, Andrew B. Barron</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å—æ˜†è™«è§†è§‰å¯å‘çš„ç”Ÿç‰©å¯å‘è§†è§‰æ¨¡å‹ï¼Œé€šè¿‡å®Œå…¨è‡ªç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ç›®æ ‡ç”Ÿæˆç¨€ç–ã€å¯åŒºåˆ†çš„è§†è§‰ç¼–ç ï¼Œè¯¥æ¨¡å‹åœ¨èŠ±æœµè¯†åˆ«ä»»åŠ¡å’Œè‡ªç„¶å›¾åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶å®ç°äº†äººå·¥ç¥ç»ç½‘ç»œå’Œè„‰å†²ç¥ç»ç½‘ç»œä¸¤ç§å®ç°å½¢å¼ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è®¸å¤šè®¡ç®—æ¨¡å‹è¿‡äºå…³æ³¨ä»»åŠ¡æ€§èƒ½è€Œå¿½è§†äº†ç”Ÿç‰©çœŸå®çš„å¤„ç†è·¯å¾„ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œé€šè¿‡æ•æ‰æ˜†è™«è§†è§‰ç³»ç»Ÿçš„åŸç†æ¥æ„å»ºæ›´ç¬¦åˆç”Ÿç‰©æœºåˆ¶çš„è§†è§‰å¤„ç†æ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç”Ÿç‰©å¯å‘è§†è§‰æ¨¡å‹ï¼Œå°†å¯†é›†è§†è§‰è¾“å…¥è½¬æ¢ä¸ºç¨€ç–ã€å¯åŒºåˆ†çš„ç¼–ç ï¼Œé‡‡ç”¨å®Œå…¨è‡ªç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ ‡æ³¨æ•°æ®å³å¯å­¦ä¹ è¡¨ç¤ºï¼Œå¹¶æ”¯æŒè·¨ä»»åŠ¡é‡ç”¨è€Œæ— éœ€ä¾èµ–ç‰¹å®šé¢†åŸŸåˆ†ç±»å™¨ï¼ŒåŒæ—¶å®ç°äº†äººå·¥ç¥ç»ç½‘ç»œå’Œè„‰å†²ç¥ç»ç½‘ç»œä¸¤ç§å®ç°å½¢å¼ã€‚</p>
<p><strong>Result:</strong> æ¨¡å‹åœ¨èŠ±æœµè¯†åˆ«ä»»åŠ¡å’Œè‡ªç„¶å›¾åƒåŸºå‡†æµ‹è¯•ä¸­æŒç»­äº§ç”Ÿå¯é çš„ç¨€ç–ç¼–ç ï¼Œèƒ½å¤ŸåŒºåˆ†è§†è§‰ç›¸ä¼¼çš„è¾“å…¥ï¼Œåœ¨æ¨¡æ‹Ÿå®šä½è®¾ç½®ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºç®€å•çš„å›¾åƒä¸‹é‡‡æ ·åŸºçº¿ï¼Œçªæ˜¾äº†æ•´åˆç¥ç»å½¢æ€è§†è§‰å¤„ç†è·¯å¾„çš„åŠŸèƒ½ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›ç»“æœæ¨è¿›äº†æ˜†è™«è®¡ç®—å»ºæ¨¡é¢†åŸŸï¼Œæä¾›äº†ä¸€ä¸ªé€šç”¨çš„ç”Ÿç‰©å¯å‘è§†è§‰æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­æ‰§è¡Œç¨€ç–è®¡ç®—ï¼Œä¸ºç†è§£ç”Ÿç‰©è§†è§‰å¤„ç†æä¾›äº†æ–°çš„è®¡ç®—æ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†è‡ªç›‘ç£å­¦ä¹ åœ¨ç”Ÿç‰©å¯å‘æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Insect vision supports complex behaviors including associative learning, navigation, and object detection, and has long motivated computational models for understanding biological visual processing. However, many contemporary models prioritize task performance while neglecting biologically grounded processing pathways. Here, we introduce a bio-inspired vision model that captures principles of the insect visual system to transform dense visual input into sparse, discriminative codes. The model is trained using a fully self-supervised contrastive objective, enabling representation learning without labeled data and supporting reuse across tasks without reliance on domain-specific classifiers. We evaluated the resulting representations on flower recognition tasks and natural image benchmarks. The model consistently produced reliable sparse codes that distinguish visually similar inputs. To support different modelling and deployment uses, we have implemented the model as both an artificial neural network and a spiking neural network. In a simulated localization setting, our approach outperformed a simple image downsampling comparison baseline, highlighting the functional benefit of incorporating neuromorphic visual processing pathways. Collectively, these results advance insect computational modelling by providing a generalizable bio-inspired vision model capable of sparse computation across diverse tasks.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="2-sparse-spike-encoding-of-channel-responses-for-energy-efficient-human-activity-recognition">[2] <a href="https://arxiv.org/abs/2602.06766">Sparse Spike Encoding of Channel Responses for Energy Efficient Human Activity Recognition</a></h3>
<p><em>Eleonora Cicciarella, Riccardo Mazzieri, Jacopo Pegoraro, Michele Rossi</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé›†æˆä¼ æ„Ÿä¸é€šä¿¡ï¼ˆISACï¼‰ä¸­äººç±»æ´»åŠ¨è¯†åˆ«çš„è„‰å†²å·ç§¯è‡ªç¼–ç å™¨ï¼ˆSCAEï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä¿¡é“å†²æ¿€å“åº”ï¼ˆCIRï¼‰çš„ç¨€ç–è„‰å†²ç¼–ç è¡¨ç¤ºï¼Œå¹¶ä¸è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰è”åˆè®­ç»ƒï¼Œåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶æ˜¾è‘—é™ä½èƒ½è€—ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é›†æˆä¼ æ„Ÿä¸é€šä¿¡ï¼ˆISACï¼‰ç³»ç»Ÿéœ€è¦è¾¹ç¼˜è®¾å¤‡è¿›è¡ŒæŒç»­ç›‘æµ‹ï¼Œä½†ç°ä»£ä¼ æ„Ÿç®—æ³•é€šå¸¸è¿‡äºå¤æ‚ï¼Œæ— æ³•æ»¡è¶³èƒ½é‡å—é™è¾¹ç¼˜è®¾å¤‡çš„èƒ½æ•ˆè¦æ±‚ã€‚è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰å› å…¶å¤„ç†ç¨€ç–äºŒè¿›åˆ¶è„‰å†²åºåˆ—çš„ç‰¹æ€§ï¼Œæœ‰æœ›å°†èƒ½è€—é™ä½æ•°ä¸ªæ•°é‡çº§ï¼Œä½†éœ€è¦å¼€å‘èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ä¼ æ„Ÿæ•°æ®è¡¨ç¤ºçš„è„‰å†²ç¼–ç æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§è„‰å†²å·ç§¯è‡ªç¼–ç å™¨ï¼ˆSCAEï¼‰ï¼Œä¸“é—¨å­¦ä¹ ä¿¡é“å†²æ¿€å“åº”ï¼ˆCIRï¼‰æ•°æ®çš„å®šåˆ¶åŒ–è„‰å†²ç¼–ç è¡¨ç¤ºã€‚è¯¥æ¨¡å‹ä¸ç”¨äºäººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰çš„è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰è¿›è¡Œè”åˆè®­ç»ƒï¼Œä»è€Œæ¶ˆé™¤äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¯¹å¤šæ™®å‹’åŸŸé¢„å¤„ç†çš„éœ€æ±‚ï¼Œå®ç°äº†ä»åŸå§‹CIRæ•°æ®åˆ°åˆ†ç±»ç»“æœçš„ç«¯åˆ°ç«¯è„‰å†²å¤„ç†ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒSCAE-SNNæ¨¡å‹åœ¨äººç±»æ´»åŠ¨è¯†åˆ«ä»»åŠ¡ä¸­è¾¾åˆ°äº†ä¸æ··åˆæ–¹æ³•ç›¸å½“çš„F1åˆ†æ•°ï¼ˆæ¥è¿‘96%ï¼‰ï¼ŒåŒæ—¶ç”Ÿæˆäº†æ˜¾è‘—æ›´ç¨€ç–çš„è„‰å†²ç¼–ç ï¼ˆ81.1%çš„ç¨€ç–åº¦ï¼‰ã€‚åœ¨åˆ†ç±»å‰å¯¹CIRæ•°æ®è¿›è¡Œç¼–ç ä¸ä»…æé«˜äº†HARçš„å‡†ç¡®æ€§ï¼Œè¿˜æ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„èƒ½æ•ˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è„‰å†²ç¼–ç è‡ªç¼–ç å™¨åœ¨ISACç³»ç»Ÿä¸­å¤„ç†ä¼ æ„Ÿæ•°æ®çš„æœ‰æ•ˆæ€§ï¼Œä¸ºèƒ½é‡å—é™è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶æ„ŸçŸ¥ä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚è”åˆè®­ç»ƒæ–¹æ³•æ¶ˆé™¤äº†é¢„å¤„ç†éœ€æ±‚ï¼Œç®€åŒ–äº†ç³»ç»Ÿæ¶æ„ï¼Œè€Œé«˜ç¨€ç–åº¦çš„è„‰å†²ç¼–ç ä¸ºå®ç°è¶…ä½åŠŸè€—çš„ä¼ æ„Ÿç®—æ³•å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>ISAC enables pervasive monitoring, but modern sensing algorithms are often too complex for energy-constrained edge devices. This motivates the development of learning techniques that balance accuracy performance and energy efficiency. Spiking Neural Networks (SNNs) are a promising alternative, processing information as sparse binary spike trains and potentially reducing energy consumption by orders of magnitude. In this work, we propose a spiking convolutional autoencoder (SCAE) that learns tailored spike-encoded representations of channel impulse responses (CIR), jointly trained with an SNN for human activity recognition (HAR), thereby eliminating the need for Doppler domain preprocessing. The results show that our SCAE-SNN achieves F1 scores comparable to a hybrid approach (almost 96%), while producing substantially sparser spike encoding (81.1% sparsity). We also show that encoding CIR data prior to classification improves both HAR accuracy and efficiency. The code is available at https://github.com/ele-ciccia/SCAE-SNN-HAR.</p>
  </article>
</body>
</html>
