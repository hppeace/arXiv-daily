<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-03.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.NE">cs.NE</a> [Total: 2]</li>
</ul>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="1-spiking-neural-networks-the-future-of-brain-inspired-computing">[1] <a href="https://arxiv.org/abs/2510.27379">Spiking Neural Networks: The Future of Brain-Inspired Computing</a></h3>
<p><em>Sales G. Aribe Jr</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å¯¹è„‰å†²ç¥ç»ç½‘ç»œè¿›è¡Œäº†å…¨é¢åˆ†æï¼Œè¡¨æ˜SNNåœ¨ä¿æŒæ¥è¿‘ANNç²¾åº¦çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå®ç°æ˜¾è‘—çš„èƒ½é‡æ•ˆç‡å’Œä½å»¶è¿Ÿç‰¹æ€§ï¼Œç‰¹åˆ«é€‚åˆèƒ½é‡å—é™çš„è¾¹ç¼˜AIåº”ç”¨ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è„‰å†²ç¥ç»ç½‘ç»œä½œä¸ºæ–°ä¸€ä»£ç¥ç»è®¡ç®—æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿäººå·¥ç¥ç»ç½‘ç»œåœ¨èƒ½é‡æ•ˆç‡å’Œæ—¶åºåŠ¨æ€ç‰¹æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºèƒ½é‡å—é™ã€å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨åœºæ™¯æä¾›æ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ·±å…¥åˆ†æäº†LIFç­‰å…³é”®ç¥ç»å…ƒæ¨¡å‹ï¼Œå¹¶è¯„ä¼°äº†å¤šç§è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬æ›¿ä»£æ¢¯åº¦ä¸‹é™æ³•ã€ANNåˆ°SNNè½¬æ¢æ–¹æ³•ä»¥åŠåŸºäºè„‰å†²æ—¶åºä¾èµ–å¯å¡‘æ€§çš„æ— ç›‘ç£å­¦ä¹ ç®—æ³•ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºæ›¿ä»£æ¢¯åº¦è®­ç»ƒçš„SNNåœ¨ç²¾åº¦ä¸Šä¸ANNç›¸å·®ä»…1-2%ï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå»¶è¿Ÿä½è‡³10æ¯«ç§’ï¼›è½¬æ¢çš„SNNæ€§èƒ½ç«äº‰æ€§ä½†éœ€è¦æ›´å¤šè„‰å†²ï¼›STDPè®­ç»ƒçš„SNNè™½ç„¶æ”¶æ•›è¾ƒæ…¢ï¼Œä½†å…·æœ‰æœ€ä½çš„è„‰å†²è®¡æ•°å’Œèƒ½é‡æ¶ˆè€—ï¼Œæ¯æ¨ç†ä»…éœ€5æ¯«ç„¦è€³ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯å®SNNç‰¹åˆ«é€‚åˆæœºå™¨äººæŠ€æœ¯ã€ç¥ç»å½¢æ€è§†è§‰å’Œè¾¹ç¼˜AIç³»ç»Ÿç­‰åº”ç”¨åœºæ™¯ï¼Œå°½ç®¡åœ¨ç¡¬ä»¶æ ‡å‡†åŒ–å’Œå¯æ‰©å±•è®­ç»ƒæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†ç»è¿‡è¿›ä¸€æ­¥ä¼˜åŒ–åæœ‰æœ›æ¨åŠ¨ç¥ç»å½¢æ€è®¡ç®—çš„ä¸‹ä¸€ä¸ªå‘å±•é˜¶æ®µã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Spiking Neural Networks (SNNs) represent the latest generation of neural
computation, offering a brain-inspired alternative to conventional Artificial
Neural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals,
SNNs operate using distinct spike events, making them inherently more
energy-efficient and temporally dynamic. This study presents a comprehensive
analysis of SNN design models, training algorithms, and multi-dimensional
performance metrics, including accuracy, energy consumption, latency, spike
count, and convergence behavior. Key neuron models such as the Leaky
Integrate-and-Fire (LIF) and training strategies, including surrogate gradient
descent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP),
are examined in depth. Results show that surrogate gradient-trained SNNs
closely approximate ANN accuracy (within 1-2%), with faster convergence by the
20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve
competitive performance but require higher spike counts and longer simulation
windows. STDP-based SNNs, though slower to converge, exhibit the lowest spike
counts and energy consumption (as low as 5 millijoules per inference), making
them optimal for unsupervised and low-power tasks. These findings reinforce the
suitability of SNNs for energy-constrained, latency-sensitive, and adaptive
applications such as robotics, neuromorphic vision, and edge AI systems. While
promising, challenges persist in hardware standardization and scalable
training. This study concludes that SNNs, with further refinement, are poised
to propel the next phase of neuromorphic computing.</p>
<h3 id="2-exploiting-heterogeneous-delays-for-efficient-computation-in-low-bit-neural-networks">[2] <a href="https://arxiv.org/abs/2510.27434">Exploiting heterogeneous delays for efficient computation in low-bit neural networks</a></h3>
<p><em>Pengfei Sun, Jascha Achterberg, Zhe Su, Dan F. M. Goodman, Danyal Akarca</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºé€šè¿‡å…±åŒå­¦ä¹ çªè§¦æƒé‡å’Œå»¶è¿Ÿå‚æ•°ï¼Œåˆ©ç”¨å»¶è¿Ÿå¼‚è´¨æ€§å®ç°è„‰å†²ç¥ç»ç½‘ç»œåœ¨æ—¶é—´å¤æ‚ä»»åŠ¡ä¸Šçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå³ä½¿åœ¨æƒé‡ç²¾åº¦æä½ï¼ˆ1.58ä½ä¸‰å…ƒç²¾åº¦ï¼‰çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œä¸ºç¥ç»å½¢æ€è®¡ç®—æä¾›äº†é«˜æ•ˆçš„å†…å­˜è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿç¥ç»ç½‘ç»œä¸»è¦å…³æ³¨çªè§¦æƒé‡å­¦ä¹ ï¼Œä½†å¿½ç•¥äº†å…¶ä»–å¯å­¦ä¹ çš„ç¥ç»å‚æ•°å¦‚å»¶è¿Ÿï¼Œè€Œå¤§è„‘è¡¨ç°å‡ºå¤æ‚çš„å¼‚è´¨æ€§å»¶è¿Ÿæ—¶é—´åŠ¨æ€ï¼Œå…¶ä¸­ä¿¡å·åœ¨ç¥ç»å…ƒé—´å¼‚æ­¥ä¼ è¾“ï¼Œè¿™ç§å»¶è¿Ÿå¼‚è´¨æ€§å¯èƒ½åœ¨å…·èº«ç¯å¢ƒä¸­è¢«åˆ©ç”¨æ¥å¤„ç†æ—¶é—´åŸŸä¸­çš„ä»»åŠ¡ç›¸å…³ä¿¡æ¯ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶è®­ç»ƒè„‰å†²ç¥ç»ç½‘ç»œåŒæ—¶ä¿®æ”¹æƒé‡å’Œä¸åŒç²¾åº¦æ°´å¹³çš„å»¶è¿Ÿå‚æ•°ï¼Œæ¢ç´¢å»¶è¿Ÿå’Œæƒé‡åœ¨å¤šç§ç²¾åº¦é…ç½®ä¸‹çš„ååŒå­¦ä¹ æœºåˆ¶ï¼Œåˆ†æå»¶è¿Ÿå¼‚è´¨æ€§å¯¹ç½‘ç»œæ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> å»¶è¿Ÿå¼‚è´¨æ€§ä½¿å¾—è„‰å†²ç¥ç»ç½‘ç»œåœ¨æ—¶é—´å¤æ‚ç¥ç»å½¢æ€é—®é¢˜ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå³ä½¿åœ¨æƒé‡ç²¾åº¦æä½ï¼ˆ1.58ä½ä¸‰å…ƒç²¾åº¦ï¼‰æ—¶ä¹Ÿèƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œå®ç°æ¯”å…¸å‹ä»…æƒé‡ç½‘ç»œæ›´æ¿€è¿›çš„å‹ç¼©ï¼ˆè¶…è¿‡ä¸€ä¸ªæ•°é‡çº§ï¼‰ï¼Œä¸”æ—¶é—´å¤æ‚ä»»åŠ¡éœ€è¦æ›´é•¿çš„å»¶è¿Ÿåˆ†å¸ƒã€‚</p>
<p><strong>Conclusion:</strong> æ—¶é—´å¼‚è´¨æ€§æ˜¯é«˜æ•ˆè®¡ç®—çš„é‡è¦åŸåˆ™ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡ç›¸å…³ä¿¡æ¯å…·æœ‰æ—¶é—´ç‰¹æ€§çš„ç‰©ç†ä¸–ç•Œä¸­ï¼Œå»¶è¿Ÿå¼‚è´¨æ€§å…è®¸åœ¨ä¿æŒæœ€å…ˆè¿›ç²¾åº¦çš„åŒæ—¶å®ç°å†…å­˜é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¯¹å…·èº«æ™ºèƒ½ç³»ç»Ÿå’Œç¥ç»å½¢æ€ç¡¬ä»¶å…·æœ‰é‡è¦å¯ç¤ºæ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Neural networks rely on learning synaptic weights. However, this overlooks
other neural parameters that can also be learned and may be utilized by the
brain. One such parameter is the delay: the brain exhibits complex temporal
dynamics with heterogeneous delays, where signals are transmitted
asynchronously between neurons. It has been theorized that this delay
heterogeneity, rather than a cost to be minimized, can be exploited in embodied
contexts where task-relevant information naturally sits contextually in the
time domain. We test this hypothesis by training spiking neural networks to
modify not only their weights but also their delays at different levels of
precision. We find that delay heterogeneity enables state-of-the-art
performance on temporally complex neuromorphic problems and can be achieved
even when weights are extremely imprecise (1.58-bit ternary precision: just
positive, negative, or absent). By enabling high performance with extremely
low-precision weights, delay heterogeneity allows memory-efficient solutions
that maintain state-of-the-art accuracy even when weights are compressed over
an order of magnitude more aggressively than typically studied weight-only
networks. We show how delays and time-constants adaptively trade-off, and
reveal through ablation that task performance depends on task-appropriate delay
distributions, with temporally-complex tasks requiring longer delays. Our
results suggest temporal heterogeneity is an important principle for efficient
computation, particularly when task-relevant information is temporal - as in
the physical world - with implications for embodied intelligent systems and
neuromorphic hardware.</p>
  </article>
</body>
</html>
