<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-03.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 2]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-exploring-the-potentials-of-spiking-neural-networks-for-image-deraining">[1] <a href="https://arxiv.org/abs/2512.02258">Exploring the Potentials of Spiking Neural Networks for Image Deraining</a></h3>
<p><em>Shuang Chen, Tomas Krajnik, Farshad Arvin, Amir Atapour-Abarghouei</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒå»é›¨ä»»åŠ¡çš„è§†è§‰LIFï¼ˆVLIFï¼‰ç¥ç»å…ƒå’Œè„‰å†²ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œé€šè¿‡åˆ›æ–°çš„è„‰å†²åˆ†è§£å¢å¼ºæ¨¡å—å’Œå¤šå°ºåº¦å•å…ƒï¼Œåœ¨æ˜¾è‘—é™ä½èƒ½è€—çš„åŒæ—¶å®ç°äº†ä¼˜äºç°æœ‰SNNæ–¹æ³•çš„å»é›¨æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰åœ¨ä½å±‚è§†è§‰ä»»åŠ¡ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒå»é›¨ç­‰ä»»åŠ¡ä¸­ï¼Œä¼ ç»Ÿè„‰å†²ç¥ç»å…ƒç¼ºä¹ç©ºé—´ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ä¸”å­˜åœ¨é¢‘åŸŸé¥±å’Œé™åˆ¶ï¼Œè¿™é˜»ç¢äº†SNNsåœ¨ä½å±‚è§†è§‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æå‡ºäº†è§†è§‰LIFï¼ˆVLIFï¼‰ç¥ç»å…ƒæ¥å…‹æœä¼ ç»Ÿè„‰å†²ç¥ç»å…ƒåœ¨ç©ºé—´ä¸Šä¸‹æ–‡ç†è§£ä¸Šçš„ä¸è¶³ï¼Œå¹¶è®¾è®¡äº†è„‰å†²åˆ†è§£å¢å¼ºæ¨¡å—å’Œè½»é‡çº§è„‰å†²å¤šå°ºåº¦å•å…ƒï¼Œé€šè¿‡åˆ†å±‚å¤šå°ºåº¦è¡¨ç¤ºå­¦ä¹ æ¥è§£å†³é¢‘åŸŸé¥±å’Œé—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªåŸºå‡†å»é›¨æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„åŸºäºSNNçš„å»é›¨æ–¹æ³•ï¼ŒåŒæ—¶ä»…æ¶ˆè€—å…¶13%çš„èƒ½è€—ï¼Œå®ç°äº†é«˜æ€§èƒ½ä¸ä½èƒ½è€—çš„å¹³è¡¡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºåœ¨é«˜æ€§èƒ½ã€ä½èƒ½è€—çš„ä½å±‚è§†è§‰ä»»åŠ¡ä¸­éƒ¨ç½²SNNså¥ å®šäº†åšå®åŸºç¡€ï¼Œè¯æ˜äº†ç”Ÿç‰©å¯å‘çš„è„‰å†²ç¥ç»ç½‘ç»œæ¡†æ¶åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­çš„å®ç”¨æ€§å’Œæ•ˆç‡ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.</p>
<h3 id="2-temporal-dynamics-enhancer-for-directly-trained-spiking-object-detectors">[2] <a href="https://arxiv.org/abs/2512.02447">Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors</a></h3>
<p><em>Fan Luo, Zeyu Gao, Xinhao Luo, Kai Zhao, Yanfeng Lu</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTemporal Dynamics Enhancerï¼ˆTDEï¼‰çš„æ–¹æ³•æ¥å¢å¼ºè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„æ—¶åºä¿¡æ¯å»ºæ¨¡èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•é€šè¿‡Spiking Encoderç”Ÿæˆå¤šæ ·åŒ–çš„æ—¶é—´æ­¥è¾“å…¥åˆºæ¿€ï¼Œå¹¶é‡‡ç”¨Spike-Driven Attentioné™ä½æ³¨æ„åŠ›æœºåˆ¶çš„èƒ½é‡æ¶ˆè€—ï¼Œæ˜¾è‘—æå‡äº†SNNåœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰é€šå¸¸ç›´æ¥å¤åˆ¶è¾“å…¥æˆ–åœ¨å›ºå®šé—´éš”å†…èšåˆè¾“å…¥å¸§ï¼Œå¯¼è‡´ç¥ç»å…ƒåœ¨ä¸åŒæ—¶é—´æ­¥æ¥æ”¶å‡ ä¹ç›¸åŒçš„åˆºæ¿€ï¼Œä¸¥é‡é™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ï¼‰ä¸­çš„è¡¨è¾¾èƒ½åŠ›ï¼Œéœ€è¦å¢å¼ºSNNsçš„æ—¶åºä¿¡æ¯å»ºæ¨¡èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºTemporal Dynamics Enhancerï¼ˆTDEï¼‰æ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šSpiking Encoderï¼ˆSEï¼‰ç”¨äºåœ¨ä¸åŒæ—¶é—´æ­¥ç”Ÿæˆå¤šæ ·åŒ–çš„è¾“å…¥åˆºæ¿€ï¼Œä»¥åŠAttention Gating Moduleï¼ˆAGMï¼‰åŸºäºæ—¶é—´é—´ä¾èµ–æ€§æŒ‡å¯¼SEçš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œä¸ºäº†æ¶ˆé™¤AGMå¼•å…¥çš„é«˜èƒ½è€—ä¹˜æ³•æ“ä½œï¼Œæå‡ºäº†Spike-Driven Attentionï¼ˆSDAï¼‰æ¥æ˜¾è‘—é™ä½æ³¨æ„åŠ›ç›¸å…³çš„èƒ½é‡æ¶ˆè€—ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜TDEå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„SNNæ£€æµ‹å™¨ä¸­ï¼Œåœ¨é™æ€PASCAL VOCæ•°æ®é›†ä¸Šè¾¾åˆ°57.7%çš„mAP50-95åˆ†æ•°ï¼Œåœ¨ç¥ç»å½¢æ€EvDET200Kæ•°æ®é›†ä¸Šè¾¾åˆ°47.6%çš„mAP50-95åˆ†æ•°ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚åœ¨èƒ½è€—æ–¹é¢ï¼ŒSDAä»…æ¶ˆè€—ä¼ ç»Ÿæ³¨æ„åŠ›æ¨¡å—0.240å€çš„èƒ½é‡ã€‚</p>
<p><strong>Conclusion:</strong> TDEæ–¹æ³•æœ‰æ•ˆå¢å¼ºäº†SNNsçš„æ—¶åºå»ºæ¨¡èƒ½åŠ›ï¼Œè§£å†³äº†è¾“å…¥åˆºæ¿€å•ä¸€åŒ–çš„é—®é¢˜ï¼ŒåŒæ—¶é€šè¿‡Spike-Driven Attentionå®ç°äº†é«˜æ•ˆèƒ½è®¡ç®—ã€‚è¯¥ç ”ç©¶ä¸ºSNNsåœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°æ€è·¯ï¼Œå¹³è¡¡äº†æ€§èƒ½ä¸èƒ½è€—ï¼Œæ¨åŠ¨äº†ç¥ç»å½¢æ€è®¡ç®—åœ¨ç°å®åœºæ™¯ä¸­çš„å®ç”¨åŒ–å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="3-efficient-eye-based-emotion-recognition-via-neural-architecture-search-of-time-to-first-spike-coded-spiking-neural-networks">[3] <a href="https://arxiv.org/abs/2512.02459">Efficient Eye-based Emotion Recognition via Neural Architecture Search of Time-to-First-Spike-Coded Spiking Neural Networks</a></h3>
<p><em>Qianhui Liu, Jing Yang, Miao Yu, Trevor E. Carlson, Gang Pan, Haizhou Li, Zhumin Chen</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†TNAS-ERï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹çœ¼åŸºæƒ…æ„Ÿè¯†åˆ«çš„TTFSç¼–ç è„‰å†²ç¥ç»ç½‘ç»œçš„ç¥ç»æ¶æ„æœç´¢æ¡†æ¶ï¼Œé€šè¿‡ANNè¾…åŠ©æœç´¢ç­–ç•¥å’Œè¿›åŒ–ç®—æ³•ä¼˜åŒ–æ¶æ„ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†èƒ½æ•ˆã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> çœ¼åŸºæƒ…æ„Ÿè¯†åˆ«åœ¨èµ„æºå—é™çš„åµŒå…¥å¼ç¡¬ä»¶ä¸Šéƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ï¼Œè™½ç„¶TTFSç¼–ç çš„SNNsæä¾›äº†èƒ½æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨è®­ç»ƒç®—æ³•æ”¹è¿›ï¼Œè€Œç½‘ç»œæ¶æ„çš„å½±å“è¢«å¿½è§†ï¼Œéœ€è¦ä¸“é—¨é’ˆå¯¹TTFS SNNsçš„æ¶æ„ä¼˜åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> TNAS-ERæå‡ºäº†ä¸€ç§æ–°é¢–çš„ANNè¾…åŠ©æœç´¢ç­–ç•¥ï¼Œåˆ©ç”¨ä¸TTFS SNNå…±äº«æ’ç­‰æ˜ å°„çš„ReLUåŸºANNå¯¹åº”ç‰©æ¥æŒ‡å¯¼æ¶æ„ä¼˜åŒ–ï¼Œé‡‡ç”¨è¿›åŒ–ç®—æ³•å¹¶ä»¥åŠ æƒå’ŒéåŠ æƒå¹³å‡å¬å›ç‡ä½œä¸ºæƒ…æ„Ÿè¯†åˆ«çš„é€‚åº”åº¦ç›®æ ‡è¿›è¡Œè”åˆä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜TNAS-ERåœ¨ä¿æŒé«˜è¯†åˆ«æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ•ˆç‡ï¼Œåœ¨ç¥ç»å½¢æ€ç¡¬ä»¶ä¸Šéƒ¨ç½²æ—¶å®ç°äº†48æ¯«ç§’çš„ä½å»¶è¿Ÿå’Œ0.05ç„¦è€³çš„èƒ½é‡æ¶ˆè€—ï¼Œè¯å®äº†å…¶ä¼˜è¶Šçš„èƒ½æ•ˆå’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é’ˆå¯¹ç‰¹å®šSNNç¼–ç æ–¹æ¡ˆè¿›è¡Œæ¶æ„æœç´¢çš„é‡è¦æ€§ï¼ŒTNAS-ERæ¡†æ¶ä¸ºèµ„æºå—é™è®¾å¤‡ä¸Šçš„æƒ…æ„Ÿè¯†åˆ«åº”ç”¨æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†ç¥ç»å½¢æ€è®¡ç®—åœ¨å¯ç©¿æˆ´è®¾å¤‡ä¸­çš„å®é™…åº”ç”¨å‰æ™¯ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Eye-based emotion recognition enables eyewear devices to perceive users' emotional states and support emotion-aware interaction, yet deploying such functionality on their resource-limited embedded hardware remains challenging. Time-to-first-spike (TTFS)-coded spiking neural networks (SNNs) offer a promising solution, as each neuron emits at most one binary spike, resulting in extremely sparse and energy-efficient computation. While prior works have primarily focused on improving TTFS SNN training algorithms, the impact of network architecture has been largely overlooked. In this paper, we propose TNAS-ER, the first neural architecture search (NAS) framework tailored to TTFS SNNs for eye-based emotion recognition. TNAS-ER presents a novel ANN-assisted search strategy that leverages a ReLU-based ANN counterpart sharing an identity mapping with the TTFS SNN to guide architecture optimization. TNAS-ER employs an evolutionary algorithm, with weighted and unweighted average recall jointly defined as fitness objectives for emotion recognition. Extensive experiments demonstrate that TNAS-ER achieves high recognition performance with significantly improved efficiency. Furthermore, when deployed on neuromorphic hardware, TNAS-ER attains a low latency of 48 ms and an energy consumption of 0.05 J, confirming its superior energy efficiency and strong potential for practical applications.</p>
  </article>
</body>
</html>
