{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86SNN\u4e2d\u66ff\u4ee3\u68af\u5ea6\u7684\u659c\u7387\u8bbe\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7279\u6743\u5f15\u5bfc\u7b56\u7565\u7684\u65b0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5c06\u5e73\u5747\u56de\u62a5\u4ece-200\u70b9\u63d0\u5347\u81f3400\u70b9\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3SNN\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4e00\u662f\u8109\u51b2\u795e\u7ecf\u5143\u7684\u4e0d\u53ef\u5fae\u7279\u6027\u9700\u8981\u4f7f\u7528\u66ff\u4ee3\u68af\u5ea6\u4f46\u5176\u4f18\u5316\u7279\u6027\u4e0d\u660e\u786e\uff0c\u4e8c\u662fSNN\u7684\u72b6\u6001\u52a8\u6001\u9700\u8981\u5728\u5e8f\u5217\u4e0a\u8bad\u7ec3\uff0c\u800c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u65e9\u671f\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u6709\u9650\u4f1a\u963b\u788d\u7f51\u7edc\u5ea6\u8fc7\u9884\u70ed\u671f\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u7279\u6743\u5f15\u5bfc\u7b56\u7565\u6765\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u4ecd\u7136\u5229\u7528\u8109\u51b2\u7b56\u7565\u7684\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u8f83\u6d45\u7684\u659c\u7387\u6216\u8c03\u5ea6\u659c\u7387\u4f7f\u8bad\u7ec3\u548c\u6700\u7ec8\u90e8\u7f72\u6027\u80fd\u63d0\u9ad8\u4e862.1\u500d\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86400\u70b9\u7684\u5e73\u5747\u56de\u62a5\uff0c\u663e\u8457\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\u548cTD3BC\u7b49\u65b9\u6cd5\uff0c\u540e\u8005\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u6700\u591a\u53ea\u80fd\u8fbe\u5230-200\u70b9\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5bf9SNN\u4e2d\u66ff\u4ee3\u68af\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u4e3a\u9ad8\u6548\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2510.24231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24231", "abs": "https://arxiv.org/abs/2510.24231", "authors": ["Waseem Shariff", "Timothy Hanley", "Maciej Stec", "Hossein Javidnia", "Peter Corcoran"], "title": "Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation", "comment": "Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference", "summary": "Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5fae\u773c\u8df3\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7528\u4e8e\u5fae\u773c\u8df3\u89d2\u5ea6\u4f4d\u79fb\u5206\u7c7b\uff0c\u5b9e\u73b0\u4e86\u7ea690%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4e3a\u7cbe\u7ec6\u8fd0\u52a8\u8bc6\u522b\u548c\u4e8b\u4ef6\u89c6\u89c9\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u51c6\u3002", "motivation": "\u4f20\u7edf\u5fae\u773c\u8df3\u7814\u7a76\u4f9d\u8d56\u773c\u52a8\u4eea\u6216\u57fa\u4e8e\u5e27\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u867d\u7136\u7cbe\u786e\u4f46\u6210\u672c\u9ad8\u6602\u4e14\u65f6\u7a7a\u5206\u8fa8\u7387\u6709\u9650\uff0c\u800c\u4e8b\u4ef6\u611f\u77e5\u6280\u672f\u63d0\u4f9b\u4e86\u9ad8\u901f\u3001\u4f4e\u5ef6\u8fdf\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u6355\u6349\u7cbe\u7ec6\u7684\u65f6\u7a7a\u53d8\u5316\u3002", "method": "\u4f7f\u7528Blender\u6e32\u67d3\u9ad8\u4fdd\u771f\u773c\u52a8\u573a\u666f\uff0c\u6a21\u62df0.5\u81f32.0\u5ea6\u89d2\u4f4d\u79fb\u7684\u5fae\u773c\u8df3\u5e76\u5206\u4e3a\u4e03\u7c7b\uff0c\u901a\u8fc7v2e\u8f6c\u6362\u4e3a\u4e8b\u4ef6\u6d41\uff1b\u8bc4\u4f30\u4e86Spiking-VGG11/13/16\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u5149\u5b66\u6d41\u589e\u5f3a\u7684Spiking-VGG16Flow\u53d8\u4f53\uff0c\u5728SpikingJelly\u4e2d\u5b9e\u73b0\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u7ea690%\u7684\u5e73\u5747\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6210\u529f\u6839\u636e\u89d2\u5ea6\u4f4d\u79fb\u5bf9\u5fae\u773c\u8df3\u8fdb\u884c\u5206\u7c7b\uff0c\u4e14\u5206\u7c7b\u6027\u80fd\u4e0e\u4e8b\u4ef6\u6570\u91cf\u6216\u6301\u7eed\u65f6\u95f4\u65e0\u5173\uff0c\u5fae\u773c\u8df3\u6301\u7eed\u65f6\u95f4\u8303\u56f4\u4e3a0.25\u6beb\u79d2\u81f32.25\u6beb\u79d2\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u7cbe\u7ec6\u8fd0\u52a8\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u51c6\uff0c\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u8bad\u7ec3\u6a21\u578b\u5c06\u516c\u5f00\u63d0\u4f9b\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2510.24637", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24637", "abs": "https://arxiv.org/abs/2510.24637", "authors": ["Andrea Castagnetti", "Alain Pegatoquet", "Beno\u00eet Miramond"], "title": "All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level Spiking Neural Networks", "comment": null, "summary": "Spiking Neural Networks (SNNs) are one of the most promising bio-inspired\nneural networks models and have drawn increasing attention in recent years. The\nevent-driven communication mechanism of SNNs allows for sparse and\ntheoretically low-power operations on dedicated neuromorphic hardware. However,\nthe binary nature of instantaneous spikes also leads to considerable\ninformation loss in SNNs, resulting in accuracy degradation. To address this\nissue, we propose a multi-level spiking neuron model able to provide both\nlow-quantization error and minimal inference latency while approaching the\nperformance of full precision Artificial Neural Networks (ANNs). Experimental\nresults with popular network architectures and datasets, show that multi-level\nspiking neurons provide better information compression, allowing therefore a\nreduction in latency without performance loss. When compared to binary SNNs on\nimage classification scenarios, multi-level SNNs indeed allow reducing by 2 to\n3 times the energy consumption depending on the number of quantization\nintervals. On neuromorphic data, our approach allows us to drastically reduce\nthe inference latency to 1 timestep, which corresponds to a compression factor\nof 10 compared to previously published results. At the architectural level, we\npropose a new residual architecture that we call Sparse-ResNet. Through a\ncareful analysis of the spikes propagation in residual connections we highlight\na spike avalanche effect, that affects most spiking residual architectures.\nUsing our Sparse-ResNet architecture, we can provide state-of-the-art accuracy\nresults in image classification while reducing by more than 20% the network\nactivity compared to the previous spiking ResNets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\u548c\u7a00\u758fResNet\u67b6\u6784\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4e8620%\u4ee5\u4e0a\u7684\u7f51\u7edc\u6d3b\u52a8\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u5177\u6709\u7a00\u758f\u8ba1\u7b97\u548c\u4f4e\u529f\u8017\u7684\u7406\u8bba\u4f18\u52bf\uff0c\u4f46\u5176\u4e8c\u5143\u8109\u51b2\u7279\u6027\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u4fe1\u606f\u635f\u5931\u548c\u7cbe\u5ea6\u4e0b\u964d\uff0c\u8fd9\u9650\u5236\u4e86SNNs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u548c\u80fd\u6548\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u7ea7\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\u4ee5\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u5e76\u6700\u5c0f\u5316\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u7a00\u758fResNet\u67b6\u6784\u6765\u89e3\u51b3\u6b8b\u5dee\u8fde\u63a5\u4e2d\u7684\u8109\u51b2\u96ea\u5d29\u6548\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790\u8109\u51b2\u5728\u6b8b\u5dee\u8fde\u63a5\u4e2d\u7684\u4f20\u64ad\u7279\u6027\u4f18\u5316\u4e86\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u591a\u7ea7SNNs\u76f8\u6bd4\u4e8c\u5143SNNs\u53ef\u5c06\u80fd\u8017\u964d\u4f4e2-3\u500d\uff0c\u5728\u795e\u7ecf\u5f62\u6001\u6570\u636e\u4e0a\u80fd\u5c06\u63a8\u7406\u5ef6\u8fdf\u538b\u7f29\u81f31\u4e2a\u65f6\u95f4\u6b65\u957f\uff0c\u76f8\u6bd4\u5148\u524d\u7ed3\u679c\u5b9e\u73b0\u4e8610\u500d\u7684\u538b\u7f29\u56e0\u5b50\uff0c\u540c\u65f6\u7a00\u758fResNet\u67b6\u6784\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u7cbe\u5ea6\u7684\u57fa\u7840\u4e0a\u51cf\u5c11\u4e8620%\u4ee5\u4e0a\u7684\u7f51\u7edc\u6d3b\u52a8\u3002", "conclusion": "\u591a\u7ea7\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\u80fd\u591f\u6709\u6548\u5e73\u8861\u91cf\u5316\u8bef\u5dee\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u7ed3\u5408\u4f18\u5316\u7684\u7a00\u758fResNet\u67b6\u6784\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4e3aSNNs\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
