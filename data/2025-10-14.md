<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 15]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation](https://arxiv.org/abs/2510.09867)
*Zhi Chen, Xin Yu, Xiaohui Tao, Yan Li, Zi Huang*

#### 🧩 TL;DR
本文提出了集群感知提示集成学习（CAPEL）框架，通过将提示集成从特征空间转移到分类logits空间，并引入集群保持正则化和自适应提示加权技术，有效解决了传统提示集成方法中特征平均导致类别中心偏移的问题。

---

#### 📘 Detailed Summary
**Motivation:** 传统视觉语言模型中的提示集成方法通过平均文本特征来组合上下文提示，但这种方法往往产生次优结果，因为特征平均会使类别中心偏离真实的类别分布，无法有效保留上下文提示的集群特性。

**Method:** CAPEL框架将图像分类到多个类别集群中，每个集群由不同的提示表示，在分类logits空间而非特征空间进行提示集成，同时引入集群保持正则化项来维持提示的区分性，并采用自适应提示加权技术动态调整有缺陷或模糊提示的注意力权重。

**Result:** 该方法在多个数据集和任务上展现出鲁棒性能，通过保持提示的集群特性和优化集成策略，显著提升了零样本分类的准确性和稳定性。

**Conclusion:** 研究证明了在logits空间进行提示集成的有效性，强调了保持提示集群区分性的重要性，为视觉语言模型的提示优化提供了新的技术路径，具有广泛的适用性和推广价值。

---

#### 📄 Abstract
Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across
various tasks by pre-training on numerous image-text pairs. These models often
benefit from using an ensemble of context prompts to represent a class. Despite
being effective, conventional prompt ensembling that averages textual features
of context prompts often yields suboptimal results. This is because feature
averaging shifts the class centroids away from the true class distribution. To
address this issue, we propose the Cluster-Aware Prompt Ensemble Learning
(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL
classifies images into one of several class clusters, each represented by a
distinct prompt. Instead of ensembling prompts in the feature space, we perform
ensembling in the classification logits space, aligning better with the visual
feature distribution. To further optimize prompt fine-tuning while maintaining
cluster-specific discriminative power, we introduce a cluster-preserving
regularization term. This ensures that prompts remain distinct and specialized
for different clusters, preventing collapse into a uniform direction.
Additionally, we integrate an adaptive prompt weighting technique to
dynamically adjust the attention weights for flawed or ambiguous prompts,
ensuring robust performance across diverse datasets and tasks.


### [2] [Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting](https://arxiv.org/abs/2510.10257)
*Abdelrhman Elrawy, Emad A. Mohammed*

#### 🧩 TL;DR
本文提出了一种改进3D高斯泼溅在少样本场景下效率的框架，通过新颖的致密化触发机制和保守剪枝策略，在保持重建质量的同时显著减少基元数量，在少样本视图合成任务中建立了质量与效率的新帕累托前沿。

---

#### 📘 Detailed Summary
**Motivation:** 3D高斯泼溅在少样本场景下存在标准自适应密度控制导致过拟合和膨胀重建的问题，现有方法如FSGS虽然提高了质量但显著增加了基元数量，需要一种能够优先考虑效率的核心优化框架。

**Method:** 采用基于不透明度梯度的新型致密化触发机制替代标准位置梯度启发式方法，将其作为渲染误差的轻量级代理，并与更保守的剪枝策略相结合以防止破坏性优化循环，同时使用标准深度相关性损失提供几何指导。

**Result:** 在3视图LLFF数据集上，模型比FSGS紧凑40%以上（32k vs. 57k基元），在Mip-NeRF 360数据集上实现了约70%的基元数量减少，这些显著的紧凑性提升仅以重建指标的适度权衡为代价。

**Conclusion:** 该框架在少样本视图合成的质量与效率帕累托前沿上建立了新的最先进水平，证明了通过重新设计核心优化策略可以在保持重建质量的同时实现显著的效率提升，为资源受限的3D重建应用提供了实用解决方案。

---

#### 📄 Abstract
3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its
standard adaptive density control (ADC) can lead to overfitting and bloated
reconstructions. While state-of-the-art methods like FSGS improve quality, they
often do so by significantly increasing the primitive count. This paper
presents a framework that revises the core 3DGS optimization to prioritize
efficiency. We replace the standard positional gradient heuristic with a novel
densification trigger that uses the opacity gradient as a lightweight proxy for
rendering error. We find this aggressive densification is only effective when
paired with a more conservative pruning schedule, which prevents destructive
optimization cycles. Combined with a standard depth-correlation loss for
geometric guidance, our framework demonstrates a fundamental improvement in
efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k
vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a
reduction of approximately 70%. This dramatic gain in compactness is achieved
with a modest trade-off in reconstruction metrics, establishing a new
state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view
synthesis.


### [3] [AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration](https://arxiv.org/abs/2510.10395)
*Xinlong Chen, Yue Ding, Weihong Lin, Jingyun Hua, Linli Yao, Yang Shi, Bozhou Li, Yuanxing Zhang, Qiang Liu, Pengfei Wan, Liang Wang, Tieniu Tan*

#### 🧩 TL;DR
本文提出AVoCaDO，一种通过音频和视觉模态时间编排驱动的强大视听视频描述生成器，在多个基准测试中显著优于现有开源模型。

---

#### 📘 Detailed Summary
**Motivation:** 视听视频描述旨在生成语义丰富的描述，同时实现视觉和听觉事件之间的时间对齐，这对视频理解和生成都有益处。现有方法在时间对齐和模态协调方面存在不足，需要更有效的多模态融合机制。

**Method:** 采用两阶段后训练流程：第一阶段AVoCaDO SFT在新构建的10.7万高质量时间对齐视听描述数据集上进行微调；第二阶段AVoCaDO GRPO利用定制奖励函数增强时间一致性和对话准确性，同时规范化描述长度并减少崩溃。

**Result:** AVoCaDO在四个视听视频描述基准测试中显著优于现有开源模型，在仅视觉设置的VDC和DREAM-1K基准测试中也达到竞争性性能。

**Conclusion:** 该研究表明通过精心设计的时间编排和两阶段训练策略，可以有效提升视听视频描述的质量和时间对齐精度，为多模态视频理解提供了新的技术路径。

---

#### 📄 Abstract
Audiovisual video captioning aims to generate semantically rich descriptions
with temporal alignment between visual and auditory events, thereby benefiting
both video understanding and generation. In this paper, we present AVoCaDO, a
powerful audiovisual video captioner driven by the temporal orchestration
between audio and visual modalities. We propose a two-stage post-training
pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated
dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)
AVoCaDO GRPO, which leverages tailored reward functions to further enhance
temporal coherence and dialogue accuracy while regularizing caption length and
reducing collapse. Experimental results demonstrate that AVoCaDO significantly
outperforms existing open-source models across four audiovisual video
captioning benchmarks, and also achieves competitive performance on the VDC and
DREAM-1K benchmark under visual-only settings.


### [4] [GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction](https://arxiv.org/abs/2510.10546)
*Zuha Fatima, Muhammad Anser Sohaib, Muhammad Talha, Sidra Sultana, Ayesha Kanwal, Nazia Perwaiz*

#### 🧩 TL;DR
本研究提出了GLOFNet，一个用于冰川湖溃决洪水监测和预测的多模态数据集，整合了多光谱影像、冰川运动速度和地表温度数据，为罕见灾害预测提供了结构化基准。

---

#### 📘 Detailed Summary
**Motivation:** 冰川湖溃决洪水是高山地区罕见但破坏性强的灾害，现有研究受限于碎片化和单模态数据，缺乏结合视觉指标与物理前兆的协调数据集，阻碍了预测能力的发展。

**Method:** GLOFNet整合了三种互补数据源：Sentinel-2多光谱影像用于空间监测、NASA ITS_LIVE速度产品用于冰川运动学、MODIS地表温度记录跨越二十多年，并进行了云掩膜、质量过滤、归一化、时间插值、数据增强和循环编码等预处理。

**Result:** 探索性分析揭示了冰川速度的季节性周期、每十年约0.8K的长期升温趋势以及冰冻圈条件的空间异质性，数据集成功解决了类别不平衡、云污染和粗分辨率等挑战。

**Conclusion:** GLOFNet为罕见灾害预测提供了结构化基础，支持多模态深度学习方法在冰川灾害预测中的基准测试，公开可用性将促进未来灾害预测研究的发展。

---

#### 📄 Abstract
Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high
mountain regions, yet predictive research is hindered by fragmented and
unimodal data. Most prior efforts emphasize post-event mapping, whereas
forecasting requires harmonized datasets that combine visual indicators with
physical precursors. We present GLOFNet, a multimodal dataset for GLOF
monitoring and prediction, focused on the Shisper Glacier in the Karakoram. It
integrates three complementary sources: Sentinel-2 multispectral imagery for
spatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and
MODIS Land Surface Temperature records spanning over two decades. Preprocessing
included cloud masking, quality filtering, normalization, temporal
interpolation, augmentation, and cyclical encoding, followed by harmonization
across modalities. Exploratory analysis reveals seasonal glacier velocity
cycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in
cryospheric conditions. The resulting dataset, GLOFNet, is publicly available
to support future research in glacial hazard prediction. By addressing
challenges such as class imbalance, cloud contamination, and coarse resolution,
GLOFNet provides a structured foundation for benchmarking multimodal deep
learning approaches to rare hazard prediction.


### [5] [Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes](https://arxiv.org/abs/2510.10577)
*Haonan Wang, Hanyu Zhou, Haoyue Liu, Luxin Yan*

#### 🧩 TL;DR
本文提出了一种基于扩散模型的帧-事件外观边界融合光流估计框架Diff-ABFlow，通过扩散模型学习从噪声流到清晰流的映射，解决了高速和低光场景下运动模糊和光照不足导致的光流估计挑战。

---

#### 📘 Detailed Summary
**Motivation:** 传统光流估计方法在高速和低光场景中面临运动模糊和光照不足的挑战，导致纹理减弱、噪声放大以及帧相机的外观饱和度和边界完整性恶化。现有方法虽然通过特征融合或域适应引入事件相机来改善边界完整性，但外观特征仍然恶化，严重影响判别式模型和生成式模型的性能。

**Method:** 提出Diff-ABFlow框架，基于扩散模型实现帧-事件外观边界融合。该方法利用扩散模型学习从噪声流到清晰流的映射过程，不依赖于恶化的视觉特征，结合帧相机提供的外观饱和度和事件相机提供的边界完整性优势。

**Result:** 该方法在高速和低光退化场景中有效改善了光流估计性能，通过扩散模型对噪声流的去噪过程生成清晰的光流场，克服了传统方法在视觉特征恶化情况下的性能限制。

**Conclusion:** 扩散模型为光流估计提供了新的范式，通过不依赖恶化视觉特征的学习机制，在挑战性场景中实现了更鲁棒的估计性能。帧-事件传感器的互补特性结合扩散模型的生成能力，为计算机视觉在极端条件下的应用开辟了新方向。

---

#### 📄 Abstract
Optical flow estimation has achieved promising results in conventional scenes
but faces challenges in high-speed and low-light scenes, which suffer from
motion blur and insufficient illumination. These conditions lead to weakened
texture and amplified noise and deteriorate the appearance saturation and
boundary completeness of frame cameras, which are necessary for motion feature
matching. In degraded scenes, the frame camera provides dense appearance
saturation but sparse boundary completeness due to its long imaging time and
low dynamic range. In contrast, the event camera offers sparse appearance
saturation, while its short imaging time and high dynamic range gives rise to
dense boundary completeness. Traditionally, existing methods utilize feature
fusion or domain adaptation to introduce event to improve boundary
completeness. However, the appearance features are still deteriorated, which
severely affects the mostly adopted discriminative models that learn the
mapping from visual features to motion fields and generative models that
generate motion fields based on given visual features. So we introduce
diffusion models that learn the mapping from noising flow to clear flow, which
is not affected by the deteriorated visual features. Therefore, we propose a
novel optical flow estimation framework Diff-ABFlow based on diffusion models
with frame-event appearance-boundary fusion.


### [6] [A Machine Learning Perspective on Automated Driving Corner Cases](https://arxiv.org/abs/2510.10653)
*Sebastian Schmidt, Julius Körner, Stephan Günnemann*

#### 🧩 TL;DR
本文提出了一种基于数据分布视角的机器学习方法，用于自动驾驶等高风险应用中的角点案例识别，通过统一现有场景分类并实现有效的个体样本感知检测。

---

#### 📘 Detailed Summary
**Motivation:** 传统角点案例处理方法采用基于示例的分类方式，缺乏可扩展性且忽略了机器学习模型训练数据的泛化能力，无法从数据覆盖角度系统解决高风险应用中的安全问题。

**Method:** 提出了一种考虑底层数据分布的新型机器学习框架，该框架基于分布视角统一现有场景分类，并针对个体样本实现有效的角点案例识别。

**Result:** 该方法在标准基准测试中表现出强大的角点案例检测性能，统一了现有场景分类体系，并通过新引入的雾增强Lost & Found数据集支持组合角点案例分析。

**Conclusion:** 研究为角点案例识别提供了原则性基础，强调了无需人工规范的定义方式，为高风险应用的安全操作提供了系统化的解决方案。

---

#### 📄 Abstract
For high-stakes applications, like autonomous driving, a safe operation is
necessary to prevent harm, accidents, and failures. Traditionally, difficult
scenarios have been categorized into corner cases and addressed individually.
However, this example-based categorization is not scalable and lacks a data
coverage perspective, neglecting the generalization to training data of machine
learning models. In our work, we propose a novel machine learning approach that
takes the underlying data distribution into account. Based on our novel
perspective, we present a framework for effective corner case recognition for
perception on individual samples. In our evaluation, we show that our approach
(i) unifies existing scenario-based corner case taxonomies under a
distributional perspective, (ii) achieves strong performance on corner case
detection tasks across standard benchmarks for which we extend established
out-of-distribution detection benchmarks, and (iii) enables analysis of
combined corner cases via a newly introduced fog-augmented Lost & Found
dataset. These results provide a principled basis for corner case recognition,
underlining our manual specification-free definition.


### [7] [Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection](https://arxiv.org/abs/2510.10750)
*Laura Weihl, Nejc Novak, Stefan H. Bengtson, Malte Pedersen*

#### 🧩 TL;DR
本文提出了AURA——首个多标注者水下视觉异常检测基准数据集，评估了四种VAD模型在两种海洋场景中的性能，揭示了当前模型对训练数据量和正常场景视觉内容变异性的高度敏感性。

---

#### 📘 Detailed Summary
**Motivation:** 水下视频监测是评估海洋生物多样性的有效策略，但海量无事件视频使得人工检查极不实用，需要开发自动化的视觉异常检测方法来识别有趣或异常事件。

**Method:** 研究引入了AURA多标注者基准数据集，评估了四种基于深度神经网络的视觉异常检测模型，并强调了鲁棒帧选择策略对于提取有意义视频片段的重要性。

**Result:** 实验结果表明当前VAD模型的性能差异显著，对训练数据量和定义正常场景的视觉内容变异性高度敏感，同时验证了软标签和共识标签的价值。

**Conclusion:** 该研究为支持科学探索和可扩展的生物多样性监测提供了实用方法，强调了多标注者基准在评估视觉异常检测模型中的重要性，并为水下监控系统的开发提供了关键见解。

---

#### 📄 Abstract
Underwater video monitoring is a promising strategy for assessing marine
biodiversity, but the vast volume of uneventful footage makes manual inspection
highly impractical. In this work, we explore the use of visual anomaly
detection (VAD) based on deep neural networks to automatically identify
interesting or anomalous events. We introduce AURA, the first multi-annotator
benchmark dataset for underwater VAD, and evaluate four VAD models across two
marine scenes. We demonstrate the importance of robust frame selection
strategies to extract meaningful video segments. Our comparison against
multiple annotators reveals that VAD performance of current models varies
dramatically and is highly sensitive to both the amount of training data and
the variability in visual content that defines "normal" scenes. Our results
highlight the value of soft and consensus labels and offer a practical approach
for supporting scientific exploration and scalable biodiversity monitoring.


### [8] [Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation](https://arxiv.org/abs/2510.11005)
*Kai Han, Siqi Ma, Chengxuan Qian, Jun Chen, Chongwen Lyu, Yuqing Song, Zhe Liu*

#### 🧩 TL;DR
本文提出了前景感知频谱分割（FASS）框架，通过前景感知模块、基于小波变换的频率增强模块和边缘约束模块，显著提升了复杂低对比度医学图像中肿瘤分割的准确性和鲁棒性。

---

#### 📘 Detailed Summary
**Motivation:** 基础模型在分割任务中通常表现良好，但在复杂低对比度背景下难以聚焦前景区域，特别是当恶性肿瘤与正常器官形态相似时，上下文区分变得困难，这限制了医学图像分割在手术规划和肿瘤分期中的准确性。

**Method:** 提出了前景感知频谱分割（FASS）框架，包含三个核心模块：前景感知模块用于放大背景与整个体积空间的区分度，基于小波变换的特征级频率增强模块提取判别性高频特征以增强边界识别和细节感知，以及边缘约束模块用于保持分割边界的几何连续性。

**Result:** 在多个医学数据集上的广泛实验表明，该框架在所有指标上均表现出优越性能，特别是在复杂条件下的鲁棒性和精细结构识别方面验证了其有效性，显著提升了低对比度图像的分割质量。

**Conclusion:** 该框架显著增强了低对比度医学图像的分割能力，为更广泛和复杂的医学成像场景应用铺平了道路，在手术规划和肿瘤分期等关键医疗任务中具有重要应用价值。

---

#### 📄 Abstract
Accurate segmentation of tumors and adjacent normal tissues in medical images
is essential for surgical planning and tumor staging. Although foundation
models generally perform well in segmentation tasks, they often struggle to
focus on foreground areas in complex, low-contrast backgrounds, where some
malignant tumors closely resemble normal organs, complicating contextual
differentiation. To address these challenges, we propose the Foreground-Aware
Spectrum Segmentation (FASS) framework. First, we introduce a foreground-aware
module to amplify the distinction between background and the entire volume
space, allowing the model to concentrate more effectively on target areas.
Next, a feature-level frequency enhancement module, based on wavelet transform,
extracts discriminative high-frequency features to enhance boundary recognition
and detail perception. Eventually, we introduce an edge constraint module to
preserve geometric continuity in segmentation boundaries. Extensive experiments
on multiple medical datasets demonstrate superior performance across all
metrics, validating the effectiveness of our framework, particularly in
robustness under complex conditions and fine structure recognition. Our
framework significantly enhances segmentation of low-contrast images, paving
the way for applications in more diverse and complex medical imaging scenarios.


### [9] [LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation](https://arxiv.org/abs/2510.11063)
*Chang Liu, Henghui Ding, Kaining Ying, Lingyi Hong, Ning Xu, Linjie Yang, Yuchen Fan, Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han, Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Chang Soo Lim, Joonyoung Moon, Donghyeon Cho, Tingmin Li, Yixuan Li, Yang Yang, An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu, Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan, Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji, Ran Hong, Feng Lu, Leilei Cao, An Yan, Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian Leibe*

#### 🧩 TL;DR
本文介绍了ICCV 2025第七届大规模视频对象分割挑战赛，在传统VOS和RVOS赛道基础上新增了复杂度更高的MOSEv2赛道，通过引入更现实的挑战场景推动长期一致性和泛化能力的发展。

---

#### 📘 Detailed Summary
**Motivation:** 该研究旨在解决当前视频对象分割在现实复杂场景中的局限性，传统基准测试难以覆盖密集小目标、频繁消失重现、严重遮挡、恶劣天气和光照等挑战性条件，需要推动算法在非精心策划的真实环境中的长期一致性和泛化能力。

**Method:** 挑战赛保留了VOS和RVOS的传统赛道评估标准，同时新引入MOSEv2赛道采用J&Ḟ作为主要排名指标以更好地评估跨尺度和消失情况下的对象分割性能，重点关注LLM/MLLM组件和内存感知传播等新兴技术趋势。

**Result:** 挑战赛采用标准J、F和J&F指标评估VOS和RVOS赛道性能，而MOSEv2赛道采用J&Ḟ作为主要排名指标，通过更复杂的现实场景设置显著提升了评估难度，为算法在真实环境中的鲁棒性提供了更全面的基准测试。

**Conclusion:** 该挑战赛通过引入更具挑战性的现实场景和评估指标，为视频对象分割领域指明了未来发展方向，特别是语言感知视频分割在野外环境中的韧性发展，强调了LLM/MLLM组件和内存感知传播技术的重要性。

---

#### 📄 Abstract
This report presents an overview of the 7th Large-scale Video Object
Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2). Building upon prior
insights, MOSEv2 substantially increases difficulty, introducing more
challenging but realistic scenarios including denser small objects, frequent
disappear/reappear events, severe occlusions, adverse weather and lighting,
etc., pushing long-term consistency and generalization beyond curated
benchmarks. The challenge retains standard ${J}$, $F$, and ${J\&F}$ metrics for
VOS and RVOS, while MOSEv2 adopts ${J\&\dot{F}}$ as the primary ranking metric
to better evaluate objects across scales and disappearance cases. We summarize
datasets and protocols, highlight top-performing solutions, and distill
emerging trends, such as the growing role of LLM/MLLM components and
memory-aware propagation, aiming to chart future directions for resilient,
language-aware video segmentation in the wild.


### [10] [LightPneumoNet: Lightweight Pneumonia Classifier](https://arxiv.org/abs/2510.11232)
*Neilansh Chauhan, Piyush Kumar Gupta, Faraz Doja*

#### 🧩 TL;DR
本研究提出了LightPneumoNet，一种轻量级卷积神经网络，专门用于在资源受限环境中实现高效准确的肺炎X光诊断。该模型仅含388,082个参数，在保持高敏感度的同时显著降低了计算需求。

---

#### 📘 Detailed Summary
**Motivation:** 现有肺炎诊断深度学习模型通常计算量大且部署成本高，难以在资源有限的医疗环境中应用。本研究旨在开发一种轻量级但准确的解决方案，为医疗资源匮乏地区提供可访问的计算机辅助诊断工具。

**Method:** 研究构建了自定义轻量级CNN架构LightPneumoNet，包含四个卷积块堆叠，仅388,082个可训练参数。采用224x224图像尺寸、灰度转换和像素归一化预处理，并通过旋转、缩放和剪切等数据增强技术防止过拟合。

**Result:** 在5,856张胸部X光数据集上的测试显示，模型达到0.942的总体准确率、0.92的精确率和0.96的F1分数。关键指标敏感度达到0.99，几乎完美识别真实肺炎病例，显著优于现有需要更重架构的方法。

**Conclusion:** LightPneumoNet证明了轻量级架构在医学影像诊断中的有效性，其1.48MB的内存占用使其能够在低成本硬件上部署。该模型可作为可靠的第二意见工具，改善医疗服务不足地区的患者诊疗结果，推动了可访问医疗AI的发展。

---

#### 📄 Abstract
Effective pneumonia diagnosis is often challenged by the difficulty of
deploying large, computationally expensive deep learning models in
resource-limited settings. This study introduces LightPneumoNet, an efficient,
lightweight convolutional neural network (CNN) built from scratch to provide an
accessible and accurate diagnostic solution for pneumonia detection from chest
X-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.
Preprocessing included image resizing to 224x224, grayscale conversion, and
pixel normalization, with data augmentation (rotation, zoom, shear) to prevent
overfitting. The custom architecture features four blocks of stacked
convolutional layers and contains only 388,082 trainable parameters, resulting
in a minimal 1.48 MB memory footprint. On the independent test set, our model
delivered exceptional performance, achieving an overall accuracy of 0.942,
precision of 0.92, and an F1-Score of 0.96. Critically, it obtained a
sensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify
true pneumonia cases and minimize clinically significant false negatives.
Notably, LightPneumoNet achieves this high recall on the same dataset where
existing approaches typically require significantly heavier architectures or
fail to reach comparable sensitivity levels. The model's efficiency enables
deployment on low-cost hardware, making advanced computer-aided diagnosis
accessible in underserved clinics and serving as a reliable second-opinion tool
to improve patient outcomes.


### [11] [When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models](https://arxiv.org/abs/2510.11302)
*Samer Al-Hamadani*

#### 🧩 TL;DR
本文首次对监督学习检测器(YOLO)与零样本视觉语言模型(Gemini Flash 2.5)进行成本效益分析，建立了基于推理量、类别稳定性和预算约束的架构选择决策框架，揭示了监督学习仅在超过5500万次推理时具有成本优势。

---

#### 📘 Detailed Summary
**Motivation:** 传统监督目标检测需要大量人工标注成本，而零样本视觉语言模型虽然无需标注但精度较低，当前缺乏系统性的成本效益分析来指导在实际部署中如何在这两种范式之间进行选择。

**Method:** 采用系统化评估方法，在1000张分层采样的COCO图像和200张多样化产品图像上进行对比实验，结合详细的总拥有成本建模，建立了量化的盈亏平衡阈值来指导架构选择。

**Result:** 监督YOLO在标准类别上达到91.2%准确率，显著高于零样本Gemini的68.5%，但需要10800美元的标注成本；在多样化产品类别上，Gemini达到52.3%准确率而YOLO为0%；每正确检测成本分析显示Gemini在10万次推理时成本仅为0.00050美元，远低于YOLO的0.143美元。

**Conclusion:** 最优架构选择关键取决于部署规模、类别稳定性、预算约束和精度要求而非纯技术性能指标，监督学习仅在推理量超过5500万次（相当于每日15.1万张图像持续一年）时具有成本合理性，为零样本视觉语言模型在特定场景下的应用提供了量化决策依据。

---

#### 📄 Abstract
Object detection systems have traditionally relied on supervised learning
with manually annotated bounding boxes, achieving high accuracy at the cost of
substantial annotation investment. The emergence of Vision-Language Models
(VLMs) offers an alternative paradigm enabling zero-shot detection through
natural language queries, eliminating annotation requirements but operating
with reduced accuracy. This paper presents the first comprehensive
cost-effectiveness analysis comparing supervised detection (YOLO) with
zero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on
1,000 stratified COCO images and 200 diverse product images spanning consumer
electronics and rare categories, combined with detailed Total Cost of Ownership
modeling, we establish quantitative break-even thresholds governing
architecture selection. Our findings reveal that supervised YOLO achieves 91.2%
accuracy versus 68.5% for zero-shot Gemini on standard categories, representing
a 22.7 percentage point advantage that costs $10,800 in annotation for
100-category systems. However, this advantage justifies investment only beyond
55 million inferences, equivalent to 151,000 images daily for one year.
Zero-shot Gemini demonstrates 52.3% accuracy on diverse product categories
(ranging from highly web-prevalent consumer electronics at 75-85% to rare
specialized equipment at 25-40%) where supervised YOLO achieves 0% due to
architectural constraints preventing detection of untrained classes. Cost per
Correct Detection analysis reveals substantially lower per-detection costs for
Gemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We
develop decision frameworks demonstrating that optimal architecture selection
depends critically on deployment volume, category stability, budget
constraints, and accuracy requirements rather than purely technical performance
metrics.


### [12] [Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation](https://arxiv.org/abs/2510.11305)
*Jean-Paul Travert, Cédric Goeury, Sébastien Boyaval, Vito Bacchi, Fabrice Zaoui*

#### 🧩 TL;DR
本研究评估了SAR图像预处理、洪水制图和水深估计方法的选择及其超参数对洪水模拟结果的影响，发现在整个处理流程中采用集成方法并考虑方法不确定性至关重要。洪水制图方法的选择对结果影响最大，而水深估计则主要受洪水制图步骤输出的影响。

---

#### 📘 Detailed Summary
**Motivation:** 本研究旨在解决SAR图像洪水制图和水深估计中方法选择及其超参数对结果影响的不确定性问题，特别是评估不同预处理方法、洪水制图算法和水深估计技术在洪水模拟中的综合影响。

**Method:** 研究采用集成方法评估多种SAR图像预处理技术、洪水制图方法和水深估计算法，包括斑点噪声滤波、监督与非监督分类方法、局部阈值法和变化检测等，并在法国加龙河2019和2021年两次洪水事件上进行验证。

**Result:** 结果显示斑点滤波器的选择会导致洪水范围估计出现数平方公里的变化，监督方法优于非监督方法但调优后的非监督方法可达到相似性能，预处理和洪水制图步骤的累积不确定性导致水深场估计存在高度变异性。

**Conclusion:** 研究强调必须考虑包含预处理、洪水制图和水深估计的完整处理流程及其超参数，建议采用集成方法而非单一配置，洪水制图方法选择影响最大，而水深估计主要受洪水制图步骤输出和方法超参数影响。

---

#### 📄 Abstract
Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)
imagery are crucial for calibrating and validating hydraulic models. This study
uses SAR imagery to evaluate various preprocessing (especially speckle noise
reduction), flood mapping, and water depth estimation methods. The impact of
the choice of method at different steps and its hyperparameters is studied by
considering an ensemble of preprocessed images, flood maps, and water depth
fields. The evaluation is conducted for two flood events on the Garonne River
(France) in 2019 and 2021, using hydrodynamic simulations and in-situ
observations as reference data. Results show that the choice of speckle filter
alters flood extent estimations with variations of several square kilometers.
Furthermore, the selection and tuning of flood mapping methods also affect
performance. While supervised methods outperformed unsupervised ones, tuned
unsupervised approaches (such as local thresholding or change detection) can
achieve comparable results. The compounded uncertainty from preprocessing and
flood mapping steps also introduces high variability in the water depth field
estimates. This study highlights the importance of considering the entire
processing pipeline, encompassing preprocessing, flood mapping, and water depth
estimation methods and their associated hyperparameters. Rather than relying on
a single configuration, adopting an ensemble approach and accounting for
methodological uncertainty should be privileged. For flood mapping, the method
choice has the most influence. For water depth estimation, the most influential
processing step was the flood map input resulting from the flood mapping step
and the hyperparameters of the methods.


### [13] [SNAP: Towards Segmenting Anything in Any Point Cloud](https://arxiv.org/abs/2510.11565)
*Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang*

#### 🧩 TL;DR
本文提出了SNAP，一个统一的交互式3D点云分割模型，支持跨室内、室外和航空领域的点基和文本基提示，通过多数据集训练和领域自适应归一化实现卓越的跨域泛化能力。

---

#### 📘 Detailed Summary
**Motivation:** 当前交互式3D点云分割方法存在领域限制（仅支持室内或室外）和交互方式单一（仅支持空间点击或文本提示）的问题，且多数据集训练常导致负迁移，缺乏通用性。

**Method:** 采用多数据集训练策略，在7个跨领域数据集上进行训练，使用领域自适应归一化防止负迁移；对于文本提示分割，自动生成掩码提案并与CLIP嵌入的文本查询进行匹配，实现全景和开放词汇分割。

**Result:** 在9个零样本空间提示分割基准中，8个达到最先进性能；在5个文本提示基准中均取得竞争性结果，证明统一模型可匹敌或超越专业领域特定方法。

**Conclusion:** 研究表明统一模型能够匹配或超越专业领域特定方法，为可扩展的3D标注提供了实用工具，证明了跨域通用交互分割的可行性。

---

#### 📄 Abstract
Interactive 3D point cloud segmentation enables efficient annotation of
complex 3D scenes through user-guided prompts. However, current approaches are
typically restricted in scope to a single domain (indoor or outdoor), and to a
single form of user interaction (either spatial clicks or textual prompts).
Moreover, training on multiple datasets often leads to negative transfer,
resulting in domain-specific tools that lack generalizability. To address these
limitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in
\textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3D
segmentation that supports both point-based and text-based prompts across
diverse domains. Our approach achieves cross-domain generalizability by
training on 7 datasets spanning indoor, outdoor, and aerial environments, while
employing domain-adaptive normalization to prevent negative transfer. For
text-prompted segmentation, we automatically generate mask proposals without
human intervention and match them against CLIP embeddings of textual queries,
enabling both panoptic and open-vocabulary segmentation. Extensive experiments
demonstrate that SNAP consistently delivers high-quality segmentation results.
We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for
spatial-prompted segmentation and demonstrate competitive results on all 5
text-prompted benchmarks. These results show that a unified model can match or
exceed specialized domain-specific approaches, providing a practical tool for
scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/


### [14] [MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.11579)
*Hongyu Zhu, Lin Chen, Mounim A. El-Yacoubi, Mingsheng Shang*

#### 🧩 TL;DR
本文提出MS-Mix，一种自适应情感敏感的多模态数据增强框架，通过情感感知样本选择、情感强度引导的混合比例计算和情感对齐损失，有效解决了多模态情感分析中Mixup增强导致的标签模糊和语义不一致问题。

---

#### 📘 Detailed Summary
**Motivation:** 多模态情感分析面临标注数据稀缺的挑战，而传统Mixup增强方法直接应用于多模态场景会因随机混合导致标签模糊和语义不一致，缺乏针对情感感知的混合机制。

**Method:** MS-Mix框架包含三个关键组件：情感感知样本选择策略防止情感矛盾样本混合造成语义混淆；情感强度引导模块使用多头自注意力动态计算模态特定混合比例；情感对齐损失通过KL散度正则化联合训练情感强度预测器和主干网络。

**Result:** 在三个基准数据集和六个最先进主干网络上的广泛实验表明，MS-Mix持续优于现有方法，为鲁棒的多模态情感增强建立了新标准。

**Conclusion:** 该研究证明了情感感知混合机制在多模态数据增强中的有效性，为缓解多模态情感分析的数据稀缺问题提供了新思路，同时通过模态间情感对齐提升了模型的泛化能力。

---

#### 📄 Abstract
Multimodal Sentiment Analysis (MSA) aims to identify and interpret human
emotions by integrating information from heterogeneous data sources such as
text, video, and audio. While deep learning models have advanced in network
architecture design, they remain heavily limited by scarce multimodal annotated
data. Although Mixup-based augmentation improves generalization in unimodal
tasks, its direct application to MSA introduces critical challenges: random
mixing often amplifies label ambiguity and semantic inconsistency due to the
lack of emotion-aware mixing mechanisms. To overcome these issues, we propose
MS-Mix, an adaptive, emotion-sensitive augmentation framework that
automatically optimizes sample mixing in multimodal settings. The key
components of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)
strategy that effectively prevents semantic confusion caused by mixing samples
with contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module
using multi-head self-attention to compute modality-specific mixing ratios
dynamically based on their respective emotional intensities. (3) a Sentiment
Alignment Loss (SAL) that aligns the prediction distributions across
modalities, and incorporates the Kullback-Leibler-based loss as an additional
regularization term to train the emotion intensity predictor and the backbone
network jointly. Extensive experiments on three benchmark datasets with six
state-of-the-art backbones confirm that MS-Mix consistently outperforms
existing methods, establishing a new standard for robust multimodal sentiment
augmentation. The source code is available at:
https://github.com/HongyuZhu-s/MS-Mix.


### [15] [Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams](https://arxiv.org/abs/2510.11717)
*Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik*

#### 🧩 TL;DR
本文提出了Ev4DGS，这是首个从单目事件流中实现非刚性变形物体新视角渲染的方法，通过回归可变形3D高斯泼溅表示，仅使用事件数据而无需RGB输入。

---

#### 📘 Detailed Summary
**Motivation:** 现有事件相机方法在处理非刚性物体时需要额外的稀疏RGB输入，这在实践中存在显著限制，目前尚不清楚能否仅从事件流中学习类似模型，本文旨在解决这一开放性问题。

**Method:** 该方法通过两种机制回归可变形3D高斯泼溅表示：一是将估计模型输出与2D事件观测空间关联的损失函数，二是从事件生成的二值掩码训练得到的粗粒度3D变形模型。

**Result:** 在现有合成数据集和新录制的真实非刚性物体数据集上的实验比较表明，Ev4DGS的有效性及其相对于多种可应用于该设置的朴素基线的优越性能。

**Conclusion:** 研究证明了仅从事件流学习非刚性变形物体新视角渲染的可行性，为事件相机在动态场景建模中的应用开辟了新方向，并将发布模型和评估数据集供研究使用。

---

#### 📄 Abstract
Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi, Zheyuan Zhang, Zhengqing Yuan, Keerthiram Murugesan, Vincent Galass, Chuxu Zhang, Yanfang Ye*

#### 🧩 TL;DR
本文提出了NG-Router框架，将营养问答建模为基于知识图谱的多智能体协作问题，通过图神经网络学习任务感知的路由分布，有效解决了单智能体推理能力有限和多智能体架构设计复杂的问题。

---

#### 📘 Detailed Summary
**Motivation:** 现有营养问答方法面临两个基本挑战：单智能体系统的推理能力有限，以及设计有效的多智能体架构的复杂性，同时上下文过载问题阻碍了准确的决策制定，这限制了个性化饮食指导和预防饮食相关慢性疾病的应用潜力。

**Method:** NG-Router将智能体节点集成到异构知识图谱中，采用图神经网络学习基于智能体经验性能的任务感知路由分布，并提出基于梯度的子图检索机制以识别训练过程中的关键证据，从而增强多跳和关系推理能力。

**Result:** 在多个基准测试和骨干模型上的广泛实验表明，NG-Router在性能上持续优于单智能体和集成基线方法，为复杂营养健康任务提供了领域感知多智能体推理的原则性解决方案。

**Conclusion:** 该研究为复杂营养健康任务提供了领域感知多智能体推理的原则性方法，展示了知识图谱引导的多智能体协作在解决专业领域复杂问答问题中的有效性，为个性化健康指导系统的发展开辟了新途径。

---

#### 📄 Abstract
Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.


### [17] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton, Matthew Wilkens, Andrew Piper*

#### 🧩 TL;DR
本文提出了NarraBench——一个基于理论构建的叙事理解任务分类法，并对该领域的78个现有基准进行了系统调查，揭示了当前评估在叙事事件、风格、视角和启示等关键方面的严重缺失。

---

#### 📘 Detailed Summary
**Motivation:** 当前叙事理解领域缺乏系统性的任务分类和基准评估，许多重要的叙事方面如事件、风格、视角和启示在现有评估中几乎完全缺失，且现有基准仅覆盖约27%的叙事任务，无法有效评估构成性主观和视角化方面的理解能力。

**Method:** 研究提出了理论驱动的叙事理解任务分类法NarraBench，并系统调查了该领域的78个现有基准，开发了评估叙事理解能力的方法论框架，特别关注构成性主观和视角化方面的评估需求。

**Result:** 调查发现现有基准仅能覆盖27%的叙事任务，叙事事件、风格、视角和启示等关键维度在现有评估中几乎完全缺失，同时识别出需要开发能够评估构成性主观和视角化方面的基准的迫切需求。

**Conclusion:** 该研究为NLP研究者测试大语言模型的叙事理解能力提供了有价值的分类法、调查和方法论，强调了开发覆盖更全面叙事维度评估基准的重要性，特别是针对主观性和视角化理解方面的评估工具。

---

#### 📄 Abstract
We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.


### [18] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee, Shivam Sonawane, Amey Hengle, Aditya Tanna, Sourish Dasgupta, Tanmoy Chakraborty*

#### 🧩 TL;DR
本文提出PerAugy数据增强技术，通过跨轨迹混洗和摘要内容扰动解决个性化摘要训练数据稀缺问题，显著提升了四种最先进用户编码器的准确性，并增强了两个摘要框架的个性化性能。

---

#### 📘 Detailed Summary
**Motivation:** 个性化摘要面临训练数据稀缺的挑战，现有MS/CAS PENS数据集仅包含用户偏好历史而缺乏目标摘要，无法进行端到端监督学习，且主题转换多样性有限限制了模型泛化能力。

**Method:** 提出PerAugy数据增强技术，采用跨轨迹混洗和摘要内容扰动方法，增强训练数据的多样性，并引入三个数据集多样性度量指标TP、RTC和DegreeD来量化诱导的多样性。

**Result:** PerAugy显著提升了四种最先进用户编码器的准确性（最佳结果：AUC提升0.132），增强后的摘要框架个性化性能平均提升61.2%（基于PSE-SU4指标），TP和DegreeD多样性指标与用户编码器性能呈强相关性。

**Conclusion:** 数据集多样性是驱动性能提升的关键因素，PerAugy通过有效的数据增强解决了个性化摘要的数据稀缺问题，为个性化摘要系统的开发提供了重要技术支撑。

---

#### 📄 Abstract
Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.


### [19] [You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News](https://arxiv.org/abs/2510.10658)
*Guy Mor-Lan, Tamir Sheafer, Shaul R. Shenhav*

#### 🧩 TL;DR
本研究通过大规模比较CNN和福克斯新闻，量化了党派媒体在构建事实报道时采用的不同认识论策略，为媒体偏见研究增加了新的维度。研究发现CNN报道包含更多事实陈述并更倾向于引用外部来源，而福克斯新闻则偏好新闻报道和直接引语。

---

#### 📘 Detailed Summary
**Motivation:** 虽然媒体偏见被广泛研究，但事实报道背后的认识论策略在计算层面仍未被充分探索。本研究旨在填补这一空白，通过分析不同媒体在报道相同事件时采用的不同认识论策略来理解媒体如何构建现实。

**Method:** 本研究采用文章匹配策略比较同一事件的报道，应用FactAppeal框架分析超过47万篇文章，涵盖COVID-19大流行和以色列-哈马斯战争两个高度政治化时期。该方法能够将报道风格与主题选择分离开来。

**Result:** 研究发现CNN的报道包含更多事实陈述，且更倾向于将其建立在外部来源基础上。两家媒体表现出截然不同的来源模式：CNN通过引用专家和专家文件来建立可信度，构建对正式权威的诉求，而福克斯新闻则偏好新闻报道和直接引语。

**Conclusion:** 这项工作量化了党派媒体如何采用系统性不同的认识论策略来构建现实，为媒体偏见研究增加了新的维度。研究结果表明，媒体不仅在选择报道主题上存在差异，在如何构建和呈现事实方面也存在根本性的认识论分歧。

---

#### 📄 Abstract
While media bias is widely studied, the epistemic strategies behind factual
reporting remain computationally underexplored. This paper analyzes these
strategies through a large-scale comparison of CNN and Fox News. To isolate
reporting style from topic selection, we employ an article matching strategy to
compare reports on the same events and apply the FactAppeal framework to a
corpus of over 470K articles covering two highly politicized periods: the
COVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting
contains more factual statements and is more likely to ground them in external
sources. The outlets also exhibit sharply divergent sourcing patterns: CNN
builds credibility by citing Experts} and Expert Documents, constructing an
appeal to formal authority, whereas Fox News favors News Reports and direct
quotations. This work quantifies how partisan outlets use systematically
different epistemic strategies to construct reality, adding a new dimension to
the study of media bias.


### [20] [Towards Real-Time Fake News Detection under Evidence Scarcity](https://arxiv.org/abs/2510.11277)
*Guangyu Wei, Ke Han, Yueming Lyu, Yu Luo, Yue Jiang, Caifeng Shan, Nicu Sebe*

#### 🧩 TL;DR
本文提出了EASE框架，通过评估感知的专家选择机制来解决实时假新闻检测中的证据稀缺问题，该框架动态整合证据、推理和情感三种评估视角，在多个基准测试中实现了最先进的性能表现。

---

#### 📘 Detailed Summary
**Motivation:** 现有假新闻检测方法严重依赖外部证据，在证据稀缺的实时场景下泛化能力不足，特别是对于缺乏充分支持证据的新兴事件，传统方法难以有效应对。

**Method:** EASE框架采用顺序评估机制，包含三个独立视角：基于证据的评估仅在证据充分支持时纳入决策，基于推理的评估在可靠性足够时利用LLM的世界知识，基于情感的回退机制在前两者不可靠时整合情感线索，并通过指令调优和伪标签提升评估准确性。

**Result:** 在多个基准测试中，EASE实现了最先进的性能表现，特别是在新构建的RealTimeNews-25基准上显著提升了实时新闻的泛化能力，证明了该框架在证据稀缺环境下的有效性。

**Conclusion:** 该研究表明评估感知的决策机制能够有效应对证据稀缺的挑战，为实时假新闻检测提供了新的解决方案，同时构建的新基准为未来研究提供了重要的评估标准。

---

#### 📄 Abstract
Fake news detection becomes particularly challenging in real-time scenarios,
where emerging events often lack sufficient supporting evidence. Existing
approaches often rely heavily on external evidence and therefore struggle to
generalize under evidence scarcity. To address this issue, we propose
Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time
fake news detection that dynamically adapts its decision-making process
according to the assessed sufficiency of available evidence. EASE introduces a
sequential evaluation mechanism comprising three independent perspectives: (1)
Evidence-based evaluation, which assesses evidence and incorporates it into
decision-making only when the evidence is sufficiently supportive; (2)
Reasoning-based evaluation, which leverages the world knowledge of large
language models (LLMs) and applies them only when their reliability is
adequately established; and (3) Sentiment-based fallback, which integrates
sentiment cues when neither evidence nor reasoning is reliable. To enhance the
accuracy of evaluation processes, EASE employs instruction tuning with pseudo
labels to guide each evaluator in justifying its perspective-specific knowledge
through interpretable reasoning. Furthermore, the expert modules integrate the
evaluators' justified assessments with the news content to enable
evaluation-aware decision-making, thereby enhancing overall detection accuracy.
Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news
for evaluating model generalization on emerging news with limited evidence.
Extensive experiments demonstrate that EASE not only achieves state-of-the-art
performance across multiple benchmarks, but also significantly improves
generalization to real-time news. The code and dataset are available:
https://github.com/wgyhhhh/EASE.


### [21] [Do LLMs "Feel"? Emotion Circuits Discovery and Control](https://arxiv.org/abs/2510.11328)
*Chenxi Wang, Yixuan Zhang, Ruiji Yu, Yufei Zheng, Lang Gao, Zirui Song, Zixiang Xu, Gus Xia, Huishuai Zhang, Dongyan Zhao, Xiuying Chen*

#### 🧩 TL;DR
本研究首次系统性地揭示并验证了大语言模型中的情感电路机制，通过识别局部神经元和注意力头的情感计算功能，构建了驱动情感表达的全局情感电路，实现了99.65%的情感表达准确率，为模型可解释性和可控情感智能提供了新见解。

---

#### 📘 Detailed Summary
**Motivation:** 随着大语言模型对情感智能需求的增长，理解情感表达的内部机制并控制生成文本中的情感成为关键挑战。本研究旨在解决三个核心问题：LLMs是否包含塑造情感表达的上下文无关机制、这些机制的具体形式，以及能否用于通用情感控制。

**Method:** 研究首先构建了受控数据集SEV以激发跨情感的可比较内部状态，随后提取上下文无关的情感方向，通过分析性分解和因果分析识别实现情感计算的局部神经元和注意力头，并通过消融和增强干预验证其因果作用，最后整合局部组件构建驱动情感表达的全局情感电路。

**Result:** 研究揭示了跨上下文一致的情感编码机制，量化了每个子层对最终情感表征的因果影响，通过直接调制情感电路在测试集上实现了99.65%的情感表达准确率，超越了基于提示和导向的方法。

**Conclusion:** 这是首个系统性揭示和验证LLMs中情感电路的研究，为模型可解释性和可控情感智能提供了新见解，所识别的情感电路机制为实现精确的情感控制开辟了新途径。

---

#### 📄 Abstract
As the demand for emotional intelligence in large language models (LLMs)
grows, a key challenge lies in understanding the internal mechanisms that give
rise to emotional expression and in controlling emotions in generated text.
This study addresses three core questions: (1) Do LLMs contain context-agnostic
mechanisms shaping emotional expression? (2) What form do these mechanisms
take? (3) Can they be harnessed for universal emotion control? We first
construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit
comparable internal states across emotions. Subsequently, we extract
context-agnostic emotion directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons and attention heads that locally
implement emotional computation through analytical decomposition and causal
analysis, and validate their causal roles via ablation and enhancement
interventions. Next, we quantify each sublayer's causal influence on the
model's final emotion representation and integrate the identified local
components into coherent global emotion circuits that drive emotional
expression (Q2). Directly modulating these circuits achieves 99.65%
emotion-expression accuracy on the test set, surpassing prompting- and
steering-based methods (Q3). To our knowledge, this is the first systematic
study to uncover and validate emotion circuits in LLMs, offering new insights
into interpretability and controllable emotional intelligence.


### [22] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma, Hailin Zhang, Liang Zhao, Yifan Song, Yudong Wang, Zhifang Sui, Fuli Luo*

#### 🧩 TL;DR
本文提出Rollout Routing Replay (R3)方法来解决混合专家模型在强化学习训练中的路由不稳定性问题，通过记录推理阶段的路由分布并在训练时重放，显著提升了训练稳定性并防止训练崩溃。

---

#### 📘 Detailed Summary
**Motivation:** 混合专家模型中的路由机制在强化学习训练过程中经常引入不稳定性，甚至导致灾难性的训练崩溃。研究发现训练和推理阶段存在显著的路由行为不一致性，即使在相同条件下，路由框架也可能在重复前向传播中产生不同的专家选择，这种基础性不一致是导致训练不稳定的根本原因。

**Method:** 提出Rollout Routing Replay (R3)方法，该方法从推理引擎记录路由分布并在训练阶段进行重放。R3显著减少了训练-推理策略KL散度，缓解了极端差异，同时不损害训练速度，为解决MoE模型中的路由不一致性提供了有效解决方案。

**Result:** 在各种设置下的大量实验证实，R3成功稳定了强化学习训练，防止了训练崩溃，并且在性能上超越了GSPO和TIS等方法。该方法在保持训练效率的同时，显著改善了MoE模型在RL训练中的稳定性表现。

**Conclusion:** 这项工作为稳定MoE模型中的强化学习训练提供了新的解决方案，揭示了路由不一致性是导致训练不稳定的关键因素，R3方法通过确保训练-推理一致性为未来MoE模型的稳定训练开辟了新的技术路径。

---

#### 📄 Abstract
Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.


### [23] [StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models](https://arxiv.org/abs/2510.11618)
*Zehao Chen, Rong Pan, Haoran Li*

#### 🧩 TL;DR
本文提出了一种基于多智能体仿真的混合自底向上长故事生成方法，通过智能体在动态沙盒环境中的交互产生涌现事件，能够生成超过10,000字的长篇故事并保持连贯性，在多个指标上达到最先进性能。

---

#### 📘 Detailed Summary
**Motivation:** 传统自上而下的故事生成方法存在结构僵化的问题，无法模拟人类作家从整体心理场景出发、通过角色与环境交互自然展开故事的创作过程，当前故事生成模型在长篇故事生成中面临连贯性和一致性的关键挑战。

**Method:** 采用混合自底向上的长故事生成方法，通过多智能体仿真在动态沙盒环境中进行交互，智能体之间的行为及其与环境的互动产生涌现事件，这些事件构成故事的基础，实现有机的角色发展和情节推进。

**Result:** 系统能够生成超过10,000字的长篇故事，同时保持故事的连贯性和一致性，在多个评估指标上实现了最先进的性能表现，有效解决了当前故事生成模型面临的关键挑战。

**Conclusion:** 该方法为创建动态、沉浸式的长篇故事提供了可扩展的创新解决方案，通过智能体驱动的交互实现故事的有机演化，突破了传统故事生成方法的局限性，为未来故事生成研究开辟了新的方向。

---

#### 📄 Abstract
Human writers often begin their stories with an overarching mental scene,
where they envision the interactions between characters and their environment.
Inspired by this creative process, we propose a novel approach to long-form
story generation, termed hybrid bottom-up long-form story generation, using
multi-agent simulations. In our method, agents interact within a dynamic
sandbox environment, where their behaviors and interactions with one another
and the environment generate emergent events. These events form the foundation
for the story, enabling organic character development and plot progression.
Unlike traditional top-down approaches that impose rigid structures, our hybrid
bottom-up approach allows for the natural unfolding of events, fostering more
spontaneous and engaging storytelling. The system is capable of generating
stories exceeding 10,000 words while maintaining coherence and consistency,
addressing some of the key challenges faced by current story generation models.
We achieve state-of-the-art performance across several metrics. This approach
offers a scalable and innovative solution for creating dynamic, immersive
long-form stories that evolve organically from agent-driven interactions.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang, Kaitong Cai, Qinglin Zeng, Ningyuan Liu, Stephen Fan, Ziliang Chen, Keze Wang*

#### 🧩 TL;DR
本文提出了一种新的LLM工作流优化范式，将优化目标从最大化标量分数重新定义为直接最小化工作流在失败签名空间中的期望失败质量，通过CE-Graph框架实现了对失败分布的系统性学习和重塑。

---

#### 📘 Detailed Summary
**Motivation:** 现有LLM工作流优化方法存在信息坍塌问题，将丰富的多步骤执行轨迹简化为简单的成功/失败信号，导致无法建模工作流的失败分布结构，从根本上限制了优化效果。

**Method:** 提出CE-Graph框架，通过反例池近似失败分布，识别最密集区域作为重复失败模式，并采用提出-验证机制进行目标明确的图编辑操作，贪婪地减少失败质量。

**Result:** 在数学、代码和问答基准测试中，CE-Graph相比强基线方法以显著更低的成本实现了更高的鲁棒性，验证了分布优化方法的有效性。

**Conclusion:** 系统可靠性并非来自避免失败，而是源于系统性地学习和重塑其失败分布的几何结构，这为LLM工作流优化提供了新的理论基础和实践路径。

---

#### 📄 Abstract
Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.


### [25] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li, Hongjun Liu, Leyi Zhao, Zisu Li, Jiawei Li, Jiajun Jiang, Linning Xu, Chen Zhao, Mingming Fan, Chen Liang*

#### 🧩 TL;DR
SwarmSys是一个受群体智能启发的分布式多智能体推理框架，通过探索者、工作者和验证者三种角色的迭代交互实现闭环推理，在符号推理、研究综合和科学编程任务中显著优于基线方法。

---

#### 📘 Detailed Summary
**Motivation:** 现有大型语言模型多智能体框架通常依赖固定角色或集中控制，这在长时程推理中限制了系统的可扩展性和适应性，需要一种能够支持动态任务分配和自组织收敛的新范式。

**Method:** 该框架集成了自适应智能体和事件配置文件、基于嵌入的概率匹配以及受信息素启发的强化机制，通过探索者、工作者和验证者三种专门角色的连续循环实现探索、利用和验证的迭代交互。

**Result:** 在符号推理、研究综合和科学编程任务上的实验表明，SwarmSys在准确性和推理稳定性方面均持续优于基线方法，证明了其优越性能。

**Conclusion:** 群体启发的协调机制为可扩展、鲁棒和自适应的多智能体推理提供了一个有前景的范式，表明协调扩展可能与模型扩展在推进LLM智能方面具有同等重要性。

---

#### 📄 Abstract
Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.


### [26] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng, Yujuan Fu, Sitong Zhou, Zixuan Yu, Lucas Jing Liu, Jun Wen, Matthew Thompson, Ruth Etzioni, Meliha Yetisgen*

#### 🧩 TL;DR
本文提出了Traj-CoA，一种用于患者轨迹建模的多智能体系统，通过链式智能体处理电子健康记录数据，在零样本肺癌风险预测任务中优于基线方法。该系统通过工作智能体分块处理数据和管理智能体综合信息，实现了临床对齐的时间推理能力。

---

#### 📘 Detailed Summary
**Motivation:** 大型语言模型为患者轨迹建模提供了通用方法，但面临电子健康记录数据长且噪声大的挑战，特别是在时间推理方面存在困难。现有方法难以有效处理复杂的时序医疗数据，需要解决数据噪声和时间关系建模的问题。

**Method:** Traj-CoA采用多智能体系统，包含工作智能体链和长期记忆模块EHRMem。工作智能体按顺序处理可管理的数据块，将关键事件提取到共享记忆中减少噪声并保留完整时间线，最后由管理智能体综合工作智能体摘要和EHRMem中的时间线进行预测。

**Result:** 在基于五年电子健康记录的零样本一年肺癌风险预测任务中，Traj-CoA在四类基线方法上表现优异。分析表明该系统展现出临床对齐的时间推理能力，证明了其在复杂患者轨迹建模中的有效性。

**Conclusion:** Traj-CoA为复杂患者轨迹建模提供了一种稳健且可推广的方法，其多智能体架构和长期记忆机制能够有效处理电子健康记录中的时序挑战。该方法展示了在医疗时间序列数据分析中的潜力，为基于LLM的医疗预测系统提供了新思路。

---

#### 📄 Abstract
Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.


### [27] [Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce](https://arxiv.org/abs/2510.11604)
*Sanjula De Alwis, Indrajith Ekanayake*

#### 🧩 TL;DR
本研究提出了一个集成可解释AI、生存分析和RFM分析的三组件框架，用于客户流失预测和保留策略制定。该框架能够识别流失驱动因素、估计干预时间窗口并优先处理高风险客户群体，从而支持减少客户流失和增强客户忠诚度的策略。

---

#### 📘 Detailed Summary
**Motivation:** 当前客户流失模型通常作为不透明的黑箱运行，限制了企业对流失决定因素、保留机会时机以及高风险客户群体识别的洞察能力。因此，需要从单纯预测转向基于可解释证据的个性化保留策略设计。

**Method:** 本研究提出了一个三组件框架，集成可解释AI来量化特征贡献，生存分析来建模事件时间流失风险，以及RFM分析来根据交易行为对客户进行细分。这些方法结合使用能够实现流失驱动因素归因、干预窗口估计和针对性行动细分群体优先排序。

**Result:** 该框架能够实现流失驱动因素的归因分析、干预时间窗口的准确估计，以及高风险客户群体的有效识别和优先处理，为制定针对性保留策略提供了可靠依据。

**Conclusion:** 该研究强调了从单纯预测转向基于可解释证据的个性化保留策略的重要性，提出的集成框架为减少客户流失和增强客户忠诚度提供了有效的技术支持，推动了客户关系管理向更智能、更精准的方向发展。

---

#### 📄 Abstract
In online retail, customer acquisition typically incurs higher costs than
customer retention, motivating firms to invest in churn analytics. However,
many contemporary churn models operate as opaque black boxes, limiting insight
into the determinants of attrition, the timing of retention opportunities, and
the identification of high-risk customer segments. Accordingly, the emphasis
should shift from prediction alone to the design of personalized retention
strategies grounded in interpretable evidence. This study advances a
three-component framework that integrates explainable AI to quantify feature
contributions, survival analysis to model time-to-event churn risk, and RFM
profiling to segment customers by transactional behaviour. In combination,
these methods enable the attribution of churn drivers, estimation of
intervention windows, and prioritization of segments for targeted actions,
thereby supporting strategies that reduce attrition and strengthen customer
loyalty.
