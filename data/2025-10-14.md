<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 15]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation](https://arxiv.org/abs/2510.09867)
*Zhi Chen, Xin Yu, Xiaohui Tao, Yan Li, Zi Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†èšç±»æ„ŸçŸ¥æç¤ºé›†æˆå­¦ä¹ æ¡†æ¶CAPELï¼Œé€šè¿‡å°†æç¤ºé›†æˆä»ç‰¹å¾ç©ºé—´è½¬ç§»åˆ°åˆ†ç±»logitsç©ºé—´ï¼Œå¹¶å¼•å…¥èšç±»ä¿æŒæ­£åˆ™åŒ–ï¼Œè§£å†³äº†ä¼ ç»Ÿæç¤ºé›†æˆæ–¹æ³•ä¸­ç‰¹å¾å¹³å‡å¯¼è‡´ç±»ä¸­å¿ƒåç§»çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æç¤ºé›†æˆæ–¹æ³•é€šè¿‡å¹³å‡å¤šä¸ªä¸Šä¸‹æ–‡æç¤ºçš„æ–‡æœ¬ç‰¹å¾æ¥å®ç°é›¶æ ·æœ¬åˆ†ç±»ï¼Œä½†è¿™ç§æ–¹æ³•å¾€å¾€å¯¼è‡´æ¬¡ä¼˜ç»“æœï¼Œå› ä¸ºç‰¹å¾å¹³å‡ä¼šä½¿ç±»ä¸­å¿ƒåç¦»çœŸå®çš„ç±»åˆ†å¸ƒï¼Œæ— æ³•æœ‰æ•ˆä¿ç•™ä¸Šä¸‹æ–‡æç¤ºçš„èšç±»ç‰¹æ€§ã€‚

**Method:** æå‡ºäº†èšç±»æ„ŸçŸ¥æç¤ºé›†æˆå­¦ä¹ æ¡†æ¶CAPELï¼Œè¯¥æ–¹æ³•å°†å›¾åƒåˆ†ç±»åˆ°ç”±ä¸åŒæç¤ºè¡¨ç¤ºçš„å¤šä¸ªç±»ç°‡ä¸­ï¼Œåœ¨åˆ†ç±»logitsç©ºé—´è€Œéç‰¹å¾ç©ºé—´è¿›è¡Œæç¤ºé›†æˆï¼Œæ›´å¥½åœ°ä¸è§†è§‰ç‰¹å¾åˆ†å¸ƒå¯¹é½ï¼›åŒæ—¶å¼•å…¥èšç±»ä¿æŒæ­£åˆ™åŒ–é¡¹æ¥ä¼˜åŒ–æç¤ºå¾®è°ƒï¼Œä¿æŒä¸åŒç°‡é—´æç¤ºçš„åŒºåˆ†æ€§ï¼›è¿˜é›†æˆäº†è‡ªé€‚åº”æç¤ºåŠ æƒæŠ€æœ¯ï¼ŒåŠ¨æ€è°ƒæ•´æœ‰ç¼ºé™·æˆ–æ¨¡ç³Šæç¤ºçš„æ³¨æ„åŠ›æƒé‡ã€‚

**Result:** CAPELæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†å’Œä»»åŠ¡ä¸Šå±•ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œé€šè¿‡ä¿ç•™ä¸Šä¸‹æ–‡æç¤ºçš„èšç±»ç‰¹æ€§ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬åˆ†ç±»çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿç‰¹å¾å¹³å‡æ–¹æ³•å¯¼è‡´çš„ç±»ä¸­å¿ƒåç§»é—®é¢˜ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜åœ¨åˆ†ç±»logitsç©ºé—´è¿›è¡Œæç¤ºé›†æˆæ¯”åœ¨ç‰¹å¾ç©ºé—´æ›´æœ‰æ•ˆï¼Œèšç±»ä¿æŒæœºåˆ¶å¯¹äºç»´æŒæç¤ºçš„åŒºåˆ†æ€§è‡³å…³é‡è¦ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æç¤ºä¼˜åŒ–æä¾›äº†æ–°çš„æ–¹å‘ï¼Œæœªæ¥å¯æ‰©å±•åˆ°æ›´å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across
various tasks by pre-training on numerous image-text pairs. These models often
benefit from using an ensemble of context prompts to represent a class. Despite
being effective, conventional prompt ensembling that averages textual features
of context prompts often yields suboptimal results. This is because feature
averaging shifts the class centroids away from the true class distribution. To
address this issue, we propose the Cluster-Aware Prompt Ensemble Learning
(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL
classifies images into one of several class clusters, each represented by a
distinct prompt. Instead of ensembling prompts in the feature space, we perform
ensembling in the classification logits space, aligning better with the visual
feature distribution. To further optimize prompt fine-tuning while maintaining
cluster-specific discriminative power, we introduce a cluster-preserving
regularization term. This ensures that prompts remain distinct and specialized
for different clusters, preventing collapse into a uniform direction.
Additionally, we integrate an adaptive prompt weighting technique to
dynamically adjust the attention weights for flawed or ambiguous prompts,
ensuring robust performance across diverse datasets and tasks.


### [2] [Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting](https://arxiv.org/abs/2510.10257)
*Abdelrhman Elrawy, Emad A. Mohammed*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›3Dé«˜æ–¯æ³¼æº…åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹æ•ˆç‡çš„æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨ä¸é€æ˜åº¦æ¢¯åº¦ä½œä¸ºè½»é‡çº§æ¸²æŸ“è¯¯å·®ä»£ç†æ¥æ›¿ä»£æ ‡å‡†ä½ç½®æ¢¯åº¦å¯å‘å¼ï¼Œç»“åˆä¿å®ˆå‰ªæç­–ç•¥å’Œæ·±åº¦ç›¸å…³æŸå¤±ï¼Œåœ¨ä¿æŒé‡å»ºè´¨é‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†åŸºå…ƒæ•°é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** 3Dé«˜æ–¯æ³¼æº…åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­é¢ä¸´è¿‡æ‹Ÿåˆå’Œé‡å»ºè†¨èƒ€é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¦‚FSGSè™½ç„¶æé«˜äº†è´¨é‡ä½†æ˜¾è‘—å¢åŠ äº†åŸºå…ƒæ•°é‡ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡ä¼˜åŒ–æ ¸å¿ƒ3DGSç®—æ³•æ¥ä¼˜å…ˆè€ƒè™‘æ•ˆç‡é—®é¢˜ã€‚

**Method:** é‡‡ç”¨åŸºäºä¸é€æ˜åº¦æ¢¯åº¦çš„æ–°å‹è‡´å¯†åŒ–è§¦å‘æœºåˆ¶æ›¿ä»£æ ‡å‡†ä½ç½®æ¢¯åº¦å¯å‘å¼ï¼Œç»“åˆæ›´ä¿å®ˆçš„å‰ªæç­–ç•¥æ¥é˜²æ­¢ç ´åæ€§ä¼˜åŒ–å¾ªç¯ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†æ·±åº¦ç›¸å…³æŸå¤±æä¾›å‡ ä½•æŒ‡å¯¼ã€‚

**Result:** åœ¨3è§†å›¾LLFFæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹æ¯”FSGSç´§å‡‘40%ä»¥ä¸Šï¼ˆ32k vs 57kåŸºå…ƒï¼‰ï¼Œåœ¨Mip-NeRF 360æ•°æ®é›†ä¸Šå®ç°äº†çº¦70%çš„åŸºå…ƒå‡å°‘ï¼Œä»…ä»¥é€‚åº¦çš„é‡å»ºæŒ‡æ ‡æŸå¤±ä¸ºä»£ä»·ã€‚

**Conclusion:** è¯¥æ¡†æ¶åœ¨å°‘æ ·æœ¬è§†å›¾åˆæˆçš„è´¨é‡-æ•ˆç‡å¸•ç´¯æ‰˜è¾¹ç•Œä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œè¯æ˜äº†é€šè¿‡é‡æ–°è®¾è®¡æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥å¯ä»¥å®ç°æ•ˆç‡çš„æ ¹æœ¬æ€§æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§çš„é‡å»ºæ€§èƒ½ã€‚

---

#### ğŸ“„ Abstract
3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its
standard adaptive density control (ADC) can lead to overfitting and bloated
reconstructions. While state-of-the-art methods like FSGS improve quality, they
often do so by significantly increasing the primitive count. This paper
presents a framework that revises the core 3DGS optimization to prioritize
efficiency. We replace the standard positional gradient heuristic with a novel
densification trigger that uses the opacity gradient as a lightweight proxy for
rendering error. We find this aggressive densification is only effective when
paired with a more conservative pruning schedule, which prevents destructive
optimization cycles. Combined with a standard depth-correlation loss for
geometric guidance, our framework demonstrates a fundamental improvement in
efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k
vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a
reduction of approximately 70%. This dramatic gain in compactness is achieved
with a modest trade-off in reconstruction metrics, establishing a new
state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view
synthesis.


### [3] [AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration](https://arxiv.org/abs/2510.10395)
*Xinlong Chen, Yue Ding, Weihong Lin, Jingyun Hua, Linli Yao, Yang Shi, Bozhou Li, Yuanxing Zhang, Qiang Liu, Pengfei Wan, Liang Wang, Tieniu Tan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†AVoCaDOï¼Œä¸€ç§åŸºäºè§†å¬æ—¶åºç¼–æ’çš„å¼ºå¤§å¤šæ¨¡æ€è§†é¢‘æè¿°æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µåè®­ç»ƒæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†å¬è§†é¢‘æè¿°ä»»åŠ¡æ—¨åœ¨ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œä¸”å…·æœ‰è§†è§‰å’Œå¬è§‰äº‹ä»¶æ—¶åºå¯¹é½çš„æè¿°ï¼Œè¿™å¯¹è§†é¢‘ç†è§£å’Œç”Ÿæˆå…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰æ–¹æ³•åœ¨æ—¶åºå¯¹é½å’Œæ¨¡æ€åè°ƒæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆæœºåˆ¶ã€‚

**Method:** æå‡ºä¸¤é˜¶æ®µåè®­ç»ƒæµç¨‹ï¼šAVoCaDO SFTé˜¶æ®µåœ¨æ–°æ„å»ºçš„107Ké«˜è´¨é‡æ—¶åºå¯¹é½è§†å¬æè¿°æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼›AVoCaDO GRPOé˜¶æ®µåˆ©ç”¨å®šåˆ¶çš„å¥–åŠ±å‡½æ•°è¿›ä¸€æ­¥å¢å¼ºæ—¶åºä¸€è‡´æ€§å’Œå¯¹è¯å‡†ç¡®æ€§ï¼ŒåŒæ—¶è§„èŒƒåŒ–æè¿°é•¿åº¦å¹¶å‡å°‘æ¨¡å‹åå¡Œã€‚

**Result:** AVoCaDOåœ¨å››ä¸ªè§†å¬è§†é¢‘æè¿°åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ï¼Œåœ¨ä»…è§†è§‰è®¾ç½®çš„VDCå’ŒDREAM-1KåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜åŸºäºæ—¶åºç¼–æ’çš„è§†å¬èåˆç­–ç•¥èƒ½æœ‰æ•ˆæå‡è§†é¢‘æè¿°è´¨é‡ï¼Œä¸¤é˜¶æ®µåè®­ç»ƒæ–¹æ³•ä¸ºå¤šæ¨¡æ€æ¨¡å‹ä¼˜åŒ–æä¾›äº†æœ‰æ•ˆèŒƒå¼ï¼Œæœªæ¥å¯æ‰©å±•åˆ°æ›´å¹¿æ³›çš„å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ã€‚

---

#### ğŸ“„ Abstract
Audiovisual video captioning aims to generate semantically rich descriptions
with temporal alignment between visual and auditory events, thereby benefiting
both video understanding and generation. In this paper, we present AVoCaDO, a
powerful audiovisual video captioner driven by the temporal orchestration
between audio and visual modalities. We propose a two-stage post-training
pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated
dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)
AVoCaDO GRPO, which leverages tailored reward functions to further enhance
temporal coherence and dialogue accuracy while regularizing caption length and
reducing collapse. Experimental results demonstrate that AVoCaDO significantly
outperforms existing open-source models across four audiovisual video
captioning benchmarks, and also achieves competitive performance on the VDC and
DREAM-1K benchmark under visual-only settings.


### [4] [GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction](https://arxiv.org/abs/2510.10546)
*Zuha Fatima, Muhammad Anser Sohaib, Muhammad Talha, Sidra Sultana, Ayesha Kanwal, Nazia Perwaiz*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GLOFNetï¼Œä¸€ä¸ªç”¨äºå†°å·æ¹–æºƒå†³æ´ªæ°´ç›‘æµ‹å’Œé¢„æµ‹çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ•´åˆäº†å¤šå…‰è°±å½±åƒã€å†°å·è¿åŠ¨é€Ÿåº¦å’Œåœ°è¡¨æ¸©åº¦æ•°æ®ï¼Œä¸ºç½•è§ç¾å®³é¢„æµ‹æä¾›äº†ç»“æ„åŒ–åŸºå‡†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å†°å·æ¹–æºƒå†³æ´ªæ°´æ˜¯é«˜å±±åœ°åŒºç½•è§ä½†å…·æœ‰ç ´åæ€§çš„ç¾å®³ï¼Œç°æœ‰ç ”ç©¶å—é™äºç¢ç‰‡åŒ–å’Œå•æ¨¡æ€æ•°æ®ï¼Œä¸»è¦å…³æ³¨äº‹ååˆ¶å›¾ï¼Œè€Œé¢„æµ‹éœ€è¦ç»“åˆè§†è§‰æŒ‡æ ‡ä¸ç‰©ç†å‰å…†çš„åè°ƒæ•°æ®é›†ã€‚

**Method:** ç ”ç©¶æ•´åˆäº†ä¸‰ç§äº’è¡¥æ•°æ®æºï¼šSentinel-2å¤šå…‰è°±å½±åƒç”¨äºç©ºé—´ç›‘æµ‹ã€NASA ITS_LIVEé€Ÿåº¦äº§å“ç”¨äºå†°å·è¿åŠ¨å­¦ã€MODISåœ°è¡¨æ¸©åº¦è®°å½•ç”¨äºé•¿æœŸè¶‹åŠ¿åˆ†æï¼Œå¹¶è¿›è¡Œäº†äº‘æ©è†œã€è´¨é‡è¿‡æ»¤ã€å½’ä¸€åŒ–ã€æ—¶é—´æ’å€¼ã€æ•°æ®å¢å¼ºå’Œå¾ªç¯ç¼–ç ç­‰é¢„å¤„ç†ã€‚

**Result:** æ¢ç´¢æ€§åˆ†ææ­ç¤ºäº†å†°å·é€Ÿåº¦çš„å­£èŠ‚æ€§å‘¨æœŸã€æ¯åå¹´çº¦0.8Kçš„é•¿æœŸå‡æ¸©è¶‹åŠ¿ä»¥åŠå†°å†»åœˆæ¡ä»¶çš„ç©ºé—´å¼‚è´¨æ€§ï¼Œæ•°æ®é›†å…¬å¼€å¯ç”¨ä»¥æ”¯æŒæœªæ¥å†°å·ç¾å®³é¢„æµ‹ç ”ç©¶ã€‚

**Conclusion:** GLOFNeté€šè¿‡è§£å†³ç±»åˆ«ä¸å¹³è¡¡ã€äº‘æ±¡æŸ“å’Œç²—åˆ†è¾¨ç‡ç­‰æŒ‘æˆ˜ï¼Œä¸ºåŸºå‡†æµ‹è¯•å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ç½•è§ç¾å®³é¢„æµ‹ä¸­çš„åº”ç”¨æä¾›äº†ç»“æ„åŒ–åŸºç¡€ï¼Œæ¨åŠ¨äº†å†°å·ç¾å®³é¢„æµ‹ç ”ç©¶çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high
mountain regions, yet predictive research is hindered by fragmented and
unimodal data. Most prior efforts emphasize post-event mapping, whereas
forecasting requires harmonized datasets that combine visual indicators with
physical precursors. We present GLOFNet, a multimodal dataset for GLOF
monitoring and prediction, focused on the Shisper Glacier in the Karakoram. It
integrates three complementary sources: Sentinel-2 multispectral imagery for
spatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and
MODIS Land Surface Temperature records spanning over two decades. Preprocessing
included cloud masking, quality filtering, normalization, temporal
interpolation, augmentation, and cyclical encoding, followed by harmonization
across modalities. Exploratory analysis reveals seasonal glacier velocity
cycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in
cryospheric conditions. The resulting dataset, GLOFNet, is publicly available
to support future research in glacial hazard prediction. By addressing
challenges such as class imbalance, cloud contamination, and coarse resolution,
GLOFNet provides a structured foundation for benchmarking multimodal deep
learning approaches to rare hazard prediction.


### [5] [Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes](https://arxiv.org/abs/2510.10577)
*Haonan Wang, Hanyu Zhou, Haoyue Liu, Luxin Yan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¸§-äº‹ä»¶å¤–è§‚-è¾¹ç•Œèåˆå…‰æµä¼°è®¡æ¡†æ¶Diff-ABFlowï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å­¦ä¹ ä»å™ªå£°æµåˆ°æ¸…æ™°æµçš„æ˜ å°„ï¼Œè§£å†³äº†é«˜é€Ÿå’Œä½å…‰åœºæ™¯ä¸‹ä¼ ç»Ÿå…‰æµä¼°è®¡æ–¹æ³•å› è¿åŠ¨æ¨¡ç³Šå’Œå…‰ç…§ä¸è¶³å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿå…‰æµä¼°è®¡æ–¹æ³•åœ¨é«˜é€Ÿå’Œä½å…‰åœºæ™¯ä¸‹é¢ä¸´è¿åŠ¨æ¨¡ç³Šå’Œå…‰ç…§ä¸è¶³çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´çº¹ç†å‡å¼±ã€å™ªå£°æ”¾å¤§ï¼Œå¹¶å½±å“å¸§ç›¸æœºçš„å¤–è§‚é¥±å’Œåº¦å’Œè¾¹ç•Œå®Œæ•´æ€§ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶é€šè¿‡ç‰¹å¾èåˆæˆ–åŸŸé€‚åº”å¼•å…¥äº‹ä»¶ç›¸æœºæ¥æ”¹å–„è¾¹ç•Œå®Œæ•´æ€§ï¼Œä½†å¤–è§‚ç‰¹å¾ä»ç„¶é€€åŒ–ï¼Œä¸¥é‡å½±å“äº†åŸºäºè§†è§‰ç‰¹å¾åˆ°è¿åŠ¨åœºæ˜ å°„çš„åˆ¤åˆ«æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚

**Method:** æå‡ºDiff-ABFlowæ¡†æ¶ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹æ„å»ºå¸§-äº‹ä»¶å¤–è§‚-è¾¹ç•Œèåˆçš„å…‰æµä¼°è®¡æ–¹æ³•ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ ä»å™ªå£°æµåˆ°æ¸…æ™°æµçš„æ˜ å°„è¿‡ç¨‹ï¼Œä¸ä¾èµ–äºé€€åŒ–çš„è§†è§‰ç‰¹å¾ï¼Œç»“åˆå¸§ç›¸æœºæä¾›å¯†é›†å¤–è§‚é¥±å’Œåº¦å’Œäº‹ä»¶ç›¸æœºæä¾›å¯†é›†è¾¹ç•Œå®Œæ•´æ€§çš„äº’è¡¥ä¼˜åŠ¿ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨é«˜é€Ÿå’Œä½å…‰é€€åŒ–åœºæ™¯ä¸‹æ˜¾è‘—æå‡äº†å…‰æµä¼°è®¡æ€§èƒ½ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›æœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ¶åŠ£æˆåƒæ¡ä»¶ä¸‹çš„å±€é™æ€§ï¼Œå®ç°äº†æ›´å‡†ç¡®å’Œé²æ£’çš„è¿åŠ¨åœºä¼°è®¡ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜æ‰©æ•£æ¨¡å‹ä¸ºå…‰æµä¼°è®¡æä¾›äº†ä¸€ç§ä¸ä¾èµ–äºé€€åŒ–è§†è§‰ç‰¹å¾çš„æ–°èŒƒå¼ï¼Œå¸§-äº‹ä»¶ä¼ æ„Ÿå™¨çš„äº’è¡¥èåˆç­–ç•¥ä¸ºè§£å†³æç«¯æˆåƒæ¡ä»¶ä¸‹çš„è¿åŠ¨ä¼°è®¡é—®é¢˜å¼€è¾Ÿäº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œä¸ºè®¡ç®—æœºè§†è§‰åœ¨æŒ‘æˆ˜æ€§åœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Optical flow estimation has achieved promising results in conventional scenes
but faces challenges in high-speed and low-light scenes, which suffer from
motion blur and insufficient illumination. These conditions lead to weakened
texture and amplified noise and deteriorate the appearance saturation and
boundary completeness of frame cameras, which are necessary for motion feature
matching. In degraded scenes, the frame camera provides dense appearance
saturation but sparse boundary completeness due to its long imaging time and
low dynamic range. In contrast, the event camera offers sparse appearance
saturation, while its short imaging time and high dynamic range gives rise to
dense boundary completeness. Traditionally, existing methods utilize feature
fusion or domain adaptation to introduce event to improve boundary
completeness. However, the appearance features are still deteriorated, which
severely affects the mostly adopted discriminative models that learn the
mapping from visual features to motion fields and generative models that
generate motion fields based on given visual features. So we introduce
diffusion models that learn the mapping from noising flow to clear flow, which
is not affected by the deteriorated visual features. Therefore, we propose a
novel optical flow estimation framework Diff-ABFlow based on diffusion models
with frame-event appearance-boundary fusion.


### [6] [A Machine Learning Perspective on Automated Driving Corner Cases](https://arxiv.org/abs/2510.10653)
*Sebastian Schmidt, Julius KÃ¶rner, Stephan GÃ¼nnemann*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®åˆ†å¸ƒè§†è§’çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ç­‰é«˜é£é™©åº”ç”¨ä¸­çš„è§’ç‚¹æ¡ˆä¾‹è¯†åˆ«ã€‚è¯¥æ–¹æ³•ç»Ÿä¸€äº†ç°æœ‰åœºæ™¯åˆ†ç±»æ–¹æ³•ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¼ºå¤§çš„è§’ç‚¹æ¡ˆä¾‹æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿæ–¹æ³•é€šè¿‡ç¤ºä¾‹åˆ†ç±»å¤„ç†è§’ç‚¹æ¡ˆä¾‹ï¼Œä½†è¿™ç§æ–¹æ³•ç¼ºä¹å¯æ‰©å±•æ€§ä¸”å¿½è§†äº†æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æ•°æ®è¦†ç›–è§†è§’ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†é«˜é£é™©åº”ç”¨ä¸­çš„å®‰å…¨é—®é¢˜ã€‚

**Method:** æå‡ºäº†ä¸€ç§è€ƒè™‘åº•å±‚æ•°æ®åˆ†å¸ƒçš„æ–°å‹æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¼€å‘äº†åŸºäºåˆ†å¸ƒè§†è§’çš„è§’ç‚¹æ¡ˆä¾‹è¯†åˆ«æ¡†æ¶ï¼Œèƒ½å¤Ÿå¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œæœ‰æ•ˆçš„æ„ŸçŸ¥è¯†åˆ«ã€‚

**Result:** è¯¥æ–¹æ³•æˆåŠŸç»Ÿä¸€äº†ç°æœ‰åŸºäºåœºæ™¯çš„è§’ç‚¹æ¡ˆä¾‹åˆ†ç±»æ³•ï¼Œåœ¨æ‰©å±•çš„æ ‡å‡†åˆ†å¸ƒå¤–æ£€æµ‹åŸºå‡†ä¸Šå®ç°äº†å¼ºå¤§çš„è§’ç‚¹æ¡ˆä¾‹æ£€æµ‹æ€§èƒ½ï¼Œå¹¶é€šè¿‡æ–°å¼•å…¥çš„é›¾å¢å¼ºLost & Foundæ•°æ®é›†æ”¯æŒç»„åˆè§’ç‚¹æ¡ˆä¾‹åˆ†æã€‚

**Conclusion:** ç ”ç©¶ä¸ºè§’ç‚¹æ¡ˆä¾‹è¯†åˆ«æä¾›äº†åŸåˆ™æ€§åŸºç¡€ï¼Œå¼ºè°ƒäº†æ— éœ€äººå·¥è§„èŒƒçš„å®šä¹‰æ–¹æ³•ï¼Œä¸ºé«˜é£é™©åº”ç”¨çš„å®‰å…¨æ“ä½œæä¾›äº†åˆ†å¸ƒè§†è§’çš„ç†è®ºæ”¯æ’‘å’Œå®è·µæ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
For high-stakes applications, like autonomous driving, a safe operation is
necessary to prevent harm, accidents, and failures. Traditionally, difficult
scenarios have been categorized into corner cases and addressed individually.
However, this example-based categorization is not scalable and lacks a data
coverage perspective, neglecting the generalization to training data of machine
learning models. In our work, we propose a novel machine learning approach that
takes the underlying data distribution into account. Based on our novel
perspective, we present a framework for effective corner case recognition for
perception on individual samples. In our evaluation, we show that our approach
(i) unifies existing scenario-based corner case taxonomies under a
distributional perspective, (ii) achieves strong performance on corner case
detection tasks across standard benchmarks for which we extend established
out-of-distribution detection benchmarks, and (iii) enables analysis of
combined corner cases via a newly introduced fog-augmented Lost & Found
dataset. These results provide a principled basis for corner case recognition,
underlining our manual specification-free definition.


### [7] [Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection](https://arxiv.org/abs/2510.10750)
*Laura Weihl, Nejc Novak, Stefan H. Bengtson, Malte Pedersen*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†AURAâ€”â€”é¦–ä¸ªå¤šæ ‡æ³¨è€…æ°´ä¸‹è§†è§‰å¼‚å¸¸æ£€æµ‹åŸºå‡†æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†å››ç§æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨ä¸¤ç§æµ·æ´‹åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹æ€§èƒ½å¯¹è®­ç»ƒæ•°æ®é‡å’Œè§†è§‰å†…å®¹å˜å¼‚æ€§çš„é«˜åº¦æ•æ„Ÿæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ°´ä¸‹è§†é¢‘ç›‘æµ‹æ˜¯è¯„ä¼°æµ·æ´‹ç”Ÿç‰©å¤šæ ·æ€§çš„æœ‰æ•ˆç­–ç•¥ï¼Œä½†æµ·é‡æ— äº‹ä»¶è§†é¢‘ä½¿å¾—äººå·¥æ£€æŸ¥æä¸å®ç”¨ï¼Œéœ€è¦è‡ªåŠ¨è¯†åˆ«æœ‰è¶£æˆ–å¼‚å¸¸äº‹ä»¶çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** ç ”ç©¶æ¢ç´¢äº†åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ°´ä¸‹è§†è§‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œå¼•å…¥äº†é¦–ä¸ªå¤šæ ‡æ³¨è€…åŸºå‡†æ•°æ®é›†AURAï¼Œå¹¶è¯„ä¼°äº†å››ç§VADæ¨¡å‹åœ¨ä¸¤ç§æµ·æ´‹åœºæ™¯ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶å¼ºè°ƒäº†é²æ£’å¸§é€‰æ‹©ç­–ç•¥å¯¹æå–æœ‰æ„ä¹‰è§†é¢‘ç‰‡æ®µçš„é‡è¦æ€§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜å½“å‰VADæ¨¡å‹æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œå¯¹è®­ç»ƒæ•°æ®é‡å’Œå®šä¹‰"æ­£å¸¸"åœºæ™¯çš„è§†è§‰å†…å®¹å˜å¼‚æ€§é«˜åº¦æ•æ„Ÿï¼Œä¸å¤šæ ‡æ³¨è€…çš„æ¯”è¾ƒæ­ç¤ºäº†è½¯æ ‡ç­¾å’Œå…±è¯†æ ‡ç­¾çš„ä»·å€¼ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºæ”¯æŒç§‘å­¦æ¢ç´¢å’Œå¯æ‰©å±•ç”Ÿç‰©å¤šæ ·æ€§ç›‘æµ‹æä¾›äº†å®ç”¨æ–¹æ³•ï¼Œå¼ºè°ƒäº†å¤šæ ‡æ³¨è€…åŸºå‡†å’Œé²æ£’å¸§é€‰æ‹©ç­–ç•¥åœ¨æ°´ä¸‹è§†è§‰å¼‚å¸¸æ£€æµ‹ä¸­çš„å…³é”®ä½œç”¨ã€‚

---

#### ğŸ“„ Abstract
Underwater video monitoring is a promising strategy for assessing marine
biodiversity, but the vast volume of uneventful footage makes manual inspection
highly impractical. In this work, we explore the use of visual anomaly
detection (VAD) based on deep neural networks to automatically identify
interesting or anomalous events. We introduce AURA, the first multi-annotator
benchmark dataset for underwater VAD, and evaluate four VAD models across two
marine scenes. We demonstrate the importance of robust frame selection
strategies to extract meaningful video segments. Our comparison against
multiple annotators reveals that VAD performance of current models varies
dramatically and is highly sensitive to both the amount of training data and
the variability in visual content that defines "normal" scenes. Our results
highlight the value of soft and consensus labels and offer a practical approach
for supporting scientific exploration and scalable biodiversity monitoring.


### [8] [LightPneumoNet: Lightweight Pneumonia Classifier](https://arxiv.org/abs/2510.11232)
*Neilansh Chauhan, Piyush Kumar Gupta, Faraz Doja*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†LightPneumoNetï¼Œä¸€ç§è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œï¼Œä¸“ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„è‚ºç‚Xå…‰è¯Šæ–­è®¾è®¡ï¼Œåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è‚ºç‚è¯Šæ–­é¢ä¸´å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²å›°éš¾çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—èƒ½åŠ›å’Œå­˜å‚¨ç©ºé—´æœ‰é™çš„åŒ»ç–—åœºæ™¯ä¸‹ï¼Œéœ€è¦å¼€å‘é«˜æ•ˆä¸”å‡†ç¡®çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆã€‚

**Method:** é‡‡ç”¨ä»å¤´è®¾è®¡çš„è½»é‡çº§CNNæ¶æ„ï¼ŒåŒ…å«å››ä¸ªå·ç§¯å—å †å ï¼Œä»…388,082ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œé¢„å¤„ç†åŒ…æ‹¬å›¾åƒç¼©æ”¾è‡³224x224ã€ç°åº¦è½¬æ¢å’Œåƒç´ å½’ä¸€åŒ–ï¼Œå¹¶é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

**Result:** åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šå–å¾—äº†0.942çš„æ€»ä½“å‡†ç¡®ç‡ã€0.92çš„ç²¾ç¡®ç‡å’Œ0.96çš„F1åˆ†æ•°ï¼Œç‰¹åˆ«æ˜¯è¾¾åˆ°äº†0.99çš„æ•æ„Ÿåº¦ï¼Œå‡ ä¹å®Œç¾è¯†åˆ«çœŸå®è‚ºç‚ç—…ä¾‹å¹¶æœ€å°åŒ–ä¸´åºŠé‡è¦çš„å‡é˜´æ€§ã€‚

**Conclusion:** è¯¥æ¨¡å‹çš„é«˜æ•ˆæ€§ä½¿å…¶èƒ½å¤Ÿåœ¨ä½æˆæœ¬ç¡¬ä»¶ä¸Šéƒ¨ç½²ï¼Œä¸ºèµ„æºåŒ®ä¹è¯Šæ‰€æä¾›å…ˆè¿›çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å·¥å…·ï¼Œä½œä¸ºå¯é çš„ç¬¬äºŒæ„è§å·¥å…·æ”¹å–„æ‚£è€…é¢„åï¼ŒåŒæ—¶å±•ç¤ºäº†è½»é‡çº§æ¶æ„åœ¨åŒ»ç–—å½±åƒåˆ†æä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Effective pneumonia diagnosis is often challenged by the difficulty of
deploying large, computationally expensive deep learning models in
resource-limited settings. This study introduces LightPneumoNet, an efficient,
lightweight convolutional neural network (CNN) built from scratch to provide an
accessible and accurate diagnostic solution for pneumonia detection from chest
X-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.
Preprocessing included image resizing to 224x224, grayscale conversion, and
pixel normalization, with data augmentation (rotation, zoom, shear) to prevent
overfitting. The custom architecture features four blocks of stacked
convolutional layers and contains only 388,082 trainable parameters, resulting
in a minimal 1.48 MB memory footprint. On the independent test set, our model
delivered exceptional performance, achieving an overall accuracy of 0.942,
precision of 0.92, and an F1-Score of 0.96. Critically, it obtained a
sensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify
true pneumonia cases and minimize clinically significant false negatives.
Notably, LightPneumoNet achieves this high recall on the same dataset where
existing approaches typically require significantly heavier architectures or
fail to reach comparable sensitivity levels. The model's efficiency enables
deployment on low-cost hardware, making advanced computer-aided diagnosis
accessible in underserved clinics and serving as a reliable second-opinion tool
to improve patient outcomes.


### [9] [Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation](https://arxiv.org/abs/2510.11005)
*Kai Han, Siqi Ma, Chengxuan Qian, Jun Chen, Chongwen Lyu, Yuqing Song, Zhe Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†å‰æ™¯æ„ŸçŸ¥é¢‘è°±åˆ†å‰²ï¼ˆFASSï¼‰æ¡†æ¶ï¼Œé€šè¿‡å‰æ™¯æ„ŸçŸ¥æ¨¡å—ã€å°æ³¢å˜æ¢ç‰¹å¾å¢å¼ºå’Œè¾¹ç¼˜çº¦æŸæ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†åŒ»å­¦å›¾åƒä¸­ä½å¯¹æ¯”åº¦è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŸºç¡€æ¨¡å‹åœ¨åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚ä½å¯¹æ¯”åº¦èƒŒæ™¯ä¸‹éš¾ä»¥èšç„¦å‰æ™¯åŒºåŸŸï¼Œç‰¹åˆ«æ˜¯å½“æ¶æ€§è‚¿ç˜¤ä¸æ­£å¸¸å™¨å®˜å½¢æ€ç›¸ä¼¼æ—¶ï¼Œä¸Šä¸‹æ–‡åŒºåˆ†å˜å¾—å›°éš¾ï¼Œè¿™é™åˆ¶äº†åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨æ‰‹æœ¯è§„åˆ’å’Œè‚¿ç˜¤åˆ†æœŸä¸­çš„å‡†ç¡®æ€§ã€‚

**Method:** æå‡ºFASSæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šå‰æ™¯æ„ŸçŸ¥æ¨¡å—å¢å¼ºèƒŒæ™¯ä¸æ•´ä½“ä½“ç§¯ç©ºé—´çš„åŒºåˆ†åº¦ï¼›åŸºäºå°æ³¢å˜æ¢çš„ç‰¹å¾çº§é¢‘ç‡å¢å¼ºæ¨¡å—æå–åˆ¤åˆ«æ€§é«˜é¢‘ç‰¹å¾ä»¥æ”¹å–„è¾¹ç•Œè¯†åˆ«ï¼›è¾¹ç¼˜çº¦æŸæ¨¡å—ä¿æŒåˆ†å‰²è¾¹ç•Œçš„å‡ ä½•è¿ç»­æ€§ã€‚

**Result:** åœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ¡ä»¶ä¸‹çš„é²æ£’æ€§å’Œç²¾ç»†ç»“æ„è¯†åˆ«æ–¹é¢éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†ä½å¯¹æ¯”åº¦å›¾åƒçš„åˆ†å‰²è´¨é‡ã€‚

**Conclusion:** è¯¥æ¡†æ¶æ˜¾è‘—å¢å¼ºäº†ä½å¯¹æ¯”åº¦åŒ»å­¦å›¾åƒçš„åˆ†å‰²èƒ½åŠ›ï¼Œä¸ºæ›´å¹¿æ³›å’Œå¤æ‚çš„åŒ»å­¦æˆåƒåœºæ™¯åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œåœ¨æ‰‹æœ¯è§„åˆ’å’Œè‚¿ç˜¤åˆ†æœŸç­‰ä¸´åºŠä»»åŠ¡ä¸­å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Accurate segmentation of tumors and adjacent normal tissues in medical images
is essential for surgical planning and tumor staging. Although foundation
models generally perform well in segmentation tasks, they often struggle to
focus on foreground areas in complex, low-contrast backgrounds, where some
malignant tumors closely resemble normal organs, complicating contextual
differentiation. To address these challenges, we propose the Foreground-Aware
Spectrum Segmentation (FASS) framework. First, we introduce a foreground-aware
module to amplify the distinction between background and the entire volume
space, allowing the model to concentrate more effectively on target areas.
Next, a feature-level frequency enhancement module, based on wavelet transform,
extracts discriminative high-frequency features to enhance boundary recognition
and detail perception. Eventually, we introduce an edge constraint module to
preserve geometric continuity in segmentation boundaries. Extensive experiments
on multiple medical datasets demonstrate superior performance across all
metrics, validating the effectiveness of our framework, particularly in
robustness under complex conditions and fine structure recognition. Our
framework significantly enhances segmentation of low-contrast images, paving
the way for applications in more diverse and complex medical imaging scenarios.


### [10] [LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation](https://arxiv.org/abs/2510.11063)
*Chang Liu, Henghui Ding, Kaining Ying, Lingyi Hong, Ning Xu, Linjie Yang, Yuchen Fan, Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han, Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Chang Soo Lim, Joonyoung Moon, Donghyeon Cho, Tingmin Li, Yixuan Li, Yang Yang, An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu, Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan, Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji, Ran Hong, Feng Lu, Leilei Cao, An Yan, Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian Leibe*

#### ğŸ§© TL;DR
æœ¬æ–‡ä»‹ç»äº†ICCV 2025ç¬¬ä¸ƒå±Šå¤§è§„æ¨¡è§†é¢‘ç›®æ ‡åˆ†å‰²æŒ‘æˆ˜èµ›ï¼Œåœ¨ä¼ ç»ŸVOSå’ŒRVOSèµ›é“åŸºç¡€ä¸Šæ–°å¢äº†æ›´å…·æŒ‘æˆ˜æ€§çš„MOSEv2èµ›é“ï¼Œé€šè¿‡å¼•å…¥æ›´å¤æ‚çš„çœŸå®åœºæ™¯æ¨åŠ¨é•¿æœŸä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„å‘å±•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†é¢‘ç›®æ ‡åˆ†å‰²åœ¨çœŸå®å¤æ‚åœºæ™¯ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¯†é›†å°ç›®æ ‡ã€é¢‘ç¹æ¶ˆå¤±é‡ç°ã€ä¸¥é‡é®æŒ¡ã€æ¶åŠ£å¤©æ°”å’Œå…‰ç…§ç­‰æ›´å…·æŒ‘æˆ˜æ€§ä½†æ›´ç°å®çš„åœºæ™¯ï¼Œæ¨åŠ¨æ¨¡å‹åœ¨éç²¾å¿ƒç­–åˆ’åŸºå‡†æµ‹è¯•ä¹‹å¤–çš„é•¿æœŸä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Method:** æŒ‘æˆ˜èµ›ä¿ç•™äº†VOSå’ŒRVOSä¸¤ä¸ªä¼ ç»Ÿèµ›é“ï¼Œå¹¶æ–°å¢äº†MOSEv2èµ›é“ï¼Œè¯¥èµ›é“é‡‡ç”¨J&Fä½œä¸ºä¸»è¦æ’åæŒ‡æ ‡ä»¥æ›´å¥½åœ°è¯„ä¼°è·¨å°ºåº¦å’Œæ¶ˆå¤±æƒ…å†µä¸‹çš„ç›®æ ‡åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶å¼ºè°ƒäº†LLM/MLLMç»„ä»¶å’Œå†…å­˜æ„ŸçŸ¥ä¼ æ’­ç­‰æ–°å…´æŠ€æœ¯è¶‹åŠ¿ã€‚

**Result:** æŒ‘æˆ˜èµ›é‡‡ç”¨äº†æ ‡å‡†Jã€Få’ŒJ&FæŒ‡æ ‡è¯„ä¼°VOSå’ŒRVOSèµ›é“æ€§èƒ½ï¼Œè€ŒMOSEv2èµ›é“é‡‡ç”¨J&Fä½œä¸ºä¸»è¦æ’åæŒ‡æ ‡ï¼Œé€šè¿‡æ€»ç»“é¡¶çº§è§£å†³æ–¹æ¡ˆå’Œæ–°å…´è¶‹åŠ¿ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚çœŸå®åœºæ™¯ä¸‹è§†é¢‘åˆ†å‰²çš„æœ€æ–°è¿›å±•ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºé²æ£’ã€è¯­è¨€æ„ŸçŸ¥çš„è§†é¢‘åˆ†å‰²æŠ€æœ¯æŒ‡æ˜äº†æœªæ¥å‘å±•æ–¹å‘ï¼Œå¼ºè°ƒäº†åœ¨çœŸå®å¤æ‚åœºæ™¯ä¸­æå‡æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºç¤¾åŒºæä¾›äº†è¯„ä¼°é•¿æœŸä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„æ–°åŸºå‡†å’Œåè®®ã€‚

---

#### ğŸ“„ Abstract
This report presents an overview of the 7th Large-scale Video Object
Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2). Building upon prior
insights, MOSEv2 substantially increases difficulty, introducing more
challenging but realistic scenarios including denser small objects, frequent
disappear/reappear events, severe occlusions, adverse weather and lighting,
etc., pushing long-term consistency and generalization beyond curated
benchmarks. The challenge retains standard ${J}$, $F$, and ${J\&F}$ metrics for
VOS and RVOS, while MOSEv2 adopts ${J\&\dot{F}}$ as the primary ranking metric
to better evaluate objects across scales and disappearance cases. We summarize
datasets and protocols, highlight top-performing solutions, and distill
emerging trends, such as the growing role of LLM/MLLM components and
memory-aware propagation, aiming to chart future directions for resilient,
language-aware video segmentation in the wild.


### [11] [When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models](https://arxiv.org/abs/2510.11302)
*Samer Al-Hamadani*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é¦–æ¬¡å¯¹ç›‘ç£å­¦ä¹ ç›®æ ‡æ£€æµ‹ï¼ˆYOLOï¼‰ä¸é›¶æ ·æœ¬è§†è§‰è¯­è¨€æ¨¡å‹æ£€æµ‹ï¼ˆGemini Flash 2.5ï¼‰è¿›è¡Œæˆæœ¬æ•ˆç›Šç»¼åˆåˆ†æï¼Œå»ºç«‹äº†æ¶æ„é€‰æ‹©çš„å®šé‡å¹³è¡¡ç‚¹é˜ˆå€¼ï¼Œæ­ç¤ºäº†éƒ¨ç½²è§„æ¨¡ã€ç±»åˆ«ç¨³å®šæ€§å’Œé¢„ç®—çº¦æŸå¯¹æœ€ä¼˜æ¶æ„é€‰æ‹©çš„å…³é”®å½±å“ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿç›‘ç£ç›®æ ‡æ£€æµ‹ä¾èµ–äººå·¥æ ‡æ³¨è¾¹ç•Œæ¡†ï¼Œè™½ç„¶ç²¾åº¦é«˜ä½†æ ‡æ³¨æˆæœ¬å·¨å¤§ï¼Œè€Œæ–°å…´çš„è§†è§‰è¯­è¨€æ¨¡å‹æ”¯æŒé›¶æ ·æœ¬æ£€æµ‹ä½†ç²¾åº¦è¾ƒä½ï¼Œç¼ºä¹ç³»ç»Ÿæ€§çš„æˆæœ¬æ•ˆç›Šæ¯”è¾ƒåˆ†ææ¥æŒ‡å¯¼å®é™…éƒ¨ç½²å†³ç­–ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿè¯„ä¼°æ–¹æ³•ï¼Œåœ¨1,000å¼ åˆ†å±‚COCOå›¾åƒå’Œ200å¼ å¤šæ ·åŒ–äº§å“å›¾åƒä¸Šè¿›è¡Œæµ‹è¯•ï¼Œç»“åˆè¯¦ç»†çš„æ€»æ‹¥æœ‰æˆæœ¬å»ºæ¨¡ï¼Œæ¯”è¾ƒç›‘ç£YOLOæ£€æµ‹ä¸é›¶æ ·æœ¬Gemini Flash 2.5æ¨ç†çš„æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚

**Result:** ç›‘ç£YOLOåœ¨æ ‡å‡†ç±»åˆ«ä¸Šè¾¾åˆ°91.2%å‡†ç¡®ç‡ï¼Œæ˜¾è‘—é«˜äºGeminiçš„68.5%ï¼Œä½†éœ€è¦10,800ç¾å…ƒæ ‡æ³¨æˆæœ¬ï¼›åªæœ‰åœ¨è¶…è¿‡5,500ä¸‡æ¬¡æ¨ç†æ—¶æŠ•èµ„æ‰åˆç†ï¼›Geminiåœ¨å¤šæ ·åŒ–äº§å“ç±»åˆ«ä¸Šå®ç°52.3%å‡†ç¡®ç‡ï¼Œè€ŒYOLOå› æ¶æ„é™åˆ¶å¯¹æœªè®­ç»ƒç±»åˆ«å‡†ç¡®ç‡ä¸º0%ï¼›æ¯æ­£ç¡®æ£€æµ‹æˆæœ¬åˆ†ææ˜¾ç¤ºGeminiåœ¨10ä¸‡æ¬¡æ¨ç†æ—¶æˆæœ¬æ˜¾è‘—æ›´ä½ï¼ˆ0.00050ç¾å…ƒ vs 0.143ç¾å…ƒï¼‰ã€‚

**Conclusion:** æœ€ä¼˜æ£€æµ‹æ¶æ„é€‰æ‹©å…³é”®å–å†³äºéƒ¨ç½²è§„æ¨¡ã€ç±»åˆ«ç¨³å®šæ€§ã€é¢„ç®—çº¦æŸå’Œç²¾åº¦è¦æ±‚ï¼Œè€Œéçº¯ç²¹æŠ€æœ¯æ€§èƒ½æŒ‡æ ‡ï¼›ç ”ç©¶å¼€å‘äº†å†³ç­–æ¡†æ¶ï¼Œè¯æ˜åœ¨ä½æ¨ç†é‡ã€åŠ¨æ€ç±»åˆ«æˆ–é¢„ç®—å—é™åœºæ™¯ä¸‹ï¼Œé›¶æ ·æœ¬VLMæ›´å…·æˆæœ¬æ•ˆç›Šï¼Œè€Œåœ¨é«˜æ¨ç†é‡ã€ç¨³å®šç±»åˆ«åœºæ™¯ä¸‹ç›‘ç£å­¦ä¹ æ›´ä¼˜ã€‚

---

#### ğŸ“„ Abstract
Object detection systems have traditionally relied on supervised learning
with manually annotated bounding boxes, achieving high accuracy at the cost of
substantial annotation investment. The emergence of Vision-Language Models
(VLMs) offers an alternative paradigm enabling zero-shot detection through
natural language queries, eliminating annotation requirements but operating
with reduced accuracy. This paper presents the first comprehensive
cost-effectiveness analysis comparing supervised detection (YOLO) with
zero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on
1,000 stratified COCO images and 200 diverse product images spanning consumer
electronics and rare categories, combined with detailed Total Cost of Ownership
modeling, we establish quantitative break-even thresholds governing
architecture selection. Our findings reveal that supervised YOLO achieves 91.2%
accuracy versus 68.5% for zero-shot Gemini on standard categories, representing
a 22.7 percentage point advantage that costs $10,800 in annotation for
100-category systems. However, this advantage justifies investment only beyond
55 million inferences, equivalent to 151,000 images daily for one year.
Zero-shot Gemini demonstrates 52.3% accuracy on diverse product categories
(ranging from highly web-prevalent consumer electronics at 75-85% to rare
specialized equipment at 25-40%) where supervised YOLO achieves 0% due to
architectural constraints preventing detection of untrained classes. Cost per
Correct Detection analysis reveals substantially lower per-detection costs for
Gemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We
develop decision frameworks demonstrating that optimal architecture selection
depends critically on deployment volume, category stability, budget
constraints, and accuracy requirements rather than purely technical performance
metrics.


### [12] [Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation](https://arxiv.org/abs/2510.11305)
*Jean-Paul Travert, CÃ©dric Goeury, SÃ©bastien Boyaval, Vito Bacchi, Fabrice Zaoui*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†SARå›¾åƒæ´ªæ°´åˆ¶å›¾å’Œæ°´æ·±ä¼°è®¡çš„å®Œæ•´å¤„ç†æµç¨‹ï¼Œå‘ç°æ–¹æ³•é€‰æ‹©å’Œè¶…å‚æ•°è°ƒä¼˜å¯¹ç»“æœæœ‰æ˜¾è‘—å½±å“ï¼Œå»ºè®®é‡‡ç”¨é›†æˆæ–¹æ³•è€Œéå•ä¸€é…ç½®æ¥å¤„ç†æ´ªæ°´ç›‘æµ‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ´ªæ°´åˆ¶å›¾å’Œæ°´æ·±ä¼°è®¡å¯¹äºæ ¡å‡†å’ŒéªŒè¯æ°´åŠ›æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†SARå›¾åƒæ—¶å­˜åœ¨é¢„å¤„ç†ã€æ´ªæ°´åˆ¶å›¾å’Œæ°´æ·±ä¼°è®¡å„æ­¥éª¤æ–¹æ³•é€‰æ‹©åŠè¶…å‚æ•°è°ƒä¼˜çš„ä¸ç¡®å®šæ€§ï¼Œéœ€è¦ç³»ç»Ÿè¯„ä¼°è¿™äº›å› ç´ å¯¹æœ€ç»ˆç»“æœçš„å½±å“ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨é›†æˆæ–¹æ³•è¯„ä¼°å¤šç§é¢„å¤„ç†æŠ€æœ¯ï¼ˆç‰¹åˆ«æ˜¯æ–‘ç‚¹å™ªå£°æŠ‘åˆ¶ï¼‰ã€æ´ªæ°´åˆ¶å›¾æ–¹æ³•ï¼ˆåŒ…æ‹¬ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ï¼‰å’Œæ°´æ·±ä¼°è®¡ç®—æ³•ï¼Œé€šè¿‡è€ƒè™‘é¢„å¤„ç†å›¾åƒã€æ´ªæ°´å›¾å’Œæ°´æ·±åœºçš„é›†åˆæ¥åˆ†æä¸åŒæ­¥éª¤æ–¹æ³•é€‰æ‹©åŠå…¶è¶…å‚æ•°çš„å½±å“ã€‚

**Result:** ç»“æœè¡¨æ˜æ–‘ç‚¹æ»¤æ³¢å™¨çš„é€‰æ‹©ä¼šæ”¹å˜æ•°å¹³æ–¹å…¬é‡Œçš„æ´ªæ°´èŒƒå›´ä¼°è®¡ï¼Œç›‘ç£æ–¹æ³•ä¼˜äºæ— ç›‘ç£æ–¹æ³•ä½†è°ƒä¼˜åçš„æ— ç›‘ç£æ–¹æ³•ï¼ˆå¦‚å±€éƒ¨é˜ˆå€¼æˆ–å˜åŒ–æ£€æµ‹ï¼‰å¯è¾¾åˆ°ç›¸å½“æ€§èƒ½ï¼Œé¢„å¤„ç†å’Œæ´ªæ°´åˆ¶å›¾æ­¥éª¤çš„ç´¯ç§¯ä¸ç¡®å®šæ€§å¯¼è‡´æ°´æ·±åœºä¼°è®¡å­˜åœ¨é«˜åº¦å˜å¼‚æ€§ã€‚

**Conclusion:** ç ”ç©¶å¼ºè°ƒå¿…é¡»è€ƒè™‘åŒ…å«é¢„å¤„ç†ã€æ´ªæ°´åˆ¶å›¾å’Œæ°´æ·±ä¼°è®¡æ–¹æ³•åŠå…¶ç›¸å…³è¶…å‚æ•°çš„å®Œæ•´å¤„ç†æµç¨‹ï¼Œåº”é‡‡ç”¨é›†æˆæ–¹æ³•å¹¶è€ƒè™‘æ–¹æ³•å­¦ä¸ç¡®å®šæ€§è€Œéä¾èµ–å•ä¸€é…ç½®ï¼Œæ´ªæ°´åˆ¶å›¾ä¸­æ–¹æ³•é€‰æ‹©å½±å“æœ€å¤§ï¼Œæ°´æ·±ä¼°è®¡ä¸­æœ€å…·å½±å“åŠ›çš„å¤„ç†æ­¥éª¤æ˜¯æ´ªæ°´åˆ¶å›¾æ­¥éª¤äº§ç”Ÿçš„æ´ªæ°´å›¾è¾“å…¥å’Œæ–¹æ³•è¶…å‚æ•°ã€‚

---

#### ğŸ“„ Abstract
Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)
imagery are crucial for calibrating and validating hydraulic models. This study
uses SAR imagery to evaluate various preprocessing (especially speckle noise
reduction), flood mapping, and water depth estimation methods. The impact of
the choice of method at different steps and its hyperparameters is studied by
considering an ensemble of preprocessed images, flood maps, and water depth
fields. The evaluation is conducted for two flood events on the Garonne River
(France) in 2019 and 2021, using hydrodynamic simulations and in-situ
observations as reference data. Results show that the choice of speckle filter
alters flood extent estimations with variations of several square kilometers.
Furthermore, the selection and tuning of flood mapping methods also affect
performance. While supervised methods outperformed unsupervised ones, tuned
unsupervised approaches (such as local thresholding or change detection) can
achieve comparable results. The compounded uncertainty from preprocessing and
flood mapping steps also introduces high variability in the water depth field
estimates. This study highlights the importance of considering the entire
processing pipeline, encompassing preprocessing, flood mapping, and water depth
estimation methods and their associated hyperparameters. Rather than relying on
a single configuration, adopting an ensemble approach and accounting for
methodological uncertainty should be privileged. For flood mapping, the method
choice has the most influence. For water depth estimation, the most influential
processing step was the flood map input resulting from the flood mapping step
and the hyperparameters of the methods.


### [13] [SNAP: Towards Segmenting Anything in Any Point Cloud](https://arxiv.org/abs/2510.11565)
*Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang*

#### ğŸ§© TL;DR
SNAPæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„äº¤äº’å¼3Dç‚¹äº‘åˆ†å‰²æ¨¡å‹ï¼Œæ”¯æŒè·¨åŸŸçš„ç‚¹åŸºå’Œæ–‡æœ¬åŸºæç¤ºï¼Œé€šè¿‡åœ¨7ä¸ªæ•°æ®é›†ä¸Šçš„è®­ç»ƒå’ŒåŸŸè‡ªé€‚åº”å½’ä¸€åŒ–å®ç°äº†å“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰äº¤äº’å¼3Dç‚¹äº‘åˆ†å‰²æ–¹æ³•å­˜åœ¨é¢†åŸŸé™åˆ¶ï¼ˆä»…å®¤å†…æˆ–å®¤å¤–ï¼‰å’Œäº¤äº’æ–¹å¼å•ä¸€ï¼ˆä»…ç©ºé—´ç‚¹å‡»æˆ–æ–‡æœ¬æç¤ºï¼‰çš„é—®é¢˜ï¼ŒåŒæ—¶åœ¨å¤šæ•°æ®é›†è®­ç»ƒæ—¶å®¹æ˜“äº§ç”Ÿè´Ÿè¿ç§»ï¼Œå¯¼è‡´ç¼ºä¹é€šç”¨æ€§çš„é¢†åŸŸç‰¹å®šå·¥å…·ã€‚

**Method:** æå‡ºSNAPç»Ÿä¸€æ¨¡å‹ï¼Œé‡‡ç”¨åŸŸè‡ªé€‚åº”å½’ä¸€åŒ–é˜²æ­¢è´Ÿè¿ç§»ï¼Œåœ¨7ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šè®­ç»ƒï¼›å¯¹äºæ–‡æœ¬æç¤ºåˆ†å‰²ï¼Œè‡ªåŠ¨ç”Ÿæˆæ©ç ææ¡ˆå¹¶ä¸æ–‡æœ¬æŸ¥è¯¢çš„CLIPåµŒå…¥è¿›è¡ŒåŒ¹é…ï¼Œå®ç°å…¨æ™¯å’Œå¼€æ”¾è¯æ±‡åˆ†å‰²ã€‚

**Result:** åœ¨ç©ºé—´æç¤ºåˆ†å‰²çš„9ä¸ªé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­ï¼Œ8ä¸ªè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼›åœ¨æ–‡æœ¬æç¤ºåˆ†å‰²çš„5ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—ç«äº‰æ€§ç»“æœï¼Œè¯æ˜äº†ç»Ÿä¸€æ¨¡å‹å¯ä»¥åŒ¹é…æˆ–è¶…è¶Šä¸“é—¨çš„é¢†åŸŸç‰¹å®šæ–¹æ³•ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ç»Ÿä¸€æ¨¡å‹èƒ½å¤Ÿå®ç°è·¨åŸŸé€šç”¨æ€§ï¼Œä¸ºå¯æ‰©å±•çš„3Dæ ‡æ³¨æä¾›äº†å®ç”¨å·¥å…·ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿæ–¹æ³•åœ¨é¢†åŸŸå’Œäº¤äº’æ–¹å¼ä¸Šçš„é™åˆ¶ã€‚

---

#### ğŸ“„ Abstract
Interactive 3D point cloud segmentation enables efficient annotation of
complex 3D scenes through user-guided prompts. However, current approaches are
typically restricted in scope to a single domain (indoor or outdoor), and to a
single form of user interaction (either spatial clicks or textual prompts).
Moreover, training on multiple datasets often leads to negative transfer,
resulting in domain-specific tools that lack generalizability. To address these
limitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in
\textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3D
segmentation that supports both point-based and text-based prompts across
diverse domains. Our approach achieves cross-domain generalizability by
training on 7 datasets spanning indoor, outdoor, and aerial environments, while
employing domain-adaptive normalization to prevent negative transfer. For
text-prompted segmentation, we automatically generate mask proposals without
human intervention and match them against CLIP embeddings of textual queries,
enabling both panoptic and open-vocabulary segmentation. Extensive experiments
demonstrate that SNAP consistently delivers high-quality segmentation results.
We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for
spatial-prompted segmentation and demonstrate competitive results on all 5
text-prompted benchmarks. These results show that a unified model can match or
exceed specialized domain-specific approaches, providing a practical tool for
scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/


### [14] [MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.11579)
*Hongyu Zhu, Lin Chen, Mounim A. El-Yacoubi, Mingsheng Shang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MS-Mixï¼Œä¸€ç§ç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„è‡ªé€‚åº”æƒ…æ„Ÿæ•æ„Ÿæ•°æ®å¢å¼ºæ¡†æ¶ï¼Œé€šè¿‡æƒ…æ„Ÿæ„ŸçŸ¥æ ·æœ¬é€‰æ‹©ã€æƒ…æ„Ÿå¼ºåº¦å¼•å¯¼çš„æ··åˆæ¯”ä¾‹è®¡ç®—å’Œæƒ…æ„Ÿå¯¹é½æŸå¤±ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸMixupæ–¹æ³•åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹å¯¼è‡´çš„æ ‡ç­¾æ¨¡ç³Šå’Œè¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æé¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œè€Œä¼ ç»Ÿçš„Mixupæ•°æ®å¢å¼ºæ–¹æ³•ç›´æ¥åº”ç”¨äºå¤šæ¨¡æ€åœºæ™¯æ—¶ä¼šå¼•å…¥æ ‡ç­¾æ¨¡ç³Šå’Œè¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜ï¼Œå› ä¸ºç¼ºä¹æƒ…æ„Ÿæ„ŸçŸ¥çš„æ··åˆæœºåˆ¶å¯¼è‡´éšæœºæ··åˆå¯èƒ½æ”¾å¤§æƒ…æ„ŸçŸ›ç›¾æ ·æœ¬é—´çš„è¯­ä¹‰æ··æ·†ã€‚

**Method:** MS-Mixæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæƒ…æ„Ÿæ„ŸçŸ¥æ ·æœ¬é€‰æ‹©ç­–ç•¥é˜²æ­¢æƒ…æ„ŸçŸ›ç›¾æ ·æœ¬æ··åˆå¯¼è‡´çš„è¯­ä¹‰æ··æ·†ï¼›æƒ…æ„Ÿå¼ºåº¦å¼•å¯¼æ¨¡å—ä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›åŠ¨æ€è®¡ç®—æ¨¡æ€ç‰¹å®šçš„æ··åˆæ¯”ä¾‹ï¼›æƒ…æ„Ÿå¯¹é½æŸå¤±é€šè¿‡KLæ•£åº¦æŸå¤±ä½œä¸ºæ­£åˆ™åŒ–é¡¹è”åˆè®­ç»ƒæƒ…æ„Ÿå¼ºåº¦é¢„æµ‹å™¨å’Œéª¨å¹²ç½‘ç»œã€‚

**Result:** åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œå…­ä¸ªæœ€å…ˆè¿›éª¨å¹²ç½‘ç»œä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMS-MixæŒç»­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œä¸ºé²æ£’çš„å¤šæ¨¡æ€æƒ…æ„Ÿå¢å¼ºå»ºç«‹äº†æ–°çš„æ ‡å‡†ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æƒ…æ„Ÿæ„ŸçŸ¥çš„æ•°æ®å¢å¼ºåœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„é‡è¦æ€§ï¼ŒMS-Mixæ¡†æ¶é€šè¿‡è‡ªé€‚åº”æ··åˆæœºåˆ¶æœ‰æ•ˆç¼“è§£äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼ŒåŒæ—¶ä¿æŒäº†è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä¸ºå¤šæ¨¡æ€å­¦ä¹ ä¸­çš„å¢å¼ºç­–ç•¥è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Multimodal Sentiment Analysis (MSA) aims to identify and interpret human
emotions by integrating information from heterogeneous data sources such as
text, video, and audio. While deep learning models have advanced in network
architecture design, they remain heavily limited by scarce multimodal annotated
data. Although Mixup-based augmentation improves generalization in unimodal
tasks, its direct application to MSA introduces critical challenges: random
mixing often amplifies label ambiguity and semantic inconsistency due to the
lack of emotion-aware mixing mechanisms. To overcome these issues, we propose
MS-Mix, an adaptive, emotion-sensitive augmentation framework that
automatically optimizes sample mixing in multimodal settings. The key
components of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)
strategy that effectively prevents semantic confusion caused by mixing samples
with contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module
using multi-head self-attention to compute modality-specific mixing ratios
dynamically based on their respective emotional intensities. (3) a Sentiment
Alignment Loss (SAL) that aligns the prediction distributions across
modalities, and incorporates the Kullback-Leibler-based loss as an additional
regularization term to train the emotion intensity predictor and the backbone
network jointly. Extensive experiments on three benchmark datasets with six
state-of-the-art backbones confirm that MS-Mix consistently outperforms
existing methods, establishing a new standard for robust multimodal sentiment
augmentation. The source code is available at:
https://github.com/HongyuZhu-s/MS-Mix.


### [15] [Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams](https://arxiv.org/abs/2510.11717)
*Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Ev4DGSï¼Œè¿™æ˜¯é¦–ä¸ªä»…ä»å•ç›®äº‹ä»¶æµä¸­å®ç°éåˆšæ€§å˜å½¢ç‰©ä½“æ–°è§†è§’æ¸²æŸ“çš„æ–¹æ³•ï¼Œé€šè¿‡å¯å˜å½¢3Dé«˜æ–¯æ³¼æº…è¡¨ç¤ºå’ŒåŸºäºäº‹ä»¶çš„æŸå¤±å‡½æ•°ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•éœ€è¦RGBè¾“å…¥çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºäº‹ä»¶ç›¸æœºçš„æ–°è§†è§’æ¸²æŸ“æ–¹æ³•ä¸»è¦é’ˆå¯¹åˆšæ€§åœºæ™¯ï¼Œå¯¹äºéåˆšæ€§ç‰©ä½“åˆ™éœ€è¦é¢å¤–çš„ç¨€ç–RGBè¾“å…¥ï¼Œè¿™åœ¨å®è·µä¸­å­˜åœ¨æ˜¾è‘—é™åˆ¶ï¼›æœ¬æ–‡æ—¨åœ¨æ¢ç´¢æ˜¯å¦èƒ½å¤Ÿä»…ä»äº‹ä»¶æµä¸­å­¦ä¹ ç±»ä¼¼æ¨¡å‹ï¼Œå¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** è¯¥æ–¹æ³•é€šè¿‡å›å½’å¯å˜å½¢3Dé«˜æ–¯æ³¼æº…è¡¨ç¤ºï¼Œé‡‡ç”¨ä¸¤ç§å…³é”®æŠ€æœ¯ï¼š1ï¼‰å°†ä¼°è®¡æ¨¡å‹è¾“å‡ºä¸2Däº‹ä»¶è§‚æµ‹ç©ºé—´ç›¸å…³è”çš„æŸå¤±å‡½æ•°ï¼›2ï¼‰ä»äº‹ä»¶ç”Ÿæˆçš„äºŒå€¼æ©ç ä¸­è®­ç»ƒçš„ç²—ç³™3Då˜å½¢æ¨¡å‹ï¼Œå®ç°äº†ä»…åŸºäºäº‹ä»¶æµçš„éåˆšæ€§åœºæ™¯å»ºæ¨¡ã€‚

**Result:** åœ¨ç°æœ‰çš„åˆæˆæ•°æ®é›†å’Œæ–°å½•åˆ¶çš„çœŸå®éåˆšæ€§ç‰©ä½“æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEv4DGSæ–¹æ³•æœ‰æ•ˆå¯è¡Œï¼Œå¹¶ä¸”åœ¨æˆ‘ä»¬çš„è®¾å®šä¸­ç›¸æ¯”å¤šä¸ªæœ´ç´ åŸºçº¿æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†ä»…ä½¿ç”¨äº‹ä»¶æµè¿›è¡Œéåˆšæ€§æ–°è§†è§’æ¸²æŸ“çš„å¯è¡Œæ€§ã€‚

**Conclusion:** è¿™é¡¹ç ”ç©¶è¯æ˜äº†ä»…ä»äº‹ä»¶æµä¸­å­¦ä¹ éåˆšæ€§å˜å½¢ç‰©ä½“æ–°è§†è§’æ¸²æŸ“æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä¸ºäº‹ä»¶ç›¸æœºåœ¨åŠ¨æ€åœºæ™¯å»ºæ¨¡ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼ŒåŒæ—¶å‘å¸ƒçš„æ¨¡å‹å’Œæ•°æ®é›†å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi, Zheyuan Zhang, Zhengqing Yuan, Keerthiram Murugesan, Vincent Galass, Chuxu Zhang, Yanfang Ye*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºNG-Routeræ¡†æ¶ï¼Œå°†è¥å…»é—®ç­”å»ºæ¨¡ä¸ºåŸºäºçŸ¥è¯†å›¾è°±å¼•å¯¼çš„å¤šæ™ºèƒ½ä½“åä½œé—®é¢˜ï¼Œé€šè¿‡å›¾ç¥ç»ç½‘ç»œå­¦ä¹ ä»»åŠ¡æ„ŸçŸ¥çš„è·¯ç”±åˆ†å¸ƒï¼Œæœ‰æ•ˆè§£å†³äº†å•æ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›æœ‰é™å’Œå¤šæ™ºèƒ½ä½“æ¶æ„è®¾è®¡å¤æ‚ç­‰æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è¥å…»é—®ç­”æ–¹æ³•é¢ä¸´ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šå•æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œä»¥åŠè®¾è®¡æœ‰æ•ˆå¤šæ™ºèƒ½ä½“æ¶æ„çš„å¤æ‚æ€§ï¼ŒåŒæ—¶ä¸Šä¸‹æ–‡è¿‡è½½é—®é¢˜é˜»ç¢äº†å‡†ç¡®å†³ç­–åˆ¶å®šã€‚

**Method:** NG-Routeræ¡†æ¶å°†æ™ºèƒ½ä½“èŠ‚ç‚¹é›†æˆåˆ°å¼‚æ„çŸ¥è¯†å›¾è°±ä¸­ï¼Œé‡‡ç”¨å›¾ç¥ç»ç½‘ç»œå­¦ä¹ åŸºäºç»éªŒæ™ºèƒ½ä½“æ€§èƒ½çš„è½¯ç›‘ç£ä»»åŠ¡æ„ŸçŸ¥è·¯ç”±åˆ†å¸ƒï¼Œå¹¶æå‡ºåŸºäºæ¢¯åº¦çš„å­å›¾æ£€ç´¢æœºåˆ¶æ¥è¯†åˆ«è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®è¯æ®ã€‚

**Result:** åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œéª¨å¹²æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒNG-Routerå§‹ç»ˆä¼˜äºå•æ™ºèƒ½ä½“å’Œé›†æˆåŸºçº¿æ–¹æ³•ï¼Œä¸ºå¤æ‚è¥å…»å¥åº·ä»»åŠ¡æä¾›äº†é¢†åŸŸæ„ŸçŸ¥å¤šæ™ºèƒ½ä½“æ¨ç†çš„åŸåˆ™æ€§æ–¹æ³•ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¤æ‚è¥å…»å¥åº·ä»»åŠ¡æä¾›äº†é¢†åŸŸæ„ŸçŸ¥å¤šæ™ºèƒ½ä½“æ¨ç†çš„åŸåˆ™æ€§æ–¹æ³•ï¼Œé€šè¿‡çŸ¥è¯†å›¾è°±å¼•å¯¼çš„åä½œæœºåˆ¶æœ‰æ•ˆæå‡äº†è¥å…»é—®ç­”ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›å’Œå†³ç­–å‡†ç¡®æ€§ã€‚

---

#### ğŸ“„ Abstract
Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.


### [17] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton, Matthew Wilkens, Andrew Piper*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†NarraBenchï¼Œä¸€ä¸ªç†è®ºæŒ‡å¯¼çš„å™äº‹ç†è§£ä»»åŠ¡åˆ†ç±»æ³•ï¼Œå¹¶å¯¹è¯¥é¢†åŸŸçš„78ä¸ªç°æœ‰åŸºå‡†è¿›è¡Œäº†ç³»ç»Ÿè°ƒæŸ¥ï¼Œæ­ç¤ºäº†å½“å‰è¯„ä¼°åœ¨å™äº‹ç†è§£å…³é”®ç»´åº¦ä¸Šçš„æ˜¾è‘—ä¸è¶³ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å™äº‹ç†è§£é¢†åŸŸçš„è¯„ä¼°å­˜åœ¨æ˜¾è‘—ç©ºç™½ï¼Œè®¸å¤šå…³é”®å™äº‹ç»´åº¦åœ¨ç°æœ‰åŸºå‡†ä¸­è¦ä¹ˆè¢«å¿½è§†ï¼Œè¦ä¹ˆä¸ç°æœ‰æŒ‡æ ‡ä¸åŒ¹é…ï¼Œéœ€è¦ç³»ç»Ÿæ€§åœ°è¯†åˆ«è¿™äº›è¯„ä¼°ç¼ºå£å¹¶æŒ‡å¯¼æœªæ¥åŸºå‡†å¼€å‘ã€‚

**Method:** ç ”ç©¶å¼€å‘äº†ç†è®ºæŒ‡å¯¼çš„å™äº‹ç†è§£ä»»åŠ¡åˆ†ç±»æ³•ï¼Œå¹¶å¯¹78ä¸ªç°æœ‰åŸºå‡†è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œé‡‡ç”¨ç³»ç»ŸåŒ–æ–¹æ³•è¯„ä¼°å½“å‰åŸºå‡†å¯¹å™äº‹ä»»åŠ¡çš„è¦†ç›–ç¨‹åº¦å’Œè¯„ä¼°è´¨é‡ã€‚

**Result:** ç ”ç©¶å‘ç°ä»…æœ‰27%çš„å™äº‹ä»»åŠ¡è¢«ç°æœ‰åŸºå‡†å……åˆ†è¦†ç›–ï¼Œå™äº‹äº‹ä»¶ã€é£æ ¼ã€è§†è§’å’Œå¯ç¤ºç­‰å…³é”®ç»´åº¦åœ¨ç°æœ‰è¯„ä¼°ä¸­å‡ ä¹ç¼ºå¤±ï¼Œæ„æˆæ€§ä¸»è§‚å’Œè§†è§’æ€§æ–¹é¢çš„è¯„ä¼°èƒ½åŠ›å°¤å…¶ä¸è¶³ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºNLPç ”ç©¶è€…æä¾›äº†è¯†åˆ«å™äº‹ç†è§£è¯„ä¼°ç¼ºå£çš„é‡è¦æ¡†æ¶ï¼Œå¼ºè°ƒäº†å¼€å‘èƒ½å¤Ÿè¯„ä¼°ä¸»è§‚å’Œè§†è§’æ€§å™äº‹æ–¹é¢çš„åŸºå‡†çš„è¿«åˆ‡éœ€æ±‚ï¼Œå¯¹æŒ‡å¯¼æœªæ¥LLMå™äº‹ç†è§£è¯„ä¼°å…·æœ‰é‡è¦æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.


### [18] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee, Shivam Sonawane, Amey Hengle, Aditya Tanna, Sourish Dasgupta, Tanmoy Chakraborty*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºPerAugyæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡è·¨è½¨è¿¹æ··æ´—å’Œæ‘˜è¦å†…å®¹æ‰°åŠ¨è§£å†³ä¸ªæ€§åŒ–æ‘˜è¦è®­ç»ƒæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç”¨æˆ·ç¼–ç å™¨æ€§èƒ½å’Œä¸ªæ€§åŒ–æ‘˜è¦è´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¸ªæ€§åŒ–æ‘˜è¦é¢ä¸´è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç°æœ‰MS/CAS PENSæ•°æ®é›†ä»…åŒ…å«ç”¨æˆ·åå¥½å†å²è€Œç¼ºä¹ç›®æ ‡æ‘˜è¦ï¼Œæ— æ³•æ”¯æŒç«¯åˆ°ç«¯ç›‘ç£å­¦ä¹ ï¼Œä¸”ä¸»é¢˜è½¬æ¢å¤šæ ·æ€§æœ‰é™é™åˆ¶äº†æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

**Method:** æå‡ºPerAugyæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé‡‡ç”¨è·¨è½¨è¿¹æ··æ´—å’Œæ‘˜è¦å†…å®¹æ‰°åŠ¨æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å››ç§æœ€å…ˆè¿›ç”¨æˆ·ç¼–ç å™¨çš„æ€§èƒ½ï¼Œå¹¶å¼•å…¥TPã€RTCå’ŒDegreeDä¸‰ä¸ªæ•°æ®é›†å¤šæ ·æ€§æŒ‡æ ‡é‡åŒ–å¢å¼ºæ•ˆæœã€‚

**Result:** PerAugyä½¿å››ç§SOTAç”¨æˆ·ç¼–ç å™¨çš„AUCæŒ‡æ ‡æœ€ä½³æå‡0.132ï¼Œå¢å¼ºåçš„ä¸ªæ€§åŒ–æ‘˜è¦æ¡†æ¶åœ¨PSE-SU4æŒ‡æ ‡ä¸Šå¹³å‡æå‡61.2%ï¼ŒTPå’ŒDegreeDå¤šæ ·æ€§æŒ‡æ ‡ä¸ç”¨æˆ·ç¼–ç å™¨æ€§èƒ½å‘ˆå¼ºç›¸å…³æ€§ã€‚

**Conclusion:** æ•°æ®é›†å¤šæ ·æ€§æ˜¯é©±åŠ¨æ€§èƒ½æå‡çš„å…³é”®å› ç´ ï¼ŒPerAugyé€šè¿‡æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³äº†ä¸ªæ€§åŒ–æ‘˜è¦çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä¸ºä¸ªæ€§åŒ–ä¿¡æ¯æå–ç³»ç»Ÿæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.


### [19] [You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News](https://arxiv.org/abs/2510.10658)
*Guy Mor-Lan, Tamir Sheafer, Shaul R. Shenhav*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡å¤§è§„æ¨¡æ¯”è¾ƒCNNå’Œç¦å…‹æ–¯æ–°é—»çš„æŠ¥é“é£æ ¼ï¼Œé‡åŒ–äº†å…šæ´¾åª’ä½“åœ¨æ„å»ºç°å®æ—¶é‡‡ç”¨çš„ä¸åŒè®¤çŸ¥ç­–ç•¥ï¼Œä¸ºåª’ä½“åè§ç ”ç©¶å¢æ·»äº†æ–°çš„ç»´åº¦ã€‚ç ”ç©¶å‘ç°CNNæ›´å€¾å‘äºä½¿ç”¨äº‹å®é™ˆè¿°å¹¶å¼•ç”¨å¤–éƒ¨æ¥æºï¼Œè€Œç¦å…‹æ–¯æ–°é—»åˆ™åå¥½æ–°é—»æŠ¥é“å’Œç›´æ¥å¼•è¯­ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è™½ç„¶åª’ä½“åè§å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†äº‹å®æŠ¥é“èƒŒåçš„è®¤çŸ¥ç­–ç•¥åœ¨è®¡ç®—å±‚é¢ä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œé€šè¿‡åˆ†æä¸åŒåª’ä½“åœ¨æŠ¥é“ç›¸åŒäº‹ä»¶æ—¶é‡‡ç”¨çš„è®¤çŸ¥ç­–ç•¥å·®å¼‚ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨æ–‡ç« åŒ¹é…ç­–ç•¥æ¥éš”ç¦»æŠ¥é“é£æ ¼ä¸ä¸»é¢˜é€‰æ‹©çš„å½±å“ï¼Œå°†FactAppealæ¡†æ¶åº”ç”¨äºåŒ…å«47ä¸‡ç¯‡æ–‡ç« çš„è¯­æ–™åº“ï¼Œæ¶µç›–COVID-19å¤§æµè¡Œå’Œä»¥è‰²åˆ—-å“ˆé©¬æ–¯æˆ˜äº‰ä¸¤ä¸ªé«˜åº¦æ”¿æ²»åŒ–æ—¶æœŸï¼Œæ¯”è¾ƒCNNå’Œç¦å…‹æ–¯æ–°é—»å¯¹ç›¸åŒäº‹ä»¶çš„æŠ¥é“æ–¹å¼ã€‚

**Result:** ç ”ç©¶å‘ç°CNNçš„æŠ¥é“åŒ…å«æ›´å¤šäº‹å®é™ˆè¿°ä¸”æ›´å€¾å‘äºå°†å…¶å»ºç«‹åœ¨å¤–éƒ¨æ¥æºåŸºç¡€ä¸Šï¼Œä¸¤å®¶åª’ä½“å±•ç°å‡ºæ˜æ˜¾ä¸åŒçš„æ¥æºæ¨¡å¼ï¼šCNNé€šè¿‡å¼•ç”¨ä¸“å®¶å’Œä¸“å®¶æ–‡ä»¶æ„å»ºæ­£å¼æƒå¨è¯‰æ±‚ï¼Œè€Œç¦å…‹æ–¯æ–°é—»åˆ™åçˆ±æ–°é—»æŠ¥é“å’Œç›´æ¥å¼•è¯­ã€‚

**Conclusion:** è¿™é¡¹ç ”ç©¶é‡åŒ–äº†å…šæ´¾åª’ä½“å¦‚ä½•ç³»ç»Ÿæ€§åœ°ä½¿ç”¨ä¸åŒçš„è®¤çŸ¥ç­–ç•¥æ¥æ„å»ºç°å®ï¼Œæ­ç¤ºäº†åª’ä½“æŠ¥é“ä¸ä»…åœ¨é€‰æ‹©ä¸»é¢˜ä¸Šå­˜åœ¨åè§ï¼Œåœ¨äº‹å®å‘ˆç°çš„è®¤çŸ¥ç­–ç•¥ä¸Šä¹Ÿå­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ï¼Œä¸ºç†è§£åª’ä½“åè§æä¾›äº†æ–°çš„åˆ†æç»´åº¦ã€‚

---

#### ğŸ“„ Abstract
While media bias is widely studied, the epistemic strategies behind factual
reporting remain computationally underexplored. This paper analyzes these
strategies through a large-scale comparison of CNN and Fox News. To isolate
reporting style from topic selection, we employ an article matching strategy to
compare reports on the same events and apply the FactAppeal framework to a
corpus of over 470K articles covering two highly politicized periods: the
COVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting
contains more factual statements and is more likely to ground them in external
sources. The outlets also exhibit sharply divergent sourcing patterns: CNN
builds credibility by citing Experts} and Expert Documents, constructing an
appeal to formal authority, whereas Fox News favors News Reports and direct
quotations. This work quantifies how partisan outlets use systematically
different epistemic strategies to construct reality, adding a new dimension to
the study of media bias.


### [20] [Towards Real-Time Fake News Detection under Evidence Scarcity](https://arxiv.org/abs/2510.11277)
*Guangyu Wei, Ke Han, Yueming Lyu, Yu Luo, Yue Jiang, Caifeng Shan, Nicu Sebe*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†EASEæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è¯„ä¼°è¯æ®å……åˆ†æ€§æ¥æ”¹è¿›å®æ—¶å‡æ–°é—»æ£€æµ‹ï¼Œåœ¨è¯æ®ç¨€ç¼ºæƒ…å†µä¸‹æ˜¾è‘—æå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å¼•å…¥ä¸‰é˜¶æ®µè¯„ä¼°æœºåˆ¶ï¼Œç»“åˆæŒ‡ä»¤è°ƒä¼˜å’Œä¼ªæ ‡ç­¾è®­ç»ƒï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å‡æ–°é—»æ£€æµ‹æ–¹æ³•ä¸¥é‡ä¾èµ–å¤–éƒ¨è¯æ®ï¼Œåœ¨å®æ—¶åœºæ™¯ä¸‹æ–°å…´äº‹ä»¶å¾€å¾€ç¼ºä¹è¶³å¤Ÿè¯æ®æ”¯æŒï¼Œå¯¼è‡´æ¨¡å‹åœ¨è¯æ®ç¨€ç¼ºæƒ…å†µä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚éœ€è¦å¼€å‘èƒ½å¤ŸåŠ¨æ€é€‚åº”è¯æ®å¯ç”¨æ€§çš„æ£€æµ‹æ¡†æ¶ã€‚

**Method:** æå‡ºEASEæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªç‹¬ç«‹è§†è§’çš„åºåˆ—è¯„ä¼°æœºåˆ¶ï¼šåŸºäºè¯æ®çš„è¯„ä¼°ä»…åœ¨è¯æ®å……åˆ†æ”¯æŒæ—¶æ•´åˆè¯æ®ï¼›åŸºäºæ¨ç†çš„è¯„ä¼°åœ¨å¯é æ€§è¶³å¤Ÿæ—¶åˆ©ç”¨LLMçš„ä¸–ç•ŒçŸ¥è¯†ï¼›æƒ…æ„Ÿå›é€€æœºåˆ¶åœ¨è¯æ®å’Œæ¨ç†å‡ä¸å¯é æ—¶æ•´åˆæƒ…æ„Ÿçº¿ç´¢ã€‚é€šè¿‡æŒ‡ä»¤è°ƒä¼˜å’Œä¼ªæ ‡ç­¾è®­ç»ƒæå‡è¯„ä¼°å‡†ç¡®æ€§ã€‚

**Result:** EASEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æå‡äº†å¯¹å®æ—¶æ–°é—»çš„æ³›åŒ–èƒ½åŠ›ã€‚æ„å»ºäº†RealTimeNews-25æ–°åŸºå‡†ç”¨äºè¯„ä¼°æ–°å…´æ–°é—»çš„æ£€æµ‹æ•ˆæœï¼Œå®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨è¯æ®ç¨€ç¼ºæƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚

**Conclusion:** EASEæ¡†æ¶é€šè¿‡åŠ¨æ€è¯„ä¼°è¯æ®å……åˆ†æ€§å®ç°äº†æ›´é²æ£’çš„å‡æ–°é—»æ£€æµ‹ï¼Œè¯æ˜äº†å¤šè§†è§’è¯„ä¼°æœºåˆ¶åœ¨è¯æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºå®æ—¶å‡æ–°é—»æ£€æµ‹æä¾›äº†æ–°èŒƒå¼ï¼Œå¼ºè°ƒäº†è¯„ä¼°æ„ŸçŸ¥å†³ç­–çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Fake news detection becomes particularly challenging in real-time scenarios,
where emerging events often lack sufficient supporting evidence. Existing
approaches often rely heavily on external evidence and therefore struggle to
generalize under evidence scarcity. To address this issue, we propose
Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time
fake news detection that dynamically adapts its decision-making process
according to the assessed sufficiency of available evidence. EASE introduces a
sequential evaluation mechanism comprising three independent perspectives: (1)
Evidence-based evaluation, which assesses evidence and incorporates it into
decision-making only when the evidence is sufficiently supportive; (2)
Reasoning-based evaluation, which leverages the world knowledge of large
language models (LLMs) and applies them only when their reliability is
adequately established; and (3) Sentiment-based fallback, which integrates
sentiment cues when neither evidence nor reasoning is reliable. To enhance the
accuracy of evaluation processes, EASE employs instruction tuning with pseudo
labels to guide each evaluator in justifying its perspective-specific knowledge
through interpretable reasoning. Furthermore, the expert modules integrate the
evaluators' justified assessments with the news content to enable
evaluation-aware decision-making, thereby enhancing overall detection accuracy.
Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news
for evaluating model generalization on emerging news with limited evidence.
Extensive experiments demonstrate that EASE not only achieves state-of-the-art
performance across multiple benchmarks, but also significantly improves
generalization to real-time news. The code and dataset are available:
https://github.com/wgyhhhh/EASE.


### [21] [Do LLMs "Feel"? Emotion Circuits Discovery and Control](https://arxiv.org/abs/2510.11328)
*Chenxi Wang, Yixuan Zhang, Ruiji Yu, Yufei Zheng, Lang Gao, Zirui Song, Zixiang Xu, Gus Xia, Huishuai Zhang, Dongyan Zhao, Xiuying Chen*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ­ç¤ºå¹¶éªŒè¯äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æƒ…æ„Ÿç”µè·¯æœºåˆ¶ï¼Œé€šè¿‡æ„å»ºå—æ§æ•°æ®é›†å’Œå› æœåˆ†ææ–¹æ³•ï¼Œå‘ç°äº†è·¨ä¸Šä¸‹æ–‡ä¸€è‡´çš„æƒ…æ„Ÿç¼–ç æ¨¡å¼ï¼Œå¹¶å®ç°äº†99.65%çš„æƒ…æ„Ÿè¡¨è¾¾æ§åˆ¶å‡†ç¡®ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹æƒ…æ„Ÿæ™ºèƒ½å‘å±•çš„å…³é”®æŒ‘æˆ˜åœ¨äºç†è§£æƒ…æ„Ÿè¡¨è¾¾çš„å†…éƒ¨æœºåˆ¶å¹¶å®ç°å¯¹ç”Ÿæˆæ–‡æœ¬ä¸­æƒ…æ„Ÿçš„æ§åˆ¶ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šLLMsæ˜¯å¦åŒ…å«å¡‘é€ æƒ…æ„Ÿè¡¨è¾¾çš„ä¸Šä¸‹æ–‡æ— å…³æœºåˆ¶ã€è¿™äº›æœºåˆ¶çš„å…·ä½“å½¢å¼ä»¥åŠèƒ½å¦ç”¨äºé€šç”¨æƒ…æ„Ÿæ§åˆ¶ã€‚

**Method:** ç ”ç©¶é¦–å…ˆæ„å»ºäº†å—æ§æ•°æ®é›†SEVæ¥å¼•å‘è·¨æƒ…æ„Ÿçš„å¯æ¯”å†…éƒ¨çŠ¶æ€ï¼Œéšåæå–äº†ä¸Šä¸‹æ–‡æ— å…³çš„æƒ…æ„Ÿæ–¹å‘ï¼Œé€šè¿‡åˆ†ææ€§åˆ†è§£å’Œå› æœåˆ†æè¯†åˆ«äº†å±€éƒ¨æ‰§è¡Œæƒ…æ„Ÿè®¡ç®—çš„ç¥ç»å…ƒå’Œæ³¨æ„åŠ›å¤´ï¼Œå¹¶é€šè¿‡æ¶ˆèå’Œå¢å¼ºå¹²é¢„éªŒè¯å…¶å› æœä½œç”¨ï¼Œæœ€åé‡åŒ–å„å­å±‚å¯¹æœ€ç»ˆæƒ…æ„Ÿè¡¨ç¤ºçš„å› æœå½±å“å¹¶æ•´åˆå±€éƒ¨ç»„ä»¶ä¸ºé©±åŠ¨æƒ…æ„Ÿè¡¨è¾¾çš„å…¨å±€æƒ…æ„Ÿç”µè·¯ã€‚

**Result:** ç ”ç©¶å‘ç°å­˜åœ¨è·¨ä¸Šä¸‹æ–‡ä¸€è‡´çš„æƒ…æ„Ÿç¼–ç æœºåˆ¶ï¼Œè¯†åˆ«å‡ºå±€éƒ¨æƒ…æ„Ÿè®¡ç®—ç»„ä»¶å¹¶éªŒè¯å…¶å› æœä½œç”¨ï¼Œç›´æ¥è°ƒåˆ¶è¿™äº›æƒ…æ„Ÿç”µè·¯åœ¨æµ‹è¯•é›†ä¸Šå®ç°äº†99.65%çš„æƒ…æ„Ÿè¡¨è¾¾å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†åŸºäºæç¤ºå’Œå¯¼å‘çš„æ–¹æ³•ã€‚

**Conclusion:** è¿™æ˜¯é¦–ä¸ªç³»ç»Ÿæ­ç¤ºå’ŒéªŒè¯LLMsä¸­æƒ…æ„Ÿç”µè·¯çš„ç ”ç©¶ï¼Œä¸ºæ¨¡å‹å¯è§£é‡Šæ€§å’Œå¯æ§æƒ…æ„Ÿæ™ºèƒ½æä¾›äº†æ–°çš„è§è§£ï¼Œé€šè¿‡ç†è§£å†…éƒ¨æƒ…æ„Ÿæœºåˆ¶å®ç°äº†é«˜æ•ˆçš„æƒ…æ„Ÿæ§åˆ¶ï¼Œä¸ºå¼€å‘æ›´ç²¾å‡†çš„æƒ…æ„Ÿæ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
As the demand for emotional intelligence in large language models (LLMs)
grows, a key challenge lies in understanding the internal mechanisms that give
rise to emotional expression and in controlling emotions in generated text.
This study addresses three core questions: (1) Do LLMs contain context-agnostic
mechanisms shaping emotional expression? (2) What form do these mechanisms
take? (3) Can they be harnessed for universal emotion control? We first
construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit
comparable internal states across emotions. Subsequently, we extract
context-agnostic emotion directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons and attention heads that locally
implement emotional computation through analytical decomposition and causal
analysis, and validate their causal roles via ablation and enhancement
interventions. Next, we quantify each sublayer's causal influence on the
model's final emotion representation and integrate the identified local
components into coherent global emotion circuits that drive emotional
expression (Q2). Directly modulating these circuits achieves 99.65%
emotion-expression accuracy on the test set, surpassing prompting- and
steering-based methods (Q3). To our knowledge, this is the first systematic
study to uncover and validate emotion circuits in LLMs, offering new insights
into interpretability and controllable emotional intelligence.


### [22] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma, Hailin Zhang, Liang Zhao, Yifan Song, Yudong Wang, Zhifang Sui, Fuli Luo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºRollout Routing Replay (R3)æ–¹æ³•æ¥è§£å†³æ··åˆä¸“å®¶æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è·¯ç”±æœºåˆ¶çš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œé€šè¿‡è®°å½•æ¨ç†é˜¶æ®µçš„è·¯ç”±åˆ†å¸ƒå¹¶åœ¨è®­ç»ƒé˜¶æ®µé‡æ”¾ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒ-æ¨ç†ç­–ç•¥KLæ•£åº¦ï¼Œæœ‰æ•ˆé˜²æ­¢äº†RLè®­ç»ƒå´©æºƒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ··åˆä¸“å®¶æ¨¡å‹ä¸­çš„è·¯ç”±æœºåˆ¶åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå¼•å…¥ä¸ç¨³å®šæ€§ï¼Œç”šè‡³å¯¼è‡´ç¾éš¾æ€§çš„è®­ç»ƒå´©æºƒã€‚ç ”ç©¶å‘ç°è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå­˜åœ¨æ˜¾è‘—çš„è·¯ç”±è¡Œä¸ºä¸ä¸€è‡´æ€§ï¼Œå³ä½¿åœ¨ç›¸åŒæ¡ä»¶ä¸‹ï¼Œè·¯ç”±æ¡†æ¶ä¹Ÿä¼šåœ¨é‡å¤å‰å‘ä¼ æ’­ä¸­äº§ç”Ÿä¸åŒçš„ä¸“å®¶é€‰æ‹©ã€‚

**Method:** æå‡ºäº†Rollout Routing Replay (R3)æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è®°å½•æ¨ç†å¼•æ“ä¸­çš„è·¯ç”±åˆ†å¸ƒå¹¶åœ¨è®­ç»ƒé˜¶æ®µè¿›è¡Œé‡æ”¾ã€‚R3æ˜¾è‘—é™ä½äº†è®­ç»ƒ-æ¨ç†ç­–ç•¥KLæ•£åº¦ï¼Œç¼“è§£äº†æç«¯å·®å¼‚ï¼ŒåŒæ—¶ä¸æŸå®³è®­ç»ƒé€Ÿåº¦ã€‚

**Result:** åœ¨å„ç§è®¾ç½®ä¸‹çš„å¹¿æ³›å®éªŒè¯å®ï¼ŒR3æˆåŠŸç¨³å®šäº†RLè®­ç»ƒï¼Œé˜²æ­¢äº†å´©æºƒï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†GSPOå’ŒTISç­‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†è·¯ç”±ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚

**Conclusion:** è¿™é¡¹å·¥ä½œä¸ºç¨³å®šæ··åˆä¸“å®¶æ¨¡å‹ä¸­çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç¡®ä¿è®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„è·¯ç”±ä¸€è‡´æ€§ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³äº†è·¯ç”±æœºåˆ¶çš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œä¸ºMoEæ¨¡å‹çš„RLåº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚

---

#### ğŸ“„ Abstract
Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.


### [23] [StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models](https://arxiv.org/abs/2510.11618)
*Zehao Chen, Rong Pan, Haoran Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆè‡ªåº•å‘ä¸Šçš„é•¿æ–‡æœ¬æ•…äº‹ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿå®ç°æœ‰æœºçš„æ•…äº‹æ¼”è¿›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ™ºèƒ½ä½“åœ¨åŠ¨æ€æ²™ç›’ç¯å¢ƒä¸­çš„äº¤äº’äº§ç”Ÿæ¶Œç°äº‹ä»¶ï¼Œèƒ½å¤Ÿç”Ÿæˆè¶…è¿‡10,000å­—çš„é•¿ç¯‡æ•…äº‹ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè‡ªä¸Šè€Œä¸‹çš„æ•…äº‹ç”Ÿæˆæ–¹æ³•å¼ºåŠ åˆšæ€§ç»“æ„ï¼Œé™åˆ¶äº†æ•…äº‹çš„æœ‰æœºå‘å±•ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å½“å‰æ•…äº‹ç”Ÿæˆæ¨¡å‹åœ¨é•¿ç¯‡å™äº‹ä¸­é¢ä¸´çš„è¿è´¯æ€§å’Œä¸€è‡´æ€§æŒ‘æˆ˜ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»ä½œå®¶ä»æ•´ä½“å¿ƒç†åœºæ™¯å‡ºå‘çš„åˆ›ä½œè¿‡ç¨‹ï¼Œå®ç°æ›´è‡ªç„¶çš„æ•…äº‹æ¼”è¿›ã€‚

**Method:** é‡‡ç”¨æ··åˆè‡ªåº•å‘ä¸Šçš„é•¿æ–‡æœ¬æ•…äº‹ç”Ÿæˆæ¡†æ¶ï¼ŒåŸºäºå¤šæ™ºèƒ½ä½“æ¨¡æ‹ŸæŠ€æœ¯ã€‚æ™ºèƒ½ä½“åœ¨åŠ¨æ€æ²™ç›’ç¯å¢ƒä¸­è¿›è¡Œäº¤äº’ï¼Œå…¶è¡Œä¸ºå’Œä¸ç¯å¢ƒåŠå…¶ä»–æ™ºèƒ½ä½“çš„äº’åŠ¨äº§ç”Ÿæ¶Œç°äº‹ä»¶ï¼Œè¿™äº›äº‹ä»¶æ„æˆæ•…äº‹çš„åŸºç¡€ï¼Œæ”¯æŒæœ‰æœºçš„è§’è‰²å‘å±•å’Œæƒ…èŠ‚æ¨è¿›ã€‚

**Result:** è¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆè¶…è¿‡10,000å­—çš„é•¿ç¯‡æ•…äº‹ï¼ŒåŒæ—¶ä¿æŒæ•…äº‹çš„è¿è´¯æ€§å’Œä¸€è‡´æ€§ã€‚åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨é•¿æ–‡æœ¬æ•…äº‹ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** æ··åˆè‡ªåº•å‘ä¸Šçš„æ–¹æ³•ä¸ºåˆ›å»ºåŠ¨æ€ã€æ²‰æµ¸å¼çš„é•¿ç¯‡æ•…äº‹æä¾›äº†å¯æ‰©å±•çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡æ™ºèƒ½ä½“é©±åŠ¨çš„äº¤äº’å®ç°æ•…äº‹çš„è‡ªç„¶æ¼”è¿›ï¼Œä¸ºæ•…äº‹ç”Ÿæˆé¢†åŸŸå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå±•ç¤ºäº†å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿåœ¨åˆ›é€ æ€§å†™ä½œä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Human writers often begin their stories with an overarching mental scene,
where they envision the interactions between characters and their environment.
Inspired by this creative process, we propose a novel approach to long-form
story generation, termed hybrid bottom-up long-form story generation, using
multi-agent simulations. In our method, agents interact within a dynamic
sandbox environment, where their behaviors and interactions with one another
and the environment generate emergent events. These events form the foundation
for the story, enabling organic character development and plot progression.
Unlike traditional top-down approaches that impose rigid structures, our hybrid
bottom-up approach allows for the natural unfolding of events, fostering more
spontaneous and engaging storytelling. The system is capable of generating
stories exceeding 10,000 words while maintaining coherence and consistency,
addressing some of the key challenges faced by current story generation models.
We achieve state-of-the-art performance across several metrics. This approach
offers a scalable and innovative solution for creating dynamic, immersive
long-form stories that evolve organically from agent-driven interactions.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang, Kaitong Cai, Qinglin Zeng, Ningyuan Liu, Stephen Fan, Ziliang Chen, Keze Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„LLMå·¥ä½œæµä¼˜åŒ–èŒƒå¼ï¼Œå°†é—®é¢˜é‡æ–°æ¦‚å¿µåŒ–ä¸ºåˆ†å¸ƒä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡æœ€å°åŒ–æœŸæœ›å¤±è´¥è´¨é‡è€Œéæœ€å¤§åŒ–æ ‡é‡åˆ†æ•°æ¥ç³»ç»Ÿæ€§åœ°å­¦ä¹ å¹¶é‡å¡‘å¤±è´¥åˆ†å¸ƒçš„å‡ ä½•ç»“æ„ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰LLMå·¥ä½œæµä¼˜åŒ–æ–¹æ³•å­˜åœ¨ä¿¡æ¯åç¼©é—®é¢˜ï¼Œå°†ä¸°å¯Œçš„å¤šæ­¥éª¤æ‰§è¡Œè½¨è¿¹ç®€åŒ–ä¸ºç®€å•çš„æˆåŠŸ/å¤±è´¥ä¿¡å·ï¼Œå¯¼è‡´æ— æ³•å»ºæ¨¡å·¥ä½œæµçš„å¤±è´¥åˆ†å¸ƒç»“æ„ï¼Œä»æ ¹æœ¬ä¸Šé™åˆ¶äº†ä¼˜åŒ–æ•ˆæœã€‚

**Method:** æå‡ºäº†CE-Graphæ¡†æ¶ï¼Œé€šè¿‡å¤±è´¥é©±åŠ¨çš„ç»†åŒ–è¿‡ç¨‹æ¥æ“ä½œåŒ–è¿™ä¸€èŒƒå¼ï¼šä»åä¾‹æ± ä¸­è¿‘ä¼¼å¤±è´¥åˆ†å¸ƒï¼Œè¯†åˆ«æœ€å¯†é›†åŒºåŸŸä½œä¸ºé‡å¤å¤±è´¥æ¨¡å¼ï¼Œå¹¶é€šè¿‡æå‡º-éªŒè¯æœºåˆ¶åº”ç”¨æœ‰é’ˆå¯¹æ€§çš„æ“ä½œç¬¦çº¦æŸå›¾ç¼–è¾‘æ¥è´ªå©ªåœ°å‡å°‘å¤±è´¥è´¨é‡ã€‚

**Result:** åœ¨æ•°å­¦ã€ä»£ç å’Œé—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCE-Graphä»¥æ˜¾è‘—æ›´ä½çš„æˆæœ¬å®ç°äº†æ¯”å¼ºåŸºçº¿æ›´é«˜çš„é²æ£’æ€§ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¼˜åŒ–æ•ˆç‡å’Œæ•ˆæœä¸Šçš„ä¼˜åŠ¿ã€‚

**Conclusion:** ç³»ç»Ÿçš„å¯é æ€§å¹¶éæ¥è‡ªé¿å…å¤±è´¥ï¼Œè€Œæ˜¯é€šè¿‡ç³»ç»Ÿæ€§åœ°å­¦ä¹ å’Œé‡å¡‘å…¶å¤±è´¥åˆ†å¸ƒçš„å‡ ä½•ç»“æ„æ¥å®ç°ï¼Œè¿™ä¸ºLLMå·¥ä½œæµä¼˜åŒ–æä¾›äº†æ–°çš„ç†è®ºåŸºç¡€å’Œæ–¹æ³•è®ºæ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.


### [25] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li, Hongjun Liu, Leyi Zhao, Zisu Li, Jiawei Li, Jiajun Jiang, Linning Xu, Chen Zhao, Mingming Fan, Chen Liang*

#### ğŸ§© TL;DR
SwarmSysæ˜¯ä¸€ä¸ªå—ç¾¤ä½“æ™ºèƒ½å¯å‘çš„åˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æ¢ç´¢è€…ã€å·¥ä½œè€…å’ŒéªŒè¯è€…ä¸‰ç§è§’è‰²çš„è¿­ä»£äº¤äº’å®ç°é—­ç¯æ¨ç†ï¼Œåœ¨ç¬¦å·æ¨ç†ã€ç ”ç©¶ç»¼åˆå’Œç§‘å­¦ç¼–ç¨‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¡¨æ˜åè°ƒæ‰©å±•å¯èƒ½æˆä¸ºä¸æ¨¡å‹æ‰©å±•åŒç­‰é‡è¦çš„LLMæ™ºèƒ½å‘å±•èŒƒå¼ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰LLMå¤šæ™ºèƒ½ä½“æ¡†æ¶é€šå¸¸ä¾èµ–å›ºå®šè§’è‰²æˆ–é›†ä¸­æ§åˆ¶ï¼Œåœ¨é•¿è§†é‡æ¨ç†ä¸­é™åˆ¶äº†å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿå®ç°åˆ†å¸ƒå¼ã€è‡ªé€‚åº”åä½œçš„æ¨ç†æ¡†æ¶ã€‚

**Method:** SwarmSysæ¡†æ¶é€šè¿‡ä¸‰ç§ä¸“é—¨è§’è‰²çš„è¿­ä»£äº¤äº’å®ç°åè°ƒï¼šæ¢ç´¢è€…ã€å·¥ä½œè€…å’ŒéªŒè¯è€…ï¼Œå®ƒä»¬æŒç»­å¾ªç¯æ‰§è¡Œæ¢ç´¢ã€åˆ©ç”¨å’ŒéªŒè¯è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶é›†æˆäº†è‡ªé€‚åº”æ™ºèƒ½ä½“å’Œäº‹ä»¶é…ç½®æ–‡ä»¶ã€åŸºäºåµŒå…¥çš„æ¦‚ç‡åŒ¹é…ä»¥åŠå—ä¿¡æ¯ç´ å¯å‘çš„å¼ºåŒ–æœºåˆ¶ï¼Œæ”¯æŒåŠ¨æ€ä»»åŠ¡åˆ†é…å’Œæ— å…¨å±€ç›‘ç£çš„è‡ªç»„ç»‡æ”¶æ•›ã€‚

**Result:** åœ¨ç¬¦å·æ¨ç†ã€ç ”ç©¶ç»¼åˆå’Œç§‘å­¦ç¼–ç¨‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSwarmSyså§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œæ¨ç†ç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶åœ¨å„ç§å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ç¾¤ä½“å¯å‘çš„åè°ƒæœºåˆ¶æ˜¯æ„å»ºå¯æ‰©å±•ã€é²æ£’å’Œè‡ªé€‚åº”å¤šæ™ºèƒ½ä½“æ¨ç†çš„æœ‰å‰æ™¯èŒƒå¼ã€‚åè°ƒæ‰©å±•å¯èƒ½ä¸æ¨¡å‹æ‰©å±•åœ¨æ¨è¿›LLMæ™ºèƒ½æ–¹é¢å…·æœ‰åŒç­‰é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°çš„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.


### [26] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng, Yujuan Fu, Sitong Zhou, Zixuan Yu, Lucas Jing Liu, Jun Wen, Matthew Thompson, Ruth Etzioni, Meliha Yetisgen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Traj-CoAï¼Œä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ‚£è€…è½¨è¿¹å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡é“¾å¼æ™ºèƒ½ä½“å¤„ç†ç”µå­å¥åº·è®°å½•çš„é•¿åºåˆ—æ•°æ®ï¼Œåœ¨é›¶æ ·æœ¬è‚ºç™Œé£é™©é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶ä¸ºæ‚£è€…è½¨è¿¹å»ºæ¨¡æä¾›äº†é€šç”¨æ–¹æ³•ï¼Œä½†åœ¨å¤„ç†ç”µå­å¥åº·è®°å½•çš„é•¿åºåˆ—å’Œå™ªå£°æ•°æ®æ—¶é¢ä¸´æ—¶åºæ¨ç†å›°éš¾ï¼Œéœ€è¦è§£å†³æ•°æ®é•¿åº¦å’Œå™ªå£°å¯¹æ¨¡å‹æ€§èƒ½çš„é™åˆ¶é—®é¢˜ã€‚

**Method:** Traj-CoAé‡‡ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¶æ„ï¼ŒåŒ…å«å¤šä¸ªå·¥ä½œæ™ºèƒ½ä½“æŒ‰é¡ºåºå¤„ç†å¯ç®¡ç†çš„æ•°æ®å—ï¼Œå°†å…³é”®äº‹ä»¶æç‚¼åˆ°å…±äº«é•¿æœŸè®°å¿†æ¨¡å—EHRMemä¸­ï¼Œæœ€åç”±ç®¡ç†æ™ºèƒ½ä½“ç»¼åˆå·¥ä½œæ™ºèƒ½ä½“çš„æ€»ç»“å’ŒEHRMemä¸­çš„æ—¶é—´çº¿è¿›è¡Œé¢„æµ‹ã€‚

**Result:** åœ¨åŸºäºäº”å¹´ç”µå­å¥åº·è®°å½•çš„é›¶æ ·æœ¬ä¸€å¹´æœŸè‚ºç™Œé£é™©é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒTraj-CoAåœ¨å››ç±»åŸºçº¿æ–¹æ³•ä¸Šå‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œåˆ†ææ˜¾ç¤ºè¯¥æ–¹æ³•å±•ç°å‡ºä¸ä¸´åºŠå¯¹é½çš„æ—¶åºæ¨ç†èƒ½åŠ›ã€‚

**Conclusion:** Traj-CoAä¸ºå¤æ‚æ‚£è€…è½¨è¿¹å»ºæ¨¡æä¾›äº†ä¸€ä¸ªç¨³å¥ä¸”å¯æ¨å¹¿çš„æ–¹æ³•ï¼Œå…¶å¤šæ™ºèƒ½ä½“æ¶æ„æœ‰æ•ˆè§£å†³äº†é•¿åºåˆ—ç”µå­å¥åº·è®°å½•çš„å¤„ç†æŒ‘æˆ˜ï¼Œå±•ç¤ºäº†åœ¨åŒ»ç–—æ—¶åºæ¨ç†ä»»åŠ¡ä¸­çš„å®ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.


### [27] [Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce](https://arxiv.org/abs/2510.11604)
*Sanjula De Alwis, Indrajith Ekanayake*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸‰ç»„ä»¶æ¡†æ¶ï¼Œå°†å¯è§£é‡ŠAIã€ç”Ÿå­˜åˆ†æå’ŒRFMåˆ†æç›¸ç»“åˆï¼Œç”¨äºå®¢æˆ·æµå¤±é¢„æµ‹å’Œä¿ç•™ç­–ç•¥åˆ¶å®šã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿé‡åŒ–ç‰¹å¾è´¡çŒ®ã€å»ºæ¨¡æµå¤±é£é™©æ—¶é—´åŠ¨æ€ï¼Œå¹¶è¯†åˆ«é«˜ä»·å€¼å®¢æˆ·ç»†åˆ†ï¼Œä»è€Œæ”¯æŒä¸ªæ€§åŒ–ä¿ç•™å¹²é¢„ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å®¢æˆ·æµå¤±æ¨¡å‹å¤šä¸ºé»‘ç›’æ“ä½œï¼Œé™åˆ¶äº†ä¼ä¸šå¯¹æµå¤±é©±åŠ¨å› ç´ ã€å¹²é¢„æ—¶æœºå’Œé«˜é£é™©å®¢æˆ·ç»†åˆ†çš„æ·±å…¥ç†è§£ã€‚ç ”ç©¶æ—¨åœ¨ä»å•çº¯é¢„æµ‹è½¬å‘åŸºäºå¯è§£é‡Šè¯æ®çš„ä¸ªæ€§åŒ–ä¿ç•™ç­–ç•¥è®¾è®¡ï¼Œè§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨æ´å¯ŸåŠ›å’Œå¯æ“ä½œæ€§æ–¹é¢çš„å±€é™æ€§ã€‚

**Method:** æå‡ºä¸‰ç»„ä»¶æ¡†æ¶ï¼šä½¿ç”¨å¯è§£é‡ŠAIé‡åŒ–ç‰¹å¾å¯¹æµå¤±çš„è´¡çŒ®åº¦ï¼Œåº”ç”¨ç”Ÿå­˜åˆ†æå»ºæ¨¡æ—¶é—´åˆ°äº‹ä»¶çš„æµå¤±é£é™©åŠ¨æ€ï¼Œç»“åˆRFMåˆ†ææ ¹æ®äº¤æ˜“è¡Œä¸ºå¯¹å®¢æˆ·è¿›è¡Œç»†åˆ†ã€‚è¿™äº›æ–¹æ³•ååŒå·¥ä½œå®ç°æµå¤±é©±åŠ¨å› ç´ å½’å› ã€å¹²é¢„çª—å£ä¼°è®¡å’Œç›®æ ‡ç»†åˆ†ä¼˜å…ˆçº§æ’åºã€‚

**Result:** è¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°æµå¤±é©±åŠ¨å› ç´ çš„å¯è§£é‡Šå½’å› ï¼Œå‡†ç¡®ä¼°è®¡ä¿ç•™å¹²é¢„çš„æœ€ä½³æ—¶é—´çª—å£ï¼Œå¹¶æœ‰æ•ˆè¯†åˆ«éœ€è¦ä¼˜å…ˆå…³æ³¨çš„é«˜é£é™©å®¢æˆ·ç»†åˆ†ã€‚ç»¼åˆæ–¹æ³•æ”¯æŒåˆ¶å®šå‡å°‘å®¢æˆ·æµå¤±å’Œå¢å¼ºå®¢æˆ·å¿ è¯šåº¦çš„é’ˆå¯¹æ€§ç­–ç•¥ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œå°†å¯è§£é‡Šæ€§ã€æ—¶é—´åŠ¨æ€åˆ†æå’Œè¡Œä¸ºç»†åˆ†ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å®¢æˆ·æµå¤±ç®¡ç†çš„æˆ˜ç•¥ä»·å€¼ã€‚è¯¥æ¡†æ¶ä¸ºä»é¢„æµ‹æ€§åˆ†æè½¬å‘å¯æ“ä½œçš„ä¿ç•™ç­–ç•¥æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œå¼ºè°ƒäº†è§£é‡Šæ€§åœ¨å®¢æˆ·å…³ç³»ç®¡ç†ä¸­çš„å…³é”®ä½œç”¨ã€‚

---

#### ğŸ“„ Abstract
In online retail, customer acquisition typically incurs higher costs than
customer retention, motivating firms to invest in churn analytics. However,
many contemporary churn models operate as opaque black boxes, limiting insight
into the determinants of attrition, the timing of retention opportunities, and
the identification of high-risk customer segments. Accordingly, the emphasis
should shift from prediction alone to the design of personalized retention
strategies grounded in interpretable evidence. This study advances a
three-component framework that integrates explainable AI to quantify feature
contributions, survival analysis to model time-to-event churn risk, and RFM
profiling to segment customers by transactional behaviour. In combination,
these methods enable the attribution of churn drivers, estimation of
intervention windows, and prioritization of segments for targeted actions,
thereby supporting strategies that reduce attrition and strengthen customer
loyalty.
