<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu, Shengpeng Xu, Yunbo Jia, Edmund Y. Lam*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSemantic-E2VIDæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½æœºåˆ¶ï¼Œå°†è§†è§‰åŸºç¡€æ¨¡å‹SAMçš„è¯­ä¹‰çŸ¥è¯†è¿ç§»åˆ°äº‹ä»¶ç›¸æœºè§†é¢‘é‡å»ºä¸­ï¼Œæ˜¾è‘—æå‡äº†äº‹ä»¶åˆ°è§†é¢‘é‡å»ºçš„è¯­ä¹‰ä¿¡æ¯æ¢å¤èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äº‹ä»¶ç›¸æœºè™½ç„¶å…·æœ‰ä½å»¶è¿Ÿã€é«˜åŠ¨æ€èŒƒå›´ç­‰ä¼˜åŠ¿ï¼Œä½†äº‹ä»¶åˆ°è§†é¢‘é‡å»ºä»»åŠ¡é¢ä¸´è¯­ä¹‰ä¿¡æ¯ç¼ºå¤±çš„æŒ‘æˆ˜ï¼Œå› ä¸ºäº‹ä»¶ç›¸æœºä»…æ•è·å¼ºåº¦å˜åŒ–è€Œå¿½ç•¥é™æ€å¯¹è±¡å’ŒèƒŒæ™¯ï¼Œå¯¼è‡´ç°æœ‰E2Væ–¹æ³•éš¾ä»¥æ¢å¤è¯­ä¹‰å†…å®¹ã€‚

**Method:** æå‡ºSemantic-E2VIDæ¡†æ¶ï¼ŒåŒ…å«è·¨æ¨¡æ€ç‰¹å¾å¯¹é½æ¨¡å—å°†SAMçš„è§†è§‰è¯­ä¹‰çŸ¥è¯†è¿ç§»åˆ°äº‹ä»¶ç¼–ç å™¨ï¼Œè¯­ä¹‰æ„ŸçŸ¥ç‰¹å¾èåˆå—æ•´åˆå­¦ä¹ åˆ°çš„è¯­ä¹‰ç‰¹å¾å½¢æˆä¸°å¯Œè¯­ä¹‰çš„äº‹ä»¶è¡¨ç¤ºï¼Œä»¥åŠè¯­ä¹‰æ„ŸçŸ¥E2Vç›‘ç£æœºåˆ¶åˆ©ç”¨SAMç”Ÿæˆçš„ç±»åˆ«æ ‡ç­¾æŒ‡å¯¼è¯­ä¹‰ç»†èŠ‚é‡å»ºã€‚

**Result:** åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSemantic-E2VIDæ˜¾è‘—æå‡äº†å¸§è´¨é‡ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„E2Væ–¹æ³•ï¼Œè¯æ˜äº†è¯­ä¹‰ä¿¡æ¯å¯¹äº‹ä»¶åˆ°è§†é¢‘é‡å»ºçš„é‡è¦æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å°†è§†è§‰åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰çŸ¥è¯†è¿ç§»åˆ°äº‹ä»¶æ¨¡æ€çš„æœ‰æ•ˆæ€§ï¼Œä¸ºäº‹ä»¶ç›¸æœºè§†è§‰ä»»åŠ¡æä¾›äº†æ–°çš„è¯­ä¹‰å¢å¼ºèŒƒå¼ï¼Œæœªæ¥å¯æ‰©å±•åˆ°å…¶ä»–äº‹ä»¶è§†è§‰ä»»åŠ¡ä¸­æå‡æ€§èƒ½ã€‚

---

#### ğŸ“„ Abstract
Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [2] [ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine](https://arxiv.org/abs/2510.17392)
*Sonu Kumar, Arjun S. Nair, Bhawna Chaudhary, Mukul Lokhande, Santosh Kumar Vishvakarma*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºCORDICçš„Hodgkin Huxleyç¥ç»å…ƒæ¨¡å‹å’Œçš®è´¨ç¥ç»æ± æ¶æ„ï¼Œå®ç°äº†ç”Ÿç‰©ç²¾ç¡®ã€ä½èµ„æºæ¶ˆè€—çš„è„‰å†²ç¥ç»ç½‘ç»œï¼Œåœ¨FPGAä¸Šç›¸æ¯”ç°æœ‰æŠ€æœ¯å®ç°äº†æ˜¾è‘—çš„èµ„æºä¼˜åŒ–å’Œæ€§èƒ½æå‡ã€‚è¯¥è®¾è®¡ä¸ºèµ„æºå—é™çš„è¾¹ç¼˜AIåº”ç”¨æä¾›äº†é«˜æ•ˆçš„SNNå®ç°æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºCORDICçš„æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•å­˜åœ¨èµ„æºå…±äº«å’Œæ€§èƒ½é™åˆ¶çš„é—®é¢˜ï¼Œæ— æ³•æ»¡è¶³èµ„æºå—é™è¾¹ç¼˜AIåº”ç”¨å¯¹ç”Ÿç‰©ç²¾ç¡®è„‰å†²ç¥ç»ç½‘ç»œçš„éœ€æ±‚ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§é«˜æ•ˆã€ä½èµ„æºçš„SNNå®ç°æ–¹æ¡ˆï¼Œè§£å†³ç°æœ‰è®¾è®¡åœ¨è®¡ç®—æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡æ–¹é¢çš„ä¸è¶³ã€‚

**Method:** æå‡ºäº†ä¸€ç§çš®è´¨ç¥ç»æ± æ¶æ„ï¼Œé‡‡ç”¨æ¨¡å—åŒ–å’Œæ€§èƒ½ä¼˜åŒ–çš„CORDICçº§è”ç»“æ„ï¼Œè®¾è®¡äº†é«˜é€Ÿã€èµ„æºé«˜æ•ˆçš„åŸºäºCORDICçš„Hodgkin Huxleyç¥ç»å…ƒæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å»¶è¿Ÿ-é¢ç§¯æƒè¡¡ä¼˜åŒ–ï¼Œå®ç°äº†æ¯”å…±äº«CORDICæ–¹æ³•æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚

**Result:** FPGAå®ç°æ˜¾ç¤ºï¼ŒRCHHç¥ç»å…ƒç›¸æ¯”æœ€å…ˆè¿›è®¾è®¡å‡å°‘äº†24.5%çš„LUTä½¿ç”¨ï¼Œé€Ÿåº¦æå‡35.2%ï¼Œå½’ä¸€åŒ–å‡æ–¹æ ¹è¯¯å·®æ”¹å–„70%ã€‚çš®è´¨ç¥ç»æ± åœ¨MNISTæ•°æ®é›†ä¸Šå®ç°äº†2.85å€ååé‡æå‡ï¼Œè¾¾åˆ°12.69 GOPSï¼Œä¸ç­‰æ•ˆDNNç›¸æ¯”ç²¾åº¦ä»…ä¸‹é™0.35%ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºCORDICçš„Hodgkin Huxleyç¥ç»å…ƒæ¨¡å‹èƒ½å¤Ÿå®ç°ç”Ÿç‰©ç²¾ç¡®ä¸”èµ„æºé«˜æ•ˆçš„SNNå®ç°ï¼Œä¸ºè¾¹ç¼˜AIåº”ç”¨æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚æ¶æ„çš„æ¨¡å—åŒ–è®¾è®¡å’Œæ€§èƒ½ä¼˜åŒ–ç­–ç•¥ä¸ºæœªæ¥ä½åŠŸè€—ç¥ç»å½¢æ€è®¡ç®—ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
We present a Cortical Neural Pool (CNP) architecture featuring a high-speed,
resource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike
shared CORDIC-based DNN approaches, the proposed neuron leverages modular and
performance-optimised CORDIC stages with a latency-area trade-off. The FPGA
implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved
speed, compared to SoTA designs, with 70% better normalised root mean square
error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69
GOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only
a 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The
overall results indicate that the design shows biologically accurate,
low-resource spiking neural network implementations for resource-constrained
edge AI applications.


### [3] [A Multi-Threading Kernel for Enabling Neuromorphic Edge Applications](https://arxiv.org/abs/2510.17745)
*Lars Niedermeier, Vyom Shah, Jeffrey L. Krichmar*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šçº¿ç¨‹å†…æ ¸ï¼Œä½¿è„‰å†²ç¥ç»ç½‘ç»œèƒ½å¤Ÿåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œï¼Œå®ç°äº†4å€çš„åŠ é€Ÿæ¯”å’Œ70%çš„èƒ½æ•ˆæå‡ï¼Œæ”¯æŒä½SWaPçš„ç¥ç»å½¢æ€åº”ç”¨å¼€å‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç¥ç»å½¢æ€åº”ç”¨åœ¨è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸­é¢ä¸´å¤„ç†æ•ˆç‡ä½å’Œèƒ½è€—é«˜çš„é—®é¢˜ï¼Œéœ€è¦è§£å†³åœ¨æ— äº‘æœåŠ¡ä¾èµ–æ¡ä»¶ä¸‹ç›´æ¥å¤„ç†æ„Ÿå®˜è¾“å…¥çš„æŒ‘æˆ˜ï¼Œä»¥åŠå¤šæ ¸å¤„ç†å™¨çš„è´Ÿè½½å‡è¡¡ä¼˜åŒ–éœ€æ±‚ã€‚

**Method:** ç ”ç©¶å¼€å‘äº†ä¸€ç§å¤šçº¿ç¨‹å†…æ ¸æŠ€æœ¯ï¼Œä¸“é—¨é’ˆå¯¹è„‰å†²ç¥ç»ç½‘ç»œçš„ç¨€ç–äº‹ä»¶é©±åŠ¨ç‰¹æ€§è¿›è¡Œä¼˜åŒ–ï¼Œå®ç°äº†åœ¨å¤šæ ¸ARMå¤„ç†å™¨ä¸Šçš„åŠ¨æ€è´Ÿè½½å‡è¡¡ï¼Œå–ä»£äº†ä¼ ç»Ÿçš„é™æ€æ ¸å¿ƒåˆ†é…æ–¹å¼ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥å†…æ ¸åœ¨ä¸­ç­‰è§„æ¨¡SNNä¸Šå®ç°äº†4å€çš„åŠ é€Ÿæ¯”ï¼Œåœ¨Synfireç½‘ç»œä¸Šè¾¾åˆ°1.7å€åŠ é€Ÿï¼Œç›¸æ¯”é™æ€æ ¸å¿ƒåˆ†é…èƒ½è€—é™ä½70%ï¼Œæœ‰æ•ˆå¹³è¡¡äº†å¤šæ ¸å¤„ç†å™¨çš„æ‰€æœ‰å¯ç”¨æ ¸å¿ƒã€‚

**Conclusion:** è¿™é¡¹å·¥ä½œä¸ºå¼€å‘ä½å°ºå¯¸ã€é‡é‡å’ŒåŠŸè€—çš„è¾¹ç¼˜ç¥ç»å½¢æ€åº”ç”¨æä¾›äº†å…³é”®æŠ€æœ¯æ”¯æ’‘ï¼ŒåŒæ—¶ä¸ºç¥ç»å½¢æ€èŠ¯ç‰‡çš„é›†æˆåŸå‹è®¾è®¡å¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†è¾¹ç¼˜AIè®¡ç®—çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Spiking Neural Networks (SNNs) have sparse, event driven processing that can
leverage neuromorphic applications. In this work, we introduce a
multi-threading kernel that enables neuromorphic applications running at the
edge, meaning they process sensory input directly and without any up-link to or
dependency on a cloud service. The kernel shows speed-up gains over single
thread processing by a factor of four on moderately sized SNNs and 1.7X on a
Synfire network. Furthermore, it load-balances all cores available on
multi-core processors, such as ARM, which run today's mobile devices and is up
to 70% more energy efficient compared to statical core assignment. The present
work can enable the development of edge applications that have low Size,
Weight, and Power (SWaP), and can prototype the integration of neuromorphic
chips.
