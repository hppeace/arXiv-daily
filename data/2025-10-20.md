<div id=toc></div>

# Table of Contents

- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [1] [SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware](https://arxiv.org/abs/2510.15542)
*Ivan Kartashov, Mariia Pushkareva, Iakov Karandashev*

#### ğŸ§© TL;DR
SpikeFitæ˜¯ä¸€ç§æ–°é¢–çš„è„‰å†²ç¥ç»ç½‘ç»œè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡è”åˆä¼˜åŒ–ç¦»æ•£æƒé‡é›†ä¸ç¡¬ä»¶çº¦æŸï¼Œå®ç°äº†åœ¨ç¥ç»å½¢æ€ç¡¬ä»¶ä¸Šçš„é«˜æ•ˆæ¨ç†ï¼ŒåŒæ—¶æ»¡è¶³ç¥ç»å…ƒæ•°é‡ã€çªè§¦è¿æ¥æ•°å’Œä½æ¯”ç‰¹ä½å®½è¡¨ç¤ºç­‰ä¸¥æ ¼ç¡¬ä»¶è¦æ±‚ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è„‰å†²ç¥ç»ç½‘ç»œå‹ç¼©æ–¹æ³•ä»…èƒ½è§£å†³æ•°å€¼ç²¾åº¦å—é™æˆ–ç½‘ç»œç¥ç»å…ƒæ•°é‡å—é™ç­‰éƒ¨åˆ†ç¡¬ä»¶çº¦æŸé—®é¢˜ï¼Œæ— æ³•å…¨é¢æ»¡è¶³ç¥ç»å½¢æ€å¤„ç†å™¨å¯¹ç¥ç»å…ƒæ•°é‡ã€çªè§¦è¿æ¥æ•°å’Œä½æ¯”ç‰¹ä½å®½è¡¨ç¤ºçš„å¤šé‡ä¸¥æ ¼è¦æ±‚ï¼Œé™åˆ¶äº†SNNåœ¨å¤šæ ·åŒ–ç¥ç»å½¢æ€ç¡¬ä»¶ä¸Šçš„å®Œæ•´éƒ¨ç½²ã€‚

**Method:** SpikeFitæå‡ºå°†å…è®¸çš„æƒé‡ç¦»æ•£å€¼æœ¬èº«ä½œä¸ºå¯å­¦ä¹ å‚æ•°ä¸æ¨¡å‹å…±åŒä¼˜åŒ–ï¼Œå®ç°ä½ç²¾åº¦ï¼ˆ2ä½ã€4ä½æˆ–8ä½ï¼‰ä¸‹çš„æœ€ä¼˜èšç±»æ„ŸçŸ¥è®­ç»ƒï¼ŒåŒæ—¶å¼•å…¥ç¡¬ä»¶å‹å¥½çš„Fisherè„‰å†²è´¡çŒ®å‰ªææ–¹æ³•ï¼Œé™åˆ¶å”¯ä¸€çªè§¦è¿æ¥æ•°é‡ä»¥é€‚åº”ç¥ç»å½¢æ€å¤„ç†å™¨çš„è¦æ±‚ã€‚

**Result:** åœ¨ä»…å…è®¸å››ä¸ªå”¯ä¸€çªè§¦æƒé‡å€¼ï¼ˆM=4ï¼‰çš„çº¦æŸä¸‹ï¼ŒSpikeFitä¸ä»…è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„SNNå‹ç¼©æ–¹æ³•å’Œä¼ ç»ŸåŸºçº¿æ–¹æ³•ï¼Œè¿˜æ»¡è¶³äº†æ›´å¹¿æ³›çš„ç¥ç»å½¢æ€ç¡¬ä»¶è¦æ±‚ï¼Œåœ¨å®éªŒä¸­å®ç°äº†æœ€ä½çš„èƒ½è€—ä½¿ç”¨ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜è”åˆä¼˜åŒ–ç¦»æ•£æƒé‡é›†ä¸ç¡¬ä»¶çº¦æŸçš„æ–¹æ³•èƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆçš„ç½‘ç»œå‹ç¼©ï¼Œä¸ºè„‰å†²ç¥ç»ç½‘ç»œåœ¨å¤šæ ·åŒ–ç¥ç»å½¢æ€å¤„ç†å™¨ä¸Šçš„å®Œæ•´éƒ¨ç½²æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æå‡ºçš„FSCå‰ªææ–¹æ³•å±•ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚

---

#### ğŸ“„ Abstract
This paper introduces SpikeFit, a novel training method for Spiking Neural
Networks (SNNs) that enables efficient inference on neuromorphic hardware,
considering all its stringent requirements: the number of neurons and synapses
that can fit on a single device, and lower bit-width representations (e.g.,
4-bit, 8-bit). Unlike conventional compressing approaches that address only a
subset of these requirements (limited numerical precision and limited number of
neurons in the network), SpikeFit treats the allowed weights' discrete values
themselves as learnable parameters co-optimized with the model, allowing for
optimal Clusterization-Aware Training (CAT) of the model's weights at low
precision (2-, 4-, or 8-bit) which results in higher network compression
efficiency, as well as limiting the number of unique synaptic connections to a
value required by neuromorphic processor. This joint optimization allows
SpikeFit to find a discrete weight set aligned with hardware constraints,
enabling the most complete deployment across a broader range of neuromorphic
processors than existing methods of SNN compression support. Moreover, SpikeFit
introduces a new hardware-friendly Fisher Spike Contribution (FSC) pruning
method showing the state-of-the-art performance. We demonstrate that for
spiking neural networks constrained to only four unique synaptic weight values
(M = 4), our SpikeFit method not only outperforms state-of-the-art SNNs
compression methods and conventional baselines combining extreme quantization
schemes and clustering algorithms, but also meets a wider range of neuromorphic
hardware requirements and provides the lowest energy use in experiments.
