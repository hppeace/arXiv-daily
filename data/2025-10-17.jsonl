{"id": "2510.14245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14245", "abs": "https://arxiv.org/abs/2510.14245", "authors": ["Miu Sumino", "Mayu Ishii", "Shun Kaizu", "Daisuke Hisano", "Yu Nakayama"], "title": "Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication", "comment": null, "summary": "Optical camera communication (OCC) represents a promising visible light\ncommunication technology. Nonetheless, typical OCC systems utilizing\nframe-based cameras are encumbered by limitations, including low bit rate and\nhigh processing load. To address these issues, OCC system utilizing an\nevent-based vision sensor (EVS) as receivers have been proposed. The EVS\nenables high-speed, low-latency, and robust communication due to its\nasynchronous operation and high dynamic range. In existing event-based OCC\nsystems, conventional modulation schemes such as on-off keying (OOK) and pulse\nposition modulation have been applied, however, to the best of our knowledge,\nno modulation method has been proposed that fully exploits the unique\ncharacteristics of the EVS. This paper proposes a novel modulation scheme,\ncalled the event interval modulation (EIM) scheme, specifically designed for\nevent-based OCC. EIM enables improvement in transmission speed by modulating\ninformation using the intervals between events. This paper proposes a\ntheoretical model of EIM and conducts a proof-of-concept experiment. First, the\nparameters of the EVS are tuned and customized to optimize the frequency\nresponse specifically for EIM. Then, the maximum modulation order usable in EIM\nis determined experimentally. We conduct transmission experiments based on the\nobtained parameters. Finally, we report successful transmission at 28 kbps over\n10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new\nbenchmark for bit rate in event-based OCC systems."}
{"id": "2510.14266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14266", "abs": "https://arxiv.org/abs/2510.14266", "authors": ["Miu Sumino", "Mayu Ishii", "Shun Kaizu", "Daisuke Hisano", "Yu Nakayama"], "title": "Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment", "comment": null, "summary": "We propose a robust demodulation scheme for optical camera communication\nsystems using an event-based vision sensor, combining OOK with toggle\ndemodulation and a digital phase-locked loop. This is the first report to\nachieve a $\\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor\nexperiments."}
{"id": "2510.14770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14770", "abs": "https://arxiv.org/abs/2510.14770", "authors": ["Zhang Nengbo", "Hann Woei Ho", "Ye Zhou"], "title": "MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks", "comment": null, "summary": "Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in\nenvironments, where conventional radio-based methods suffer from spectrum\ncongestion, jamming, and high power consumption. Inspired by the waggle dance\nof honeybees, which efficiently communicate the location of food sources\nwithout sound or contact, we propose a novel visual communication framework for\nMAV swarms using motion-based signaling. In this framework, MAVs convey\ninformation, such as heading and distance, through deliberate flight patterns,\nwhich are passively captured by event cameras and interpreted using a\npredefined visual codebook of four motion primitives: vertical (up/down),\nhorizontal (left/right), left-to-up-to-right, and left-to-down-to-right,\nrepresenting control symbols (``start'', ``end'', ``1'', ``0''). To decode\nthese signals, we design an event frame-based segmentation model and a\nlightweight Spiking Neural Network (SNN) for action recognition. An integrated\ndecoding algorithm then combines segmentation and classification to robustly\ninterpret MAV motion sequences. Experimental results validate the framework's\neffectiveness, which demonstrates accurate decoding and low power consumption,\nand highlights its potential as an energy-efficient alternative for MAV\ncommunication in constrained environments."}
{"id": "2510.14235", "categories": ["cs.NE", "C.1.3; I.2.6; I.2.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.14235", "abs": "https://arxiv.org/abs/2510.14235", "authors": ["Kama Svoboda", "Tosiron Adegbija"], "title": "Spiking Neural Network Architecture Search: A Survey", "comment": "16 pages, 9 figures, submitted to IEEE Computational Intelligence\n  Magazine", "summary": "This survey paper presents a comprehensive examination of Spiking Neural\nNetwork (SNN) architecture search (SNNaS) from a unique hardware/software\nco-design perspective. SNNs, inspired by biological neurons, have emerged as a\npromising approach to neuromorphic computing. They offer significant advantages\nin terms of power efficiency and real-time resource-constrained processing,\nmaking them ideal for edge computing and IoT applications. However, designing\noptimal SNN architectures poses significant challenges, due to their inherent\ncomplexity (e.g., with respect to training) and the interplay between hardware\nconstraints and SNN models. We begin by providing an overview of SNNs,\nemphasizing their operational principles and key distinctions from traditional\nartificial neural networks (ANNs). We then provide a brief overview of the\nstate of the art in NAS for ANNs, highlighting the challenges of directly\napplying these approaches to SNNs. We then survey the state-of-the-art in\nSNN-specific NAS approaches. Finally, we conclude with insights into future\nresearch directions for SNN research, emphasizing the potential of\nhardware/software co-design in unlocking the full capabilities of SNNs. This\nsurvey aims to serve as a valuable resource for researchers and practitioners\nin the field, offering a holistic view of SNNaS and underscoring the importance\nof a co-design approach to harness the true potential of neuromorphic\ncomputing."}
