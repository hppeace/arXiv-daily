{"id": "2512.21053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21053", "abs": "https://arxiv.org/abs/2512.21053", "authors": ["Zibin Liu", "Banglei Guan", "Yang Shang", "Shunkun Liang", "Zhenbao Yu", "Qifeng Yu"], "title": "Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera", "comment": "9 pages, 5 figures. In Proceedings of the 32nd ACM International Conference on Multimedia (MM '24)", "summary": "Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5149\u6d41\u5f15\u5bfc6\u81ea\u7531\u5ea6\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc72D-3D\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u7b56\u7565\u548c\u4e8b\u4ef6\u5173\u8054\u6982\u7387\u6700\u5927\u5316\u5b9e\u73b0\u7cbe\u786e\u8fd0\u52a8\u8868\u5f81\uff0c\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u4e8b\u4ef6\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u76f8\u673a\u5728\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u4e2d\u9762\u4e34\u8fd0\u52a8\u6a21\u7cca\u3001\u4f20\u611f\u5668\u566a\u58f0\u3001\u90e8\u5206\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\uff0c\u800c\u65b0\u5174\u7684\u751f\u7269\u542f\u53d1\u4f20\u611f\u5668\u7279\u522b\u662f\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u4f18\u52bf\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u5149\u6d41\u5f15\u5bfc\u76846\u81ea\u7531\u5ea6\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u6846\u67b6\uff0c\u9996\u5148\u4f7f\u75282D-3D\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u7b56\u7565\u4ece\u4e8b\u4ef6\u548c\u7269\u4f53\u6a21\u578b\u4e2d\u68c0\u6d4b\u89d2\u70b9\u548c\u8fb9\u7f18\u4ee5\u7cbe\u786e\u8868\u5f81\u7269\u4f53\u8fd0\u52a8\uff0c\u7136\u540e\u901a\u8fc7\u5728\u65f6\u7a7a\u7a97\u53e3\u5185\u6700\u5927\u5316\u4e8b\u4ef6\u5173\u8054\u6982\u7387\u6765\u641c\u7d22\u89d2\u70b9\u7684\u5149\u6d41\uff0c\u5e76\u5efa\u7acb\u5149\u6d41\u5f15\u5bfc\u4e0b\u7684\u89d2\u70b9\u4e0e\u8fb9\u7f18\u76f8\u5173\u6027\uff0c\u6700\u540e\u901a\u8fc7\u6700\u5c0f\u5316\u89d2\u70b9\u4e0e\u8fb9\u7f18\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u8fed\u4ee3\u4f18\u53166\u81ea\u7531\u5ea6\u7269\u4f53\u59ff\u6001\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u5149\u6d41\u5f15\u5bfc\u7b56\u7565\u548c\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u7684\u5149\u6d41\u5f15\u5bfc\u6846\u67b6\u548c\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u7b56\u7565\u4e3a\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u5728\u591a\u5a92\u4f53\u5e94\u7528\u4e2d\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.21284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21284", "abs": "https://arxiv.org/abs/2512.21284", "authors": ["Shihao Zou", "Jingjing Li", "Wei Ji", "Jincai Huang", "Kai Wang", "Guo Dan", "Weixin Si", "Yi Pan"], "title": "Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential", "comment": null, "summary": "Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \\textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\\times$. Notably, it delivers over $20\\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSpikeSurgSeg\uff0c\u9996\u4e2a\u9488\u5bf9\u624b\u672f\u573a\u666f\u5206\u5272\u7684\u5c16\u5cf0\u9a71\u52a8\u89c6\u9891Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u672f\u573a\u666f\u63a9\u7801\u81ea\u7f16\u7801\u9884\u8bad\u7ec3\u548c\u8f7b\u91cf\u7ea7\u5206\u5272\u5934\uff0c\u5728\u975eGPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u76f8\u6bd4ANN\u6a21\u578b\u51cf\u5c11\u81f3\u5c118\u500d\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u4ee3\u624b\u672f\u7cfb\u7edf\u4f9d\u8d56\u667a\u80fd\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u672f\u4e2d\u6001\u52bf\u611f\u77e5\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7279\u522b\u662f\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u9ad8\u3001\u529f\u8017\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u624b\u672f\u73af\u5883\u4e2d\u5b9e\u65f6\u90e8\u7f72\uff0c\u540c\u65f6\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\u53d7\u9650\u4e8e\u6807\u8bb0\u624b\u672f\u6570\u636e\u7a00\u7f3a\u548c\u624b\u672f\u89c6\u9891\u8868\u793a\u7684\u56fa\u6709\u7a00\u758f\u6027\u3002", "method": "\u63d0\u51faSpikeSurgSeg\u6846\u67b6\uff0c\u5305\u542b\u624b\u672f\u573a\u666f\u63a9\u7801\u81ea\u7f16\u7801\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u5c42\u7ba1\u72b6\u63a9\u7801\u5b9e\u73b0\u9c81\u68d2\u7684\u65f6\u7a7a\u8868\u793a\u5b66\u4e60\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u91c7\u7528\u8f7b\u91cf\u7ea7\u5c16\u5cf0\u9a71\u52a8\u5206\u5272\u5934\uff0c\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u9884\u6d4b\u540c\u65f6\u4fdd\u7559SNN\u7684\u4f4e\u5ef6\u8fdf\u7279\u6027\uff0c\u4e13\u95e8\u9488\u5bf9\u975eGPU\u5e73\u53f0\u4f18\u5316\u3002", "result": "\u5728EndoVis18\u548c\u5185\u90e8SurgBleed\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSpikeSurgSeg\u8fbe\u5230\u4e0e\u6700\u5148\u8fdbANN\u6a21\u578b\u76f8\u5f53\u7684mIoU\uff0c\u540c\u65f6\u51cf\u5c11\u81f3\u5c118\u500d\u63a8\u7406\u5ef6\u8fdf\uff1b\u76f8\u6bd4\u5927\u591a\u6570\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\u5b9e\u73b0\u8d85\u8fc720\u500d\u52a0\u901f\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u65f6\u95f4\u5173\u952e\u624b\u672f\u573a\u666f\u5206\u5272\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\u5728\u8d44\u6e90\u53d7\u9650\u624b\u672f\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u67b6\u6784\u8bbe\u8ba1\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u65f6\u624b\u672f\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8fb9\u7f18\u8ba1\u7b97\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u3002"}}
