{"id": "2510.27379", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27379", "abs": "https://arxiv.org/abs/2510.27379", "authors": ["Sales G. Aribe Jr"], "title": "Spiking Neural Networks: The Future of Brain-Inspired Computing", "comment": "17 pages, 7 figures, 4 tables, Published with International Journal\n  of Engineering Trends and Technology (IJETT)", "summary": "Spiking Neural Networks (SNNs) represent the latest generation of neural\ncomputation, offering a brain-inspired alternative to conventional Artificial\nNeural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals,\nSNNs operate using distinct spike events, making them inherently more\nenergy-efficient and temporally dynamic. This study presents a comprehensive\nanalysis of SNN design models, training algorithms, and multi-dimensional\nperformance metrics, including accuracy, energy consumption, latency, spike\ncount, and convergence behavior. Key neuron models such as the Leaky\nIntegrate-and-Fire (LIF) and training strategies, including surrogate gradient\ndescent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP),\nare examined in depth. Results show that surrogate gradient-trained SNNs\nclosely approximate ANN accuracy (within 1-2%), with faster convergence by the\n20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve\ncompetitive performance but require higher spike counts and longer simulation\nwindows. STDP-based SNNs, though slower to converge, exhibit the lowest spike\ncounts and energy consumption (as low as 5 millijoules per inference), making\nthem optimal for unsupervised and low-power tasks. These findings reinforce the\nsuitability of SNNs for energy-constrained, latency-sensitive, and adaptive\napplications such as robotics, neuromorphic vision, and edge AI systems. While\npromising, challenges persist in hardware standardization and scalable\ntraining. This study concludes that SNNs, with further refinement, are poised\nto propel the next phase of neuromorphic computing."}
{"id": "2510.27379", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27379", "abs": "https://arxiv.org/abs/2510.27379", "authors": ["Sales G. Aribe Jr"], "title": "Spiking Neural Networks: The Future of Brain-Inspired Computing", "comment": "17 pages, 7 figures, 4 tables, Published with International Journal\n  of Engineering Trends and Technology (IJETT)", "summary": "Spiking Neural Networks (SNNs) represent the latest generation of neural\ncomputation, offering a brain-inspired alternative to conventional Artificial\nNeural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals,\nSNNs operate using distinct spike events, making them inherently more\nenergy-efficient and temporally dynamic. This study presents a comprehensive\nanalysis of SNN design models, training algorithms, and multi-dimensional\nperformance metrics, including accuracy, energy consumption, latency, spike\ncount, and convergence behavior. Key neuron models such as the Leaky\nIntegrate-and-Fire (LIF) and training strategies, including surrogate gradient\ndescent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP),\nare examined in depth. Results show that surrogate gradient-trained SNNs\nclosely approximate ANN accuracy (within 1-2%), with faster convergence by the\n20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve\ncompetitive performance but require higher spike counts and longer simulation\nwindows. STDP-based SNNs, though slower to converge, exhibit the lowest spike\ncounts and energy consumption (as low as 5 millijoules per inference), making\nthem optimal for unsupervised and low-power tasks. These findings reinforce the\nsuitability of SNNs for energy-constrained, latency-sensitive, and adaptive\napplications such as robotics, neuromorphic vision, and edge AI systems. While\npromising, challenges persist in hardware standardization and scalable\ntraining. This study concludes that SNNs, with further refinement, are poised\nto propel the next phase of neuromorphic computing."}
{"id": "2510.27434", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2510.27434", "abs": "https://arxiv.org/abs/2510.27434", "authors": ["Pengfei Sun", "Jascha Achterberg", "Zhe Su", "Dan F. M. Goodman", "Danyal Akarca"], "title": "Exploiting heterogeneous delays for efficient computation in low-bit neural networks", "comment": null, "summary": "Neural networks rely on learning synaptic weights. However, this overlooks\nother neural parameters that can also be learned and may be utilized by the\nbrain. One such parameter is the delay: the brain exhibits complex temporal\ndynamics with heterogeneous delays, where signals are transmitted\nasynchronously between neurons. It has been theorized that this delay\nheterogeneity, rather than a cost to be minimized, can be exploited in embodied\ncontexts where task-relevant information naturally sits contextually in the\ntime domain. We test this hypothesis by training spiking neural networks to\nmodify not only their weights but also their delays at different levels of\nprecision. We find that delay heterogeneity enables state-of-the-art\nperformance on temporally complex neuromorphic problems and can be achieved\neven when weights are extremely imprecise (1.58-bit ternary precision: just\npositive, negative, or absent). By enabling high performance with extremely\nlow-precision weights, delay heterogeneity allows memory-efficient solutions\nthat maintain state-of-the-art accuracy even when weights are compressed over\nan order of magnitude more aggressively than typically studied weight-only\nnetworks. We show how delays and time-constants adaptively trade-off, and\nreveal through ablation that task performance depends on task-appropriate delay\ndistributions, with temporally-complex tasks requiring longer delays. Our\nresults suggest temporal heterogeneity is an important principle for efficient\ncomputation, particularly when task-relevant information is temporal - as in\nthe physical world - with implications for embodied intelligent systems and\nneuromorphic hardware."}
