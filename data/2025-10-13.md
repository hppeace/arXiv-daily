<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 38]
- [cs.CL](#cs.CL) [Total: 15]
- [cs.AI](#cs.AI) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes](https://arxiv.org/abs/2510.08589)
*Nirmal Elamon, Rouzbeh Davoudi*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ¯”è¾ƒä¼ ç»ŸCNNã€é›¶æ ·æœ¬å¤šæ¨¡æ€LLMå’Œå¾®è°ƒå¤šæ¨¡æ€LLMåœ¨å›¾åƒæ–‡æœ¬å åŠ æ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¯æ˜å¤šæ¨¡æ€LLMä»…éœ€å°‘é‡æ•°æ®å¾®è°ƒå³å¯æ˜¾è‘—æå‡æ€§èƒ½ï¼Œåœ¨ä½èµ„æºè§†è§‰ç¯å¢ƒä¸­å±•ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡å’Œé€‚åº”æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸“ä¸šè§†è§‰ä»»åŠ¡ä¸­çš„æ½œåŠ›å°šæœªå……åˆ†æŒ–æ˜ï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¾€å¾€å¯¼è‡´æ€§èƒ½æ¬ ä½³ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¦‚ä½•é€šè¿‡æœ‰é™ç›‘ç£æ•°æ®æœ‰æ•ˆé€‚é…è¯­è¨€å¼•å¯¼æ¨¡å‹ä»¥å®ç°ç²¾ç¡®çš„è§†è§‰ç†è§£ã€‚

**Method:** é‡‡ç”¨ç»¼åˆå¯¹æ¯”ç ”ç©¶æ–¹æ³•ï¼Œåœ¨äººå·¥æ–‡æœ¬å åŠ æ£€æµ‹ä»»åŠ¡ä¸Šè¯„ä¼°å¾®è°ƒä¼ ç»ŸCNNã€é›¶æ ·æœ¬é¢„è®­ç»ƒå¤šæ¨¡æ€LLMä»¥åŠå¾®è°ƒå¤šæ¨¡æ€LLMä¸‰ç§ç­–ç•¥ï¼Œç‰¹åˆ«å…³æ³¨LLMåœ¨æå°‘é‡æ•°æ®ä¸‹çš„å¾®è°ƒæ•ˆæœã€‚

**Result:** å®éªŒè¡¨æ˜å¤šæ¨¡æ€LLMä»…éœ€ä¸è¶³1000å¼ å›¾åƒå¾®è°ƒå³å¯å®ç°é«˜è¾¾36%çš„å‡†ç¡®ç‡æå‡ï¼Œæ€§èƒ½è¾¾åˆ°ç”šè‡³è¶…è¶Šéœ€è¦å¤§é‡æ•°æ®çš„CNNåŸºçº¿æ¨¡å‹ï¼Œçªæ˜¾å…¶å“è¶Šçš„æ•°æ®æ•ˆç‡ä¼˜åŠ¿ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†LLMæ–¹æ³•åœ¨çœŸå®ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­çš„å¼ºå¤§é€‚åº”æ€§å’Œæ•°æ®æ•ˆç‡ï¼Œä¸ºä½èµ„æºè§†è§‰ç¯å¢ƒä¸­åº”ç”¨å¤šæ¨¡æ€transformeræä¾›äº†å®ç”¨æŒ‡å¯¼ï¼Œæ¨åŠ¨äº†è§†è§‰ä¸è¯­è¨€æ¨¡æ€çš„é«˜æ•ˆèåˆå­¦ä¹ ç­–ç•¥å‘å±•ã€‚

---

#### ğŸ“„ Abstract
The field of object detection and understanding is rapidly evolving, driven
by advances in both traditional CNN-based models and emerging multi-modal large
language models (LLMs). While CNNs like ResNet and YOLO remain highly effective
for image-based tasks, recent transformer-based LLMs introduce new capabilities
such as dynamic context reasoning, language-guided prompts, and holistic scene
understanding. However, when used out-of-the-box, the full potential of LLMs
remains underexploited, often resulting in suboptimal performance on
specialized visual tasks. In this work, we conduct a comprehensive comparison
of fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and
fine-tuned multi-modal LLMs on the challenging task of artificial text overlay
detection in images. A key contribution of our study is demonstrating that LLMs
can be effectively fine-tuned on very limited data (fewer than 1,000 images) to
achieve up to 36% accuracy improvement, matching or surpassing CNN-based
baselines that typically require orders of magnitude more data. By exploring
how language-guided models can be adapted for precise visual understanding with
minimal supervision, our work contributes to the broader effort of bridging
vision and language, offering novel insights into efficient cross-modal
learning strategies. These findings highlight the adaptability and data
efficiency of LLM-based approaches for real-world object detection tasks and
provide actionable guidance for applying multi-modal transformers in
low-resource visual environments. To support continued progress in this area,
we have made the code used to fine-tune the models available in our GitHub,
enabling future improvements and reuse in related applications.


### [2] [Adjusting Initial Noise to Mitigate Memorization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.08625)
*Hyeonggeun Han, Sehwan Kim, Hyungjun Joo, Sangwoo Hong, Jungwoo Lee*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºé€šè¿‡è°ƒæ•´åˆå§‹å™ªå£°æ ·æœ¬æ¥ä¿ƒè¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ›´æ—©åœ°é€ƒç¦»è®°å¿†å¸å¼•ç›†ï¼Œä»è€Œåœ¨å‡å°‘è®­ç»ƒæ•°æ®è®°å¿†çš„åŒæ—¶ä¿æŒå›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚è¯¥æ–¹æ³•é€šè¿‡é›†ä½“æˆ–ä¸ªä½“æ–¹å¼ä¼˜åŒ–åˆå§‹å™ªå£°åˆ†å¸ƒï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹è®°å¿†åŒ–é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„è®­ç»ƒæ•°æ®è®°å¿†åŒ–é—®é¢˜ï¼Œå¼•å‘éšç§å’Œç‰ˆæƒæ‹…å¿§ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å»¶è¿Ÿåº”ç”¨åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼æ¥é¿å…è®°å¿†åŒ–ï¼Œä½†ä¼šå¯¼è‡´å›¾åƒä¸è¾“å…¥æç¤ºå¯¹é½ä¸ä½³ï¼Œå› æ­¤éœ€è¦ä¿ƒè¿›æ›´æ—©é€ƒç¦»è®°å¿†å¸å¼•ç›†ä»¥ä¾¿å°½æ—©åº”ç”¨CFGã€‚

**Method:** æœ¬æ–‡æå‡ºä¸¤ç§ç¼“è§£ç­–ç•¥ï¼šé›†ä½“è°ƒæ•´å’Œä¸ªä½“è°ƒæ•´åˆå§‹å™ªå£°æ ·æœ¬ã€‚åŸºäºåˆå§‹å™ªå£°å†³å®šé€ƒç¦»æ—¶é—´çš„è§‚å¯Ÿï¼Œé€šè¿‡ä¼˜åŒ–åˆå§‹å™ªå£°åˆ†å¸ƒæ¥å¯»æ‰¾èƒ½ä¿ƒè¿›æ›´æ—©é€ƒç¦»è®°å¿†å¸å¼•ç›†çš„åˆå§‹æ ·æœ¬ï¼Œä»è€Œå…è®¸åœ¨å»å™ªè¿‡ç¨‹ä¸­æ›´æ—©åº”ç”¨åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ã€‚

**Result:** å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åˆå§‹å™ªå£°è°ƒæ•´æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ¨¡å‹è®°å¿†åŒ–ç°è±¡ï¼ŒåŒæ—¶æœ‰æ•ˆä¿æŒäº†ç”Ÿæˆå›¾åƒä¸è¾“å…¥æ–‡æœ¬æç¤ºä¹‹é—´çš„å¯¹é½è´¨é‡ï¼Œè§£å†³äº†ç°æœ‰å»¶è¿ŸCFGæ–¹æ³•å¯¼è‡´çš„å›¾åƒè´¨é‡ä¸‹é™é—®é¢˜ã€‚

**Conclusion:** åˆå§‹å™ªå£°åœ¨æ‰©æ•£æ¨¡å‹è®°å¿†åŒ–ä¸­èµ·å…³é”®ä½œç”¨ï¼Œé€šè¿‡é’ˆå¯¹æ€§ä¼˜åŒ–åˆå§‹å™ªå£°å¯ä»¥å¹³è¡¡è®°å¿†åŒ–å‡å°‘å’Œå›¾åƒè´¨é‡ä¿æŒã€‚è¿™ä¸ºæ‰©æ•£æ¨¡å‹éšç§ä¿æŠ¤æä¾›äº†æ–°æ€è·¯ï¼Œå³ä»ç”Ÿæˆè¿‡ç¨‹æºå¤´è€Œéä¸­é—´æ­¥éª¤è¿›è¡Œå¹²é¢„ã€‚

---

#### ğŸ“„ Abstract
Despite their impressive generative capabilities, text-to-image diffusion
models often memorize and replicate training data, prompting serious concerns
over privacy and copyright. Recent work has attributed this memorization to an
attraction basin-a region where applying classifier-free guidance (CFG) steers
the denoising trajectory toward memorized outputs-and has proposed deferring
CFG application until the denoising trajectory escapes this basin. However,
such delays often result in non-memorized images that are poorly aligned with
the input prompts, highlighting the need to promote earlier escape so that CFG
can be applied sooner in the denoising process. In this work, we show that the
initial noise sample plays a crucial role in determining when this escape
occurs. We empirically observe that different initial samples lead to varying
escape times. Building on this insight, we propose two mitigation strategies
that adjust the initial noise-either collectively or individually-to find and
utilize initial samples that encourage earlier basin escape. These approaches
significantly reduce memorization while preserving image-text alignment.


### [3] [Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding](https://arxiv.org/abs/2510.08668)
*Songtao Jiang, Yuan Wang, Sibo Song, Tianxiang Hu, Chenyi Zhou, Bin Pu, Yan Zhang, Zhibo Yang, Yang Feng, Joey Tianyi Zhou, Jin Hao, Zijian Chen, Ruijia Wu, Tao Tang, Junhui Lv, Hongxia Xu, Hongwei Wang, Jun Xiao, Bin Feng, Fudong Zhu, Kenli Li, Weidi Xie, Jimeng Sun, Jian Wu, Zuozhu Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Hulu-Medï¼Œä¸€ä¸ªé€æ˜çš„åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„åŸºäºè¡¥ä¸çš„è§†è§‰ç¼–ç å™¨å’ŒLLMè§£ç å™¨æ¶æ„ï¼Œå®ç°äº†è·¨æ–‡æœ¬ã€2D/3Då›¾åƒå’Œè§†é¢‘çš„å¤šæ¨¡æ€ç†è§£ï¼Œåœ¨30ä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°å®ä¸–ç•Œä¸´åºŠå†³ç­–éœ€è¦æ•´åˆæ¥è‡ªä¸åŒæ•°æ®æ¨¡æ€çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŒ»å­¦æ–‡æœ¬ã€2D/3Då›¾åƒå’Œè§†é¢‘ï¼Œè¿™å¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œæ½œåœ¨çš„è¯Šæ–­é—æ¼ã€‚è™½ç„¶é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹å…·æœ‰æ½œåŠ›ï¼Œä½†å…¶åŒ»å­¦åº”ç”¨é¢ä¸´æµç¨‹ä¸é€æ˜ã€æ•°æ®ç¨€ç¼ºå’Œæ¶æ„ä¸çµæ´»çš„æŒ‘æˆ˜ã€‚

**Method:** Hulu-MedåŸºäºç»Ÿä¸€çš„åŸºäºè¡¥ä¸çš„è§†è§‰ç¼–ç å™¨å’ŒLLMè§£ç å™¨æ„å»ºï¼Œé€šè¿‡æ¸è¿›å¼è®­ç»ƒåœ¨1670ä¸‡ä¸ªæ ·æœ¬ä¸Šä»2Dæ‰©å±•åˆ°3Då’Œè§†é¢‘ç†è§£ã€‚åŒ»å­¦æ„ŸçŸ¥çš„ä»¤ç‰Œç¼©å‡æŠ€æœ¯å®ç°äº†é«˜æ•ˆè®­ç»ƒï¼Œ7Båˆ°32Bå‚æ•°å˜ä½“ä»…éœ€4000åˆ°40000 GPUå°æ—¶ã€‚

**Result:** åœ¨30ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒHulu-Medå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è§†è§‰é—®ç­”ã€åŒ»å­¦æŠ¥å‘Šç”Ÿæˆä»¥åŠå¤šè¯­è¨€å’Œç½•è§ç–¾ç—…åœºæ™¯çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¶…è¶Šäº†é¢†å…ˆçš„å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸ä¸“æœ‰ç³»ç»Ÿç«äº‰ã€‚

**Conclusion:** é€šè¿‡å¼€æºå®Œæ•´çš„æµç¨‹ï¼Œæœ¬ç ”ç©¶è¯æ˜äº†é«˜æ€§èƒ½åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹å¯ä»¥é€æ˜åœ°å®ç°ï¼Œä¸ºå¯è®¿é—®å’Œæœ‰å½±å“åŠ›çš„ä¸´åºŠAIæä¾›äº†åŸºç¡€å·¥å…·ï¼Œæ¨åŠ¨äº†åŒ»å­¦å¤šæ¨¡æ€ç†è§£çš„æ ‡å‡†åŒ–å’Œå¯å¤ç°æ€§ã€‚

---

#### ğŸ“„ Abstract
Real-world clinical decision-making grapples with integrating information
from diverse data modalities, including medical text, 2D/3D images, and video,
leading to inefficiencies and potential diagnostic oversights. While generalist
vision-language models (VLMs) offer promise, their medical development faces
challenges of opaque pipelines, data scarcity, and architectural inflexibility.
Here we present Hulu-Med, a transparent medical VLM that unifies understanding
across all these modalities. Built upon a unified patch-based vision encoder
and an LLM decoder, Hulu-Med was progressively trained on 16.7 million (M)
samples to scale from 2D to 3D and video comprehension. The medical-aware token
reduction enables efficient training, requiring only 4,000 to 40,000 GPU hours
for 7B to 32B parameter variants. Extensive evaluation across 30 benchmarks
exhibits state-of-the-art performance, surpassing leading open-source models
and competing with proprietary systems in tasks spanning visual
question-answering, medical report generation, and complex reasoning in
multilingual and rare disease scenarios. By open-sourcing our complete
pipeline, we establish that high-performance medical VLM can be achieved
transparently, providing a foundational tool for accessible and impactful
clinical AI. Code is released on
\href{https://github.com/ZJUI-AI4H/Hulu-Med}{https://github.com/ZJUI-AI4H/Hulu-Med}.


### [4] [Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation](https://arxiv.org/abs/2510.08673)
*Kang Liao, Size Wu, Zhonghua Wu, Linyi Jin, Chao Wang, Yikai Wang, Fei Wang, Wei Li, Chen Change Loy*

#### ğŸ§© TL;DR
Puffinæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç›¸æœºä¸­å¿ƒå¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡å°†ç›¸æœºè§†ä¸ºè¯­è¨€æ¥æ‰©å±•ç©ºé—´æ™ºèƒ½ï¼Œåœ¨ç›¸æœºä¸­å¿ƒçš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¸“é—¨åŒ–æ¨¡å‹ï¼Œå¹¶èƒ½å¤Ÿæ³›åŒ–åˆ°å¤šç§è·¨è§†è§’ä»»åŠ¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç›¸æœºä¸­å¿ƒçš„ç†è§£å’Œç”Ÿæˆç ”ç©¶é€šå¸¸å­¤ç«‹è¿›è¡Œï¼Œç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶æ¥åŒæ—¶å¤„ç†ç©ºé—´è§£é‡Šå’Œåœºæ™¯åˆ›å»ºä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†ç©ºé—´æ™ºèƒ½ç³»ç»Ÿåœ¨ä»»æ„è§†è§’ä¸‹çš„ç»¼åˆèƒ½åŠ›ã€‚

**Method:** Puffiné‡‡ç”¨è¯­è¨€å›å½’å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ–¹æ³•ï¼Œæå‡ºå°†ç›¸æœºè§†ä¸ºè¯­è¨€çš„æ–°èŒƒå¼ï¼Œé€šè¿‡å…¨å±€ç›¸æœºå‚æ•°å’Œåƒç´ çº§ç›¸æœºæ˜ å°„ï¼Œåœ¨Puffin-4Må¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†ç©ºé—´æ„ŸçŸ¥ä¸æ‘„å½±æœ¯è¯­çš„å¯¹é½ã€‚

**Result:** å®éªŒè¡¨æ˜Puffinåœ¨ç›¸æœºä¸­å¿ƒç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸Šä¼˜äºä¸“é—¨åŒ–æ¨¡å‹ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒä¼˜èƒ½å¤Ÿæ³›åŒ–åˆ°ç©ºé—´æƒ³è±¡ã€ä¸–ç•Œæ¢ç´¢å’Œæ‘„å½±æŒ‡å¯¼ç­‰å¤šæ ·åŒ–è·¨è§†è§’ä»»åŠ¡ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€ç©ºé—´æ™ºèƒ½æä¾›äº†ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ç›¸æœºè¯­è¨€åŒ–èŒƒå¼å®ç°äº†çµæ´»å¯é çš„ç©ºé—´ç”Ÿæˆï¼Œæ¨åŠ¨äº†ç›¸æœºç»´åº¦ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›çš„å‘å±•ï¼Œç›¸å…³ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å°†å¼€æºä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚

---

#### ğŸ“„ Abstract
Camera-centric understanding and generation are two cornerstones of spatial
intelligence, yet they are typically studied in isolation. We present Puffin, a
unified camera-centric multimodal model that extends spatial awareness along
the camera dimension. Puffin integrates language regression and diffusion-based
generation to interpret and create scenes from arbitrary viewpoints. To bridge
the modality gap between cameras and vision-language, we introduce a novel
paradigm that treats camera as language, enabling thinking with camera. This
guides the model to align spatially grounded visual cues with photographic
terminology while reasoning across geometric context. Puffin is trained on
Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets.
We incorporate both global camera parameters and pixel-wise camera maps,
yielding flexible and reliable spatial generation. Experiments demonstrate
Puffin superior performance over specialized models for camera-centric
generation and understanding. With instruction tuning, Puffin generalizes to
diverse cross-view tasks such as spatial imagination, world exploration, and
photography guidance. We will release the code, models, dataset pipeline, and
benchmark to advance multimodal spatial intelligence research.


### [5] [BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities](https://arxiv.org/abs/2510.08759)
*Yu Qi, Haibo Zhao, Ziyu Guo, Siyuan Ma, Ziyan Chen, Yaokun Han, Renrui Zhang, Zitiantao Lin, Shiji Xin, Yijian Huang, Kai Cheng, Peiheng Wang, Jiazheng Liu, Jiayi Zhang, Yizhe Zhu, Wenqing Wang, Yiran Qin, Xupeng Zhu, Haojie Huang, Lawson L. S. Wong*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†BEARåŸºå‡†æµ‹è¯•ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å…·èº«èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†BEAR-Agentä»£ç†æ¥å¢å¼ºè¿™äº›èƒ½åŠ›ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†17.5%çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºå…·èº«ä»£ç†çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç‰¹å®šé¢†åŸŸå¦‚è§„åˆ’æˆ–ç©ºé—´ç†è§£ï¼Œç¼ºä¹å¯¹åŸå­çº§å…·èº«èƒ½åŠ›çš„å…¨é¢ç³»ç»Ÿè¯„ä¼°ã€‚

**Method:** æå‡ºäº†BEARåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«4,469ä¸ªäº¤é”™å›¾åƒ-è§†é¢‘-æ–‡æœ¬æ¡ç›®ï¼Œæ¶µç›–6ä¸ªç±»åˆ«14ä¸ªé¢†åŸŸçš„ä»»åŠ¡ï¼›å¹¶å¼€å‘äº†BEAR-Agentå¤šæ¨¡æ€å¯¹è¯ä»£ç†ï¼Œé›†æˆé¢„è®­ç»ƒè§†è§‰æ¨¡å‹ä»¥å¢å¼ºæ„ŸçŸ¥ã€3Dç†è§£å’Œè§„åˆ’èƒ½åŠ›ã€‚

**Result:** å¯¹20ä¸ªä»£è¡¨æ€§MLLMçš„è¯„ä¼°æ˜¾ç¤ºå…¶åœ¨æ‰€æœ‰å…·èº«èƒ½åŠ›é¢†åŸŸå‡å­˜åœ¨æŒç»­å±€é™æ€§ï¼›BEAR-Agentåœ¨BEARåŸºå‡†ä¸Šå®ç°äº†9.12%çš„ç»å¯¹å¢ç›Šå’Œ17.5%çš„ç›¸å¯¹æ”¹è¿›ï¼ŒåŒæ—¶æå‡å…·èº«èƒ½åŠ›ä¹Ÿèƒ½æ”¹å–„æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å…·èº«ä»»åŠ¡æ€§èƒ½ã€‚

**Conclusion:** ç³»ç»Ÿè¯„ä¼°æ­ç¤ºäº†MLLMåœ¨å…·èº«èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ä¸è¶³ï¼Œæå‡ºçš„BEAR-Agentæ¡†æ¶èƒ½æœ‰æ•ˆå¢å¼ºè¿™äº›èƒ½åŠ›ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†é‡è¦æ–¹æ³•è®ºå’ŒåŸºå‡†æ”¯æŒã€‚

---

#### ğŸ“„ Abstract
Embodied capabilities refer to a suite of fundamental abilities for an agent
to perceive, comprehend, and interact with the physical world. While multimodal
large language models (MLLMs) show promise as embodied agents, a thorough and
systematic evaluation of their embodied capabilities remains underexplored, as
existing benchmarks primarily focus on specific domains such as planning or
spatial understanding. To bridge this gap, we introduce BEAR, a comprehensive
and fine-grained benchmark that evaluates MLLMs on atomic embodied
capabilities. BEAR comprises 4,469 interleaved image-video-text entries across
14 domains in 6 categories, including tasks from low-level pointing, trajectory
understanding, spatial reasoning, to high-level planning. Extensive evaluation
results of 20 representative MLLMs reveal their persistent limitations across
all domains of embodied capabilities. To tackle the shortfall, we propose
BEAR-Agent, a multimodal conversable agent that integrates pretrained vision
models to strengthen MLLM perception, 3D understanding, and planning
capabilities. It substantially enhances MLLM performance across diverse
embodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative
improvement of 17.5% on GPT-5. Furthermore, our experiments indicate that
improving MLLM embodied capabilities can benefit embodied tasks in simulated
environments. Project website: https://bear-official66.github.io/


### [6] [Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization](https://arxiv.org/abs/2510.08789)
*Shuo Xing, Soumik Dey, Mingyang Wu, Ashirbad Mishra, Hansi Wu, Binbin Li, Zhengzhong Tu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºQ-Routerï¼Œä¸€ç§åŸºäºå¤šå±‚çº§æ¨¡å‹è·¯ç”±çš„æ™ºèƒ½è§†é¢‘è´¨é‡è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹åŠ¨æ€é€‰æ‹©å’Œé›†æˆä¸“å®¶æ¨¡å‹ï¼Œå®ç°äº†è·¨ä¸åŒè§†é¢‘å†…å®¹å’Œä»»åŠ¡çš„é€šç”¨è§†é¢‘è´¨é‡è¯„ä¼°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºç›´æ¥åˆ†æ•°ç›‘ç£çš„è§†é¢‘è´¨é‡è¯„ä¼°æ¨¡å‹å­˜åœ¨ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šå¯¹ç”¨æˆ·ç”Ÿæˆå†…å®¹ã€çŸ­è§†é¢‘å’ŒAIç”Ÿæˆå†…å®¹ç­‰å¤šæ ·åŒ–å†…å®¹çš„æ³›åŒ–èƒ½åŠ›å·®ã€å¯è§£é‡Šæ€§æœ‰é™ï¼Œä»¥åŠç¼ºä¹å¯¹æ–°ç”¨ä¾‹æˆ–å†…å®¹ç±»å‹çš„å¯æ‰©å±•æ€§ï¼Œè¿™é™åˆ¶äº†è§†é¢‘è´¨é‡è¯„ä¼°ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚

**Method:** Q-Routeré‡‡ç”¨æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œæ„å»ºå¤šå±‚çº§æ¨¡å‹è·¯ç”±ç³»ç»Ÿï¼Œé›†æˆå¤šæ ·åŒ–ä¸“å®¶æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºå®æ—¶è·¯ç”±å™¨ï¼Œæ ¹æ®è¾“å…¥è§†é¢‘è¯­ä¹‰åŠ¨æ€æ¨ç†å¹¶é›†æˆæœ€åˆé€‚çš„ä¸“å®¶æ¨¡å‹ï¼Œæœ€é«˜è®¡ç®—å±‚çº§è¿˜åŒ…å«ç‰¹å®šçš„æ—¶ç©ºä¼ªå½±å®šä½ä»¥å¢å¼ºå¯è§£é‡Šæ€§ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒQ-Routeråœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€å…ˆè¿›çš„è§†é¢‘è´¨é‡è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼Œåœ¨åŸºäºè´¨é‡çš„é—®é¢˜å›ç­”åŸºå‡†Q-Bench-Videoä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½æœ‰æ•ˆå®šä½æ—¶ç©ºä¼ªå½±ï¼Œå±•ç¤ºäº†ä½œä¸ºåè®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹å¥–åŠ±å‡½æ•°çš„æ½œåŠ›ã€‚

**Conclusion:** Q-Routeré€šè¿‡æ™ºèƒ½è·¯ç”±æœºåˆ¶ç»“åˆä¸“ä¸šåŒ–ä¸“å®¶æ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œå®ç°äº†è·¨å¼‚æ„è§†é¢‘æºå’Œä»»åŠ¡çš„çµæ´»é²æ£’æ€§èƒ½ï¼Œä¸ºä¸‹ä¸€ä»£è§†é¢‘è´¨é‡è¯„ä¼°ç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„åŸºç¡€æ¡†æ¶ï¼ŒåŒæ—¶å…¶ä¼ªå½±å®šä½èƒ½åŠ›ä¸ºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ä¼˜åŒ–æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

---

#### ğŸ“„ Abstract
Video quality assessment (VQA) is a fundamental computer vision task that
aims to predict the perceptual quality of a given video in alignment with human
judgments. Existing performant VQA models trained with direct score supervision
suffer from (1) poor generalization across diverse content and tasks, ranging
from user-generated content (UGC), short-form videos, to AI-generated content
(AIGC), (2) limited interpretability, and (3) lack of extensibility to novel
use cases or content types. We propose Q-Router, an agentic framework for
universal VQA with a multi-tier model routing system. Q-Router integrates a
diverse set of expert models and employs vision--language models (VLMs) as
real-time routers that dynamically reason and then ensemble the most
appropriate experts conditioned on the input video semantics. We build a
multi-tiered routing system based on the computing budget, with the heaviest
tier involving a specific spatiotemporal artifacts localization for
interpretability. This agentic design enables Q-Router to combine the
complementary strengths of specialized experts, achieving both flexibility and
robustness in delivering consistent performance across heterogeneous video
sources and tasks. Extensive experiments demonstrate that Q-Router matches or
surpasses state-of-the-art VQA models on a variety of benchmarks, while
substantially improving generalization and interpretability. Moreover, Q-Router
excels on the quality-based question answering benchmark, Q-Bench-Video,
highlighting its promise as a foundation for next-generation VQA systems.
Finally, we show that Q-Router capably localizes spatiotemporal artifacts,
showing potential as a reward function for post-training video generation
models.


### [7] [Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2510.08791)
*Yuanhao Zou, Zhaozheng Yin*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åŒ»å­¦è§†è§‰é—®ç­”æ¡†æ¶ï¼Œé€šè¿‡å¤šçº§æ¨¡æ€å¯¹é½ã€å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜å’Œé—¨æ§äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œè§£å†³äº†Med-VQAä»»åŠ¡ä¸­çš„æ¨¡æ€å¯¹é½ä¸ç»Ÿä¸€ã€å›°éš¾è´Ÿæ ·æœ¬å’ŒçŸ¥è¯†èåˆé—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŒ»å­¦è§†è§‰é—®ç­”ä»»åŠ¡é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹ç»Ÿä¸€çš„æ¨¡æ€å¯¹é½è§£å†³æ–¹æ¡ˆï¼Œå›°éš¾è´Ÿæ ·æœ¬é—®é¢˜ç ”ç©¶ä¸è¶³ï¼Œä»¥åŠå¸¸ç”¨çŸ¥è¯†èåˆæŠ€æœ¯å¯èƒ½å¼•å…¥ä¸ç›¸å…³ä¿¡æ¯ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†Med-VQAç³»ç»Ÿçš„æ€§èƒ½æå‡å’Œå®é™…åº”ç”¨æ•ˆæœã€‚

**Method:** è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼šé‡‡ç”¨å¯¹æ¯”å­¦ä¹ å’Œæœ€ä¼˜ä¼ è¾“ç†è®ºå®ç°è·¨å¤šçº§ã€å¤šæ¨¡æ€ã€å¤šè§†å›¾å’Œå¤šé˜¶æ®µçš„å¼‚è´¨æ¨¡æ€ç»Ÿä¸€å¯¹é½ï¼›ä½¿ç”¨è½¯æ ‡ç­¾è¿›è¡Œå¤šæ¨¡æ€å¯¹é½å¹¶åŠ å¼ºå›°éš¾è´Ÿæ ·æœ¬å¯¹åˆ¤åˆ«æ€§çš„å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜æ–¹æ³•ï¼›ä»¥åŠé›†æˆç­”æ¡ˆè¯æ±‡ä½œä¸ºå…ˆéªŒçŸ¥è¯†å¹¶ä»ä¸­é€‰æ‹©ç›¸å…³ä¿¡æ¯é—¨æ§äº¤å‰æ³¨æ„åŠ›æ¨¡å—ã€‚

**Result:** è¯¥æ¡†æ¶åœ¨RAD-VQAã€SLAKEã€PathVQAå’ŒVQA-2019ç­‰å¹¿æ³›ä½¿ç”¨çš„Med-VQAæ•°æ®é›†ä¸Šå‡è¶…è¶Šäº†å…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†æ‰€æå‡ºæŠ€æœ¯åœ¨åŒ»å­¦è§†è§‰é—®ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºåŒ»å­¦è§†è§‰é—®ç­”æä¾›äº†ç»Ÿä¸€çš„æ¨¡æ€å¯¹é½è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆè§£å†³äº†å›°éš¾è´Ÿæ ·æœ¬å’ŒçŸ¥è¯†èåˆé—®é¢˜ï¼Œå±•ç¤ºäº†å¤šçº§å¯¹é½ç­–ç•¥å’Œé—¨æ§æœºåˆ¶åœ¨åŒ»å­¦å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥åŒ»å­¦AIç³»ç»Ÿçš„å‘å±•æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Medical Visual Question Answering (Med-VQA) is a challenging task that
requires a deep understanding of both medical images and textual questions.
Although recent works leveraging Medical Vision-Language Pre-training (Med-VLP)
have shown strong performance on the Med-VQA task, there is still no unified
solution for modality alignment, and the issue of hard negatives remains
under-explored. Additionally, commonly used knowledge fusion techniques for
Med-VQA may introduce irrelevant information. In this work, we propose a
framework to address these challenges through three key contributions: (1) a
unified solution for heterogeneous modality alignments across multiple levels,
modalities, views, and stages, leveraging methods like contrastive learning and
optimal transport theory; (2) a hard negative mining method that employs soft
labels for multi-modality alignments and enforces the hard negative pair
discrimination; and (3) a Gated Cross-Attention Module for Med-VQA that
integrates the answer vocabulary as prior knowledge and selects relevant
information from it. Our framework outperforms the previous state-of-the-art on
widely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019.


### [8] [D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition](https://arxiv.org/abs/2510.08818)
*Yiyang Huang, Yizhou Wang, Yun Fu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºD-CoDeæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å‹ç¼©å’Œé—®é¢˜åˆ†è§£è§£å†³å›¾åƒé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”è§†é¢‘ä»»åŠ¡æ—¶çš„æ„ŸçŸ¥ç“¶é¢ˆå’Œä»¤ç‰Œè¿‡è½½é—®é¢˜ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æ˜¾è‘—æå‡è§†é¢‘ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å°†å›¾åƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸé¢ä¸´ä¸¤å¤§å…³é”®æŒ‘æˆ˜ï¼šæ„ŸçŸ¥ç“¶é¢ˆå’Œä»¤ç‰Œè¿‡è½½ã€‚æ„ŸçŸ¥ç“¶é¢ˆæºäºæ¨¡å‹éš¾ä»¥å¤„ç†å¯†é›†ä¸”æ—¶é—´å»¶é•¿çš„è§†è§‰è¾“å…¥ï¼Œè€Œä»¤ç‰Œè¿‡è½½åˆ™ç”±äºè§†é¢‘å¸§æ•°é‡åºå¤§å¯¼è‡´è®¡ç®—èµ„æºä¸è¶³ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†å›¾åƒåŸºç¡€æ¨¡å‹åœ¨å¤æ‚è§†é¢‘è¯­è¨€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚

**Method:** D-CoDeæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŠ¨æ€å‹ç¼©å’Œé—®é¢˜åˆ†è§£ã€‚åŠ¨æ€å‹ç¼©é€šè¿‡è‡ªé€‚åº”é€‰æ‹©ä»£è¡¨æ€§å¸§å’Œå†…å®¹æ„ŸçŸ¥çš„ç©ºé—´ä»¤ç‰Œèšåˆæ¥å‡å°‘å†—ä½™å¹¶ä¿ç•™ä¿¡æ¯å†…å®¹ï¼›é—®é¢˜åˆ†è§£åˆ™å°†åŸå§‹æŸ¥è¯¢é‡æ–°è¡¨è¿°ä¸ºå­é—®é¢˜ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨è§†é¢‘çš„ä¸åŒæ–¹é¢ï¼Œå®ç°æ›´å…¨é¢çš„ç†è§£ã€‚æ•´ä¸ªæ¡†æ¶æ— éœ€é¢å¤–è®­ç»ƒå³å¯éƒ¨ç½²ã€‚

**Result:** å®éªŒè¡¨æ˜D-CoDeåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆæå‡äº†è§†é¢‘ç†è§£èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤æ‚è§†é¢‘è¯­è¨€ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚

**Conclusion:** D-CoDeå±•ç¤ºäº†æ— éœ€è®­ç»ƒå³å¯æœ‰æ•ˆæ‰©å±•å›¾åƒåŸºç¡€æ¨¡å‹åˆ°è§†é¢‘é¢†åŸŸçš„å¯è¡Œæ€§ï¼Œä¸ºè§£å†³è§†é¢‘è¯­è¨€æ¨¡å‹ä¸­çš„æ„ŸçŸ¥ç“¶é¢ˆå’Œä»¤ç‰Œè¿‡è½½é—®é¢˜æä¾›äº†æ–°æ€è·¯ã€‚è¯¥æ¡†æ¶çš„æˆåŠŸè¡¨æ˜é€šè¿‡æ™ºèƒ½å‹ç¼©å’Œåˆ†è§£ç­–ç•¥å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹å¤„ç†é•¿è§†é¢‘å†…å®¹çš„èƒ½åŠ›ï¼Œä¸ºæœªæ¥è§†é¢‘ç†è§£ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒæ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Video large language models (Vid-LLMs), which excel in diverse video-language
tasks, can be effectively constructed by adapting image-pretrained
vision-language models (VLMs). However, this adaptation remains challenging, as
it requires processing dense and temporally extended visual inputs that exceed
the capacity of image-based models. This paper identifies the perception
bottleneck and token overload as key challenges in extending image-based VLMs
to the video domain. To address these issues, we propose D-CoDe, a
training-free adaptation framework that incorporates dynamic compression and
question decomposition. Specifically, dynamic compression alleviates the
perception bottleneck through adaptive selection of representative frames and
content-aware aggregation of spatial tokens, thereby reducing redundancy while
preserving informative content. In parallel, question decomposition mitigates
token overload by reformulating the original query into sub-questions, guiding
the model to focus on distinct aspects of the video and enabling more
comprehensive understanding. Experiments demonstrate that D-CoDe effectively
improves video understanding across various benchmarks. Furthermore, strong
performance on the challenging long-video benchmark highlights the potential of
D-CoDe in handling complex video-language tasks. Code is available at
https://github.com/hukcc/D-CoDe.


### [9] [FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided Knowledge Distillation](https://arxiv.org/abs/2510.08849)
*Hongrui Wu, Zhicheng Gao, Jin Cao, Kelu Yao, Wen Shen, Zhihua Wei*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºFOLKæ–¹æ³•ï¼Œé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„çŸ¥è¯†è’¸é¦å®ç°å¿«é€Ÿå¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­2Dæ˜ å°„å¸¦æ¥çš„å™ªå£°å’Œè®¡ç®—å¼€é”€ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²æ–¹æ³•é€šå¸¸å°†3Då®ä¾‹æ˜ å°„åˆ°2D RGB-Då›¾åƒï¼Œç„¶åä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ†ç±»ï¼Œè¿™ç§æ˜ å°„ç­–ç•¥ä¼šå¼•å…¥2Dé®æŒ¡å™ªå£°ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿå¤§é‡è®¡ç®—å’Œå†…å­˜æˆæœ¬ï¼Œæ˜¾è‘—é™ä½æ¨ç†é€Ÿåº¦ã€‚

**Method:** æå‡ºFOLKæ–¹æ³•ï¼Œè®¾è®¡æ•™å¸ˆæ¨¡å‹æå–é«˜è´¨é‡å®ä¾‹åµŒå…¥å¹¶è’¸é¦å…¶å¼€æ”¾è¯æ±‡çŸ¥è¯†åˆ°3Då­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œå…·ä½“åŒ…æ‹¬è®¾è®¡æ•™å¸ˆæ¨¡å‹ç”ŸæˆåŒ…å«å¯è§æ€§å’Œè§†è§’å¤šæ ·æ€§çš„2D CLIPåµŒå…¥ä½œä¸ºè’¸é¦ç›®æ ‡ï¼Œå¼€å‘ç›´æ¥ç”Ÿæˆ3DåµŒå…¥çš„å­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶æå‡ºæ ‡ç­¾å¼•å¯¼è’¸é¦ç®—æ³•å°†æ ‡ç­¾ä¸€è‡´çš„2DåµŒå…¥çŸ¥è¯†è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚

**Result:** åœ¨ScanNet200å’ŒReplicaæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFOLKåœ¨ScanNet200æ•°æ®é›†ä¸Šè¾¾åˆ°35.7çš„AP50åˆ†æ•°ï¼Œå–å¾—æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æ¯”å…ˆå‰æ–¹æ³•å¿«çº¦6.0å€åˆ°152.2å€ã€‚

**Conclusion:** è¯¥æ–¹æ³•è¯æ˜äº†é€šè¿‡çŸ¥è¯†è’¸é¦å°†å¼€æ”¾è¯æ±‡èƒ½åŠ›ç›´æ¥é›†æˆåˆ°3Dæ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé¿å…äº†2Dæ˜ å°„çš„å™ªå£°é—®é¢˜ï¼ŒåŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ¨ç†åŠ é€Ÿï¼Œä¸ºå®æ—¶3Dåœºæ™¯ç†è§£æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Open-vocabulary 3D instance segmentation seeks to segment and classify
instances beyond the annotated label space. Existing methods typically map 3D
instances to 2D RGB-D images, and then employ vision-language models (VLMs) for
classification. However, such a mapping strategy usually introduces noise from
2D occlusions and incurs substantial computational and memory costs during
inference, slowing down the inference speed. To address the above problems, we
propose a Fast Open-vocabulary 3D instance segmentation method via Label-guided
Knowledge distillation (FOLK). Our core idea is to design a teacher model that
extracts high-quality instance embeddings and distills its open-vocabulary
knowledge into a 3D student model. In this way, during inference, the distilled
3D model can directly classify instances from the 3D point cloud, avoiding
noise caused by occlusions and significantly accelerating the inference
process. Specifically, we first design a teacher model to generate a 2D CLIP
embedding for each 3D instance, incorporating both visibility and viewpoint
diversity, which serves as the learning target for distillation. We then
develop a 3D student model that directly produces a 3D embedding for each 3D
instance. During training, we propose a label-guided distillation algorithm to
distill open-vocabulary knowledge from label-consistent 2D embeddings into the
student model. FOLK conducted experiments on the ScanNet200 and Replica
datasets, achieving state-of-the-art performance on the ScanNet200 dataset with
an AP50 score of 35.7, while running approximately 6.0x to 152.2x faster than
previous methods. All codes will be released after the paper is accepted.


### [10] [PHyCLIP: $\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning](https://arxiv.org/abs/2510.08919)
*Daiki Yoshikawa, Takashi Matsubara*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPHyCLIPæ¨¡å‹ï¼Œé€šè¿‡åœ¨å¤šæ›²ç‡åŒæ›²ç©ºé—´çš„ç¬›å¡å°”ç§¯ä¸Šå¼•å…¥â„“â‚-ä¹˜ç§¯åº¦é‡ï¼ŒåŒæ—¶æ•æ‰æ¦‚å¿µæ—å†…çš„å±‚æ¬¡ç»“æ„å’Œè·¨æ¦‚å¿µæ—çš„ç»„åˆè¯­ä¹‰ï¼Œè§£å†³äº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹éš¾ä»¥åŒæ—¶è¡¨è¾¾å±‚æ¬¡æ€§å’Œç»„åˆæ€§çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†éš¾ä»¥åŒæ—¶è¡¨è¾¾æ¦‚å¿µæ—å†…çš„å±‚æ¬¡ç»“æ„ï¼ˆå¦‚ç‹—âŠ†å“ºä¹³åŠ¨ç‰©âŠ†åŠ¨ç‰©ï¼‰å’Œè·¨æ¦‚å¿µæ—çš„ç»„åˆè¯­ä¹‰ï¼ˆå¦‚â€œè½¦é‡Œçš„ç‹—â€âŠ†ç‹—ã€è½¦ï¼‰ã€‚è™½ç„¶è¿‘æœŸå·¥ä½œä½¿ç”¨åŒæ›²ç©ºé—´æœ‰æ•ˆæ•æ‰æ ‘çŠ¶å±‚æ¬¡ç»“æ„ï¼Œä½†å…¶å¯¹ç»„åˆæ€§çš„è¡¨ç¤ºèƒ½åŠ›ä»ä¸æ˜ç¡®ã€‚

**Method:** PHyCLIPé‡‡ç”¨åœ¨å¤šæ›²ç‡åŒæ›²ç©ºé—´çš„ç¬›å¡å°”ç§¯ä¸Šå®šä¹‰â„“â‚-ä¹˜ç§¯åº¦é‡ï¼Œå…¶ä¸­æ¦‚å¿µæ—å†…çš„å±‚æ¬¡ç»“æ„åœ¨å•ä¸ªåŒæ›²å› å­ä¸­è‡ªç„¶æ¶Œç°ï¼Œè€Œè·¨æ¦‚å¿µæ—çš„ç»„åˆè¯­ä¹‰é€šè¿‡â„“â‚-ä¹˜ç§¯åº¦é‡æ•è·ï¼Œç±»ä¼¼äºå¸ƒå°”ä»£æ•°çš„ç»“æ„ã€‚

**Result:** åœ¨é›¶æ ·æœ¬åˆ†ç±»ã€æ£€ç´¢ã€å±‚æ¬¡åˆ†ç±»å’Œç»„åˆç†è§£ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPHyCLIPä¼˜äºç°æœ‰çš„å•ç©ºé—´æ–¹æ³•ï¼Œå¹¶åœ¨åµŒå…¥ç©ºé—´ä¸­æä¾›äº†æ›´å…·å¯è§£é‡Šæ€§çš„ç»“æ„è¡¨ç¤ºã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å¤šæ›²ç‡åŒæ›²ç©ºé—´çš„ä¹˜ç§¯ç»“æ„èƒ½å¤ŸåŒæ—¶æœ‰æ•ˆå»ºæ¨¡å±‚æ¬¡æ€§å’Œç»„åˆæ€§è¯­ä¹‰ï¼Œä¸ºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æä¾›äº†æ–°çš„å‡ ä½•è§†è§’ï¼Œå¹¶å±•ç¤ºäº†æ¯”ä¼ ç»Ÿå•ç©ºé—´æ–¹æ³•æ›´å¥½çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“„ Abstract
Vision-language models have achieved remarkable success in multi-modal
representation learning from large-scale pairs of visual scenes and linguistic
descriptions. However, they still struggle to simultaneously express two
distinct types of semantic structures: the hierarchy within a concept family
(e.g., dog $\preceq$ mammal $\preceq$ animal) and the compositionality across
different concept families (e.g., "a dog in a car" $\preceq$ dog, car). Recent
works have addressed this challenge by employing hyperbolic space, which
efficiently captures tree-like hierarchy, yet its suitability for representing
compositionality remains unclear. To resolve this dilemma, we propose PHyCLIP,
which employs an $\ell_1$-Product metric on a Cartesian product of Hyperbolic
factors. With our design, intra-family hierarchies emerge within individual
hyperbolic factors, and cross-family composition is captured by the
$\ell_1$-product metric, analogous to a Boolean algebra. Experiments on
zero-shot classification, retrieval, hierarchical classification, and
compositional understanding tasks demonstrate that PHyCLIP outperforms existing
single-space approaches and offers more interpretable structures in the
embedding space.


### [11] [Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition with Multimodal Training](https://arxiv.org/abs/1812.06145)
*Mahdi Abavisani, Hamid Reza Vaezi Joze, Vishal M. Patel*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¤šæ¨¡æ€çŸ¥è¯†åµŒå…¥è®­ç»ƒå•æ¨¡æ€3D-CNNçš„é«˜æ•ˆæ–¹æ³•ï¼Œç”¨äºåŠ¨æ€æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ—¶ç©ºè¯­ä¹‰å¯¹é½æŸå¤±å’Œç„¦ç‚¹æ­£åˆ™åŒ–å‚æ•°ï¼Œä½¿å•æ¨¡æ€ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å…±äº«è¯­ä¹‰è¡¨ç¤ºï¼Œè€Œæ— éœ€åœ¨æµ‹è¯•æ—¶ä½¿ç”¨å¤šæ¨¡æ€è¾“å…¥ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŠ¨æ€æ‰‹åŠ¿è¯†åˆ«æ–¹æ³•é€šå¸¸æ˜¾å¼ç»„åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œä½†è¿™ç§æ–¹æ³•åœ¨æµ‹è¯•æ—¶ä¾èµ–å¤šæ¨¡æ€è¾“å…¥ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•å°†å¤šæ¨¡æ€çŸ¥è¯†åµŒå…¥åˆ°å•æ¨¡æ€ç½‘ç»œä¸­ï¼Œä½¿æ¯ä¸ªå•æ¨¡æ€ç½‘ç»œéƒ½èƒ½è·å¾—æ€§èƒ½æå‡ï¼ŒåŒæ—¶é¿å…æµ‹è¯•æ—¶å¯¹å¤šæ¨¡æ€è¾“å…¥çš„ä¾èµ–ã€‚

**Method:** æå‡ºäº†ä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œä¸ºæ¯ä¸ªå¯ç”¨æ¨¡æ€åˆ†é…ç‹¬ç«‹ç½‘ç»œï¼Œå¹¶é€šè¿‡æ—¶ç©ºè¯­ä¹‰å¯¹é½æŸå¤±å¼ºåˆ¶è¿™äº›ç½‘ç»œåä½œå­¦ä¹ å…±äº«è¯­ä¹‰å’Œæ›´å¥½çš„è¡¨ç¤ºã€‚å¼•å…¥ç„¦ç‚¹æ­£åˆ™åŒ–å‚æ•°æ¥é¿å…è´ŸçŸ¥è¯†è¿ç§»ï¼Œç¡®ä¿ç½‘ç»œé—´çŸ¥è¯†ä¼ é€’çš„æœ‰æ•ˆæ€§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†å•æ¨¡æ€ç½‘ç»œçš„æµ‹è¯•æ—¶è¯†åˆ«å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤šä¸ªåŠ¨æ€æ‰‹åŠ¿è¯†åˆ«æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é€šè¿‡é€‚å½“çš„æ­£åˆ™åŒ–å’Œå¯¹é½æœºåˆ¶ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–æµ‹è¯•æ—¶å¤šæ¨¡æ€è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå°†å¤šæ¨¡æ€çŸ¥è¯†æœ‰æ•ˆåµŒå…¥å•æ¨¡æ€ç½‘ç»œã€‚è¿™ä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´å®ç”¨çš„æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ï¼Œå¹¶å±•ç¤ºäº†çŸ¥è¯†è¿ç§»åœ¨å•æ¨¡æ€ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
We present an efficient approach for leveraging the knowledge from multiple
modalities in training unimodal 3D convolutional neural networks (3D-CNNs) for
the task of dynamic hand gesture recognition. Instead of explicitly combining
multimodal information, which is commonplace in many state-of-the-art methods,
we propose a different framework in which we embed the knowledge of multiple
modalities in individual networks so that each unimodal network can achieve an
improved performance. In particular, we dedicate separate networks per
available modality and enforce them to collaborate and learn to develop
networks with common semantics and better representations. We introduce a
"spatiotemporal semantic alignment" loss (SSA) to align the content of the
features from different networks. In addition, we regularize this loss with our
proposed "focal regularization parameter" to avoid negative knowledge transfer.
Experimental results show that our framework improves the test time recognition
accuracy of unimodal networks, and provides the state-of-the-art performance on
various dynamic hand gesture recognition datasets.


### [12] [RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos](https://arxiv.org/abs/2510.08936)
*Zixi Yang, Jiapeng Li, Muxi Diao, Yinuo Jing, Kongming Liang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Ro-Benchï¼Œé¦–ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€åˆ†å¸ƒå¤–åäº‹å®è§†é¢‘æµ‹è¯•é›†ä¸Šé²æ£’æ€§çš„åŸºå‡†ï¼Œç ”ç©¶å‘ç°å½“å‰æ¨¡å‹åœ¨åäº‹å®è§†é¢‘å†…å®¹ä¸Šæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€Œé€šè¿‡åäº‹å®æ•°æ®å¾®è°ƒå¯æœ‰æ•ˆæå‡æ¨¡å‹é²æ£’æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨é¢å¯¹è¢«æ“çºµè§†é¢‘å†…å®¹æ—¶çš„é²æ£’æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œç°æœ‰ç ”ç©¶ç¼ºä¹é’ˆå¯¹åŠ¨æ€åˆ†å¸ƒå¤–åäº‹å®è§†é¢‘çš„ä¸“é—¨è¯„ä¼°åŸºå‡†ã€‚

**Method:** é€šè¿‡ç¼–è¾‘é£æ ¼ã€å¯¹è±¡ã€èƒŒæ™¯åŠå…¶ç»„åˆæ¥æ„å»ºé«˜è´¨é‡ã€å¤šæ ·åŒ–ä¸”æ—¶é—´ç›¸å…³çš„åäº‹å®è§†é¢‘æ•°æ®ï¼Œåˆ›å»ºRo-BenchåŸºå‡†ï¼Œå¹¶å¯¹å…«ä¸ªæœ€æ–°çš„è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ŒåŒæ—¶æ¢ç´¢ä½¿ç”¨åäº‹å®æ•°æ®å¾®è°ƒæ¨¡å‹çš„ç­–ç•¥ã€‚

**Result:** å½“å‰æ¨¡å‹åœ¨Ro-BenchåŸºå‡†ä¸Šé¢å¯¹åäº‹å®è§†é¢‘å†…å®¹æ—¶è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼Œè€Œé€šè¿‡åäº‹å®æ•°æ®å¾®è°ƒå¯ä½¿æ¨¡å‹åœ¨Ro-Benchä¸Šæ€§èƒ½æå‡21.73%ï¼Œåœ¨MVBenchæ•°æ®é›†çš„20ä¸ªä»»åŠ¡ä¸Šå¹³å‡æå‡12.78%ã€‚

**Conclusion:** åäº‹å®æ•°æ®èƒ½æœ‰æ•ˆå¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œç ”ç©¶ç»“æœå¼ºè°ƒäº†è¯„ä¼°å’Œæ”¹è¿›æ¨¡å‹åœ¨é¢å¯¹åˆ†å¸ƒå¤–è§†é¢‘å†…å®¹æ—¶é²æ£’æ€§çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥è§†é¢‘ç†è§£æ¨¡å‹çš„ç¨³å¥æ€§ç ”ç©¶æä¾›äº†é‡è¦åŸºå‡†å’Œæ–¹æ³•è®ºæŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
Recently, Multi-modal Large Language Models (MLLMs) have demonstrated
significant performance across various video understanding tasks. However,
their robustness, particularly when faced with manipulated video content,
remains largely unexplored. In this paper, we introduce Ro-Bench, the first
benchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)
counterfactual video test sets. Ro-Bench incorporates high-quality, diverse and
temporally relevant video data, by editing Style, Object, Background and their
compositions. We evaluated eight recent video MLLMs and found that current
models exhibit substantial performance degradation on Ro-Bench when exposed to
counterfactual video content. Furthermore, we demonstrate that fine-tuning
MLLMs with counterfactual data enhances robustness, achieving a 21.73%
performance increase on Ro-Bench and a 12.78% improvement across 20 tasks in
the MVBench dataset. These findings underscore the effectiveness of
counterfactual data in enhancing the video understanding ability of MLLMs. The
code and data will be released shortly.


### [13] [Unleashing Perception-Time Scaling to Multimodal Reasoning Models](https://arxiv.org/abs/2510.08964)
*Yifan Li, Zhenghao Chen, Ziheng Wu, Kun Zhou, Ruipu Luo, Can Zhang, Zhentao He, Yufei Zhan, Wayne Xin Zhao, Minghui Qiu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ„ŸçŸ¥æ—¶é—´æ‰©å±•ï¼ˆPTSï¼‰èŒƒå¼ï¼Œé€šè¿‡å°†å¤æ‚æ„ŸçŸ¥é—®é¢˜åˆ†è§£ä¸ºå¯å¤„ç†çš„å­é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä¼°è®¡ä»»åŠ¡ä¸­çš„ç²¾åº¦ï¼Œå°†é«˜ç²¾åº¦æ€§èƒ½ä»8.0%æå‡è‡³64.7%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰é™çš„ä¼°è®¡ç²¾åº¦ï¼Œæ¨ç†æ—¶é—´æ‰©å±•ä»…å¸¦æ¥è¾¹é™…æ”¶ç›Šï¼Œè¿™ä¸»è¦æºäºç°æœ‰æ¨¡å‹çš„å¿«é€Ÿæ„ŸçŸ¥èŒƒå¼å°†è§†è§‰ç†è§£è§†ä¸ºä¸€æ¬¡æ€§è¾“å‡ºï¼Œæœªèƒ½å»ºæ¨¡åº•å±‚æ„ŸçŸ¥è¿‡ç¨‹ã€‚

**Method:** æå‡ºäº†æ„ŸçŸ¥æ—¶é—´æ‰©å±•ï¼ˆPTSï¼‰èŒƒå¼ï¼Œé¼“åŠ±ä¸°å¯Œæ ‡è®°çš„æ„ŸçŸ¥è¿‡ç¨‹ï¼Œå°†å¤æ‚æ„ŸçŸ¥é—®é¢˜åˆ†è§£ä¸ºä¸­é—´å¯å¤„ç†çš„å­é—®é¢˜ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œä½¿æ„ŸçŸ¥èƒ½å¤Ÿä¸æ¨ç†æ—¶é—´æ‰©å±•å¯¹é½å¹¶ä»ä¸­å—ç›Šã€‚

**Result:** PTSæ˜¾è‘—æå‡äº†æ„ŸçŸ¥ç²¾åº¦ï¼Œåœ¨DisTANCEåŸºå‡†ä¸Šå°†é«˜ç²¾åº¦æ€§èƒ½ä»8.0%æå‡è‡³64.7%ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼›å³ä½¿ä½¿ç”¨çº¯åˆæˆæ•°æ®ï¼Œä¸æ•°å­¦æ¨ç†æ•°æ®ç»“åˆä¹Ÿèƒ½åœ¨æ¨ç†å’ŒçœŸå®ä¸–ç•Œæ„ŸçŸ¥åŸºå‡†ä¸Šè·å¾—ä¸€è‡´å¢ç›Šã€‚

**Conclusion:** PTSèŒƒå¼é€šè¿‡å¼•å…¥æ›´å¤šæ„ŸçŸ¥ç›¸å…³æ ‡è®°å¹¶å¢å¼ºæ¨¡å‹å¯¹å›¾åƒæ ‡è®°çš„å…³æ³¨ï¼Œæœ‰æ•ˆè§£å†³äº†å½“å‰LVLMsåœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç²¾åº¦é™åˆ¶ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›æå‡æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent advances in inference-time scaling, particularly those leveraging
reinforcement learning with verifiable rewards, have substantially enhanced the
reasoning capabilities of Large Vision-Language Models (LVLMs). Inspired by
this success, similar strategies have been applied to multimodal reasoning, yet
their impact on visual perception remains unclear. To investigate this gap, we
introduce DisTANCE, a perception-centric benchmark for visual estimation tasks.
Evaluation results show that LVLMs exhibit limited estimation precision, and
inference-time scaling offers only marginal gains. We attribute this to the
fast perception paradigm of current LVLMs, where visual understanding is
treated as a one-shot output without modeling the underlying perceptual
process. To address this, we propose Perception-Time Scaling (PTS), a novel
paradigm that encourages token-rich perception and decomposes complex
perception problems into intermediate tractable sub-problems, thereby enabling
perception to align with and benefit from inference-time scaling. Combined with
reinforcement learning techniques, PTS significantly improves perception
accuracy, raising high-precision performance on DisTANCE from 8.0% to 64.7%,
and generalizes well to out-of-domain tasks. Surprisingly, even though PTS data
are purely synthetic, combining them with math reasoning data yields consistent
gains in both reasoning and real-world perception benchmarks. Further analysis
reveals that PTS introduces more perception-related tokens and increases the
model's attention to image tokens. Our code and data will be publicly released.


### [14] [Hierarchical Scheduling for Multi-Vector Image Retrieval](https://arxiv.org/abs/2510.08976)
*Maoliang Li, Ke Li, Yaoyang Liu, Jiayu Chen, Zihao Zheng, Yinjun Wu, Xiang Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†HiMIRæ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚å¤šç²’åº¦å›¾åƒæ£€ç´¢å’Œè·¨å±‚æ¬¡ç›¸ä¼¼æ€§ä¸€è‡´æ€§ä¼˜åŒ–ï¼Œåœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå°†è®¡ç®—é‡å‡å°‘é«˜è¾¾3.5å€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆåº”ç”¨ä¸­å­˜åœ¨æ£€ç´¢ç²¾åº¦æœ‰é™çš„é—®é¢˜ï¼Œè€Œç°æœ‰çš„å¤šå‘é‡æ£€ç´¢æ–¹æ³•è™½ç„¶é€šè¿‡æŸ¥è¯¢åˆ†è§£å’Œå›¾åƒåˆ†æ®µåŒ¹é…æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†ä»é¢ä¸´æŸ¥è¯¢ä¸ä¸åŒå›¾åƒå¯¹è±¡å¯¹é½ä¸è¶³ä»¥åŠç»†ç²’åº¦å›¾åƒç‰‡æ®µå†—ä½™å¯¼è‡´çš„æ¬¡ä¼˜å‡†ç¡®æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚

**Method:** æå‡ºäº†HiMIRé«˜æ•ˆè°ƒåº¦æ¡†æ¶ï¼Œé‡‡ç”¨åˆ†å±‚èŒƒå¼ä½¿ç”¨å¤šä¸ªä¸­é—´ç²’åº¦å¤„ç†ä¸åŒå›¾åƒå¯¹è±¡ä»¥å¢å¼ºå¯¹é½ï¼Œåˆ©ç”¨è·¨å±‚æ¬¡ç›¸ä¼¼æ€§ä¸€è‡´æ€§å’Œå±‚æ¬¡ç¨€ç–æ€§æœ€å°åŒ–æ£€ç´¢å†—ä½™ï¼Œå¹¶ä¸ºä¸åŒæ•°æ®é›†è‡ªåŠ¨é…ç½®å‚æ•°ä»¥é€‚åº”å¤šæ ·åŒ–åº”ç”¨åœºæ™¯ã€‚

**Result:** å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒHiMIRä¸ä»…å®ç°äº†æ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ï¼Œè€Œä¸”åœ¨ç°æœ‰å¤šå‘é‡æ£€ç´¢ç³»ç»ŸåŸºç¡€ä¸Šå°†è®¡ç®—é‡å‡å°‘äº†é«˜è¾¾3.5å€ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜å¼‚çš„æ£€ç´¢æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åˆ†å±‚å¤šç²’åº¦æ£€ç´¢ç­–ç•¥åœ¨å¹³è¡¡ç²¾åº¦ä¸æ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿæä¾›äº†å®ç”¨çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒåº”ç”¨åœºæ™¯çš„éœ€æ±‚ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
To effectively leverage user-specific data, retrieval augmented generation
(RAG) is employed in multimodal large language model (MLLM) applications.
However, conventional retrieval approaches often suffer from limited retrieval
accuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by
decomposing queries and matching against segmented images. They still suffer
from sub-optimal accuracy and efficiency, overlooking alignment between the
query and varying image objects and redundant fine-grained image segments. In
this work, we present an efficient scheduling framework for image retrieval -
HiMIR. First, we introduce a novel hierarchical paradigm, employing multiple
intermediate granularities for varying image objects to enhance alignment.
Second, we minimize redundancy in retrieval by leveraging cross-hierarchy
similarity consistency and hierarchy sparsity to minimize unnecessary matching
computation. Furthermore, we configure parameters for each dataset
automatically for practicality across diverse scenarios. Our empirical study
shows that, HiMIR not only achieves substantial accuracy improvements but also
reduces computation by up to 3.5 times over the existing MVR system.


### [15] [HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images](https://arxiv.org/abs/2510.08978)
*Zichuan Wang, Bo Peng, Songlin Yang, Zhenchen Tang, Jing Dong*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªé’ˆå¯¹ç”Ÿæˆå›¾åƒä¸­æ‰‹éƒ¨åŒºåŸŸçš„è´¨é‡è¯„ä¼°ä»»åŠ¡ï¼Œå¼€å‘äº†åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ‰‹éƒ¨è´¨é‡è¯„ä¼°æ¨¡å‹HandEvalï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸åº”ç”¨ä¸­æ˜¾è‘—æå‡äº†ç”Ÿæˆæ‰‹éƒ¨çš„çœŸå®æ€§å’ŒAIGCæ£€æµ‹å‡†ç¡®æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨æ•´ä½“è§†è§‰è´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨å¤æ‚å±€éƒ¨åŒºåŸŸç‰¹åˆ«æ˜¯æ‰‹éƒ¨çš„ç»†èŠ‚ç”Ÿæˆæ–¹é¢ä»å­˜åœ¨ä¸¥é‡ä¸è¶³ï¼Œç”Ÿæˆçš„æ‰‹éƒ¨ç»å¸¸å‡ºç°ç»“æ„æ‰­æ›²å’Œä¸çœŸå®çº¹ç†ï¼Œè€Œæ‰‹éƒ¨è´¨é‡è¯„ä¼°ä»»åŠ¡å´é•¿æœŸè¢«å¿½è§†ï¼Œè¿™é™åˆ¶äº†äººç±»ä¸­å¿ƒç”Ÿæˆè´¨é‡ä¼˜åŒ–å’ŒAIGCæ£€æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½æå‡ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿé¦–å…ˆæ„å»ºäº†åŒ…å«48kå¼ é«˜è´¨é‡å’Œä½è´¨é‡æ‰‹éƒ¨é…å¯¹å›¾åƒçš„HandPairæ•°æ®é›†ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯å®ç°ä½æˆæœ¬é«˜æ•ˆç›‘ç£ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘äº†HandEvalæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¼ºå¤§è§†è§‰ç†è§£èƒ½åŠ›ï¼Œå¹¶èå…¥æ‰‹éƒ¨å…³é”®ç‚¹çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œè·å¾—å¯¹æ‰‹éƒ¨è´¨é‡çš„å¼ºæ„ŸçŸ¥èƒ½åŠ›ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒHandEvalåœ¨ä»å„ç§æœ€å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆçš„æ‰‹éƒ¨å›¾åƒæµ‹è¯•é›†ä¸Šï¼Œä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ä¼˜äºç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼Œå°†HandEvalé›†æˆåˆ°å›¾åƒç”Ÿæˆå’ŒAIGCæ£€æµ‹æµç¨‹ä¸­ï¼Œåˆ†åˆ«æ˜¾è‘—æå‡äº†ç”Ÿæˆæ‰‹éƒ¨çš„çœŸå®æ€§å’Œæ£€æµ‹å‡†ç¡®æ€§ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„æ™®é€‚æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¡«è¡¥äº†ç”Ÿæˆå›¾åƒæ‰‹éƒ¨è´¨é‡è¯„ä¼°çš„ç©ºç™½ï¼Œå±•ç¤ºäº†æ‰‹éƒ¨è´¨é‡è¯„ä¼°åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„ä¸°å¯Œåº”ç”¨ä»·å€¼ï¼Œé€šè¿‡ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œæ‰‹éƒ¨ç»“æ„å…ˆéªŒçŸ¥è¯†çš„æ–¹æ³•ï¼Œä¸ºç”Ÿæˆå†…å®¹è´¨é‡è¯„ä¼°æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨æ„ä¹‰å’Œæ¨å¹¿ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Although recent text-to-image (T2I) models have significantly improved the
overall visual quality of generated images, they still struggle in the
generation of accurate details in complex local regions, especially human
hands. Generated hands often exhibit structural distortions and unrealistic
textures, which can be very noticeable even when the rest of the body is
well-generated. However, the quality assessment of hand regions remains largely
neglected, limiting downstream task performance like human-centric generation
quality optimization and AIGC detection. To address this, we propose the first
quality assessment task targeting generated hand regions and showcase its
abundant downstream applications. We first introduce the HandPair dataset for
training hand quality assessment models. It consists of 48k images formed by
high- and low-quality hand pairs, enabling low-cost, efficient supervision
without manual annotation. Based on it, we develop HandEval, a carefully
designed hand-specific quality assessment model. It leverages the powerful
visual understanding capability of Multimodal Large Language Model (MLLM) and
incorporates prior knowledge of hand keypoints, gaining strong perception of
hand quality. We further construct a human-annotated test set with hand images
from various state-of-the-art (SOTA) T2I models to validate its quality
evaluation capability. Results show that HandEval aligns better with human
judgments than existing SOTA methods. Furthermore, we integrate HandEval into
image generation and AIGC detection pipelines, prominently enhancing generated
hand realism and detection accuracy, respectively, confirming its universal
effectiveness in downstream applications. Code and dataset will be available.


### [16] [Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation](https://arxiv.org/abs/2510.08994)
*Yao Teng, Fuyun Wang, Xian Liu, Zhekai Chen, Han Shi, Yu Wang, Zhenguo Li, Weiyang Liu, Difan Zou, Xihui Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Speculative Jacobi-Denoising Decoding (SJD2)æ¡†æ¶ï¼Œå°†å»å™ªè¿‡ç¨‹èå…¥Jacobiè¿­ä»£ä¸­ï¼Œå®ç°äº†è‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¹¶è¡Œä»¤ç‰Œç”Ÿæˆï¼Œæ˜¾è‘—åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”±äºé‡‡ç”¨é¡ºåºä»¤ç‰Œè§£ç è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦ç¼“æ…¢ï¼Œé€šå¸¸éœ€è¦æ•°åƒæ¬¡æ¨¡å‹å‰å‘ä¼ é€’æ‰èƒ½ç”Ÿæˆå•å¼ å›¾åƒï¼Œè¿™ç§ä½æ•ˆç‡é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

**Method:** æå‡ºäº†ä¸€ç§ä¸‹ä¸€å¹²å‡€ä»¤ç‰Œé¢„æµ‹èŒƒå¼ï¼Œé€šè¿‡ä½æˆæœ¬å¾®è°ƒä½¿é¢„è®­ç»ƒè‡ªå›å½’æ¨¡å‹èƒ½å¤Ÿæ¥å—å™ªå£°æ‰°åŠ¨ä»¤ç‰ŒåµŒå…¥å¹¶é¢„æµ‹ä¸‹ä¸€å¹²å‡€ä»¤ç‰Œï¼Œè¯¥å»å™ªèŒƒå¼å¼•å¯¼æ¨¡å‹æœå‘æ›´ç¨³å®šçš„Jacobiè½¨è¿¹ï¼›åœ¨æ¨ç†æ—¶ä½¿ç”¨é«˜æ–¯å™ªå£°åˆå§‹åŒ–ä»¤ç‰Œåºåˆ—ï¼Œåœ¨åµŒå…¥ç©ºé—´ä¸­è¿›è¡Œè¿­ä»£çš„ä¸‹ä¸€å¹²å‡€ä»¤ç‰Œé¢„æµ‹ï¼Œå¹¶é‡‡ç”¨æ¦‚ç‡å‡†åˆ™å¹¶è¡ŒéªŒè¯å’Œæ¥å—å¤šä¸ªä»¤ç‰Œã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€šè¿‡å‡å°‘æ¨¡å‹å‰å‘ä¼ é€’æ¬¡æ•°æ¥åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆå›¾åƒçš„è§†è§‰è´¨é‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºè‡ªå›å½’ç”Ÿæˆæ¨¡å‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å¹¶è¡ŒåŒ–è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å°†å»å™ªè¿‡ç¨‹ä¸Jacobiè¿­ä»£ç›¸ç»“åˆï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ï¼Œä¸ºå¤§è§„æ¨¡è§†è§‰å†…å®¹ç”Ÿæˆåº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
As a new paradigm of visual content generation, autoregressive text-to-image
models suffer from slow inference due to their sequential token-by-token
decoding process, often requiring thousands of model forward passes to generate
a single image. To address this inefficiency, we propose Speculative
Jacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising
process into Jacobi iterations to enable parallel token generation in
autoregressive models. Our method introduces a next-clean-token prediction
paradigm that enables the pre-trained autoregressive models to accept
noise-perturbed token embeddings and predict the next clean tokens through
low-cost fine-tuning. This denoising paradigm guides the model towards more
stable Jacobi trajectories. During inference, our method initializes token
sequences with Gaussian noise and performs iterative
next-clean-token-prediction in the embedding space. We employ a probabilistic
criterion to verify and accept multiple tokens in parallel, and refine the
unaccepted tokens for the next iteration with the denoising trajectory.
Experiments show that our method can accelerate generation by reducing model
forward passes while maintaining the visual quality of generated images.


### [17] [On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2510.09008)
*Hoigi Seo, Dong Un Kang, Hyunjin Cho, Joohoon Lee, Se Young Chun*

#### ğŸ§© TL;DR
è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡ä¿®æ”¹è§†è§‰ç¼–ç å™¨æ¥ç¼“è§£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç‰©ä½“å¹»è§‰é—®é¢˜çš„æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«å’Œå±è”½é«˜ä¸ç¡®å®šæ€§çš„è§†è§‰æ ‡è®°æ¥å‡å°‘å¹»è§‰ç°è±¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è™½ç„¶å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä»ç„¶é¢ä¸´ç‰©ä½“å¹»è§‰çš„å…³é”®æŒ‘æˆ˜ï¼Œå³ç”Ÿæˆè¾“å…¥å›¾åƒä¸­ä¸å­˜åœ¨çš„ç‰©ä½“æè¿°ã€‚ä½œè€…è®¤ä¸ºè§†è§‰ç¼–ç å™¨ä¸­çš„ä¸ç¡®å®šè§†è§‰æ ‡è®°æ˜¯å¯¼è‡´ç‰©ä½“å¹»è§‰çš„å…³é”®å› ç´ ã€‚

**Method:** æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼ŒåŒ…æ‹¬ä½¿ç”¨å¯¹æŠ—æ‰°åŠ¨ä»£ç†æ–¹æ³•é«˜æ•ˆè¯†åˆ«ä¸ç¡®å®šè§†è§‰æ ‡è®°ï¼Œå¹¶åœ¨è§†è§‰ç¼–ç å™¨çš„ä¸­é—´å±‚è‡ªæ³¨æ„åŠ›è¿‡ç¨‹ä¸­å±è”½è¿™äº›ä¸ç¡®å®šè§†è§‰æ ‡è®°ï¼Œä»è€ŒæŠ‘åˆ¶å…¶å¯¹è§†è§‰ç¼–ç çš„å½±å“ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç‰©ä½“å¹»è§‰ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¸å…¶ä»–ç°æœ‰æŠ€æœ¯ååŒå·¥ä½œã€‚ç»Ÿè®¡åˆ†æå’Œç†è®ºè¯æ˜éƒ½æ”¯æŒäº†ä¸ç¡®å®šè§†è§‰æ ‡è®°ä¸å¹»è§‰ä¹‹é—´çš„æ­£ç›¸å…³æ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜è§†è§‰ç¼–ç å™¨ä¸­ä¸ç¡®å®šè§†è§‰æ ‡è®°æ˜¯ç‰©ä½“å¹»è§‰çš„é‡è¦æ¥æºï¼Œé€šè¿‡é’ˆå¯¹æ€§å±è”½è¿™äº›æ ‡è®°å¯ä»¥æœ‰æ•ˆç¼“è§£å¹»è§‰é—®é¢˜ï¼Œä¸ºæ”¹è¿›å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Large vision-language models (LVLMs), which integrate a vision encoder (VE)
with a large language model, have achieved remarkable success across various
tasks. However, there are still crucial challenges in LVLMs such as object
hallucination, generating descriptions of objects that are not in the input
image. Here, we argue that uncertain visual tokens within the VE is a key
factor that contributes to object hallucination. Our statistical analysis found
that there are positive correlations between visual tokens with high epistemic
uncertainty and the occurrence of hallucinations. Furthermore, we show
theoretically and empirically that visual tokens in early VE layers that
exhibit large representation deviations under small adversarial perturbations
indicate high epistemic uncertainty. Based on these findings, we propose a
simple yet effective strategy to mitigate object hallucination by modifying the
VE only. Our method comprises a proxy method with adversarial perturbations for
identifying uncertain visual tokens efficiently and a method to mask these
uncertain visual tokens during the self-attention process in the middle layers
of the VE, suppressing their influence on visual encoding and thus alleviating
hallucinations. Extensive experiments show that our method significantly
reduces object hallucinations in LVLMs and can synergistically work with other
prior arts.


### [18] [Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation](https://arxiv.org/abs/2510.09094)
*Youwei Zheng, Yuxi Ren, Xin Xia, Xuefeng Xiao, Xiaohua Xie*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºDense2MoEæ–¹æ³•ï¼Œå°†å¯†é›†æ‰©æ•£å˜æ¢å™¨è½¬æ¢ä¸ºæ··åˆä¸“å®¶ç»“æ„ï¼Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å‡å°‘60%çš„æ¿€æ´»å‚æ•°ï¼Œä¸ºé«˜æ•ˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå»ºç«‹äº†æ–°èŒƒå¼ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ‰©æ•£å˜æ¢å™¨åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åºå¤§çš„å‚æ•°é‡å¯¼è‡´æ˜¾è‘—çš„æ¨ç†å¼€é”€ã€‚ç°æœ‰çš„å‚æ•°å‹ç¼©æ–¹æ³•ä¸»è¦ä¾èµ–å‰ªæï¼Œä½†æ¿€è¿›çš„å‰ªæä¼šå› æ¨¡å‹å®¹é‡å‡å°‘è€Œå¯¼è‡´ä¸¥é‡çš„æ€§èƒ½ä¸‹é™ã€‚

**Method:** å°†å¯†é›†DiTè½¬æ¢ä¸ºæ··åˆä¸“å®¶ç»“æ„è¿›è¡Œç»“æ„åŒ–ç¨€ç–åŒ–ï¼Œç”¨MoEå±‚æ›¿æ¢DiTå—ä¸­çš„å‰é¦ˆç½‘ç»œï¼Œå°†FFNæ¿€æ´»å‚æ•°å‡å°‘62.5%ã€‚æå‡ºæ··åˆå—é€‰æ‹©æ€§åœ°æ¿€æ´»DiTå—ä»¥å¢å¼ºç¨€ç–æ€§ï¼Œå¹¶è®¾è®¡äº†å¤šæ­¥è’¸é¦æµç¨‹ï¼ŒåŒ…æ‹¬åŸºäºæ³°å‹’åº¦é‡çš„ä¸“å®¶åˆå§‹åŒ–ã€è´Ÿè½½å‡è¡¡çš„çŸ¥è¯†è’¸é¦å’ŒMoBä¼˜åŒ–çš„ç»„ç‰¹å¾æŸå¤±ã€‚

**Result:** å°†å¤§å‹æ‰©æ•£å˜æ¢å™¨è½¬æ¢ä¸ºMoEç»“æ„ï¼Œæ¿€æ´»å‚æ•°å‡å°‘60%çš„åŒæ—¶ä¿æŒåŸå§‹æ€§èƒ½ï¼Œåœ¨å¹¿æ³›å®éªŒä¸­è¶…è¶Šäº†åŸºäºå‰ªæçš„æ–¹æ³•ã€‚

**Conclusion:** Dense2MoEä¸ºé«˜æ•ˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå»ºç«‹äº†æ–°èŒƒå¼ï¼Œè¯æ˜äº†é€šè¿‡ç»“æ„åŒ–ç¨€ç–åŒ–è€Œéä¼ ç»Ÿå‰ªæå¯ä»¥åœ¨ä¿æŒæ¨¡å‹å®¹é‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€ï¼Œä¸ºå¤§å‹ç”Ÿæˆæ¨¡å‹çš„éƒ¨ç½²æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Diffusion Transformer (DiT) has demonstrated remarkable performance in
text-to-image generation; however, its large parameter size results in
substantial inference overhead. Existing parameter compression methods
primarily focus on pruning, but aggressive pruning often leads to severe
performance degradation due to reduced model capacity. To address this
limitation, we pioneer the transformation of a dense DiT into a Mixture of
Experts (MoE) for structured sparsification, reducing the number of activated
parameters while preserving model capacity. Specifically, we replace the
Feed-Forward Networks (FFNs) in DiT Blocks with MoE layers, reducing the number
of activated parameters in the FFNs by 62.5\%. Furthermore, we propose the
Mixture of Blocks (MoB) to selectively activate DiT blocks, thereby further
enhancing sparsity. To ensure an effective dense-to-MoE conversion, we design a
multi-step distillation pipeline, incorporating Taylor metric-based expert
initialization, knowledge distillation with load balancing, and group feature
loss for MoB optimization. We transform large diffusion transformers (e.g.,
FLUX.1 [dev]) into an MoE structure, reducing activated parameters by 60\%
while maintaining original performance and surpassing pruning-based approaches
in extensive experiments. Overall, Dense2MoE establishes a new paradigm for
efficient text-to-image generation.


### [19] [SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding](https://arxiv.org/abs/2510.09110)
*Weikai Huang, Jieyu Zhang, Taoyang Jia, Chenhao Zheng, Ziqi Gao, Jae Sung Park, Ranjay Krishna*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSOSï¼Œä¸€ç§åŸºäºç‰©ä½“ä¸­å¿ƒåˆæˆç­–ç•¥çš„ç®€å•å¯æ‰©å±•æ•°æ®åˆæˆæµæ°´çº¿ï¼Œé€šè¿‡åœ¨çœŸå®å›¾åƒä¸­ç²˜è´´é«˜è´¨é‡åˆæˆç‰©ä½“ç‰‡æ®µå¹¶åº”ç”¨ç»“æ„åŒ–å¸ƒå±€å…ˆéªŒå’Œç”Ÿæˆå¼é‡å…‰ç…§ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹å’Œè§†è§‰å®šä½ä»»åŠ¡çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰åˆ†ç»„ä»»åŠ¡ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œä½†çœŸå®æ•°æ®æ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€è¦†ç›–èŒƒå›´å­˜åœ¨åå·®ä¸”éš¾ä»¥æ‰©å±•ï¼Œè€Œç°æœ‰åˆæˆæ•°æ®æ–¹æ³•ç¼ºä¹çµæ´»æ€§ã€å‡†ç¡®æ€§å’Œç»„åˆå¤šæ ·æ€§ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**Method:** SOSé‡‡ç”¨ç‰©ä½“ä¸­å¿ƒåˆæˆç­–ç•¥ï¼Œå°†é«˜è´¨é‡åˆæˆç‰©ä½“ç‰‡æ®µç²˜è´´åˆ°æ–°å›¾åƒä¸­ï¼Œåˆ©ç”¨ç»“æ„åŒ–å¸ƒå±€å…ˆéªŒæŒ‡å¯¼ç‰©ä½“æ”¾ç½®ï¼Œå¹¶é€šè¿‡ç”Ÿæˆå¼é‡å…‰ç…§æŠ€æœ¯ç¡®ä¿è§†è§‰ä¸€è‡´æ€§ï¼Œä»è€Œç”Ÿæˆå‡†ç¡®çš„æ©ç ã€è¾¹ç•Œæ¡†å’ŒæŒ‡ä»£è¡¨è¾¾å¼æ ‡æ³¨ã€‚

**Result:** åœ¨æ£€æµ‹å’Œè§†è§‰å®šä½ä»»åŠ¡ä¸Šï¼Œä»…ä½¿ç”¨10ä¸‡å¼ SOSåˆæˆå›¾åƒè®­ç»ƒçš„æ¨¡å‹è¶…è¶Šäº†åŸºäºæ›´å¤§è§„æ¨¡çœŸå®æ•°æ®é›†ï¼ˆGRITçš„2000ä¸‡å¼ å’ŒV3Detçš„20ä¸‡å¼ ï¼‰è®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨LVISæ£€æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°+10.9 APæå‡ï¼Œåœ¨gRefCOCOè§†è§‰å®šä½ä»»åŠ¡ä¸Šå®ç°+8.4 NAccæå‡ã€‚

**Conclusion:** SOSå®ç°äº†å¯æ§çš„æ•°æ®é›†æ„å»ºï¼Œåœ¨ä½æ•°æ®é‡å’Œå°é—­è¯æ±‡è¡¨è®¾ç½®ä¸‹å‡èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡å‘LVISå’ŒCOCOæ·»åŠ åˆæˆç‰©ä½“ç‰‡æ®µå¯åœ¨å„ç§çœŸå®æ•°æ®è§„æ¨¡ä¸‹è·å¾—å¼ºåŠ²æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ææœ‰é™çœŸå®æ•°æ®åœºæ™¯ä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºã€‚

---

#### ğŸ“„ Abstract
Visual grouping -- operationalized via instance segmentation, visual
grounding, and object detection -- underpins applications from robotic
perception to photo editing. Large annotated datasets are costly, biased in
coverage, and hard to scale. Synthetic data are promising but often lack
flexibility, accuracy, and compositional diversity.
  We present SOS, a simple and scalable data synthesis pipeline based on an
object-centric composition strategy. It pastes high-quality synthetic object
segments into new images using structured layout priors and generative
relighting, producing accurate and diverse masks, boxes, and referring
expressions. Models trained on 100000 synthetic images from SOS outperform
those trained on larger real-image datasets such as GRIT (20M) and V3Det (200K)
on detection and grounding tasks, achieving +10.9 AP on LVIS detection and +8.4
$N_{\text{Acc}}$ on gRefCOCO grounding. SOS enables controllable dataset
construction and improves generalization in both low-data and closed-vocabulary
settings. Augmenting LVIS and COCO with synthetic object segments yields strong
performance across real-data scales and even larger gains under extremely
limited real data (for example, +3.83 $AP_{\text{rare}}$ on LVIS instance
segmentation and +6.59 AP with a 1 percent COCO setup). This controllability
also supports targeted data generation for challenging intra-class referring in
visual grounding.


### [20] [MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation](https://arxiv.org/abs/2510.09121)
*Dominik Winter, Mai Bui, Monica Azqueta Gavaldon, Nicolas Triltsch, Marco Rosati, Nicolas Brieu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è¯­ä¹‰æ‰©æ•£æ¨¡å‹MSDMï¼Œç”¨äºç”Ÿæˆåƒç´ çº§ç²¾ç¡®çš„ç»†èƒå’Œç»†èƒæ ¸åˆ†å‰²å›¾åƒ-æ©ç å¯¹ï¼Œé€šè¿‡æ•´åˆå½¢æ€å­¦ã€é¢œè‰²ç‰¹å¾å’Œå…ƒæ•°æ®ç­‰å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†åˆ†å‰²æ¨¡å‹åœ¨ç¨€æœ‰ç»†èƒç±»å‹ä¸Šçš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è®¡ç®—ç—…ç†å­¦ä¸­æ ‡æ³¨æ•°æ®ç¨€ç¼ºï¼Œç‰¹åˆ«æ˜¯å¯¹äºç½•è§æˆ–éå…¸å‹å½¢æ€çš„ç»†èƒå’Œç»†èƒæ ¸ï¼Œæ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œè€Œåˆæˆæ•°æ®æä¾›äº†ä¸€ç§ç»æµæœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚

**Method:** æå‡ºå¤šæ¨¡æ€è¯­ä¹‰æ‰©æ•£æ¨¡å‹MSDMï¼Œé€šè¿‡æ•´åˆç»†èƒ/ç»†èƒæ ¸å½¢æ€å­¦ç‰¹å¾ï¼ˆä½¿ç”¨æ°´å¹³å’Œå‚ç›´æ˜ å°„ï¼‰ã€RGBé¢œè‰²ç‰¹æ€§ä»¥åŠBERTç¼–ç çš„æ£€æµ‹/æŒ‡ç¤ºå…ƒæ•°æ®ï¼Œåˆ©ç”¨å¤šå¤´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶èåˆè¿™äº›å¼‚è´¨æ¨¡æ€ï¼Œå®ç°å¯¹ç”Ÿæˆå›¾åƒçš„ç»†ç²’åº¦æ§åˆ¶ã€‚

**Result:** å®šé‡åˆ†æè¡¨æ˜åˆæˆå›¾åƒä¸çœŸå®æ•°æ®é«˜åº¦åŒ¹é…ï¼Œåœ¨åŒ¹é…ç”Ÿç‰©æ¡ä»¶ä¸‹ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒçš„åµŒå…¥ä¹‹é—´å…·æœ‰è¾ƒä½çš„Wassersteinè·ç¦»ï¼Œç‰¹åˆ«æ˜¯åœ¨æŸ±çŠ¶ç»†èƒç­‰ç¨€æœ‰ç±»å‹ä¸Šï¼Œåˆæˆæ ·æœ¬çš„åŠ å…¥æ˜¾è‘—æå‡äº†åˆ†å‰²æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

**Conclusion:** å¤šæ¨¡æ€æ‰©æ•£å¢å¼ºç­–ç•¥èƒ½å¤Ÿç³»ç»Ÿæ€§åœ°ä¸°å¯Œæ•°æ®é›†ï¼Œç›´æ¥é’ˆå¯¹æ¨¡å‹ç¼ºé™·è¿›è¡Œæ”¹è¿›ï¼Œä¸ºæå‡ç»†èƒå’Œç»†èƒæ ¸åˆ†å‰²æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œæ¨åŠ¨äº†ç”Ÿæˆæ¨¡å‹åœ¨è®¡ç®—ç—…ç†å­¦ä¸­çš„æ›´å¹¿æ³›åº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Scarcity of annotated data, particularly for rare or atypical morphologies,
present significant challenges for cell and nuclei segmentation in
computational pathology. While manual annotation is labor-intensive and costly,
synthetic data offers a cost-effective alternative. We introduce a Multimodal
Semantic Diffusion Model (MSDM) for generating realistic pixel-precise
image-mask pairs for cell and nuclei segmentation. By conditioning the
generative process with cellular/nuclear morphologies (using horizontal and
vertical maps), RGB color characteristics, and BERT-encoded assay/indication
metadata, MSDM generates datasests with desired morphological properties. These
heterogeneous modalities are integrated via multi-head cross-attention,
enabling fine-grained control over the generated images. Quantitative analysis
demonstrates that synthetic images closely match real data, with low
Wasserstein distances between embeddings of generated and real images under
matching biological conditions. The incorporation of these synthetic samples,
exemplified by columnar cells, significantly improves segmentation model
accuracy on columnar cells. This strategy systematically enriches data sets,
directly targeting model deficiencies. We highlight the effectiveness of
multimodal diffusion-based augmentation for advancing the robustness and
generalizability of cell and nuclei segmentation models. Thereby, we pave the
way for broader application of generative models in computational pathology.


### [21] [Towards Safer and Understandable Driver Intention Prediction](https://arxiv.org/abs/2510.09200)
*Mukilan Karuppasamy, Shankar Gangisetty, Shyam Nandan Rai, Carlo Masone, C V Jawahar*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†å¯è§£é‡Šçš„é©¾é©¶å‘˜æ„å›¾é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶å¼€å‘äº†VCBMæ¡†æ¶å’ŒDAAD-Xæ•°æ®é›†ï¼Œé€šè¿‡æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ç”Ÿæˆæ—¶ç©ºä¸€è‡´çš„è§£é‡Šï¼Œè¯æ˜äº†Transformeræ¨¡å‹æ¯”ä¼ ç»ŸCNNæ¨¡å‹å…·æœ‰æ›´å¥½çš„å¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸äººç±»äº¤äº’çš„å¢åŠ ï¼Œé©¾é©¶ç³»ç»Ÿå†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§å¯¹äºç¡®ä¿å®‰å…¨é©¾é©¶æ“ä½œå˜å¾—è‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ ç³»ç»Ÿåœ¨ç†è§£ç¯å¢ƒè¡¨ç¤ºå’Œé©¾é©¶ä»»åŠ¡æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³é©¾é©¶å‘˜æ„å›¾é¢„æµ‹çš„å¯è§£é‡Šæ€§é—®é¢˜ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†DAAD-Xå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæä¾›é©¾é©¶å‘˜å†³ç­–çš„å±‚æ¬¡åŒ–æ–‡æœ¬è§£é‡Šï¼Œå¹¶æå‡ºäº†è§†é¢‘æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆVCBMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå›ºæœ‰åœ°ç”Ÿæˆæ—¶ç©ºä¸€è‡´çš„è§£é‡Šï¼Œè€Œä¸ä¾èµ–åå¤„ç†æŠ€æœ¯ã€‚

**Result:** åœ¨DAAD-Xæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºTransformerçš„æ¨¡å‹æ¯”ä¼ ç»ŸCNNæ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„å¯è§£é‡Šæ€§ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†å¤šæ ‡ç­¾t-SNEå¯è§†åŒ–æŠ€æœ¯æ¥å±•ç¤ºå¤šä¸ªè§£é‡Šä¹‹é—´çš„è§£ç¼ å’Œå› æœç›¸å…³æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¯è§£é‡Šçš„é©¾é©¶å‘˜æ„å›¾é¢„æµ‹æä¾›äº†æ–°çš„æ•°æ®é›†å’Œæ¡†æ¶ï¼Œè¯æ˜äº†Transformeræ¶æ„åœ¨å¯è§£é‡Šæ€§æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸ºç†è§£é©¾é©¶å†³ç­–çš„å› æœæœºåˆ¶æä¾›äº†å¯è§†åŒ–å·¥å…·ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå®‰å…¨æ€§å’Œå¯ä¿¡åº¦çš„æå‡ã€‚

---

#### ğŸ“„ Abstract
Autonomous driving (AD) systems are becoming increasingly capable of handling
complex tasks, mainly due to recent advances in deep learning and AI. As
interactions between autonomous systems and humans increase, the
interpretability of decision-making processes in driving systems becomes
increasingly crucial for ensuring safe driving operations. Successful
human-machine interaction requires understanding the underlying representations
of the environment and the driving task, which remains a significant challenge
in deep learning-based systems. To address this, we introduce the task of
interpretability in maneuver prediction before they occur for driver safety,
i.e., driver intent prediction (DIP), which plays a critical role in AD
systems. To foster research in interpretable DIP, we curate the eXplainable
Driving Action Anticipation Dataset (DAAD-X), a new multimodal, ego-centric
video dataset to provide hierarchical, high-level textual explanations as
causal reasoning for the driver's decisions. These explanations are derived
from both the driver's eye-gaze and the ego-vehicle's perspective. Next, we
propose Video Concept Bottleneck Model (VCBM), a framework that generates
spatio-temporally coherent explanations inherently, without relying on post-hoc
techniques. Finally, through extensive evaluations of the proposed VCBM on the
DAAD-X dataset, we demonstrate that transformer-based models exhibit greater
interpretability than conventional CNN-based models. Additionally, we introduce
a multilabel t-SNE visualization technique to illustrate the disentanglement
and causal correlation among multiple explanations. Our data, code and models
are available at: https://mukil07.github.io/VCBM.github.io/


### [22] [Cattle-CLIP: A Multimodal Framework for Cattle Behaviour Recognition](https://arxiv.org/abs/2510.09203)
*Huimin Liu, Jing Gao, Daria Baran, AxelX Montout, Neill W Campbell, Andrew W Dowsey*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Cattle-CLIPï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶çš„ç‰›åªè¡Œä¸ºè¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰çº¿ç´¢å¢å¼ºè§†é¢‘ç‰¹å¾è¯†åˆ«æ€§èƒ½ï¼Œåœ¨ç›‘ç£å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºè‰²ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºè§†é¢‘çš„ç‰›åªè¡Œä¸ºç›‘æµ‹è™½ç„¶èƒ½æä¾›é«˜ç²¾åº¦è¯†åˆ«ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºçš„è¡Œä¸ºè¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®å†œåœºç›‘æ§è§†é¢‘ä¸é¢„è®­ç»ƒæ¨¡å‹ç½‘ç»œæ•°æ®ä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·çš„é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†è§£å†³ã€‚

**Method:** è¯¥æ–¹æ³•åŸºäºå¤§è§„æ¨¡å›¾åƒ-è¯­è¨€æ¨¡å‹CLIPè¿›è¡Œé€‚é…ï¼Œæ·»åŠ äº†æ—¶é—´æ•´åˆæ¨¡å—æ¥å¤„ç†è§†é¢‘æ—¶åºä¿¡æ¯ï¼Œå¹¶é’ˆå¯¹é¢†åŸŸå·®è·é—®é¢˜å¼•å…¥äº†å®šåˆ¶åŒ–æ•°æ®å¢å¼ºç­–ç•¥å’Œä¸“é—¨è®¾è®¡çš„æ–‡æœ¬æç¤ºè¯ã€‚

**Result:** åœ¨å®Œå…¨ç›‘ç£è®¾ç½®ä¸‹ï¼ŒCattle-CLIPåœ¨å…­ä¸ªè¡Œä¸ºç±»åˆ«ä¸Šè¾¾åˆ°96.1%çš„æ€»ä½“å‡†ç¡®ç‡ï¼Œå…¶ä¸­è¿›é£Ÿã€é¥®æ°´å’Œç«™ç«‹ååˆè¡Œä¸ºçš„å¬å›ç‡æ¥è¿‘100%ï¼Œå¹¶åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹å±•ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€å­¦ä¹ åœ¨å†œä¸šå’ŒåŠ¨ç‰©è¡Œä¸ºåˆ†æä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹ä»èƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œä¸ºç²¾å‡†ç•œç‰§ä¸šç›‘æµ‹æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Cattle behaviour is a crucial indicator of an individual animal health,
productivity and overall well-being. Video-based monitoring, combined with deep
learning techniques, has become a mainstream approach in animal biometrics, and
it can offer high accuracy in some behaviour recognition tasks. We present
Cattle-CLIP, a multimodal deep learning framework for cattle behaviour
recognition, using semantic cues to improve the performance of video-based
visual feature recognition. It is adapted from the large-scale image-language
model CLIP by adding a temporal integration module. To address the domain gap
between web data used for the pre-trained model and real-world cattle
surveillance footage, we introduce tailored data augmentation strategies and
specialised text prompts. Cattle-CLIP is evaluated under both fully-supervised
and few-shot learning scenarios, with a particular focus on data-scarce
behaviour recognition - an important yet under-explored goal in livestock
monitoring. To evaluate the proposed method, we release the CattleBehaviours6
dataset, which comprises six types of indoor behaviours: feeding, drinking,
standing-self-grooming, standing-ruminating, lying-self-grooming and
lying-ruminating. The dataset consists of 1905 clips collected from our John
Oldacre Centre dairy farm research platform housing 200 Holstein-Friesian cows.
Experiments show that Cattle-CLIP achieves 96.1% overall accuracy across six
behaviours in a supervised setting, with nearly 100% recall for feeding,
drinking and standing-ruminating behaviours, and demonstrates robust
generalisation with limited data in few-shot scenarios, highlighting the
potential of multimodal learning in agricultural and animal behaviour analysis.


### [23] [Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras](https://arxiv.org/abs/2510.09230)
*Jindong Hong, Wencheng Zhang, Shiqin Qiao, Jianhai Chen, Jianing Qiu, Chuanyang Zheng, Qian Xu, Yun Ji, Qianyue Wen, Weiwei Sun, Hao Li, Huizhen Li, Huichao Wang, Kai Wu, Meng Li, Yijun He, Lingjie Luo, Jiankai Sun*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†HMVDxæ··åˆè¿åŠ¨è§†é¢‘è¯Šæ–­æ¡†æ¶ï¼Œé€šè¿‡å°†åŠ¨ä½œç†è§£ä¸ç–¾ç—…è¯Šæ–­ä»»åŠ¡åˆ†ç¦»å¹¶ç”±ä¸¤ä¸ªMLLMåˆ†åˆ«å®Œæˆï¼Œæ˜¾è‘—æå‡äº†è‚©å…³èŠ‚éšœç¢çš„åˆæ­¥è¯Šæ–­å‡†ç¡®ç‡ï¼Œä¸ºåŒ»ç–—èµ„æºåŒ®ä¹åœ°åŒºæä¾›äº†ä½æˆæœ¬å¯æ‰©å±•çš„è¾…åŠ©è¯Šæ–­è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è‚©å…³èŠ‚éšœç¢å¦‚å†»ç»“è‚©æ˜¯å…¨çƒå¸¸è§ç–¾ç—…ï¼Œåœ¨åŒ»ç–—èµ„æºç¨€ç¼ºåœ°åŒºå®ç°æ—©æœŸå‡†ç¡®è¯Šæ–­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè¿«åˆ‡éœ€è¦ä½æˆæœ¬ä¸”æ˜“äºæ‰©å±•çš„è¾…åŠ©è¯Šæ–­è§£å†³æ–¹æ¡ˆï¼Œæœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨æ¶ˆè´¹çº§è®¾å¤‡æ‹æ‘„çš„è§†é¢‘ä½œä¸ºè¯Šæ–­åŸºç¡€ä»¥é™ä½ç”¨æˆ·æˆæœ¬ã€‚

**Method:** æå‡ºäº†HMVDxæ··åˆè¿åŠ¨è§†é¢‘è¯Šæ–­æ¡†æ¶ï¼Œå°†åŠ¨ä½œç†è§£å’Œç–¾ç—…è¯Šæ–­ä¸¤ä¸ªä»»åŠ¡åˆ†ç¦»ï¼Œåˆ†åˆ«ç”±ä¸¤ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®Œæˆï¼Œå¹¶æå‡ºäº†åŸºäºåŒ»ç–—å†³ç­–é€»è¾‘è¿‡ç¨‹çš„æ–°å‹è¯„ä¼°æŒ‡æ ‡â€”â€”å¯ç”¨æ€§æŒ‡æ•°ï¼Œä»å®Œæ•´åŒ»ç–—è¯Šæ–­è·¯å¾„è§’åº¦è¯„ä¼°MLLMåœ¨åŒ»ç–—é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚

**Result:** åœ¨å®éªŒæ¯”è¾ƒä¸­ï¼ŒHMVDxåœ¨è¯Šæ–­è‚©å…³èŠ‚æŸä¼¤æ–¹é¢çš„å‡†ç¡®ç‡ç›¸æ¯”ç›´æ¥è§†é¢‘è¯Šæ–­æé«˜äº†79.6%ï¼Œæ˜¾è‘—æå‡äº†è¯Šæ–­æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨åŒ»ç–—è§†é¢‘ç†è§£åº”ç”¨ä¸­çš„æŠ€æœ¯è´¡çŒ®ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†ä½æˆæœ¬MLLMåœ¨åŒ»ç–—åº”ç”¨ä¸­ä¸ºåŒ»ç–—ä»ä¸šè€…å¸¦æ¥çš„æ½œåœ¨ä»·å€¼ï¼Œä¸ºæœªæ¥MLLMåœ¨åŒ»ç–—é¢†åŸŸè§†é¢‘ç†è§£åº”ç”¨ç ”ç©¶æä¾›äº†é‡è¦çš„æŠ€æœ¯è´¡çŒ®å’Œæ–¹å‘æŒ‡å¼•ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„è¾…åŠ©è¯Šæ–­æ–¹æ¡ˆå¼€å‘ã€‚

---

#### ğŸ“„ Abstract
Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),
are common conditions affecting the health of people worldwide, and have a high
incidence rate among the elderly and workers engaged in repetitive shoulder
tasks. In regions with scarce medical resources, achieving early and accurate
diagnosis poses significant challenges, and there is an urgent need for
low-cost and easily scalable auxiliary diagnostic solutions. This research
introduces videos captured by consumer-grade devices as the basis for
diagnosis, reducing the cost for users. We focus on the innovative application
of Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of
shoulder disorders and propose a Hybrid Motion Video Diagnosis framework
(HMVDx). This framework divides the two tasks of action understanding and
disease diagnosis, which are respectively completed by two MLLMs. In addition
to traditional evaluation indicators, this work proposes a novel metric called
Usability Index by the logical process of medical decision-making (action
recognition, movement diagnosis, and final diagnosis). This index evaluates the
effectiveness of MLLMs in the medical field from the perspective of the entire
medical diagnostic pathway, revealing the potential value of low-cost MLLMs in
medical applications for medical practitioners. In experimental comparisons,
the accuracy of HMVDx in diagnosing shoulder joint injuries has increased by
79.6\% compared with direct video diagnosis, a significant technical
contribution to future research on the application of MLLMs for video
understanding in the medical field.


### [24] [Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation](https://arxiv.org/abs/2510.09228)
*Vijay M. Galshetwar, Praful Hambarde, Prashant W. Patil, Akshay Dudhane, Sachin Chaudhary, Santosh Kumar Vipparathi, Subrahmanyam Murala*

#### ğŸ§© TL;DR
æœ¬è°ƒæŸ¥è®ºæ–‡å¯¹å›¾åƒå’Œè§†é¢‘å¤©æ°”é€€åŒ–æ¢å¤æŠ€æœ¯è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œç³»ç»Ÿåˆ†ç±»äº†ä¼ ç»Ÿå…ˆéªŒæ–¹æ³•å’Œç°ä»£æ•°æ®é©±åŠ¨æ¨¡å‹ï¼Œå¹¶è®¨è®ºäº†æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­å¤©æ°”å¼¹æ€§è§†è§‰ç³»ç»Ÿçš„æœªæ¥å‘å±•æ–¹å‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é›¾éœ¾ã€é›¨é›ªç­‰æ¶åŠ£å¤©æ°”æ¡ä»¶ä¼šæ˜¾è‘—é™ä½å›¾åƒå’Œè§†é¢‘è´¨é‡ï¼Œå¯¹ä¾èµ–è§†è§‰è¾“å…¥çš„æ™ºèƒ½äº¤é€šç³»ç»Ÿï¼ˆåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€äº¤é€šç›‘æ§å’Œå®‰é˜²åº”ç”¨ï¼‰æ„æˆä¸¥é‡æŒ‘æˆ˜ï¼Œéœ€è¦å¼€å‘æœ‰æ•ˆçš„æ¢å¤æŠ€æœ¯æ¥ç¼“è§£å¤©æ°”å¼•èµ·çš„è§†è§‰æŸä¼¤ã€‚

**Method:** å°†ç°æœ‰æ–¹æ³•åˆ†ç±»ä¸ºä¼ ç»Ÿå…ˆéªŒæ–¹æ³•å’Œç°ä»£æ•°æ®é©±åŠ¨æ¨¡å‹ï¼ŒåŒ…æ‹¬CNNã€Transformerã€æ‰©æ•£æ¨¡å‹å’Œæ–°å…´çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼›æ¢å¤ç­–ç•¥è¿›ä¸€æ­¥æŒ‰èŒƒå›´åˆ†ä¸ºå•ä»»åŠ¡æ¨¡å‹ã€å¤šä»»åŠ¡/å¤šå¤©æ°”ç³»ç»Ÿä»¥åŠèƒ½å¤Ÿå¤„ç†å¤šç§é€€åŒ–çš„å…¨èƒ½æ¡†æ¶ã€‚

**Result:** è°ƒæŸ¥æ¶µç›–äº†æ—¥é—´å’Œå¤œé—´æ¢å¤æŒ‘æˆ˜ã€åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œç³»ç»Ÿåˆ†æäº†ä¸åŒæ–¹æ³•åœ¨åº”å¯¹å¤©æ°”é€€åŒ–æ–¹é¢çš„æ€§èƒ½è¡¨ç°å’ŒæŠ€æœ¯ç‰¹ç‚¹ã€‚

**Conclusion:** å½“å‰ç ”ç©¶å­˜åœ¨æ··åˆ/å¤åˆé€€åŒ–æ¢å¤ã€å®æ—¶éƒ¨ç½²å’Œæ™ºèƒ½AIæ¡†æ¶ç­‰å±€é™æ€§ï¼Œæœªæ¥æ–¹å‘åŒ…æ‹¬å¼€å‘æ›´å¼ºå¤§çš„æ¢å¤æ¨¡å‹ã€å®ç°å®æ—¶åº”ç”¨ä»¥åŠæ„å»ºæ™ºèƒ½äº¤é€šç¯å¢ƒä¸­çš„å¤©æ°”å¼¹æ€§è§†è§‰ç³»ç»Ÿå‚è€ƒæ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
Adverse weather conditions such as haze, rain, and snow significantly degrade
the quality of images and videos, posing serious challenges to intelligent
transportation systems (ITS) that rely on visual input. These degradations
affect critical applications including autonomous driving, traffic monitoring,
and surveillance. This survey presents a comprehensive review of image and
video restoration techniques developed to mitigate weather-induced visual
impairments. We categorize existing approaches into traditional prior-based
methods and modern data-driven models, including CNNs, transformers, diffusion
models, and emerging vision-language models (VLMs). Restoration strategies are
further classified based on their scope: single-task models,
multi-task/multi-weather systems, and all-in-one frameworks capable of handling
diverse degradations. In addition, we discuss day and night time restoration
challenges, benchmark datasets, and evaluation protocols. The survey concludes
with an in-depth discussion on limitations in current research and outlines
future directions such as mixed/compound-degradation restoration, real-time
deployment, and agentic AI frameworks. This work aims to serve as a valuable
reference for advancing weather-resilient vision systems in smart
transportation environments. Lastly, to stay current with rapid advancements in
this field, we will maintain regular updates of the latest relevant papers and
their open-source implementations at
https://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration


### [25] [CapGeo: A Caption-Assisted Approach to Geometric Reasoning](https://arxiv.org/abs/2510.09302)
*Yuying Li, Siyi Qian, Hao Liang, Leqi Zheng, Ruichuan An, Yongzhen Guo, Wentao Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CapGeoæ¡†æ¶ï¼Œé€šè¿‡å°†å‡ ä½•å›¾å½¢è½¬æ¢ä¸ºæ–‡æœ¬æè¿°æ¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‡ ä½•æ¨ç†èƒ½åŠ›ï¼Œå¹¶å»ºç«‹äº†CapGeo-BenchåŸºå‡†æ•°æ®é›†ç”¨äºç³»ç»Ÿè¯„ä¼°å‡ ä½•æ ‡æ³¨æ¨¡å‹çš„è´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å‡ ä½•æ¨ç†æ–¹é¢ä»ç„¶å­˜åœ¨æ˜¾è‘—ç“¶é¢ˆï¼Œå³ä½¿å¦‚GPT-4oå’ŒGemini-2.5-Proç­‰é—­æºç³»ç»Ÿåœ¨è§£å†³å‡ ä½•é—®é¢˜æ—¶ä¹Ÿè¡¨ç°ä¸ä½³ï¼Œè¿™è¡¨æ˜é—®é¢˜çš„æ ¸å¿ƒåœ¨äºå¯¹å‡ ä½•å›¾å½¢çš„ç†è§£è€Œéæ¨ç†èƒ½åŠ›æœ¬èº«ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†CapGeoæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ ‡æ³¨è¾…åŠ©çš„æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡å°†è§†è§‰å†…å®¹è½¬æ¢ä¸ºç®€æ´çš„æ–‡æœ¬æè¿°æ¥æ¡¥æ¥è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œå¹¶å¼€å‘äº†åŒ…å«4,641ä¸ªç²¾å¿ƒç­›é€‰çš„å›¾å½¢-æ ‡æ³¨å¯¹çš„CapGeo-Benchæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åŸºäºå…³é”®ç‚¹çš„è¯„ä¼°æŒ‡æ ‡ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼Œé…å¤‡æ ‡æ³¨åæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼šQwen2.5-VL-72Bä»ä»…è§†è§‰çš„8.6%æå‡è‡³59.0%ï¼ŒClaude-Opus-4ä»44.8%æå‡è‡³73.0%ï¼ŒåŒæ—¶CapGeo-Benchä¸­çš„å…³é”®ç‚¹è¯„ä¼°æŒ‡æ ‡ä¸ä¸‹æ¸¸CapGeoæ€§èƒ½é«˜åº¦ç›¸å…³ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†é€šè¿‡è§†è§‰åˆ°æ–‡æœ¬è½¬æ¢æå‡å‡ ä½•æ¨ç†èƒ½åŠ›çš„æ–°é€”å¾„ï¼Œå¼ºè°ƒäº†å‡ ä½•æ ‡æ³¨è´¨é‡å¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å…³é”®å½±å“ï¼Œä¸ºæ¨è¿›å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‡ ä½•æ¨ç†èƒ½åŠ›æä¾›äº†ç³»ç»Ÿæ€§çš„è¯„ä¼°æ¡†æ¶å’Œå‘å±•æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Geometric reasoning remains a core challenge for Multimodal Large Language
Models (MLLMs). Even the most advanced closed-source systems, such as GPT-O3
and Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite
exhibiting strong textual reasoning abilities on tasks like the International
Mathematical Olympiad (IMO). This gap suggests that the bottleneck lies in
understanding geometric diagrams rather than reasoning itself. Since geometric
figures can often be faithfully described in concise textual form, converting
visual content into captions offers a promising direction. Motivated by this
insight, we introduce CapGeo, a caption-assisted reasoning framework that
bridges visual and textual modalities. Experiments show substantial
improvements when models are equipped with captions: Qwen2.5-VL-72B improves
from 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to
73.0%. To systematically evaluate and identify high-quality geometric
captioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated
figure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based
evaluation metric that correlates strongly with downstream CapGeo performance,
enabling reliable assessment of geometric captioning ability. Together, our
framework and benchmark highlight a new pathway toward advancing geometric
reasoning in MLLMs.


### [26] [StreamingVLM: Real-Time Understanding for Infinite Video Streams](https://arxiv.org/abs/2510.09608)
*Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, Song Han*

#### ğŸ§© TL;DR
StreamingVLMæ˜¯ä¸€ä¸ªä¸“ä¸ºå®æ—¶ç†è§£æ— é™è§†è§‰è¾“å…¥è€Œè®¾è®¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„è®­ç»ƒæ¨ç†å¯¹é½æ¡†æ¶å’Œç´§å‡‘KVç¼“å­˜æœºåˆ¶ï¼Œè§£å†³äº†é•¿è§†é¢‘å¤„ç†ä¸­çš„è®¡ç®—å¤æ‚åº¦å’Œå»¶è¿Ÿé—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ— é™è§†é¢‘æµæ—¶é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼šå®Œæ•´æ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´äºŒæ¬¡è®¡ç®—æˆæœ¬å’Œé«˜å†…å­˜ä½¿ç”¨ï¼Œè€Œç®€å•æ»‘åŠ¨çª—å£æ–¹æ³•ä¼šç ´åè¿è´¯æ€§æˆ–äº§ç”Ÿé«˜å»¶è¿Ÿå†—ä½™è®¡ç®—ï¼Œæ— æ³•æ»¡è¶³å®æ—¶åŠ©æ‰‹å’Œè‡ªä¸»ä»£ç†çš„éœ€æ±‚ã€‚

**Method:** æå‡ºç»Ÿä¸€è®­ç»ƒæ¨ç†å¯¹é½æ¡†æ¶ï¼Œåœ¨æ¨ç†æ—¶é€šè¿‡é‡ç”¨æ³¨æ„åŠ›æ±‡èšç‚¹çŠ¶æ€ã€è¿‘æœŸè§†è§‰æ ‡è®°çŸ­çª—å£å’Œæ–‡æœ¬æ ‡è®°é•¿çª—å£æ¥ç»´æŠ¤ç´§å‡‘KVç¼“å­˜ï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒç­–ç•¥åœ¨çŸ­é‡å è§†é¢‘å—ä¸Šåº”ç”¨å®Œæ•´æ³¨æ„åŠ›ï¼Œæœ‰æ•ˆæ¨¡æ‹Ÿæ¨ç†æ—¶æ³¨æ„åŠ›æ¨¡å¼è€Œæ— éœ€è®­ç»ƒè¶…é•¿ä¸Šä¸‹æ–‡ã€‚

**Result:** åœ¨æ–°å»ºçš„Inf-Streams-EvalåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°66.18%èƒœç‡å¯¹æŠ—GPT-4O miniï¼Œåœ¨å•å¼ NVIDIA H100ä¸Šä¿æŒç¨³å®šå®æ—¶æ€§èƒ½è¾¾8 FPSï¼Œç›‘ç£å¾®è°ƒç­–ç•¥è¿˜æå‡äº†é€šç”¨VQAèƒ½åŠ›ï¼Œåœ¨LongVideoBenchå’ŒOVOBench Realtimeä¸Šåˆ†åˆ«æå‡+4.30å’Œ+5.96ã€‚

**Conclusion:** StreamingVLMè¯æ˜äº†é€šè¿‡è®­ç»ƒæ¨ç†å¯¹é½å’Œç´§å‡‘ç¼“å­˜æœºåˆ¶å¯å®ç°æ— é™è§†é¢‘æµçš„å®æ—¶ç¨³å®šç†è§£ï¼Œå…¶ç›‘ç£å¾®è°ƒç­–ç•¥ä¸ä»…èƒ½æå‡æµå¼å¤„ç†èƒ½åŠ›ï¼Œè¿˜èƒ½å¢å¼ºé€šç”¨è§†è§‰é—®ç­”æ€§èƒ½ï¼Œä¸ºå®æ—¶è§†è§‰è¯­è¨€åº”ç”¨æä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) could power real-time assistants and autonomous
agents, but they face a critical challenge: understanding near-infinite video
streams without escalating latency and memory usage. Processing entire videos
with full attention leads to quadratic computational costs and poor performance
on long videos. Meanwhile, simple sliding window methods are also flawed, as
they either break coherence or suffer from high latency due to redundant
recomputation. In this paper, we introduce StreamingVLM, a model designed for
real-time, stable understanding of infinite visual input. Our approach is a
unified framework that aligns training with streaming inference. During
inference, we maintain a compact KV cache by reusing states of attention sinks,
a short window of recent vision tokens, and a long window of recent text
tokens. This streaming ability is instilled via a simple supervised fine-tuning
(SFT) strategy that applies full attention on short, overlapped video chunks,
which effectively mimics the inference-time attention pattern without training
on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a
new benchmark with videos averaging over two hours that requires dense,
per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM
achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time
performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy
also enhances general VQA abilities without any VQA-specific fine-tuning,
improving performance on LongVideoBench by +4.30 and OVOBench Realtime by
+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.


### [27] [Zero-shot image privacy classification with Vision-Language Models](https://arxiv.org/abs/2510.09253)
*Alina Elena Baia, Alessio Xompero, Andrea Cavallaro*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å»ºç«‹äº†å›¾åƒéšç§åˆ†ç±»çš„é›¶æ ·æœ¬åŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸ä¸“ç”¨æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”ã€‚ç ”ç©¶å‘ç°å°½ç®¡VLMså‚æ•°è§„æ¨¡åºå¤§ä¸”æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œä½†åœ¨éšç§é¢„æµ‹å‡†ç¡®æ€§ä¸Šä»è½åäºä¸“ç”¨çš„å°å‹æ¨¡å‹ï¼Œä½†åœ¨å›¾åƒæ‰°åŠ¨é²æ£’æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡çŒ®è¶Šæ¥è¶Šå€¾å‘äºé‡‡ç”¨ä¸ºé€šç”¨ä»»åŠ¡è®¾è®¡çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè€Œå¿½è§†äº†ä¸“ç”¨æ¨¡å‹å¯èƒ½è¾¾åˆ°çš„æ€§èƒ½ä¸Šé™ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹ç³»ç»Ÿæ€§çš„è¯„ä¼°æ¡†æ¶ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå»ºç«‹å…¬å¹³çš„æ¯”è¾ƒåŸºå‡†æ¥è¯„ä¼°VLMsåœ¨å›¾åƒéšç§é¢„æµ‹ä»»åŠ¡ä¸­çš„å®é™…è¡¨ç°ã€‚

**Method:** ç ”ç©¶å»ºç«‹äº†å›¾åƒéšç§åˆ†ç±»çš„é›¶æ ·æœ¬åŸºå‡†ï¼Œè¯„ä¼°äº†æ ¹æ®éšç§åŸºå‡†æ’åå‰ä¸‰çš„å¼€æºVLMsï¼Œä½¿ç”¨ä»»åŠ¡å¯¹é½çš„æç¤ºè¯ï¼Œå¹¶å°†å®ƒä»¬çš„æ€§èƒ½ã€æ•ˆç‡å’Œé²æ£’æ€§ä¸å·²å»ºç«‹çš„çº¯è§†è§‰å’Œå¤šæ¨¡æ€æ–¹æ³•è¿›è¡Œå¯¹æ¯”åˆ†æã€‚

**Result:** ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡VLMså…·æœ‰èµ„æºå¯†é›†çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬é«˜å‚æ•°æ•°é‡å’Œè¾ƒæ…¢çš„æ¨ç†é€Ÿåº¦ï¼Œä½†åœ¨éšç§é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢ç›®å‰ä»è½åäºä¸“ç”¨çš„å°å‹æ¨¡å‹ã€‚åŒæ—¶å‘ç°VLMså¯¹å›¾åƒæ‰°åŠ¨è¡¨ç°å‡ºæ›´é«˜çš„é²æ£’æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†åœ¨å›¾åƒéšç§é¢„æµ‹è¿™ä¸€ç‰¹å®šä»»åŠ¡ä¸­ï¼Œä¸“ç”¨æ¨¡å‹ä»ç„¶å…·æœ‰ä¸å¯æ›¿ä»£çš„ä¼˜åŠ¿ï¼Œè€ŒVLMsè™½ç„¶åœ¨æŸäº›æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä¸ºæœªæ¥æ¨¡å‹é€‰æ‹©å’Œå‘å±•æ–¹å‘æä¾›äº†é‡è¦å‚è€ƒï¼Œå¼ºè°ƒäº†ä»»åŠ¡ä¸“ç”¨æ€§ä¸é€šç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚

---

#### ğŸ“„ Abstract
While specialized learning-based models have historically dominated image
privacy prediction, the current literature increasingly favours adopting large
Vision-Language Models (VLMs) designed for generic tasks. This trend risks
overlooking the performance ceiling set by purpose-built models due to a lack
of systematic evaluation. To address this problem, we establish a zero-shot
benchmark for image privacy classification, enabling a fair comparison. We
evaluate the top-3 open-source VLMs, according to a privacy benchmark, using
task-aligned prompts and we contrast their performance, efficiency, and
robustness against established vision-only and multi-modal methods.
Counter-intuitively, our results show that VLMs, despite their
resource-intensive nature in terms of high parameter count and slower
inference, currently lag behind specialized, smaller models in privacy
prediction accuracy. We also find that VLMs exhibit higher robustness to image
perturbations.


### [28] [Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy](https://arxiv.org/abs/2510.09256)
*Patrick Wienholt, Sophie Caselitz, Robert Siepmann, Philipp Bruners, Keno Bressem, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºä½¿ç”¨ç¦»æ•£è¯­ä¹‰ç†µæ¥æ£€æµ‹å’Œè¿‡æ»¤å¯èƒ½äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†é»‘ç›’è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ”¾å°„å­¦è§†è§‰é—®ç­”ä»»åŠ¡ä¸­çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡åŒ–è¯­ä¹‰ä¸ä¸€è‡´æ€§å®ç°äº†å¯é çš„å¹»è§‰æ£€æµ‹ï¼Œä¸ºä¸´åºŠVLMåº”ç”¨æä¾›äº†æœ‰æ•ˆçš„è¿‡æ»¤ç­–ç•¥ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³é»‘ç›’è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ”¾å°„å­¦å›¾åƒè§†è§‰é—®ç­”ä»»åŠ¡ä¸­å®¹æ˜“äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ï¼Œé€šè¿‡å¼€å‘å¯é çš„å¹»è§‰æ£€æµ‹æ–¹æ³•æ¥æé«˜æ¨¡å‹åœ¨ä¸´åºŠè¯Šæ–­åº”ç”¨ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç¦»æ•£è¯­ä¹‰ç†µæ–¹æ³•ï¼Œé€šè¿‡åŒå‘è•´å«æ£€æŸ¥å°†æ„ä¹‰ç­‰æ•ˆçš„å›ç­”åˆ†ç»„ï¼Œå¹¶ä»ç”Ÿæˆçš„è¯­ä¹‰ç°‡çš„ç›¸å¯¹é¢‘ç‡è®¡ç®—DSEã€‚ä½¿ç”¨GPT-4oå’ŒGPT-4.1æ¨¡å‹åœ¨æ¸©åº¦1.0ä¸‹å¯¹æ¯ä¸ªé—®é¢˜å›ç­”15æ¬¡ï¼Œå¹¶é€šè¿‡æ’é™¤DSEé«˜äº0.6æˆ–0.3çš„é—®é¢˜æ¥é‡æ–°è®¡ç®—å‡†ç¡®ç‡ã€‚

**Result:** åœ¨706ä¸ªå›¾åƒ-é—®é¢˜å¯¹ä¸­ï¼ŒåŸºçº¿å‡†ç¡®ç‡ä¸ºGPT-4oçš„51.7%å’ŒGPT-4.1çš„54.8%ã€‚è¿‡æ»¤æ‰é«˜ç†µé—®é¢˜åï¼ŒGPT-4oåœ¨å‰©ä½™334ä¸ªé—®é¢˜ä¸Šçš„å‡†ç¡®ç‡æå‡è‡³76.3%ï¼ŒGPT-4.1åœ¨499ä¸ªé—®é¢˜ä¸Šçš„å‡†ç¡®ç‡æå‡è‡³63.8%ï¼Œä¸¤è€…å‡å…·æœ‰ç»Ÿè®¡å­¦æ˜¾è‘—æ€§ã€‚

**Conclusion:** ç¦»æ•£è¯­ä¹‰ç†µèƒ½å¤Ÿé€šè¿‡é‡åŒ–è¯­ä¹‰ä¸ä¸€è‡´æ€§å®ç°é»‘ç›’è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¯é å¹»è§‰æ£€æµ‹ï¼Œæ˜¾è‘—æé«˜è¯Šæ–­ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºä¸´åºŠVLMåº”ç”¨æä¾›æœ‰æ•ˆçš„è¿‡æ»¤ç­–ç•¥ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
To determine whether using discrete semantic entropy (DSE) to reject
questions likely to generate hallucinations can improve the accuracy of
black-box vision-language models (VLMs) in radiologic image based visual
question answering (VQA). This retrospective study evaluated DSE using two
publicly available, de-identified datasets: (i) the VQA-Med 2019 benchmark (500
images with clinical questions and short-text answers) and (ii) a diagnostic
radiology dataset (206 cases: 60 computed tomography scans, 60 magnetic
resonance images, 60 radiographs, 26 angiograms) with corresponding
ground-truth diagnoses. GPT-4o and GPT-4.1 answered each question 15 times
using a temperature of 1.0. Baseline accuracy was determined using
low-temperature answers (temperature 0.1). Meaning-equivalent responses were
grouped using bidirectional entailment checks, and DSE was computed from the
relative frequencies of the resulting semantic clusters. Accuracy was
recalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and
95% confidence intervals were obtained using bootstrap resampling and a
Bonferroni-corrected threshold of p < .004 for statistical significance. Across
706 image-question pairs, baseline accuracy was 51.7% for GPT-4o and 54.8% for
GPT-4.1. After filtering out high-entropy questions (DSE > 0.3), accuracy on
the remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and
63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains
were observed across both datasets and largely remained statistically
significant after Bonferroni correction. DSE enables reliable hallucination
detection in black-box VLMs by quantifying semantic inconsistency. This method
significantly improves diagnostic answer accuracy and offers a filtering
strategy for clinical VLM applications.


### [29] [Spotlight on Token Perception for Multimodal Reinforcement Learning](https://arxiv.org/abs/2510.09285)
*Siyuan Huang, Xiaoye Qu, Yafu Li, Yun Luo, Zefeng He, Daizong Liu, Yu Cheng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–æ–¹æ³•VPPOï¼Œé€šè¿‡å¼•å…¥ä»¤ç‰Œæ„ŸçŸ¥è§†è§’æ¥å¢å¼ºå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œåœ¨å…«ä¸ªæ„ŸçŸ¥ä¸æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­å¿½è§†äº†è§†è§‰æ„ŸçŸ¥åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹å¯¹ç”Ÿæˆä»¤ç‰Œè§†è§‰ä¾èµ–æ€§çš„ç»†ç²’åº¦åˆ†æï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨è§†è§‰åŸºç¡€æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡ã€‚

**Method:** æå‡ºäº†è§†è§‰æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ç®—æ³•VPPOï¼Œè¯¥ç®—æ³•é€šè¿‡åŒé‡æœºåˆ¶ä¼˜åŒ–å­¦ä¹ ä¿¡å·ï¼šåŸºäºè½¨è¿¹æ•´ä½“è§†è§‰ä¾èµ–åº¦é‡æ–°åŠ æƒä¼˜åŠ¿å‡½æ•°ï¼Œå¹¶ä»…å¯¹æ„ŸçŸ¥å…³é”®ä»¤ç‰Œè¿›è¡Œç­–ç•¥æ›´æ–°ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è§†è§‰ä¿¡æ¯è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚

**Result:** åœ¨å…«ä¸ªå…¨é¢çš„æ„ŸçŸ¥ä¸æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVPPOç›¸æ¯”é¢†å…ˆçš„å¼€æºRLè°ƒä¼˜æ¨¡å‹å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå…¶æœ‰æ•ˆæ€§åœ¨7Bå’Œ32Bæ¨¡å‹è§„æ¨¡ä¸Šå‡å¾—åˆ°ä¸€è‡´éªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å®¹é‡ä¸‹çš„æ™®é€‚æ€§ã€‚

**Conclusion:** æœ¬ç ”ç©¶ä¸ä»…ä¸ºåˆ†æå¤šæ¨¡æ€RLVRå»ºç«‹äº†æ–°çš„ä»¤ç‰Œçº§æ„ŸçŸ¥è§†è§’ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°é¢–æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºLVLMsçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the
reasoning capabilities of Large Vision-Language Models (LVLMs), most existing
methods in multimodal reasoning neglect the critical role of visual perception
within the RLVR optimization process. In this paper, we undertake a pioneering
exploration of multimodal RLVR through the novel perspective of token
perception, which measures the visual dependency of each generated token. With
a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key
insights: first, token perception in a rollout trajectory is sparsely
distributed, where only a small fraction of tokens have high visual dependency
for visually-grounded reasoning; second, different trajectories exhibit
significant divergence in their overall visual dependency. Based on these
observations, we propose Visually-Perceptive Policy Optimization (VPPO), a
novel policy gradient algorithm that explicitly leverages token perception to
refine the learning signal. Specifically, VPPO achieves this through a dual
mechanism: it reweights a trajectory's advantage by its overall visual
dependency, and focuses policy updates exclusively on perceptually pivotal
tokens. On a comprehensive suite of eight perception and reasoning benchmarks,
VPPO demonstrates substantial gains over leading open-source RL-tuned models,
with its effectiveness consistently validated across 7B and 32B model scales.
Our findings not only establish a new token-level perceptual perspective for
analyzing multimodal RLVR but also present a novel and effective optimization
strategy to significantly enhance the multimodal reasoning capabilities of
LVLMs.


### [30] [Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation](https://arxiv.org/abs/2510.09320)
*Wenyao Zhang, Hongsi Liu, Bohan Li, Jiawei He, Zekun Qi, Yunnan Wang, Shengyang Zhao, Xinqiang Yu, Wenjun Zeng, Xin Jin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºHybrid-depthæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆCLIPå’ŒDINOåŸºç¡€æ¨¡å‹æ¥æå–è§†è§‰å…ˆéªŒï¼Œè§£å†³è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡ä¸­è¯­ä¹‰-ç©ºé—´çŸ¥è¯†æå–ä¸è¶³çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ·±åº¦ä¼°è®¡æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ç”±äºè¯­ä¹‰-ç©ºé—´çŸ¥è¯†æå–ä¸è¶³è€Œé¢ä¸´æ€§èƒ½é™åˆ¶ï¼Œéœ€è¦æ›´æœ‰æ•ˆåœ°æ•´åˆå…¨å±€è¯­ä¹‰å’Œå±€éƒ¨ç©ºé—´ç»†èŠ‚æ¥æå‡æ·±åº¦ä¼°è®¡ç²¾åº¦ã€‚

**Method:** æå‡ºç²—åˆ°ç»†æ¸è¿›å­¦ä¹ æ¡†æ¶ï¼šé¦–å…ˆåœ¨å¯¹æ¯”è¯­è¨€å¼•å¯¼ä¸‹èšåˆCLIPçš„å…¨å±€è¯­ä¹‰å’ŒDINOçš„å±€éƒ¨ç©ºé—´ç‰¹å¾ï¼Œè®¾è®¡è¿‘è·ç¦»-è¿œè·ç¦»å›¾åƒå—å¯¹æ¯”ä»£ç†ä»»åŠ¡ï¼›ç„¶åæ•´åˆç›¸æœºä½å§¿ä¿¡æ¯å’Œåƒç´ çº§è¯­è¨€å¯¹é½æ¥ç»†åŒ–æ·±åº¦é¢„æµ‹ï¼Œå¯ä½œä¸ºå³æ’å³ç”¨æ¨¡å—ä¸ç°æœ‰è‡ªç›‘ç£MDEæµç¨‹é›†æˆã€‚

**Result:** åœ¨KITTIåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼ŒåŒæ—¶æœ‰æ•ˆæå‡äº†BEVæ„ŸçŸ¥ç­‰ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚

**Conclusion:** é€šè¿‡è¯­è¨€å¼•å¯¼èšåˆCLIPçš„è¯­ä¹‰ä¸Šä¸‹æ–‡å’ŒDINOçš„ç©ºé—´ç»†èŠ‚ï¼Œæœ‰æ•ˆè§£å†³äº†ç‰¹å¾ç²’åº¦ä¸åŒ¹é…é—®é¢˜ï¼Œä¸ºè‡ªç›‘ç£æ·±åº¦ä¼°è®¡æä¾›äº†æ–°çš„åŸºç¡€æ¨¡å‹é›†æˆèŒƒå¼ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Current self-supervised monocular depth estimation (MDE) approaches encounter
performance limitations due to insufficient semantic-spatial knowledge
extraction. To address this challenge, we propose Hybrid-depth, a novel
framework that systematically integrates foundation models (e.g., CLIP and
DINO) to extract visual priors and acquire sufficient contextual information
for MDE. Our approach introduces a coarse-to-fine progressive learning
framework: 1) Firstly, we aggregate multi-grained features from CLIP (global
semantics) and DINO (local spatial details) under contrastive language
guidance. A proxy task comparing close-distant image patches is designed to
enforce depth-aware feature alignment using text prompts; 2) Next, building on
the coarse features, we integrate camera pose information and pixel-wise
language alignment to refine depth predictions. This module seamlessly
integrates with existing self-supervised MDE pipelines (e.g., Monodepth2,
ManyDepth) as a plug-and-play depth encoder, enhancing continuous depth
estimation. By aggregating CLIP's semantic context and DINO's spatial details
through language guidance, our method effectively addresses feature granularity
mismatches. Extensive experiments on the KITTI benchmark demonstrate that our
method significantly outperforms SOTA methods across all metrics, which also
indeed benefits downstream tasks like BEV perception. Code is available at
https://github.com/Zhangwenyao1/Hybrid-depth.


### [31] [Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models](https://arxiv.org/abs/2510.09358)
*Qihang Ma, Shengyu Li, Jie Tang, Dingkang Yang, Shaodong Chen, Yingyi Zhang, Chao Feng, Jiao Ran*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€å…³é”®è¯é¢„æµ‹æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€æ€ç»´é“¾ç­–ç•¥è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨ç¼ºå¤±å’Œæœªè§åœºæ™¯ä¸‹çš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿå¤šæ¨¡æ€æ–¹æ³•åœ¨å¤„ç†ç¼ºå¤±å’Œæœªè§åœºæ™¯æ—¶å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼ŒåŒæ—¶ç°æœ‰åŸºå‡†æµ‹è¯•ç”±äºè®­ç»ƒæµ‹è¯•é›†é‡å ä¸¥é‡è€Œé«˜ä¼°äº†æ¨¡å‹èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜å¹¶æå‡å¤šæ¨¡æ€å…³é”®è¯é¢„æµ‹çš„æ€§èƒ½ã€‚

**Method:** é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€å…³é”®è¯é¢„æµ‹ï¼Œé¦–å…ˆä½¿ç”¨é›¶æ ·æœ¬å’Œç›‘ç£å¾®è°ƒè¯„ä¼°æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œç„¶åé‡‡ç”¨Fine-tune-CoTç­–ç•¥åˆ©ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„é«˜è´¨é‡æ€ç»´é“¾æ•°æ®å¾®è°ƒå°æ¨¡å‹ï¼Œæœ€åæå‡ºåŠ¨æ€CoTç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”æ³¨å…¥æ€ç»´é“¾æ•°æ®ã€‚

**Result:** åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æœ‰æ•ˆæå‡äº†å¤šæ¨¡æ€å…³é”®è¯é¢„æµ‹çš„æ€§èƒ½ï¼ŒåŠ¨æ€CoTç­–ç•¥ç‰¹åˆ«è§£å†³äº†æ¨¡å‹çš„"è¿‡åº¦æ€è€ƒ"ç°è±¡ï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µèƒ½å¤Ÿçµæ´»è¿ç”¨å…¶æ¨ç†èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€å…³é”®è¯é¢„æµ‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŠ¨æ€æ€ç»´é“¾ç­–ç•¥ä¸ºè§£å†³å¤æ‚æ¨ç†é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€ç†è§£ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only
methods by incorporating multiple modalities of input information to produce a
set of conclusive phrases. Traditional multi-modal approaches have been proven
to have significant limitations in handling the challenging absence and unseen
scenarios. Additionally, we identify shortcomings in existing benchmarks that
overestimate model capability due to significant overlap in training tests. In
this work, we propose leveraging vision-language models (VLMs) for the MMKP
task. Firstly, we use two widely-used strategies, e.g., zero-shot and
supervised fine-tuning (SFT) to assess the lower bound performance of VLMs.
Next, to improve the complex reasoning capabilities of VLMs, we adopt
Fine-tune-CoT, which leverages high-quality CoT reasoning data generated by a
teacher model to finetune smaller models. Finally, to address the
"overthinking" phenomenon, we propose a dynamic CoT strategy which adaptively
injects CoT data during training, allowing the model to flexibly leverage its
reasoning capabilities during the inference stage. We evaluate the proposed
strategies on various datasets and the experimental results demonstrate the
effectiveness of the proposed approaches. The code is available at
https://github.com/bytedance/DynamicCoT.


### [32] [BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception](https://arxiv.org/abs/2510.09361)
*Junyan Ye, Dongzhi Jiang, Jun He, Baichuan Zhou, Zilong Huang, Zhiyuan Yan, Hongsheng Li, Conghui He, Weijia Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†BLINK-Twiceï¼Œä¸€ä¸ªåŸºäºæŒ‘æˆ˜æ€§æ„ŸçŸ¥ä»»åŠ¡çš„è§†è§‰ä¸­å¿ƒæ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä»…ä¾èµ–è§†è§‰å†…å®¹è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œè€Œéä¾èµ–å¤–éƒ¨çŸ¥è¯†æˆ–è¯­è¨€ä¸»å¯¼çš„æ¨ç†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ¨ç†åŸºå‡†ä¸»è¦è¯„ä¼°åŸºäºè¯­è¨€çš„æ¨ç†èƒ½åŠ›ï¼Œé€šå¸¸å°†è§†è§‰è¾“å…¥è§†ä¸ºå¯æ›¿æ¢çš„ä¸Šä¸‹æ–‡ï¼Œç¼ºä¹å¯¹çº¯ç²¹è§†è§‰å†…å®¹æ¨ç†èƒ½åŠ›çš„ä¸“é—¨è¯„ä¼°ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œéœ€è¦å¼€å‘ä¸€ä¸ªä¸“æ³¨äºè§†è§‰ä¸­å¿ƒæ¨ç†çš„åŸºå‡†ï¼Œä»è¯­è¨€ä¸»å¯¼æ¨ç†è½¬å‘å›¾åƒåŸºç¡€æ¨ç†ã€‚

**Method:** BLINK-TwiceåŸºå‡†åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸ƒç§è§†è§‰æŒ‘æˆ˜ç±»å‹ç”¨äºæµ‹è¯•è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œè‡ªç„¶å¯¹æŠ—å›¾åƒå¯¹å¼ºåˆ¶æ¨¡å‹ä¾èµ–è§†è§‰å†…å®¹ï¼Œä»¥åŠå¸¦æ³¨é‡Šçš„æ¨ç†é“¾ç”¨äºå¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚è¯¥åŸºå‡†è¯„ä¼°äº†20ä¸ªé¢†å…ˆçš„MLLMï¼ŒåŒ…æ‹¬12ä¸ªåŸºç¡€æ¨¡å‹å’Œ8ä¸ªæ¨ç†å¢å¼ºæ¨¡å‹ã€‚

**Result:** BLINK-Twiceå¯¹å½“å‰æ¨¡å‹æ„æˆäº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œç°æœ‰è¯­è¨€ç©ºé—´çš„æ¨ç†ç­–ç•¥å¦‚æ€ç»´é“¾æˆ–è‡ªæˆ‘æ‰¹è¯„è™½èƒ½æå‡æ€§èƒ½ï¼Œä½†å¾€å¾€å¯¼è‡´ä¸ç¨³å®šå’Œå†—ä½™çš„æ¨ç†ã€‚å®éªŒè§‚å¯Ÿåˆ°é‡å¤å›¾åƒè§‚å¯Ÿèƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œè€Œåƒo3æ¨¡å‹å±•ç¤ºçš„ä¸»åŠ¨è§†è§‰äº¤äº’çªæ˜¾äº†è§†è§‰æ¨ç†æ–°èŒƒå¼çš„éœ€æ±‚ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å½“å‰MLLMåœ¨è§†è§‰ä¸­å¿ƒæ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œéœ€è¦å¼€å‘æ–°çš„è§†è§‰æ¨ç†èŒƒå¼ã€‚é‡å¤å›¾åƒè§‚å¯Ÿå’Œä¸»åŠ¨è§†è§‰äº¤äº’æ˜¯æå‡æ€§èƒ½çš„å…³é”®æ–¹å‘ï¼Œå¼ºè°ƒäº†è¶…è¶Šæµ…å±‚æ„ŸçŸ¥å‘ç»†ç²’åº¦è§‚å¯Ÿå’Œåˆ†ææ¨ç†è½¬å˜çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Recently, Multimodal Large Language Models (MLLMs) have made rapid progress,
particularly in enhancing their reasoning capabilities. However, existing
reasoning benchmarks still primarily assess language-based reasoning, often
treating visual input as replaceable context. To address this gap, we introduce
BLINK-Twice, a vision-centric reasoning benchmark grounded in challenging
perceptual tasks. Instead of relying on external knowledge, our tasks require
models to reason from visual content alone, shifting the focus from
language-based to image-grounded reasoning. Compared to prior perception
benchmarks, it moves beyond shallow perception ("see") and requires
fine-grained observation and analytical reasoning ("observe"). BLINK-Twice
integrates three core components: seven types of visual challenges for testing
visual reasoning, natural adversarial image pairs that enforce reliance on
visual content, and annotated reasoning chains for fine-grained evaluation of
the reasoning process rather than final answers alone. We evaluate 20 leading
MLLMs, including 12 foundation models and 8 reasoning-enhanced models.
BLINK-Twice poses a significant challenge to current models. While existing
reasoning strategies in the language space-such as chain-of-thought or
self-criticism can improve performance, they often result in unstable and
redundant reasoning. We observe that repeated image observation improves
performance across models, and active visual interaction, as demonstrated by
models like o3, highlights the need for a new paradigm for vision reasoning.
The dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice


### [33] [Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians](https://arxiv.org/abs/2510.09438)
*Jin-Chuan Shi, Chengye Su, Jiajun Wang, Ariel Shamir, Miao Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Mono4DEditoræ¡†æ¶ï¼Œé€šè¿‡å°†3Dé«˜æ–¯ä¸é‡åŒ–CLIPç‰¹å¾ç»“åˆå½¢æˆè¯­è¨€åµŒå…¥åŠ¨æ€è¡¨ç¤ºï¼Œå®ç°äº†åŸºäºæ–‡æœ¬æç¤ºçš„å•ç›®è§†é¢‘4Dåœºæ™¯ç¼–è¾‘ï¼Œåœ¨ä¿æŒæœªç¼–è¾‘å†…å®¹å®Œæ•´æ€§çš„åŒæ—¶å®ç°è¯­ä¹‰ç²¾ç¡®çš„å±€éƒ¨åŒºåŸŸç¼–è¾‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä»å•ç›®è§†é¢‘é‡å»ºçš„4Dåœºæ™¯åŸºäºæ–‡æœ¬æç¤ºè¿›è¡Œç¼–è¾‘çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œä¸»è¦å›°éš¾åœ¨äºå¦‚ä½•åœ¨å¤æ‚åŠ¨æ€åœºæ™¯çš„å±€éƒ¨åŒºåŸŸå®ç°è¯­ä¹‰ç²¾ç¡®çš„ç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒæœªç¼–è¾‘å†…å®¹çš„å®Œæ•´æ€§ã€‚

**Method:** è¯¥æ–¹æ³•é€šè¿‡å°†3Dé«˜æ–¯ä¸é‡åŒ–CLIPç‰¹å¾ç»“åˆå½¢æˆè¯­è¨€åµŒå…¥åŠ¨æ€è¡¨ç¤ºï¼Œæ”¯æŒä»»æ„ç©ºé—´åŒºåŸŸçš„é«˜æ•ˆè¯­ä¹‰æŸ¥è¯¢ï¼›æå‡ºä¸¤é˜¶æ®µç‚¹çº§å®šä½ç­–ç•¥ï¼Œé¦–å…ˆé€šè¿‡CLIPç›¸ä¼¼åº¦é€‰æ‹©å€™é€‰é«˜æ–¯ï¼Œç„¶åç²¾ç‚¼å…¶ç©ºé—´èŒƒå›´ä»¥æé«˜å‡†ç¡®æ€§ï¼›æœ€åä½¿ç”¨åŸºäºæ‰©æ•£çš„è§†é¢‘ç¼–è¾‘æ¨¡å‹å¯¹å±€éƒ¨åŒºåŸŸè¿›è¡Œé’ˆå¯¹æ€§ç¼–è¾‘ï¼Œé€šè¿‡æµå’Œæ¶‚é¸¦å¼•å¯¼ç¡®ä¿ç©ºé—´ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMono4DEditorèƒ½å¤Ÿåœ¨å¤šæ ·åœºæ™¯å’Œå¯¹è±¡ç±»å‹ä¸Šå®ç°é«˜è´¨é‡çš„æ–‡æœ¬é©±åŠ¨ç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒæœªç¼–è¾‘åŒºåŸŸçš„å¤–è§‚å’Œå‡ ä½•ç‰¹æ€§ï¼Œåœ¨çµæ´»æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è¯­è¨€åµŒå…¥åŠ¨æ€è¡¨ç¤ºä¸ä¸¤é˜¶æ®µå®šä½ç­–ç•¥ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸º4Dåœºæ™¯ç¼–è¾‘æä¾›äº†çµæ´»ä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨å†…å®¹åˆ›ä½œå’Œè™šæ‹Ÿç¯å¢ƒé¢†åŸŸå…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼Œå¹¶ä¸ºæœªæ¥åŠ¨æ€åœºæ™¯ç¼–è¾‘ç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Editing 4D scenes reconstructed from monocular videos based on text prompts
is a valuable yet challenging task with broad applications in content creation
and virtual environments. The key difficulty lies in achieving semantically
precise edits in localized regions of complex, dynamic scenes, while preserving
the integrity of unedited content. To address this, we introduce Mono4DEditor,
a novel framework for flexible and accurate text-driven 4D scene editing. Our
method augments 3D Gaussians with quantized CLIP features to form a
language-embedded dynamic representation, enabling efficient semantic querying
of arbitrary spatial regions. We further propose a two-stage point-level
localization strategy that first selects candidate Gaussians via CLIP
similarity and then refines their spatial extent to improve accuracy. Finally,
targeted edits are performed on localized regions using a diffusion-based video
editing model, with flow and scribble guidance ensuring spatial fidelity and
temporal coherence. Extensive experiments demonstrate that Mono4DEditor enables
high-quality, text-driven edits across diverse scenes and object types, while
preserving the appearance and geometry of unedited areas and surpassing prior
approaches in both flexibility and visual fidelity.


### [34] [D-TPT: Dimensional Entropy Maximization for Calibrating Test-Time Prompt Tuning in Vision-Language Models](https://arxiv.org/abs/2510.09473)
*Jisu Han, Wonjun Hwang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»´åº¦ç†µæœ€å¤§åŒ–æ–¹æ³•ï¼Œé€šè¿‡æ­£åˆ™åŒ–æ–‡æœ¬ç‰¹å¾åˆ†å¸ƒæ¥ç¼“è§£å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ä¸»å¯¼ç»´åº¦çš„å½±å“ï¼Œä»è€Œæ”¹å–„æµ‹è¯•æ—¶æç¤ºè°ƒä¼˜ä¸­çš„æ ¡å‡†æ€§èƒ½é€€åŒ–é—®é¢˜ï¼Œä¸ºVLMåœ¨çœŸå®éƒ¨ç½²åœºæ™¯ä¸­çš„å¯é æ€§æä¾›äº†ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç”±è·¨æ¨¡æ€å•ä¸€ä¸»å¯¼ç‰¹å¾ç»´åº¦å¼•èµ·çš„æ¨¡æ€é—´éš™é—®é¢˜ï¼Œè¯¥é—®é¢˜å¯¼è‡´æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¸­çš„ä¸»å¯¼ç»´åº¦è¡¨ç°å‡ºé«˜é¢„æµ‹æ•æ„Ÿæ€§ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨æµ‹è¯•æ—¶æç¤ºè°ƒä¼˜ä¸­çš„æ ¡å‡†æ€§èƒ½ã€‚

**Method:** æå‡ºäº†ç»´åº¦ç†µæœ€å¤§åŒ–æ–¹æ³•ï¼Œé€šè¿‡æ­£åˆ™åŒ–æ–‡æœ¬ç‰¹å¾åˆ†å¸ƒä½¿å…¶è¶‹å‘å‡åŒ€åˆ†å¸ƒï¼Œä»è€Œå‡è½»ä¸»å¯¼ç»´åº¦çš„ä¾èµ–æ€§ï¼Œè¯¥æ–¹æ³•ä¸“é—¨é’ˆå¯¹æµ‹è¯•æ—¶æç¤ºè°ƒä¼˜åœºæ™¯è®¾è®¡ï¼Œé€šè¿‡çº¦æŸä¸»å¯¼ç»´åº¦çš„å½±å“æ¥æ”¹å–„æ¨¡å‹æ ¡å‡†è¯¯å·®ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†æµ‹è¯•æ—¶æç¤ºè°ƒä¼˜ä¸­æ ¡å‡†æ€§èƒ½çš„é€€åŒ–é—®é¢˜ï¼Œé€šè¿‡é™ä½ä¸»å¯¼ç»´åº¦çš„ä¾èµ–æ€§æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„æ ¡å‡†è¯¯å·®ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®éƒ¨ç½²åœºæ™¯ä¸­çš„å¯é æ€§æä¾›äº†å®è¯æ”¯æŒã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜çº¦æŸå¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ä¸»å¯¼ç»´åº¦çš„å½±å“å¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹æ ¡å‡†æ€§èƒ½ï¼Œç»´åº¦ç†µæœ€å¤§åŒ–ä¸ºæµ‹è¯•æ—¶è‡ªé€‚åº”æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ­£åˆ™åŒ–ç­–ç•¥ï¼Œå¯¹æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç°å®åº”ç”¨ä¸­çš„å¯é æ€§å…·æœ‰é‡è¦ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Test-time adaptation paradigm provides flexibility towards domain shifts by
performing immediate adaptation on unlabeled target data from the source model.
Vision-Language Models (VLMs) leverage their generalization capabilities for
diverse downstream tasks, and test-time prompt tuning has emerged as a
prominent solution for adapting VLMs. In this work, we explore contrastive VLMs
and identify the modality gap caused by a single dominant feature dimension
across modalities. We observe that the dominant dimensions in both text and
image modalities exhibit high predictive sensitivity, and that constraining
their influence can improve calibration error. Building on this insight, we
propose dimensional entropy maximization that regularizes the distribution of
textual features toward uniformity to mitigate the dependency of dominant
dimensions. Our method alleviates the degradation of calibration performance in
test-time prompt tuning, offering a simple yet effective solution to enhance
the reliability of VLMs in real-world deployment scenarios.


### [35] [Few-shot multi-token DreamBooth with LoRa for style-consistent character generation](https://arxiv.org/abs/2510.09475)
*Ruben Pascual, Mikel Sesma-Sara, Aranzazu Jurio, Daniel Paternain, Mikel Galar*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºDreamBoothçš„å¤šä»¤ç‰Œç­–ç•¥ï¼Œç»“åˆLoRAå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œè§£å†³äº†ä»å°‘é‡å‚è€ƒè§’è‰²ç”Ÿæˆæ— é™æ•°é‡æ–°è§’è‰²åŒæ—¶ä¿æŒè‰ºæœ¯é£æ ¼ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œåœ¨åŠ¨ç”»å’Œæ¸¸æˆé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„åˆ›æ„ç”Ÿæˆèƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†å¬è¡Œä¸šéœ€è¦è§£å†³ä»å°‘é‡äººå·¥è®¾è®¡å‚è€ƒè§’è‰²ç”Ÿæˆæ— é™æ•°é‡æ–°è§’è‰²æ—¶ä¿æŒè‰ºæœ¯é£æ ¼å’Œå…±äº«è§†è§‰ç‰¹å¾ä¸€è‡´æ€§çš„æŒ‘æˆ˜ï¼Œä»¥æ‰©å±•åŠ¨ç”»ã€æ¸¸æˆç­‰é¢†åŸŸçš„åˆ›ä½œå¯èƒ½æ€§ã€‚

**Method:** è¯¥æ–¹æ³•åŸºäºDreamBoothå¾®è°ƒæŠ€æœ¯ï¼Œé‡‡ç”¨å¤šä»¤ç‰Œç­–ç•¥é€šè¿‡èšç±»ä¸ºå•ä¸ªè§’è‰²å’Œé›†ä½“é£æ ¼åˆ†é…ç‹¬ç«‹ä»¤ç‰Œï¼Œç»“åˆLoRAå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œç§»é™¤ç±»åˆ«ç‰¹å®šæ­£åˆ™åŒ–é›†å¹¶åœ¨ç”Ÿæˆæ—¶å¼•å…¥éšæœºä»¤ç‰Œå’ŒåµŒå…¥ã€‚

**Result:** åœ¨äº”ä¸ªå°å‹ä¸“ä¸šæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§’è‰²åŒæ—¶ä¿æŒå‚è€ƒè§’è‰²çš„ç‹¬ç‰¹ç¾å­¦ç‰¹å¾ï¼Œäººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥æ–¹æ³•è¯æ˜äº†åœ¨ä¿æŒè‰ºæœ¯é£æ ¼ä¸€è‡´æ€§çš„å‰æä¸‹å®ç°æ— é™è§’è‰²ç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œä¸ºåˆ›æ„äº§ä¸šæä¾›äº†å¼ºå¤§çš„å·¥å…·ï¼Œå¹¶å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨é£æ ¼ä¿æŒç”Ÿæˆä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
The audiovisual industry is undergoing a profound transformation as it is
integrating AI developments not only to automate routine tasks but also to
inspire new forms of art. This paper addresses the problem of producing a
virtually unlimited number of novel characters that preserve the artistic style
and shared visual traits of a small set of human-designed reference characters,
thus broadening creative possibilities in animation, gaming, and related
domains. Our solution builds upon DreamBooth, a well-established fine-tuning
technique for text-to-image diffusion models, and adapts it to tackle two core
challenges: capturing intricate character details beyond textual prompts and
the few-shot nature of the training data. To achieve this, we propose a
multi-token strategy, using clustering to assign separate tokens to individual
characters and their collective style, combined with LoRA-based
parameter-efficient fine-tuning. By removing the class-specific regularization
set and introducing random tokens and embeddings during generation, our
approach allows for unlimited character creation while preserving the learned
style. We evaluate our method on five small specialized datasets, comparing it
to relevant baselines using both quantitative metrics and a human evaluation
study. Our results demonstrate that our approach produces high-quality, diverse
characters while preserving the distinctive aesthetic features of the reference
characters, with human evaluation further reinforcing its effectiveness and
highlighting the potential of our method.


### [36] [PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs](https://arxiv.org/abs/2510.09507)
*Zixin Zhang, Kanghao Chen, Xingwang Lin, Lutao Jiang, Xu Zheng, Yuanhuiyi Lyu, Litao Guo, Yinchuan Li, Ying-Cong Chen*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†PhysToolBenchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹ç‰©ç†å·¥å…·ç†è§£èƒ½åŠ›çš„åŸºå‡†ï¼Œé€šè¿‡åŒ…å«1000å¤šä¸ªå›¾åƒ-æ–‡æœ¬å¯¹çš„è§†è§‰é—®ç­”æ•°æ®é›†ï¼Œæ­ç¤ºäº†å½“å‰MLLMsåœ¨å·¥å…·ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è™½ç„¶ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åˆ©ç”¨å…¶å¹¿æ³›å¸¸è¯†çŸ¥è¯†åœ¨å…·èº«AIå’Œä¸‹æ¸¸è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­è¿›è¡Œé«˜çº§è§„åˆ’ï¼Œä½†å®ƒä»¬å¯¹ç‰©ç†å·¥å…·çš„çœŸæ­£ç†è§£ç¨‹åº¦å°šæœªå¾—åˆ°é‡åŒ–è¯„ä¼°ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** è¯¥åŸºå‡†é‡‡ç”¨è§†è§‰é—®ç­”æ•°æ®é›†å½¢å¼ï¼ŒåŒ…å«1000å¤šä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ï¼Œè¯„ä¼°èƒ½åŠ›åˆ†ä¸ºä¸‰ä¸ªéš¾åº¦çº§åˆ«ï¼šå·¥å…·è¯†åˆ«ï¼ˆè¯†åˆ«å·¥å…·ä¸»è¦åŠŸèƒ½ï¼‰ã€å·¥å…·ç†è§£ï¼ˆç†è§£å·¥å…·æ“ä½œåŸç†ï¼‰å’Œå·¥å…·åˆ›å»ºï¼ˆåœ¨ä¼ ç»Ÿå·¥å…·ä¸å¯ç”¨æ—¶åˆ©ç”¨å‘¨å›´ç‰©ä½“åˆ›é€ æ–°å·¥å…·ï¼‰ã€‚

**Result:** å¯¹32ä¸ªMLLMsï¼ˆåŒ…æ‹¬ä¸“æœ‰ã€å¼€æºã€ä¸“ç”¨å…·èº«æ¨¡å‹å’ŒVLAéª¨å¹²æ¨¡å‹ï¼‰çš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨å·¥å…·ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å·¥å…·æ“ä½œåŸç†å’Œåˆ›é€ æ–°å·¥å…·æ–¹é¢è¡¨ç°ä¸è¶³ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†å·¥å…·ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥æ”¹è¿›æä¾›äº†é‡è¦åŸºå‡†å’Œåˆæ­¥è§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†åœ¨å…·èº«æ™ºèƒ½ç³»ç»Ÿä¸­åŠ å¼ºç‰©ç†å¸¸è¯†æ¨ç†èƒ½åŠ›çš„å¿…è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
The ability to use, understand, and create tools is a hallmark of human
intelligence, enabling sophisticated interaction with the physical world. For
any general-purpose intelligent agent to achieve true versatility, it must also
master these fundamental skills. While modern Multimodal Large Language Models
(MLLMs) leverage their extensive common knowledge for high-level planning in
embodied AI and in downstream Vision-Language-Action (VLA) models, the extent
of their true understanding of physical tools remains unquantified. To bridge
this gap, we present PhysToolBench, the first benchmark dedicated to evaluating
the comprehension of physical tools by MLLMs. Our benchmark is structured as a
Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.
It assesses capabilities across three distinct difficulty levels: (1) Tool
Recognition: Requiring the recognition of a tool's primary function. (2) Tool
Understanding: Testing the ability to grasp the underlying principles of a
tool's operation. (3) Tool Creation: Challenging the model to fashion a new
tool from surrounding objects when conventional options are unavailable. Our
comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,
specialized embodied, and backbones in VLAs-reveals a significant deficiency in
tool understanding. Furthermore, we provide an in-depth analysis and propose
preliminary solutions. Code and dataset are publicly available.


### [37] [Vision Language Models: A Survey of 26K Papers](https://arxiv.org/abs/2510.09586)
*Fengming Lin*

#### ğŸ§© TL;DR
æœ¬æ–‡é€šè¿‡å¯¹2023-2025å¹´é—´26,104ç¯‡CVPRã€ICLRå’ŒNeurIPSè®ºæ–‡çš„ç³»ç»Ÿåˆ†æï¼Œé‡åŒ–äº†AIç ”ç©¶é¢†åŸŸçš„ä¸‰å¤§å®è§‚è¶‹åŠ¿ï¼šå¤šæ¨¡æ€è§†è§‰-è¯­è¨€-å¤§è¯­è¨€æ¨¡å‹å·¥ä½œçš„æ€¥å‰§å¢é•¿ã€ç”Ÿæˆå¼æ–¹æ³•çš„ç¨³æ­¥æ‰©å±•ä»¥åŠ3Då’Œè§†é¢‘æ´»åŠ¨çš„æŒç»­æ´»è·ƒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡é€æ˜ã€å¯å¤ç°çš„æ–¹æ³•é‡åŒ–AIç ”ç©¶è¶‹åŠ¿ï¼Œè§£å†³å½“å‰ç¼ºä¹ç³»ç»Ÿæ€§è·¨é¢†åŸŸç ”ç©¶åŠ¨æ€åˆ†æçš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è®¡ç®—æœºè§†è§‰ã€æœºå™¨å­¦ä¹ å’Œç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿç­‰é¡¶çº§ä¼šè®®çš„ç ”ç©¶æ¼”å˜è¿›è¡Œå¤§è§„æ¨¡æµ‹é‡ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨è§„èŒƒåŒ–å¤„ç†ã€çŸ­è¯­ä¿æŠ¤å’Œæ‰‹å·¥æ„å»ºè¯å…¸åŒ¹é…çš„æ–¹æ³•ï¼Œå¯¹è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œåˆ†æï¼Œåˆ†é…å¤šè¾¾35ä¸ªä¸»é¢˜æ ‡ç­¾ï¼Œå¹¶æŒ–æ˜å…³äºä»»åŠ¡ã€æ¶æ„ã€è®­ç»ƒæœºåˆ¶ã€ç›®æ ‡å‡½æ•°ã€æ•°æ®é›†å’Œå…±æåŠæ¨¡æ€çš„ç»†ç²’åº¦çº¿ç´¢ã€‚

**Result:** åˆ†ææ­ç¤ºäº†ä¸‰å¤§å®è§‚è½¬å˜ï¼šå¤šæ¨¡æ€è§†è§‰-è¯­è¨€-LLMå·¥ä½œçš„æ€¥å‰§å¢é•¿ï¼Œå°†ç»å…¸æ„ŸçŸ¥é‡æ–°å®šä¹‰ä¸ºæŒ‡ä»¤è·Ÿéšå’Œå¤šæ­¥æ¨ç†ï¼›ç”Ÿæˆå¼æ–¹æ³•çš„ç¨³æ­¥æ‰©å±•ï¼Œæ‰©æ•£ç ”ç©¶å›´ç»•å¯æ§æ€§ã€è’¸é¦å’Œé€Ÿåº¦è¿›è¡Œæ•´åˆï¼›3Då’Œè§†é¢‘æ´»åŠ¨çš„æŒç»­æ´»è·ƒï¼Œç»„åˆæ–¹æ³•ä»NeRFè½¬å‘é«˜æ–¯æº…å°„ï¼Œå¹¶æ›´åŠ å…³æ³¨ä»¥äººå’Œæ™ºèƒ½ä½“ä¸ºä¸­å¿ƒçš„ç†è§£ã€‚

**Conclusion:** ç ”ç©¶æä¾›äº†AIç ”ç©¶è¶‹åŠ¿çš„é‡åŒ–è¯æ®ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€å’Œç”Ÿæˆå¼æ–¹æ³•çš„å¿«é€Ÿå´›èµ·ï¼ŒåŒæ—¶é‡Šæ”¾äº†è¯å…¸å’Œæ–¹æ³•è®ºä»¥æ”¯æŒå®¡è®¡å’Œæ‰©å±•ï¼Œä¸ºç†è§£é¢†åŸŸæ¼”å˜å’ŒæŒ‡å¯¼æœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†ç³»ç»Ÿæ€§çš„å®è¯åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
We present a transparent, reproducible measurement of research trends across
26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles
and abstracts are normalized, phrase-protected, and matched against a
hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained
cues about tasks, architectures, training regimes, objectives, datasets, and
co-mentioned modalities. The analysis quantifies three macro shifts: (1) a
sharp rise of multimodal vision-language-LLM work, which increasingly reframes
classic perception as instruction following and multi-step reasoning; (2)
steady expansion of generative methods, with diffusion research consolidating
around controllability, distillation, and speed; and (3) resilient 3D and video
activity, with composition moving from NeRFs to Gaussian splatting and a
growing emphasis on human- and agent-centric understanding. Within VLMs,
parameter-efficient adaptation like prompting/adapters/LoRA and lightweight
vision-language bridges dominate; training practice shifts from building
encoders from scratch to instruction tuning and finetuning strong backbones;
contrastive objectives recede relative to cross-entropy/ranking and
distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and
ICLR the highest VLM share, while reliability themes such as efficiency or
robustness diffuse across areas. We release the lexicon and methodology to
enable auditing and extension. Limitations include lexicon recall and
abstract-only scope, but the longitudinal signals are consistent across venues
and years.


### [38] [VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation](https://arxiv.org/abs/2510.09607)
*Shaoqi Dong, Chaoyou Fu, Haihan Gao, Yi-Fan Zhang, Chi Yan, Chu Wu, Xiaoyu Liu, Yunhang Shen, Jing Huo, Deqiang Jiang, Haoyu Cao, Yang Gao, Xing Sun, Ran He, Caifeng Shan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè’¸é¦çš„æ¡†æ¶ï¼Œé€šè¿‡ä»é¢„è®­ç»ƒçš„å°å‹åŠ¨ä½œæ¨¡å‹è½¬ç§»çŸ¥è¯†ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹èµ‹äºˆåŠ¨ä½œæ‰§è¡Œèƒ½åŠ›ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬å¹¶æå‡äº†æ“ä½œæ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨LIBEROåŸºå‡†ä¸Šå®ç°äº†97.3%çš„å¹³å‡æˆåŠŸç‡ï¼Œåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­è¾¾åˆ°82.0%çš„æˆåŠŸç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å¼ºå¤§æ„ŸçŸ¥èƒ½åŠ›æ˜¾è‘—æ¨è¿›äº†æœºå™¨äººæ“ä½œï¼Œä½†ä»å¤´è®­ç»ƒè¿™äº›æ¨¡å‹æˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³VLAæ¨¡å‹è®­ç»ƒæˆæœ¬é«˜çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒå…¶æ³›åŒ–èƒ½åŠ›ã€‚

**Method:** æå‡ºè’¸é¦æ¡†æ¶ï¼Œä¿ç•™åŸå§‹VLMç»“æ„ï¼Œä»…æ·»åŠ åŠ¨ä½œä»¤ç‰Œå’ŒçŠ¶æ€ç¼–ç å™¨æ¥æ•´åˆç‰©ç†è¾“å…¥ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆé€šè¿‡è½»é‡çº§å¯¹é½å°†VLMéšè—çŠ¶æ€æ˜ å°„åˆ°å°å‹åŠ¨ä½œæ¨¡å‹çš„åŠ¨ä½œç©ºé—´ï¼Œé‡ç”¨å…¶é¢„è®­ç»ƒåŠ¨ä½œè§£ç å™¨ï¼›ç„¶åé€‰æ‹©æ€§å¾®è°ƒè¯­è¨€æ¨¡å‹ã€çŠ¶æ€ç¼–ç å™¨å’ŒåŠ¨ä½œæ¨¡å—ï¼Œå®ç°å¤šæ¨¡æ€è¾“å…¥ä¸ç²¾ç¡®åŠ¨ä½œç”Ÿæˆçš„é›†æˆã€‚

**Result:** åœ¨LIBEROåŸºå‡†ä¸Šè¾¾åˆ°97.3%çš„å¹³å‡æˆåŠŸç‡ï¼ˆæå‡11.8%ï¼‰ï¼Œåœ¨LIBERO-LONGä¸Šè¾¾åˆ°93.5%ï¼ˆæå‡24.5%ï¼‰ã€‚åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­ï¼Œä»¥82.0%çš„æˆåŠŸç‡æŒç»­è¶…è¶Šæ•™å¸ˆæ¨¡å‹ï¼ˆæå‡17%ï¼‰ã€‚

**Conclusion:** åŠ¨ä½œè’¸é¦æœ‰æ•ˆä½¿VLMèƒ½å¤Ÿç”Ÿæˆç²¾ç¡®åŠ¨ä½œï¼ŒåŒæ—¶å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†é€šè¿‡çŸ¥è¯†è½¬ç§»è€Œéä»å¤´è®­ç»ƒæ¥å¢å¼ºå¤§å‹æ¨¡å‹åŠŸèƒ½çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé«˜æ•ˆæœºå™¨äººå­¦ä¹ æä¾›äº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Action (VLA) models significantly advance robotic
manipulation by leveraging the strong perception capabilities of pretrained
vision-language models (VLMs). By integrating action modules into these
pretrained models, VLA methods exhibit improved generalization. However,
training them from scratch is costly. In this work, we propose a simple yet
effective distillation-based framework that equips VLMs with action-execution
capability by transferring knowledge from pretrained small action models. Our
architecture retains the original VLM structure, adding only an action token
and a state encoder to incorporate physical inputs. To distill action
knowledge, we adopt a two-stage training strategy. First, we perform
lightweight alignment by mapping VLM hidden states into the action space of the
small action model, enabling effective reuse of its pretrained action decoder
and avoiding expensive pretraining. Second, we selectively fine-tune the
language model, state encoder, and action modules, enabling the system to
integrate multimodal inputs with precise action generation. Specifically, the
action token provides the VLM with a direct handle for predicting future
actions, while the state encoder allows the model to incorporate robot dynamics
not captured by vision alone. This design yields substantial efficiency gains
over training large VLA models from scratch. Compared with previous
state-of-the-art methods, our method achieves 97.3% average success rate on
LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In
real-world experiments across five manipulation tasks, our method consistently
outperforms the teacher model, achieving 82.0% success rate (17% improvement),
which demonstrate that action distillation effectively enables VLMs to generate
precise actions while substantially reducing training costs.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [39] [Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech](https://arxiv.org/abs/2510.08593)
*Yuxin Li, Eng Siong Chng, Cuntai Guan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºHAREN-CTCï¼Œä¸€ç§æ•´åˆå¤šå±‚è‡ªç›‘ç£å­¦ä¹ ç‰¹å¾çš„å¤šä»»åŠ¡å­¦ä¹ æ¶æ„ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å’ŒCTCæŸå¤±å¤„ç†ç¨€ç–æ—¶é—´ç›‘ç£ï¼Œåœ¨è¯­éŸ³æŠ‘éƒæ£€æµ‹ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è¯­éŸ³æŠ‘éƒæ£€æµ‹æ–¹æ³•ä¸»è¦é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šéš¾ä»¥æå–æœ‰æ„ä¹‰çš„ç‰¹å¾ä»¥åŠæ•æ‰ç¨€ç–ã€å¼‚è´¨çš„æŠ‘éƒæ—¶é—´çº¿ç´¢ã€‚è™½ç„¶é¢„è®­ç»ƒè‡ªç›‘ç£å­¦ä¹ æ¨¡å‹å¦‚WavLMæä¾›äº†ä¸°å¯Œçš„å¤šå±‚è¯­éŸ³è¡¨ç¤ºï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä»…ä½¿ç”¨æœ€ç»ˆå±‚æˆ–å¯»æ‰¾å•ä¸€æœ€ä½³å±‚ï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆç‰¹å®šæ•°æ®é›†ä¸”æ— æ³•å……åˆ†åˆ©ç”¨æ£€æµ‹ç»†å¾®æŒä¹…æŠ‘éƒä¿¡å·æ‰€éœ€çš„å®Œæ•´å±‚æ¬¡ç»“æ„ã€‚

**Method:** HAREN-CTCæ¶æ„æ•´åˆå¤šå±‚SSLç‰¹å¾ï¼Œé‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ç»“åˆè¿æ¥æ—¶åºåˆ†ç±»æŸå¤±æ¥å¤„ç†ç¨€ç–æ—¶é—´ç›‘ç£ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šå±‚æ¬¡è‡ªé€‚åº”èšç±»æ¨¡å—å°†SSLç‰¹å¾é‡ç»„ä¸ºäº’è¡¥åµŒå…¥ï¼Œä»¥åŠè·¨æ¨¡æ€èåˆæ¨¡å—é€šè¿‡äº¤å‰æ³¨æ„åŠ›å»ºæ¨¡å±‚é—´ä¾èµ–å…³ç³»ã€‚CTCç›®æ ‡å®ç°å¯¹é½æ„ŸçŸ¥è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿½è¸ªæŠ‘éƒè¯­éŸ³çº¿ç´¢çš„ä¸è§„åˆ™æ—¶é—´æ¨¡å¼ã€‚

**Result:** åœ¨æ ‡å‡†æ•°æ®åˆ†å‰²çš„ä¸Šç•Œè®¾ç½®å’Œäº”æŠ˜äº¤å‰éªŒè¯çš„æ³›åŒ–è®¾ç½®ä¸‹è¿›è¡Œè¯„ä¼°ï¼ŒHAREN-CTCåœ¨DAIC-WOZæ•°æ®é›†ä¸Šè¾¾åˆ°0.81çš„å®F1åˆ†æ•°ï¼Œåœ¨MODMAæ•°æ®é›†ä¸Šè¾¾åˆ°0.82çš„å®F1åˆ†æ•°ï¼Œåœ¨ä¸¤ä¸ªè¯„ä¼°åœºæ™¯ä¸‹å‡ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒSSLæ¨¡å‹çš„å¤šå±‚ç‰¹å¾å¯¹äºæ£€æµ‹ç»†å¾®æŠ‘éƒä¿¡å·è‡³å…³é‡è¦ï¼Œæå‡ºçš„å±‚æ¬¡ç‰¹å¾æ•´åˆå’ŒCTCå¯¹é½æœºåˆ¶ä¸ºè§£å†³è¯­éŸ³æŠ‘éƒæ£€æµ‹ä¸­çš„ç¨€ç–æ—¶é—´ç›‘ç£é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œä¸ºä¸´åºŠè¾…åŠ©è¯Šæ–­ç³»ç»Ÿçš„å‘å±•æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚

---

#### ğŸ“„ Abstract
Speech-based depression detection (SDD) is a promising, non-invasive
alternative to traditional clinical assessments. However, it remains limited by
the difficulty of extracting meaningful features and capturing sparse,
heterogeneous depressive cues over time. Pretrained self-supervised learning
(SSL) models such as WavLM provide rich, multi-layer speech representations,
yet most existing SDD methods rely only on the final layer or search for a
single best-performing one. These approaches often overfit to specific datasets
and fail to leverage the full hierarchical structure needed to detect subtle
and persistent depression signals.
  To address this challenge, we propose HAREN-CTC, a novel architecture that
integrates multi-layer SSL features using cross-attention within a multitask
learning framework, combined with Connectionist Temporal Classification loss to
handle sparse temporal supervision. HAREN-CTC comprises two key modules: a
Hierarchical Adaptive Clustering module that reorganizes SSL features into
complementary embeddings, and a Cross-Modal Fusion module that models
inter-layer dependencies through cross-attention. The CTC objective enables
alignment-aware training, allowing the model to track irregular temporal
patterns of depressive speech cues.
  We evaluate HAREN-CTC under both an upper-bound setting with standard data
splits and a generalization setting using five-fold cross-validation. The model
achieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on
MODMA, outperforming prior methods across both evaluation scenarios.


### [40] [Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection](https://arxiv.org/abs/2510.08602)
*Cong Zeng, Shengkun Tang, Yuanzhou Chen, Zhiqiang Shen, Wenchao Yu, Xujiang Zhao, Haifeng Chen, Wei Cheng, Zhiqiang Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºå°†AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºç¦»ç¾¤æ£€æµ‹é—®é¢˜ï¼Œé‡‡ç”¨å•ç±»å­¦ä¹ å’ŒåŸºäºåˆ†æ•°çš„å­¦ä¹ æ–¹æ³•æ¥æé«˜æ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜å¼‚çš„æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹æ–¹æ³•å¤§å¤šå°†ä»»åŠ¡è§†ä¸ºäºŒåˆ†ç±»é—®é¢˜ï¼Œè¿™ç§äºŒå…ƒåŒ–è¡¨è¿°é”™è¯¯åœ°å‡è®¾äººç±»æ–‡æœ¬æ„æˆç»Ÿä¸€çš„åˆ†å¸ƒï¼Œå¯¼è‡´æ£€æµ‹å™¨å®¹æ˜“è®°å¿†è§‚å¯Ÿåˆ°çš„åˆ†å¸ƒå¤–ç‰¹å¾è€Œéå­¦ä¹ çœŸæ­£çš„'éåˆ†å¸ƒå†…'è¡Œä¸ºæœ¬è´¨ï¼Œä»è€Œé™åˆ¶äº†åœ¨æœªè§äººç±»æ–‡æœ¬è¾“å…¥ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

**Method:** æœ¬æ–‡æå‡ºå°†æ£€æµ‹ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºç¦»ç¾¤æ£€æµ‹é—®é¢˜ï¼Œå°†äººç±»æ–‡æœ¬è§†ä¸ºåˆ†å¸ƒå¤–æ ·æœ¬è€Œæœºå™¨ç”Ÿæˆæ–‡æœ¬ä½œä¸ºåˆ†å¸ƒå†…æ ·æœ¬ï¼Œå¼€å‘äº†åŸºäºå•ç±»å­¦ä¹ çš„æ–¹æ³•åŒ…æ‹¬DeepSVDDå’ŒHRNï¼Œä»¥åŠåŸºäºåˆ†æ•°çš„å­¦ä¹ æŠ€æœ¯å¦‚èƒ½é‡æ–¹æ³•ï¼Œæ„å»ºäº†é²æ£’ä¸”å¯æ³›åŒ–çš„æ£€æµ‹æ¡†æ¶ã€‚

**Result:** åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†åŸºäºç¦»ç¾¤æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨DeepFakeæ•°æ®é›†ä¸Šå®ç°äº†98.3%çš„AUROCå’ŒAUPRï¼ŒFPR95ä»…ä¸º8.9%ï¼Œå¹¶åœ¨å¤šè¯­è¨€ã€å—æ”»å‡»ä»¥åŠæœªè§æ¨¡å‹å’Œé¢†åŸŸæ–‡æœ¬è®¾ç½®ä¸‹æµ‹è¯•äº†æ£€æµ‹æ¡†æ¶ï¼Œè¯æ˜äº†å…¶é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å°†AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹é‡æ–°å®šä¹‰ä¸ºç¦»ç¾¤æ£€æµ‹é—®é¢˜èƒ½å¤Ÿæœ‰æ•ˆè§£å†³ä¼ ç»ŸäºŒåˆ†ç±»æ–¹æ³•çš„æ³›åŒ–é™åˆ¶ï¼ŒåŸºäºå•ç±»å­¦ä¹ å’Œåˆ†æ•°å­¦ä¹ çš„æ–¹æ³•èƒ½å¤Ÿæ•æ‰æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„æœ¬è´¨ç‰¹å¾ï¼Œä¸ºæ„å»ºæ›´é²æ£’çš„æ£€æµ‹ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œç†è®ºè§†è§’ã€‚

---

#### ğŸ“„ Abstract
The rapid advancement of large language models (LLMs) such as ChatGPT,
DeepSeek, and Claude has significantly increased the presence of AI-generated
text in digital communication. This trend has heightened the need for reliable
detection methods to distinguish between human-authored and machine-generated
content. Existing approaches both zero-shot methods and supervised classifiers
largely conceptualize this task as a binary classification problem, often
leading to poor generalization across domains and models. In this paper, we
argue that such a binary formulation fundamentally mischaracterizes the
detection task by assuming a coherent representation of human-written texts. In
reality, human texts do not constitute a unified distribution, and their
diversity cannot be effectively captured through limited sampling. This causes
previous classifiers to memorize observed OOD characteristics rather than learn
the essence of `non-ID' behavior, limiting generalization to unseen
human-authored inputs. Based on this observation, we propose reframing the
detection task as an out-of-distribution (OOD) detection problem, treating
human-written texts as distributional outliers while machine-generated texts
are in-distribution (ID) samples. To this end, we develop a detection framework
using one-class learning method including DeepSVDD and HRN, and score-based
learning techniques such as energy-based method, enabling robust and
generalizable performance. Extensive experiments across multiple datasets
validate the effectiveness of our OOD-based approach. Specifically, the
OOD-based method achieves 98.3% AUROC and AUPR with only 8.9% FPR95 on DeepFake
dataset. Moreover, we test our detection framework on multilingual, attacked,
and unseen-model and -domain text settings, demonstrating the robustness and
generalizability of our framework. Code, pretrained weights, and demo will be
released.


### [41] [Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations](https://arxiv.org/abs/2510.08606)
*Yu Liu, Hanlei Shi, Haoxun Li, Yuqing Sun, Yuxuan Ding, Linlin Gong, Leyuan Qu, Taihao Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»¥æƒ…æ„Ÿçƒ­ç‚¹ä¸ºä¸­å¿ƒçš„å¯¹è¯æƒ…æ„Ÿè¯†åˆ«ç»Ÿä¸€æ¨¡å‹ï¼Œé€šè¿‡çƒ­ç‚¹é—¨æ§èåˆå’Œè·¯ç”±æ··åˆå¯¹é½å™¨æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€æ•°æ®ä¸­çš„ç¨€ç–ã€å±€éƒ¨å’Œå¼‚æ­¥æƒ…æ„Ÿè¯æ®ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜äºå¼ºåŸºçº¿çš„æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¯¹è¯æƒ…æ„Ÿè¯†åˆ«é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºæƒ…æ„Ÿè¯æ®åœ¨å¤šæ¨¡æ€æ•°æ®ä¸­å‘ˆç°ç¨€ç–æ€§ã€å±€éƒ¨æ€§å’Œå¼‚æ­¥æ€§åˆ†å¸ƒï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰è¿™äº›åˆ†æ•£ä¸”è·¨æ¨¡æ€ä¸å¯¹é½çš„å…³é”®æƒ…æ„Ÿçº¿ç´¢ã€‚

**Method:** è¯¥æ¨¡å‹é€šè¿‡æ£€æµ‹æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ä¸­çš„é€è¯è¯­æƒ…æ„Ÿçƒ­ç‚¹ï¼Œé‡‡ç”¨çƒ­ç‚¹é—¨æ§èåˆå°†å±€éƒ¨çƒ­ç‚¹ä¸å…¨å±€ç‰¹å¾ç»“åˆï¼Œä½¿ç”¨è·¯ç”±æ··åˆå¯¹é½å™¨è§£å†³æ¨¡æ€å¯¹é½é—®é¢˜ï¼Œå¹¶æ„å»ºè·¨æ¨¡æ€å›¾ç¼–ç å¯¹è¯ç»“æ„ä»¥ä¿æŒä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**Result:** åœ¨æ ‡å‡†å¯¹è¯æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºå¼ºåŸºçº¿æ¨¡å‹å–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œæ¶ˆèç ”ç©¶è¯å®äº†çƒ­ç‚¹é—¨æ§èåˆå’Œæ··åˆå¯¹é½å™¨ç»„ä»¶çš„å…³é”®è´¡çŒ®ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ä»¥æƒ…æ„Ÿçƒ­ç‚¹ä¸ºä¸­å¿ƒçš„è§†è§’èƒ½å¤Ÿä¸ºæœªæ¥å¤šæ¨¡æ€å­¦ä¹ æä¾›æ–°æ€è·¯ï¼Œä¸ºæ¨¡æ€èåˆæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå¼ºè°ƒäº†èšç„¦å…³é”®æƒ…æ„ŸåŒºåŸŸåœ¨å¤„ç†å¤šæ¨¡æ€å¯¹è¯æ•°æ®ä¸­çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Emotion Recognition in Conversations (ERC) is hard because discriminative
evidence is sparse, localized, and often asynchronous across modalities. We
center ERC on emotion hotspots and present a unified model that detects
per-utterance hotspots in text, audio, and video, fuses them with global
features via Hotspot-Gated Fusion, and aligns modalities using a routed
Mixture-of-Aligners; a cross-modal graph encodes conversational structure. This
design focuses modeling on salient spans, mitigates misalignment, and preserves
context. Experiments on standard ERC benchmarks show consistent gains over
strong baselines, with ablations confirming the contributions of HGF and MoA.
Our results point to a hotspot-centric view that can inform future multimodal
learning, offering a new perspective on modality fusion in ERC.


### [42] [MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation](https://arxiv.org/abs/2510.08608)
*Weihua Zheng, Zhengyuan Liu, Tanmoy Chakraborty, Weiwen Xu, Xiaoxue Gao, Bryan Chen Zhengyu Tan, Bowei Zou, Chang Liu, Yujia Hu, Xing Xie, Xiaoyuan Yi, Jing Yao, Chaojun Wang, Long Li, Rui Liu, Huiyao Liu, Koji Inoue, Ryuichi Sumida, Tatsuya Kawahara, Fan Xu, Lingyu Ye, Wei Tian, Dongjun Kim, Jimin Jung, Jaehyung Seo, Nadya Yuki Wangsajaya, Pham Minh Duc, Ojasva Saxena, Palash Nandi, Xiyan Tao, Wiwik Karlina, Tuan Luong, Keertana Arun Vasan, Roy Ka-Wei Lee, Nancy F. Chen*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†MMA-ASIAæ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹äºšæ´²æ–‡åŒ–èƒŒæ™¯çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«27,000ä¸ªå¤šè¯­è¨€å¤šæ¨¡æ€å¯¹é½é—®é¢˜ï¼Œæ­ç¤ºäº†LLMsåœ¨éè¥¿æ–¹æ–‡åŒ–ç¯å¢ƒä¸­çš„ç†è§£é€€åŒ–é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸Šï¼Œåœ¨éè¥¿æ–¹é«˜èµ„æºç¯å¢ƒï¼ˆç‰¹åˆ«æ˜¯äºšæ´²æ–‡åŒ–èƒŒæ™¯ï¼‰ä¸­è¡¨ç°æ˜¾è‘—é€€åŒ–ï¼Œç¼ºä¹é’ˆå¯¹æ–‡åŒ–æ„è¯†çš„ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ã€‚

**Method:** æå‡ºäº†MMA-ASIAè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬äººç±»æ ‡æ³¨çš„å¤šè¯­è¨€å¤šæ¨¡æ€å¯¹é½åŸºå‡†ï¼ˆæ¶µç›–8ä¸ªäºšæ´²å›½å®¶å’Œ10ç§è¯­è¨€ï¼Œ27,000ä¸ªé—®é¢˜ï¼‰ï¼Œäº”ç»´è¯„ä¼°åè®®ï¼Œä»¥åŠæ–‡åŒ–æ„è¯†åŸºç¡€éªŒè¯æ¨¡å—æ¥æ£€æµ‹æ·å¾„å­¦ä¹ ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›è¿½è¸ªå’Œè§†è§‰æ¶ˆèå‰ç¼€é‡æ”¾æ–¹æ³•åˆ†ææ¨¡å‹è¡Œä¸ºã€‚

**Result:** åŸºå‡†ä¸­è¶…è¿‡79%çš„é—®é¢˜éœ€è¦åŸºäºæ–‡åŒ–èƒŒæ™¯çš„å¤šæ­¥æ¨ç†ï¼Œå»ºç«‹äº†é¦–ä¸ªåœ¨æ–‡æœ¬ã€å›¾åƒå’Œè¯­éŸ³ä¸‰ç§æ¨¡æ€è¾“å…¥å±‚é¢å®Œå…¨å¯¹é½çš„æ•°æ®é›†ï¼Œèƒ½å¤Ÿç›´æ¥æµ‹è¯•è·¨æ¨¡æ€è¿ç§»èƒ½åŠ›ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†LLMsåœ¨è·¨è¯­è¨€å’Œè·¨æ¨¡æ€çš„æ–‡åŒ–ç†è§£ä¸­å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸ºæ„å»ºæ–‡åŒ–å¯é çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†å¯æ“ä½œçš„è§è§£å’Œç³»ç»Ÿæ€§è¯„ä¼°æ–¹æ³•ã€‚

---

#### ğŸ“„ Abstract
Large language models (LLMs) are now used worldwide, yet their multimodal
understanding and reasoning often degrade outside Western, high-resource
settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs'
cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a
human-curated, multilingual, and multimodally aligned multiple-choice benchmark
covering 8 Asian countries and 10 languages, comprising 27,000 questions; over
79 percent require multi-step reasoning grounded in cultural context, moving
beyond simple memorization. To our knowledge, this is the first dataset aligned
at the input level across three modalities: text, image (visual question
answering), and speech. This enables direct tests of cross-modal transfer.
Building on this benchmark, we propose a five-dimensional evaluation protocol
that measures: (i) cultural-awareness disparities across countries, (ii)
cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural
knowledge generalization, and (v) grounding validity. To ensure rigorous
assessment, a Cultural Awareness Grounding Validation Module detects "shortcut
learning" by checking whether the requisite cultural knowledge supports correct
answers. Finally, through comparative model analysis, attention tracing, and an
innovative Vision-ablated Prefix Replay (VPR) method, we probe why models
diverge across languages and modalities, offering actionable insights for
building culturally reliable multimodal LLMs.


### [43] [FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs](https://arxiv.org/abs/2510.08886)
*Yan Wang, Keyi Wang, Shanshan Yang, Jaisal Patel, Jeff Zhao, Fengran Mo, Xueqing Peng, Lingfei Qian, Jimin Huang, Guojun Xiong, Xiao-Yang Liu, Jian-Yun Nie*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†FinAuditingï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è´¢åŠ¡å®¡è®¡ä»»åŠ¡çš„ã€ä¸åˆ†ç±»æ³•å¯¹é½çš„ã€ç»“æ„æ„ŸçŸ¥çš„å¤šæ–‡æ¡£åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ç»“æ„åŒ–è´¢åŠ¡æ–‡æ¡£æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ­ç¤ºäº†ç°ä»£LLMsåœ¨åŸºäºåˆ†ç±»æ³•çš„è´¢åŠ¡æ¨ç†ä¸­å­˜åœ¨çš„ç³»ç»Ÿæ€§å±€é™ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é€šç”¨ä¼šè®¡å‡†åˆ™çš„å¤æ‚æ€§å’ŒXBRLç”³æŠ¥æ–‡ä»¶çš„å±‚æ¬¡åŒ–ç»“æ„ä½¿å¾—è´¢åŠ¡å®¡è®¡éš¾ä»¥è‡ªåŠ¨åŒ–å’ŒéªŒè¯ã€‚å°½ç®¡LLMsåœ¨éç»“æ„åŒ–æ–‡æœ¬ç†è§£æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ç»“æ„åŒ–ã€ç›¸äº’ä¾èµ–ä¸”åŸºäºåˆ†ç±»æ³•çš„è´¢åŠ¡æ–‡æ¡£ä¸Šçš„æ¨ç†èƒ½åŠ›ä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚

**Method:** åŸºäºçœŸå®çš„US-GAAPå…¼å®¹XBRLç”³æŠ¥æ–‡ä»¶æ„å»ºFinAuditingåŸºå‡†ï¼Œå®šä¹‰äº†ä¸‰ä¸ªäº’è¡¥çš„å­ä»»åŠ¡ï¼šFinSMç”¨äºè¯­ä¹‰ä¸€è‡´æ€§ã€FinREç”¨äºå…³ç³»ä¸€è‡´æ€§ã€FinFinMRç”¨äºæ•°å€¼ä¸€è‡´æ€§ï¼Œæ¯ä¸ªå­ä»»åŠ¡é’ˆå¯¹ç»“æ„åŒ–å®¡è®¡æ¨ç†çš„ä¸åŒæ–¹é¢ã€‚è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ•´åˆäº†æ£€ç´¢ã€åˆ†ç±»å’Œæ¨ç†æŒ‡æ ‡ã€‚

**Result:** å¯¹13ä¸ªæœ€å…ˆè¿›LLMsè¿›è¡Œçš„é›¶æ ·æœ¬å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨è¯­ä¹‰ã€å…³ç³»å’Œæ•°å­¦ç»´åº¦ä¸Šè¡¨ç°ä¸ä¸€è‡´ï¼Œå½“åœ¨å±‚æ¬¡åŒ–å¤šæ–‡æ¡£ç»“æ„ä¸Šè¿›è¡Œæ¨ç†æ—¶ï¼Œå‡†ç¡®ç‡ä¸‹é™é«˜è¾¾60-90%ã€‚å®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨ç»“æ„åŒ–è´¢åŠ¡æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚

**Conclusion:** ç ”ç©¶ç»“æœæ­ç¤ºäº†ç°ä»£LLMsåœ¨åŸºäºåˆ†ç±»æ³•çš„è´¢åŠ¡æ¨ç†ä¸­çš„ç³»ç»Ÿæ€§å±€é™ï¼Œå¹¶å°†FinAuditingç¡®ç«‹ä¸ºå¼€å‘å¯ä¿¡èµ–ã€ç»“æ„æ„ŸçŸ¥ä¸”ç¬¦åˆç›‘ç®¡è¦æ±‚çš„è´¢åŠ¡æ™ºèƒ½ç³»ç»Ÿçš„åŸºç¡€ã€‚è¯¥åŸºå‡†ä¸ºæœªæ¥æ”¹è¿›LLMsåœ¨ç»“æ„åŒ–è´¢åŠ¡æ–‡æ¡£å¤„ç†èƒ½åŠ›æä¾›äº†é‡è¦å‚è€ƒæ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
The complexity of the Generally Accepted Accounting Principles (GAAP) and the
hierarchical structure of eXtensible Business Reporting Language (XBRL) filings
make financial auditing increasingly difficult to automate and verify. While
large language models (LLMs) have demonstrated strong capabilities in
unstructured text understanding, their ability to reason over structured,
interdependent, and taxonomy-driven financial documents remains largely
unexplored. To fill this gap, we introduce FinAuditing, the first
taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs
on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,
FinAuditing defines three complementary subtasks, FinSM for semantic
consistency, FinRE for relational consistency, and FinMR for numerical
consistency, each targeting a distinct aspect of structured auditing reasoning.
We further propose a unified evaluation framework integrating retrieval,
classification, and reasoning metrics across these subtasks. Extensive
zero-shot experiments on 13 state-of-the-art LLMs reveal that current models
perform inconsistently across semantic, relational, and mathematical
dimensions, with accuracy drops of up to 60-90% when reasoning over
hierarchical multi-document structures. Our findings expose the systematic
limitations of modern LLMs in taxonomy-grounded financial reasoning and
establish FinAuditing as a foundation for developing trustworthy,
structure-aware, and regulation-aligned financial intelligence systems. The
benchmark dataset is available at Hugging Face.


### [44] [A Unified Biomedical Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2510.08902)
*Tengxiao Lv, Ling Luo, Juntao Li, Yanhua Wang, Yuchen Pan, Chao Liu, Yanan Wang, Yan Jiang, Huiyi Lv, Yuanyuan Sun, Jian Wang, Hongfei Lin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡å°†BioNERé‡æ„ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡å¹¶é‡‡ç”¨ç¬¦å·åŒ–æ ‡è®°ç­–ç•¥ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„è·¨è¯­è¨€é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«æ–¹æ³•åœ¨å¤„ç†åµŒå¥—å®ä½“ã€å®ä½“è¾¹ç•Œæ¨¡ç³Šæ€§å’Œè·¨è¯­è¨€æ³›åŒ–æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†åŒ»å­¦ä¿¡æ¯æŠ½å–å’ŒçŸ¥è¯†å‘ç°çš„å‡†ç¡®æ€§ã€‚

**Method:** è¯¥æ–¹æ³•å°†BioNERé‡æ„ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œè®¾è®¡äº†ç¬¦å·åŒ–æ ‡è®°ç­–ç•¥æ¥è”åˆå¤„ç†å¹³é¢å’ŒåµŒå¥—å®ä½“ï¼Œé‡‡ç”¨åŒè¯­è”åˆå¾®è°ƒå¢å¼ºå¤šè¯­è¨€å¤šä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„å®ä½“é€‰æ‹©å™¨æ¥è¿‡æ»¤é”™è¯¯é¢„æµ‹ã€‚

**Result:** åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªæœªè§è¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨è·¨è¯­è¨€é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶åœ¨ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†å¤æ‚å®ä½“ç»“æ„å’Œè·¨è¯­è¨€åœºæ™¯æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Accurate recognition of biomedical named entities is critical for medical
information extraction and knowledge discovery. However, existing methods often
struggle with nested entities, entity boundary ambiguity, and cross-lingual
generalization. In this paper, we propose a unified Biomedical Named Entity
Recognition (BioNER) framework based on Large Language Models (LLMs). We first
reformulate BioNER as a text generation task and design a symbolic tagging
strategy to jointly handle both flat and nested entities with explicit boundary
annotation. To enhance multilingual and multi-task generalization, we perform
bilingual joint fine-tuning across multiple Chinese and English datasets.
Additionally, we introduce a contrastive learning-based entity selector that
filters incorrect or spurious predictions by leveraging boundary-sensitive
positive and negative samples. Experimental results on four benchmark datasets
and two unseen corpora show that our method achieves state-of-the-art
performance and robust zero-shot generalization across languages. The source
codes are freely available at https://github.com/dreamer-tx/LLMNER.


### [45] [CrisiText: A dataset of warning messages for LLM training in emergency communication](https://arxiv.org/abs/2510.09243)
*Giacomo Gonella, Gian Maria Campedelli, Stefano Menini, Marco Guerini*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CrisiTextï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘13ç§å±æœºåœºæ™¯çš„å¤§è§„æ¨¡é¢„è­¦æ¶ˆæ¯ç”Ÿæˆæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡40ä¸‡æ¡é¢„è­¦æ¶ˆæ¯ï¼Œå¹¶ç³»ç»Ÿæ¯”è¾ƒäº†ç›‘ç£å¾®è°ƒã€åå¥½å¯¹é½ã€é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç­‰ä¸åŒè‡ªç„¶è¯­è¨€ç”Ÿæˆæ–¹æ³•åœ¨å±æœºé¢„è­¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åœ¨å±æœºæƒ…å¢ƒä¸‹ä½¿ç”¨NLPæŠ€æœ¯ä»ç„¶æœ‰é™ï¼Œä¸»è¦é›†ä¸­äºåˆ†ç±»ä»»åŠ¡ï¼Œè€Œåˆ©ç”¨è‡ªç„¶è¯­è¨€ç”Ÿæˆæ¶æ„è¿›è¡ŒåŠæ—¶é¢„è­¦æ¶ˆæ¯ç”Ÿæˆçš„å·¨å¤§æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†AIåœ¨è‡ªç„¶ç¾å®³æˆ–æš´åŠ›æ”»å‡»ç­‰ç´§æ€¥æƒ…å†µä¸‹æœ‰æ•ˆä¿æŠ¤æ°‘ä¼—çš„èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†åŒ…å«40å¤šä¸‡æ¡é¢„è­¦æ¶ˆæ¯çš„CrisiTextæ•°æ®é›†ï¼Œè¦†ç›–è¿‘1.8ä¸‡ä¸ªå±æœºæƒ…å¢ƒï¼Œé€šè¿‡ä»ç°æœ‰å±æœºæè¿°åˆ›å»ºäº‹ä»¶é“¾å¹¶ä¸ºæ¯ä¸ªäº‹ä»¶é…å¯¹é¢„è­¦æ¶ˆæ¯ï¼Œéµå¾ªä¸“å®¶æŒ‡å—ç¡®ä¿æœ¯è¯­æ­£ç¡®æ€§å’Œäº‹å®å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¸ºæ¯æ¡æ¶ˆæ¯æä¾›ä¸‰ç§æ¬¡ä¼˜é¢„è­¦ç±»å‹ä»¥æ”¯æŒä¸åŒNLGæ–¹æ³•ç ”ç©¶ã€‚

**Result:** å®éªŒæ¯”è¾ƒäº†ç›‘ç£å¾®è°ƒè®¾ç½®ä¸åå¥½å¯¹é½ã€é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ–¹æ³•çš„æ€§èƒ½ï¼Œè¯„ä¼°äº†æ¨¡å‹åœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå¹¶æµ‹è¯•äº†è‡ªåŠ¨åç¼–è¾‘å™¨çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå±æœºé¢„è­¦æ¶ˆæ¯ç”Ÿæˆæä¾›äº†å…¨é¢çš„åŸºå‡†è¯„ä¼°ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¡«è¡¥äº†å±æœºé¢„è­¦NLGé¢†åŸŸçš„ç©ºç™½ï¼ŒCrisiTextæ•°æ®é›†ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„å±æœºå“åº”ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œè¯æ˜äº†ä¸åŒNLGæ–¹æ³•åœ¨ç´§æ€¥æƒ…å†µä¸‹çš„é€‚ç”¨æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†å’Œæ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Effectively identifying threats and mitigating their potential damage during
crisis situations, such as natural disasters or violent attacks, is paramount
for safeguarding endangered individuals. To tackle these challenges, AI has
been used in assisting humans in emergency situations. Still, the use of NLP
techniques remains limited and mostly focuses on classification tasks. The
significant potential of timely warning message generation using NLG
architectures, however, has been largely overlooked. In this paper we present
CrisiText, the first large-scale dataset for the generation of warning messages
across 13 different types of crisis scenarios. The dataset contains more than
400,000 warning messages (spanning almost 18,000 crisis situations) aimed at
assisting civilians during and after such events. To generate the dataset, we
started from existing crisis descriptions and created chains of events related
to the scenarios. Each event was then paired with a warning message. The
generations follow experts' written guidelines to ensure correct terminology
and factuality of their suggestions. Additionally, each message is accompanied
by three suboptimal warning types to allow for the study of different NLG
approaches. To this end, we conducted a series of experiments comparing
supervised fine-tuning setups with preference alignment, zero-shot, and
few-shot approaches. We further assessed model performance in
out-of-distribution scenarios and evaluated the effectiveness of an automatic
post-editor.


### [46] [CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09266)
*Kaiwen Wei, Xiao Liu, Jie Zhang, Zijian Wang, Ruida Liu, Yuming Yang, Xin Xiao, Xiao Sun, Haoyang Zeng, Changzai Pan, Yidan Zhang, Jiang Zhong, Peijin Wang, Yingchao Feng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CFVBenchåŸºå‡†æµ‹è¯•å’Œè‡ªé€‚åº”è§†è§‰ä¼˜åŒ–æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†åœ¨æ¨¡æ€è¦†ç›–å’Œæ ¼å¼å¤šæ ·æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹ç»†ç²’åº¦è§†è§‰ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†å­˜åœ¨æ¨¡æ€è¦†ç›–ä¸è¶³å’Œæ ¼å¼å¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ï¼Œä¸»è¦å…³æ³¨å•æ¨¡æ€æˆ–æœ‰é™æ¨¡æ€ä»»åŠ¡ï¼Œä»¥åŠç²—ç²’åº¦çš„åœºæ™¯ç†è§£ï¼Œæ— æ³•å……åˆ†è¯„ä¼°æ¨¡å‹åœ¨é•¿æ—¶åºè§†é¢‘ä¸­æ£€ç´¢å’Œæ¨ç†ç»†ç²’åº¦å¤šæ¨¡æ€ä¿¡æ¯çš„èƒ½åŠ›ã€‚

**Method:** æå‡ºäº†CFVBenchå¤§è§„æ¨¡äººå·¥éªŒè¯åŸºå‡†ï¼ŒåŒ…å«599ä¸ªå…¬å¼€è§†é¢‘çš„5360ä¸ªå¼€æ”¾å¼é—®ç­”å¯¹ï¼Œæ¶µç›–å›¾è¡¨å¯†é›†æŠ¥å‘Šã€æ–°é—»å¹¿æ’­å’Œè½¯ä»¶æ•™ç¨‹ç­‰é«˜å¯†åº¦æ ¼å¼é¢†åŸŸï¼›åŒæ—¶è®¾è®¡äº†è‡ªé€‚åº”è§†è§‰ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”å¢åŠ å¸§é‡‡æ ·å¯†åº¦å’Œé€‰æ‹©æ€§è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥å¢å¼ºç»†ç²’åº¦å¤šæ¨¡æ€ç†è§£ã€‚

**Result:** å¯¹7ç§æ£€ç´¢æ–¹æ³•å’Œ14ä¸ªå¸¸ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨æ•æ‰ç¬æ€ä½†å…³é”®çš„ç»†ç²’åº¦å¤šæ¨¡æ€ç»†èŠ‚æ–¹é¢å­˜åœ¨æ˜¾è‘—ç“¶é¢ˆï¼›è‡ªé€‚åº”è§†è§‰ä¼˜åŒ–æ¡†æ¶å®éªŒè¯æ˜èƒ½å¤ŸæŒç»­å¢å¼ºç»†ç²’åº¦å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œå¹¶æå‡æ‰€æœ‰è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰ä¿¡æ¯ç†è§£æ–¹é¢çš„æ ¹æœ¬å±€é™æ€§ï¼Œæå‡ºçš„è‡ªé€‚åº”è§†è§‰ä¼˜åŒ–æ¡†æ¶ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„æ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Multimodal Retrieval-Augmented Generation (MRAG) enables Multimodal Large
Language Models (MLLMs) to generate responses with external multimodal
evidence, and numerous video-based MRAG benchmarks have been proposed to
evaluate model capabilities across retrieval and generation stages. However,
existing benchmarks remain limited in modality coverage and format diversity,
often focusing on single- or limited-modality tasks, or coarse-grained scene
understanding. To address these gaps, we introduce CFVBench, a large-scale,
manually verified benchmark constructed from 599 publicly available videos,
yielding 5,360 open-ended QA pairs. CFVBench spans high-density formats and
domains such as chart-heavy reports, news broadcasts, and software tutorials,
requiring models to retrieve and reason over long temporal video spans while
maintaining fine-grained multimodal information. Using CFVBench, we
systematically evaluate 7 retrieval methods and 14 widely-used MLLMs, revealing
a critical bottleneck: current models (even GPT5 or Gemini) struggle to capture
transient yet essential fine-grained multimodal details. To mitigate this, we
propose Adaptive Visual Refinement (AVR), a simple yet effective framework that
adaptively increases frame sampling density and selectively invokes external
tools when necessary. Experiments show that AVR consistently enhances
fine-grained multimodal comprehension and improves performance across all
evaluated MLLMs


### [47] [One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations](https://arxiv.org/abs/2510.09293)
*Kohei Oda, Po-Min Chuang, Kiyoaki Shirai, Natthawut Kertkeidkachorn*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDualCSEï¼Œä¸€ç§ä¸ºæ¯ä¸ªå¥å­åˆ†é…ä¸¤ä¸ªåµŒå…¥å‘é‡çš„å¥å­åµŒå…¥æ–¹æ³•ï¼Œåˆ†åˆ«è¡¨ç¤ºæ˜¾å¼è¯­ä¹‰å’Œéšå¼è¯­ä¹‰ï¼Œä»¥å…‹æœä¼ ç»Ÿå•å‘é‡è¡¨ç¤ºæ–¹æ³•åœ¨æ•æ‰éšå¼è¯­ä¹‰æ–¹é¢çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿå¥å­åµŒå…¥æ–¹æ³•ä¸ºæ¯ä¸ªå¥å­ä»…åˆ†é…å•ä¸ªå‘é‡ï¼Œå­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰å¥å­ä¸­çš„éšå¼è¯­ä¹‰ï¼Œè¿™é™åˆ¶äº†å¥å­è¡¨ç¤ºçš„è´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚

**Method:** æå‡ºDualCSEæ–¹æ³•ï¼Œä¸ºæ¯ä¸ªå¥å­åˆ†é…ä¸¤ä¸ªå…±å­˜äºå…±äº«ç©ºé—´çš„åµŒå…¥å‘é‡ï¼šä¸€ä¸ªè¡¨ç¤ºæ˜¾å¼è¯­ä¹‰ï¼Œå¦ä¸€ä¸ªè¡¨ç¤ºéšå¼è¯­ä¹‰ï¼Œä½¿å¾—å¯ä»¥æ ¹æ®ç‰¹å®šä»»åŠ¡éœ€æ±‚é€‰æ‹©ç›¸åº”çš„è¯­ä¹‰è¡¨ç¤ºã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜DualCSEèƒ½å¤Ÿæœ‰æ•ˆç¼–ç æ˜¾å¼å’Œéšå¼è¯­ä¹‰ï¼Œå¹¶åœ¨ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­æå‡æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜åŒå‘é‡è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿæ›´å…¨é¢åœ°æ•æ‰å¥å­è¯­ä¹‰ï¼Œä¸ºå¥å­åµŒå…¥ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Sentence embedding methods have made remarkable progress, yet they still
struggle to capture the implicit semantics within sentences. This can be
attributed to the inherent limitations of conventional sentence embedding
methods that assign only a single vector per sentence. To overcome this
limitation, we propose DualCSE, a sentence embedding method that assigns two
embeddings to each sentence: one representing the explicit semantics and the
other representing the implicit semantics. These embeddings coexist in the
shared space, enabling the selection of the desired semantics for specific
purposes such as information retrieval and text classification. Experimental
results demonstrate that DualCSE can effectively encode both explicit and
implicit meanings and improve the performance of the downstream task.


### [48] [The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach](https://arxiv.org/abs/2510.09424)
*Nizar El Ghazal, Antoine CaubriÃ¨re, Valentin Vielzeuf*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿæ¯”è¾ƒäº†ç«¯åˆ°ç«¯å£è¯­å¯¹è¯çŠ¶æ€è·Ÿè¸ªä¸­çš„ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥ï¼Œå‘ç°å®Œæ•´å£è¯­å¯¹è¯å†å²è¾“å…¥å¯è·å¾—æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶åŸºäºæ³¨æ„åŠ›æ± åŒ–çš„å‹ç¼©æ–¹æ³•åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ˜¾è‘—å‡å°ä¸Šä¸‹æ–‡è§„æ¨¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å£è¯­å¯¹è¯çŠ¶æ€è·Ÿè¸ªé¢†åŸŸç¼ºä¹å¯¹ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥çš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å£è¯­å¯¹è¯å†å²ä¿¡æ¯æ¥æå‡Speech-LLMæ¨¡å‹çš„æ€§èƒ½ï¼Œéœ€è¦æ¢ç´¢ä¸åŒä¸Šä¸‹æ–‡è¡¨ç¤ºæ–¹æ³•çš„ä¼˜åŠ£ã€‚

**Method:** ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†ä¸‰ç§ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥ï¼šä¼ ç»Ÿå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼ˆç»“åˆæ–‡æœ¬å†å²å’Œå½“å‰å£è¯­è½®æ¬¡ï¼‰ã€å®Œæ•´å£è¯­å†å²ä»¥åŠåŸºäºæ³¨æ„åŠ›æ± åŒ–å‹ç¼©çš„å£è¯­å†å²æ–¹æ³•ï¼Œåœ¨SpokenWOZè¯­æ–™åº“ä¸Šè¿›è¡Œäº†å¯¹æ¯”å®éªŒã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œæä¾›å®Œæ•´å£è¯­å¯¹è¯ä½œä¸ºè¾“å…¥åœ¨ç›¸ä¼¼è§„æ¨¡æ¨¡å‹ä¸­è·å¾—äº†æœ€é«˜æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šå…ˆå‰æ–¹æ³•ï¼›æ³¨æ„åŠ›æ± åŒ–å‹ç¼©æ–¹æ³•åœ¨ä¿æŒç«äº‰åŠ›çš„å‡†ç¡®ç‡çš„åŒæ—¶æœ‰æ•ˆå‡å°äº†ä¸Šä¸‹æ–‡è§„æ¨¡ã€‚

**Conclusion:** ç ”ç©¶è¯å®æ€§èƒ½æå‡æºäºæ›´æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡åˆ©ç”¨ï¼Œå®Œæ•´å£è¯­å†å²æä¾›äº†æœ€ä¸°å¯Œçš„å¯¹è¯ä¿¡æ¯ï¼Œè€Œå‹ç¼©æ–¹æ³•ä¸ºå®é™…åº”ç”¨æä¾›äº†è®¡ç®—æ•ˆç‡ä¸æ€§èƒ½çš„å¹³è¡¡æ–¹æ¡ˆï¼Œä¸ºå£è¯­å¯¹è¯ç³»ç»Ÿçš„ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥é€‰æ‹©æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
This paper presents a comparative study of context management strategies for
end-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically
evaluate traditional multimodal context (combining text history and spoken
current turn), full spoken history, and compressed spoken history approaches.
Our experiments on the SpokenWOZ corpus demonstrate that providing the full
spoken conversation as input yields the highest performance among models of
similar size, significantly surpassing prior methods. Furthermore, we show that
attention-pooling-based compression of the spoken history offers a strong
trade-off, maintaining competitive accuracy with reduced context size. Detailed
analysis confirms that improvements stem from more effective context
utilization.


### [49] [Multimodal Policy Internalization for Conversational Agents](https://arxiv.org/abs/2510.09474)
*Zhenhailong Wang, Jiateng Liu, Amin Fazel, Ritesh Sarkhel, Xing Fan, Xiang Li, Chenlei Guo, Heng Ji, Ruhi Sarikaya*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€ç­–ç•¥å†…åŒ–ï¼ˆMPIï¼‰ä»»åŠ¡ï¼Œé€šè¿‡å°†æ¨ç†å¯†é›†å‹å¤šæ¨¡æ€ç­–ç•¥å†…åŒ–åˆ°æ¨¡å‹å‚æ•°ä¸­ï¼Œå®ç°æ— éœ€æ¨ç†æ—¶åŒ…å«ç­–ç•¥çš„æ›´å¼ºç­–ç•¥éµå¾ªèƒ½åŠ›ã€‚ä½œè€…å¼€å‘äº†TriMPIä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œåœ¨ç«¯åˆ°ç«¯å‡†ç¡®æ€§ã€æ³›åŒ–æ€§å’ŒæŠ—é—å¿˜æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°ä»£å¯¹è¯ç³»ç»Ÿä¾èµ–é¢„å®šä¹‰ç­–ç•¥æ¥æŒ‡å®šå…ƒæ•°æ®ã€å“åº”é£æ ¼å’Œå·¥å…·ä½¿ç”¨è§„åˆ™ï¼Œéšç€LLMç³»ç»Ÿæ‰©å±•æ”¯æŒå¤šæ ·åŒ–ä¸šåŠ¡å’Œç”¨æˆ·æŸ¥è¯¢ï¼Œè¿™äº›é€šå¸¸ä½œä¸ºä¸Šä¸‹æ–‡æç¤ºå®ç°çš„ç­–ç•¥å˜å¾—æ—¥ç›Šå¤æ‚å†—é•¿ï¼Œå¯¼è‡´å¿ å®éµå¾ªå›°éš¾å¹¶äº§ç”Ÿå¤§é‡å›ºå®šè®¡ç®—æˆæœ¬ã€‚å¤šæ¨¡æ€ä»£ç†ä¸­ç®¡ç†è§†è§‰å’Œå¤šæ¨¡æ€è¡Œä¸ºçš„ç­–ç•¥è‡³å…³é‡è¦ä½†ç ”ç©¶ä¸è¶³ï¼Œç°æœ‰æç¤ºå‹ç¼©å·¥ä½œä¸»è¦ç¼©çŸ­ä»»åŠ¡æ¨¡æ¿å’Œæ¼”ç¤ºï¼Œè€Œç­–ç•¥å¯¹é½ç ”ç©¶ä»…å…³æ³¨åŸºäºæ–‡æœ¬çš„å®‰å…¨è§„åˆ™ã€‚

**Method:** æœ¬æ–‡æå‡ºTriMPIä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆé€šè¿‡æŒç»­é¢„è®­ç»ƒæ³¨å…¥ç­–ç•¥çŸ¥è¯†ï¼Œç„¶åè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæœ€ååº”ç”¨PolicyRolloutâ€”â€”ä¸€ç§GRPOé£æ ¼çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•ï¼Œé€šè¿‡ç­–ç•¥æ„ŸçŸ¥å“åº”å¢å¼ºrolloutsä»¥å®ç°æœ‰åŸºç¡€çš„æ¢ç´¢ã€‚ä½œè€…æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–åˆæˆå’ŒçœŸå®ä¸–ç•Œå†³ç­–åˆ¶å®šä¸å·¥å…·ä½¿ç”¨ä»»åŠ¡ã€‚

**Result:** TriMPIåœ¨ç«¯åˆ°ç«¯å‡†ç¡®æ€§ã€æ³›åŒ–æ€§å’ŒæŠ—é—å¿˜é²æ£’æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚ä½œä¸ºé¦–ä¸ªå¤šæ¨¡æ€ç­–ç•¥å†…åŒ–å·¥ä½œï¼Œä½œè€…æä¾›äº†æ•°æ®é›†ã€è®­ç»ƒæ–¹æ³•å’Œå…¨é¢è¯„ä¼°ï¼Œä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚è¯¥æ–¹æ³•åœ¨æ¨ç†å¯†é›†å‹å¤šæ¨¡æ€ç­–ç•¥å†…åŒ–ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚

**Conclusion:** æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ¢ç´¢äº†å¤šæ¨¡æ€ç­–ç•¥å†…åŒ–é—®é¢˜ï¼Œæå‡ºçš„TriMPIæ¡†æ¶é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒæœ‰æ•ˆè§£å†³äº†ç­–ç•¥å†…åŒ–çš„æ•°æ®å’Œç®—æ³•æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤šæ¨¡æ€ä»£ç†çš„ç­–ç•¥éµå¾ªæä¾›äº†æ–°èŒƒå¼ï¼Œé€šè¿‡å‚æ•°å†…åŒ–ç­–ç•¥çŸ¥è¯†é¿å…äº†æ¨ç†æ—¶çš„å›ºå®šè®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸ºæœªæ¥å¤šæ¨¡æ€ç­–ç•¥å­¦ä¹ ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Modern conversational agents like ChatGPT and Alexa+ rely on predefined
policies specifying metadata, response styles, and tool-usage rules. As these
LLM-based systems expand to support diverse business and user queries, such
policies, often implemented as in-context prompts, are becoming increasingly
complex and lengthy, making faithful adherence difficult and imposing large
fixed computational costs. With the rise of multimodal agents, policies that
govern visual and multimodal behaviors are critical but remain understudied.
Prior prompt-compression work mainly shortens task templates and
demonstrations, while existing policy-alignment studies focus only on
text-based safety rules. We introduce Multimodal Policy Internalization (MPI),
a new task that internalizes reasoning-intensive multimodal policies into model
parameters, enabling stronger policy-following without including the policy
during inference. MPI poses unique data and algorithmic challenges. We build
two datasets spanning synthetic and real-world decision-making and tool-using
tasks and propose TriMPI, a three-stage training framework. TriMPI first
injects policy knowledge via continual pretraining, then performs supervised
finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement
learning extension that augments rollouts with policy-aware responses for
grounded exploration. TriMPI achieves notable gains in end-to-end accuracy,
generalization, and robustness to forgetting. As the first work on multimodal
policy internalization, we provide datasets, training recipes, and
comprehensive evaluations to foster future research. Project page:
https://mikewangwzhl.github.io/TriMPI.


### [50] [Can We Reliably Rank Model Performance across Domains without Labeled Data?](https://arxiv.org/abs/2510.09519)
*Veronica Rammouz, Aaron Gonzalez, Carlos Cruzportillo, Adrian Tan, Nicole Beebe, Anthony Rios*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶åˆ†æäº†æ— æ ‡ç­¾æƒ…å†µä¸‹æ¨¡å‹æ€§èƒ½ä¼°è®¡æ–¹æ³•çš„å¯é æ€§ï¼Œå‘ç°åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é”™è¯¯é¢„æµ‹å™¨åœ¨è·¨åŸŸæ€§èƒ½æ’åºä¸­æ¯”æ¼‚ç§»åŸºå‡†å’Œé›¶æ ·æœ¬æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œæ­ç¤ºäº†æ€§èƒ½å·®å¼‚å¤§å°å’Œé”™è¯¯æ¨¡å¼å¯¹é½æ˜¯å½±å“æ’åºå¯é æ€§çš„å…³é”®å› ç´ ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶æå‡ºäº†åŸºäºæ•°æ®é›†ç›¸ä¼¼æ€§æˆ–é¢„æµ‹æ­£ç¡®æ€§çš„æ¨¡å‹æ€§èƒ½ä¼°è®¡æ–¹æ³•ï¼Œä½†å°šä¸æ¸…æ¥šè¿™äº›ä¼°è®¡åœ¨è·¨åŸŸæƒ…å†µä¸‹ä½•æ—¶èƒ½äº§ç”Ÿå¯é çš„æ€§èƒ½æ’åºï¼Œå› æ­¤éœ€è¦åˆ†æå½±å“æ’åºå¯é æ€§çš„å› ç´ ã€‚

**Method:** é‡‡ç”¨å››æ­¥è¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨å››ç§åŸºç¡€åˆ†ç±»å™¨å’Œå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ä½œä¸ºé”™è¯¯é¢„æµ‹å™¨ï¼Œåœ¨GeoOLIDå’ŒAmazon Reviewsæ•°æ®é›†çš„15ä¸ªé¢†åŸŸä¸Šè¿›è¡Œå®éªŒï¼Œæ¯”è¾ƒåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é”™è¯¯é¢„æµ‹å™¨ä¸æ¼‚ç§»åŸºå‡†å’Œé›¶æ ·æœ¬æ–¹æ³•çš„æ€§èƒ½ã€‚

**Result:** å®éªŒè¡¨æ˜åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é”™è¯¯é¢„æµ‹å™¨ä¸çœŸå®å‡†ç¡®ç‡ä¹‹é—´çš„ç§©ç›¸å…³æ€§æ›´å¼ºä¸”æ›´ä¸€è‡´ï¼Œä¼˜äºæ¼‚ç§»åŸºå‡†å’Œé›¶æ ·æœ¬æ–¹æ³•ï¼Œå¹¶å‘ç°å½“è·¨åŸŸæ€§èƒ½å·®å¼‚è¾ƒå¤§ä¸”é”™è¯¯æ¨¡å‹é¢„æµ‹ä¸åŸºç¡€æ¨¡å‹çœŸå®å¤±è´¥æ¨¡å¼å¯¹é½æ—¶æ’åºæ›´å¯é ã€‚

**Conclusion:** ç ”ç©¶æ˜ç¡®äº†æ€§èƒ½ä¼°è®¡æ–¹æ³•åœ¨è·¨åŸŸæ¨¡å‹è¯„ä¼°ä¸­çš„å¯ä¿¡æ¡ä»¶ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†æŒ‡å¯¼ï¼Œæ­ç¤ºäº†æ€§èƒ½å·®å¼‚å’Œé”™è¯¯æ¨¡å¼å¯¹é½æ˜¯ç¡®ä¿å¯é æ’åºçš„å…³é”®å› ç´ ã€‚

---

#### ğŸ“„ Abstract
Estimating model performance without labels is an important goal for
understanding how NLP models generalize. While prior work has proposed measures
based on dataset similarity or predicted correctness, it remains unclear when
these estimates produce reliable performance rankings across domains. In this
paper, we analyze the factors that affect ranking reliability using a two-step
evaluation setup with four base classifiers and several large language models
as error predictors. Experiments on the GeoOLID and Amazon Reviews datasets,
spanning 15 domains, show that large language model-based error predictors
produce stronger and more consistent rank correlations with true accuracy than
drift-based or zero-shot baselines. Our analysis reveals two key findings:
ranking is more reliable when performance differences across domains are
larger, and when the error model's predictions align with the base model's true
failure patterns. These results clarify when performance estimation methods can
be trusted and provide guidance for their use in cross-domain model evaluation.


### [51] [Hierarchical Indexing with Knowledge Enrichment for Multilingual Video Corpus Retrieval](https://arxiv.org/abs/2510.09553)
*Yu Wang, Tianhao Tan, Yifei Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µæ¡†æ¶ç”¨äºå¤šè¯­è¨€è§†é¢‘è¯­æ–™åº“æ£€ç´¢ï¼Œé€šè¿‡ç»“åˆå¤šè¯­è¨€è¯­ä¹‰ã€é¢†åŸŸæœ¯è¯­å’Œé«˜æ•ˆé•¿æ–‡æœ¬å¤„ç†ï¼Œåœ¨åŒ»å­¦è§†é¢‘é›†åˆä¸­å®ç°äº†å‡†ç¡®ä¸”å¯æ‰©å±•çš„å¤šè¯­è¨€æ£€ç´¢ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚æ ‘ç´¢å¼•å’Œè½»é‡çº§LLMé‡æ’åºï¼Œé¿å…äº†è¯¦å°½çš„äº¤å‰ç¼–ç å™¨è¯„åˆ†åŒæ—¶ä¿æŒäº†å—çº§ç²¾åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç³»ç»Ÿåœ¨å¤„ç†å¤šè¯­è¨€åŒ»å­¦è§†é¢‘æ£€ç´¢æ—¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šè¦ä¹ˆå°†å°æ—¶é•¿çš„è§†é¢‘å‹ç¼©ä¸ºç²—ç³™çš„åµŒå…¥è¡¨ç¤ºï¼Œè¦ä¹ˆè¿›è¡Œç»†ç²’åº¦åŒ¹é…æ—¶äº§ç”Ÿè¿‡é«˜è®¡ç®—æˆæœ¬ã€‚å¤šè¯­è¨€è§†é¢‘è¯­æ–™åº“æ£€ç´¢ä»»åŠ¡éœ€è¦è§£å†³è·¨è¯­è¨€è¾¹ç•Œçš„å¤æ‚å¤šè·³é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸“ä¸šåŒ»å­¦è§†é¢‘é›†åˆä¸­ã€‚

**Method:** æå‡ºå¤šé˜¶æ®µæ¡†æ¶ï¼Œå°†è§†é¢‘å­—å¹•åˆ†å‰²æˆè¯­ä¹‰è¿è´¯çš„å—ï¼Œå¹¶ç”¨ç®€æ´çš„çŸ¥è¯†å›¾è°±äº‹å®è¿›è¡Œä¸°å¯Œï¼Œç»„ç»‡æˆå±‚æ¬¡æ ‘ç»“æ„ã€‚ä½¿ç”¨è¯­è¨€æ— å…³çš„å¤šè¯­è¨€ç¼–ç å™¨ç”ŸæˆèŠ‚ç‚¹åµŒå…¥ï¼ŒæŸ¥è¯¢æ—¶é‡‡ç”¨ç²—åˆ°ç»†çš„æ ‘æœç´¢ç­–ç•¥å‰ªææ— å…³åˆ†æ”¯ï¼Œä»…å¯¹æ’åé å‰çš„å—ä½¿ç”¨è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé‡æ’åºã€‚

**Result:** åœ¨mVCRæµ‹è¯•é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¶ˆèç ”ç©¶è¯å®äº†çŸ¥è¯†å›¾è°±ä¸°å¯Œã€åˆ†å±‚ç´¢å¼•å’Œç›®æ ‡LLMé‡æ’åºçš„äº’è¡¥è´¡çŒ®ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—æé«˜äº†æ£€ç´¢æ•ˆç‡ã€‚

**Conclusion:** è¯¥æ–¹æ³•ä¸ºä¸“ä¸šåŒ»å­¦è§†é¢‘é›†åˆä¸­çš„å¤šè¯­è¨€æ£€ç´¢æä¾›äº†å‡†ç¡®ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†åˆ†å±‚å¤„ç†ä¸è½»é‡çº§é‡æ’åºçš„æœ‰æ•ˆç»“åˆèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²ç²¾åº¦çš„æƒ…å†µä¸‹æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œä¸ºé•¿è§†é¢‘å¤šè¯­è¨€æ£€ç´¢å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Retrieving relevant instructional videos from multilingual medical archives
is crucial for answering complex, multi-hop questions across language
boundaries. However, existing systems either compress hour-long videos into
coarse embeddings or incur prohibitive costs for fine-grained matching. We
tackle the Multilingual Video Corpus Retrieval (mVCR) task in the NLPCC-2025
M4IVQA challenge with a multi-stage framework that integrates multilingual
semantics, domain terminology, and efficient long-form processing. Video
subtitles are divided into semantically coherent chunks, enriched with concise
knowledge-graph (KG) facts, and organized into a hierarchical tree whose node
embeddings are generated by a language-agnostic multilingual encoder. At query
time, the same encoder embeds the input question; a coarse-to-fine tree search
prunes irrelevant branches, and only the top-ranked chunks are re-scored by a
lightweight large language model (LLM). This design avoids exhaustive
cross-encoder scoring while preserving chunk-level precision. Experiments on
the mVCR test set demonstrate state-of-the-art performance, and ablation
studies confirm the complementary contributions of KG enrichment, hierarchical
indexing, and targeted LLM re-ranking. The proposed method offers an accurate
and scalable solution for multilingual retrieval in specialized medical video
collections.


### [52] [AutoPR: Let's Automate Your Academic Promotion!](https://arxiv.org/abs/2510.09558)
*Qiguang Chen, Zheng Yan, Mingda Yang, Libo Qin, Yixin Yuan, Hanjing Li, Jinhao Liu, Yiyan Ji, Dengyun Peng, Jiannan Guan, Mengkang Hu, Yantao Du, Wanxiang Che*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†AutoPRä»»åŠ¡ï¼Œå°†ç ”ç©¶è®ºæ–‡è‡ªåŠ¨è½¬åŒ–ä¸ºé«˜è´¨é‡çš„æ¨å¹¿å†…å®¹ï¼Œå¹¶å¼€å‘äº†PRAgentå¤šæ™ºèƒ½ä½“æ¡†æ¶å’ŒPRBenchåŸºå‡†æµ‹è¯•ï¼Œæ˜¾è‘—æå‡äº†å­¦æœ¯æ¨å¹¿çš„æ•ˆç‡å’Œæ•ˆæœã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€åŒè¡Œè¯„å®¡ç ”ç©¶æ•°é‡çš„æ¿€å¢ï¼Œå­¦è€…ä»¬è¶Šæ¥è¶Šä¾èµ–ç¤¾äº¤å¹³å°è¿›è¡Œè®ºæ–‡å‘ç°ï¼Œè€Œä½œè€…éœ€è¦æŠ•å…¥å¤§é‡ç²¾åŠ›è¿›è¡Œæ¨å¹¿ä»¥ç¡®ä¿å¯è§æ€§å’Œå¼•ç”¨ã€‚ä¸ºäº†ç®€åŒ–è¿™ä¸€è¿‡ç¨‹å¹¶å‡å°‘å¯¹äººåŠ›çš„ä¾èµ–ï¼Œéœ€è¦è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ¥å°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºå‡†ç¡®ã€å¸å¼•äººä¸”åŠæ—¶çš„å…¬å…±å†…å®¹ã€‚

**Method:** æå‡ºäº†AutoPRä»»åŠ¡å’ŒPRAgentå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šä½¿ç”¨å¤šæ¨¡æ€å‡†å¤‡çš„å†…å®¹æå–ã€åä½œåˆæˆç”Ÿæˆç²¾ç‚¼è¾“å‡ºã€ä»¥åŠå¹³å°ç‰¹å®šé€‚é…ä»¥ä¼˜åŒ–è§„èŒƒã€è¯­æ°”å’Œæ ‡ç­¾ã€‚åŒæ—¶å‘å¸ƒäº†PRBenchå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œä»ä¿çœŸåº¦ã€å‚ä¸åº¦å’Œå¯¹é½åº¦ä¸‰ä¸ªç»´åº¦è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ã€‚

**Result:** ä¸ç›´æ¥LLMæµæ°´çº¿ç›¸æ¯”ï¼ŒPRAgentåœ¨PRBenchä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬æ€»è§‚çœ‹æ—¶é—´å¢åŠ 604%ã€ç‚¹èµæ•°å¢é•¿438%ï¼Œæ•´ä½“å‚ä¸åº¦è‡³å°‘æå‡2.9å€ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜å¹³å°å»ºæ¨¡å’Œå®šå‘æ¨å¹¿å¯¹è¿™äº›å¢ç›Šè´¡çŒ®æœ€å¤§ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜AutoPRæ˜¯ä¸€ä¸ªå¯å¤„ç†ã€å¯è¡¡é‡çš„ç ”ç©¶é—®é¢˜ï¼Œå¹¶ä¸ºå¯æ‰©å±•ã€æœ‰å½±å“åŠ›çš„è‡ªåŠ¨åŒ–å­¦æœ¯äº¤æµæä¾›äº†è·¯çº¿å›¾ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜å­¦æœ¯å†…å®¹çš„ä¼ æ’­æ•ˆæœï¼Œå‡å°‘ä½œè€…æ¨å¹¿è´Ÿæ‹…ï¼Œä¿ƒè¿›ç ”ç©¶æˆæœçš„å¹¿æ³›ä¼ æ’­ã€‚

---

#### ğŸ“„ Abstract
As the volume of peer-reviewed research surges, scholars increasingly rely on
social platforms for discovery, while authors invest considerable effort in
promoting their work to ensure visibility and citations. To streamline this
process and reduce the reliance on human effort, we introduce Automatic
Promotion (AutoPR), a novel task that transforms research papers into accurate,
engaging, and timely public content. To enable rigorous evaluation, we release
PRBench, a multimodal benchmark that links 512 peer-reviewed articles to
high-quality promotional posts, assessing systems along three axes: Fidelity
(accuracy and tone), Engagement (audience targeting and appeal), and Alignment
(timing and channel optimization). We also introduce PRAgent, a multi-agent
framework that automates AutoPR in three stages: content extraction with
multimodal preparation, collaborative synthesis for polished outputs, and
platform-specific adaptation to optimize norms, tone, and tagging for maximum
reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates
substantial improvements, including a 604% increase in total watch time, a 438%
rise in likes, and at least a 2.9x boost in overall engagement. Ablation
studies show that platform modeling and targeted promotion contribute the most
to these gains. Our results position AutoPR as a tractable, measurable research
problem and provide a roadmap for scalable, impactful automated scholarly
communication.


### [53] [Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation](https://arxiv.org/abs/2510.09599)
*Sondos Mahmoud Bsharat, Zhiqiang Shen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPrompting Test-Time Scaling (P-TTS)ï¼Œä¸€ç§é€šè¿‡æµ‹è¯•æ—¶æ•°æ®å¢å¼ºæ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¾®è°ƒæ–¹æ³•ï¼Œä»…éœ€90ä¸ªæ‰‹åŠ¨é€‰æ‹©çš„æ¨ç†å®ä¾‹å³å¯æ˜¾è‘—æå‡æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨æä¾›æ€ç»´é“¾ç¤ºä¾‹æ—¶å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æ„å»ºå¤§è§„æ¨¡æ¨ç†æ•°æ®é›†éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨å’Œè®¡ç®—èµ„æºï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨èµ„æºå—é™æˆ–å¿«é€Ÿæ¼”åŒ–é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚

**Method:** P-TTSé‡‡ç”¨æµ‹è¯•æ—¶æ•°æ®å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°å˜åŒ–æŒ‡ä»¤æç¤ºå¼ºåº¦æ¥åˆæˆå¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ä¸Šä¸‹æ–‡ï¼Œä»…ä½¿ç”¨90ä¸ªæ‰‹åŠ¨é€‰æ‹©çš„æ¨ç†å®ä¾‹ï¼Œç„¶åå¯¹å„ç§è§„æ¨¡çš„Qwen-2.5æ¨¡å‹åœ¨P-TTSæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚

**Result:** åœ¨AIME2024ã€AIME2025ã€MATH500å’ŒGPQA-Diamondç­‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒP-TTS-7Bå’Œ32Bæ¨¡å‹æ˜¾è‘—è¶…è¶Šäº†S1å’ŒS1.1ç­‰ç«äº‰åŸºçº¿ï¼Œåœ¨AIME'24ä¸Šåˆ†åˆ«è·å¾—+26.66%å’Œ+30.00%çš„ç»å¯¹å‡†ç¡®ç‡æå‡ï¼Œåœ¨AIME'25ä¸Šåˆ†åˆ«è·å¾—+13.34%å’Œ+6.67%çš„æå‡ï¼ŒåŒæ—¶åœ¨é›¶æ ·æœ¬æ³›åŒ–æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚

**Conclusion:** æµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥æœ‰æ•ˆæ¢ç´¢äº†æ¨ç†æ¨¡å¼çš„æ½œåœ¨ç©ºé—´ï¼Œä»¥æœ€å°çš„æ ‡æ³¨å¼€é”€æ”¾å¤§äº†å¤§è¯­è¨€æ¨¡å‹çš„è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œä¸ºèµ„æºå—é™æˆ–å¿«é€Ÿæ¼”åŒ–é¢†åŸŸæä¾›äº†ä¸€ç§å®ç”¨ã€ä½æˆæœ¬çš„LLMæ¨ç†æ¿€å‘æ–¹æ³•ã€‚

---

#### ğŸ“„ Abstract
Large language models (LLMs) have demonstrated impressive reasoning
capabilities when provided with chain-of-thought exemplars, but curating large
reasoning datasets remains laborious and resource-intensive. In this work, we
introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective
inference-time data augmentation strategy for enhancing LLM reasoning through
finetuning. Rather than collecting thousands or even millions of examples,
P-TTS leverages a small pool of only 90 manually selected reasoning instances
and systematically varies exemplar augmentation through principled instruction
prompting intensities at test time to synthesize diverse reasoning trajectory
contexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.
Across a suite of mathematical reasoning AIME2024 & 25, MATH500, and
GPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive
baselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of
+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);
P-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and
+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better
performance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances
zero-shot generalization accuracy on out-of-domain reasoning benchmarks of
Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our
analysis suggests that test-time scaling effectively explores the latent space
of reasoning patterns, amplifying LLM problem-solving with minimal annotation
overhead, and further unlocking the reasoning potential and capabilities of
LLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit
LLM reasoning in resource-constrained or rapidly evolving domains.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation](https://arxiv.org/abs/2510.08713)
*Yifei Dong, Fengyi Wu, Guangyu Chen, Zhi-Qi Cheng, Qiyu Hu, Yuxuan Zhou, Jingdong Sun, Jun-Yan He, Qi Dai, Alexander G Hauptmann*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºUniWMï¼Œä¸€ç§ç»Ÿä¸€çš„å†…å­˜å¢å¼ºä¸–ç•Œæ¨¡å‹ï¼Œå°†è‡ªæˆ‘ä¸­å¿ƒè§†è§‰é¢„è§å’Œå¯¼èˆªè§„åˆ’é›†æˆåˆ°å•ä¸€å¤šæ¨¡æ€è‡ªå›å½’éª¨å¹²ä¸­ï¼Œæ˜¾è‘—æå‡äº†å…·èº«å¯¼èˆªçš„æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æœ€å…ˆè¿›çš„å…·èº«å¯¼èˆªæ–¹æ³•é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œå°†å¯¼èˆªè§„åˆ’ä¸è§†è§‰ä¸–ç•Œå»ºæ¨¡åˆ†ç¦»ï¼Œå¯¼è‡´çŠ¶æ€-åŠ¨ä½œä¸å¯¹é½ä»¥åŠåœ¨æ–°é¢–æˆ–åŠ¨æ€åœºæ™¯ä¸­çš„æœ‰é™é€‚åº”æ€§ã€‚è¿™ç§åˆ†ç¦»æ¶æ„å­˜åœ¨æ ¹æœ¬æ€§é™åˆ¶ï¼Œéœ€è¦ç»Ÿä¸€æ¡†æ¶æ¥ç¡®ä¿é¢„æµ‹ä¸æ§åˆ¶ä¹‹é—´çš„ç´§å¯†å¯¹é½ã€‚

**Method:** UniWMé‡‡ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€è‡ªå›å½’éª¨å¹²ç½‘ç»œï¼Œå°†è‡ªæˆ‘ä¸­å¿ƒè§†è§‰é¢„è§å’Œè§„åˆ’é›†æˆåˆ°å•ä¸€æ¡†æ¶ä¸­ï¼Œé€šè¿‡åˆ†å±‚è®°å¿†æœºåˆ¶æ•´åˆè¯¦ç»†çš„çŸ­æœŸæ„ŸçŸ¥çº¿ç´¢ä¸é•¿æœŸè½¨è¿¹ä¸Šä¸‹æ–‡ï¼Œå®ç°åœ¨æ‰©å±•è§†é‡ä¸Šçš„ç¨³å®šè¿è´¯æ¨ç†ã€‚è¯¥æ–¹æ³•æ˜ç¡®åœ°å°†åŠ¨ä½œå†³ç­–å»ºç«‹åœ¨è§†è§‰æƒ³è±¡çš„æœªæ¥çŠ¶æ€åŸºç¡€ä¸Šã€‚

**Result:** åœ¨å››ä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ï¼ˆGo Stanfordã€ReConã€SCANDã€HuRoNï¼‰ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUniWMå°†å¯¼èˆªæˆåŠŸç‡æé«˜äº†é«˜è¾¾30%ï¼Œæ˜¾è‘—å‡å°‘äº†è½¨è¿¹è¯¯å·®ï¼Œå¹¶åœ¨æœªè§è¿‡çš„TartanDriveæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** UniWMä»£è¡¨äº†å‘ç»Ÿä¸€ã€æƒ³è±¡åŠ›é©±åŠ¨çš„å…·èº«å¯¼èˆªçš„åŸåˆ™æ€§è¿›å±•ï¼Œè¯æ˜äº†å°†è§†è§‰é¢„è§ä¸è§„åˆ’ç´§å¯†é›†æˆåœ¨å•ä¸€æ¡†æ¶ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´é²æ£’å’Œé€šç”¨çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Enabling embodied agents to effectively imagine future states is critical for
robust and generalizable visual navigation. Current state-of-the-art
approaches, however, adopt modular architectures that separate navigation
planning from visual world modeling, leading to state-action misalignment and
limited adaptability in novel or dynamic scenarios. To overcome this
fundamental limitation, we propose UniWM, a unified, memory-augmented world
model integrating egocentric visual foresight and planning within a single
multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly
grounds action decisions in visually imagined outcomes, ensuring tight
alignment between prediction and control. A hierarchical memory mechanism
further integrates detailed short-term perceptual cues with longer-term
trajectory context, enabling stable, coherent reasoning over extended horizons.
Extensive experiments across four challenging benchmarks (Go Stanford, ReCon,
SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success
rates by up to 30%, significantly reduces trajectory errors compared to strong
baselines, and exhibits impressive zero-shot generalization on the unseen
TartanDrive dataset. These results highlight UniWM as a principled step toward
unified, imagination-driven embodied navigation.


### [55] [LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition](https://arxiv.org/abs/2510.08928)
*Yushuo Zheng, Zicheng Zhang, Xiongkuo Min, Huiyu Duan, Guangtao Zhai*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†LM Fight Arenaæ¡†æ¶ï¼Œé€šè¿‡åœ¨æ ¼æ–—æ¸¸æˆã€ŠçœŸäººå¿«æ‰“IIã€‹ä¸­è®©å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç›¸äº’å¯¹æŠ—ï¼Œè¯„ä¼°å…¶åœ¨å®æ—¶å¯¹æŠ—ç¯å¢ƒä¸­çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨åŠ¨æ€äº¤äº’è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åŸºå‡†æµ‹è¯•å¾€å¾€æ— æ³•æ•æ‰å…¶åœ¨å®æ—¶å¯¹æŠ—ç¯å¢ƒä¸­çš„çœŸå®è¡¨ç°ï¼Œå­˜åœ¨è¯„ä¼°é™æ€åŒ–ã€ç¼ºä¹äº¤äº’æ€§çš„å±€é™æ€§ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿè¯„ä¼°åŠ¨æ€æˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†ã€‚

**Method:** æå‡ºäº†LM Fight Arenaæ¡†æ¶ï¼Œå°†å…­ä¸ªé¢†å…ˆçš„å¼€æºå’Œé—­æºæ¨¡å‹ç½®äºã€ŠçœŸäººå¿«æ‰“IIã€‹æ¸¸æˆç¯å¢ƒä¸­ç›¸äº’å¯¹æŠ—ï¼Œé€šè¿‡è§£ææ¸¸æˆç”»é¢å’ŒçŠ¶æ€æ•°æ®æ¥é€‰æ‹©åŠ¨ä½œï¼Œé‡‡ç”¨ç›¸åŒè§’è‰²æ§åˆ¶ç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼Œå®ç°äº†å…¨è‡ªåŠ¨ã€å¯å¤ç°çš„å®¢è§‚è¯„ä¼°ã€‚

**Result:** è¯¥æ¡†æ¶åœ¨å—æ§é”¦æ ‡èµ›ä¸­æµ‹è¯•äº†å¤šä¸ªé¢†å…ˆæ¨¡å‹ï¼Œæä¾›äº†å¯¹æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­æˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°ï¼Œç›¸æ¯”é™æ€è¯„ä¼°æ›´èƒ½åæ˜ æ¨¡å‹åœ¨å®æ—¶å¯¹æŠ—åœºæ™¯ä¸­çš„çœŸå®æ€§èƒ½ã€‚

**Conclusion:** LM Fight Arenaå¼•å…¥äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä¸”å¼•äººå…¥èƒœçš„åŸºå‡†æµ‹è¯•ï¼Œå¼¥åˆäº†AIè¯„ä¼°ä¸äº¤äº’å¨±ä¹ä¹‹é—´çš„å·®è·ï¼Œä¸ºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›æä¾›äº†æ–°èŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Existing benchmarks for large multimodal models (LMMs) often fail to capture
their performance in real-time, adversarial environments. We introduce LM Fight
Arena (Large Model Fight Arena), a novel framework that evaluates LMMs by
pitting them against each other in the classic fighting game Mortal Kombat II,
a task requiring rapid visual understanding and tactical, sequential
decision-making. In a controlled tournament, we test six leading open- and
closed-source models, where each agent operates controlling the same character
to ensure a fair comparison. The models are prompted to interpret game frames
and state data to select their next actions. Unlike static evaluations, LM
Fight Arena provides a fully automated, reproducible, and objective assessment
of an LMM's strategic reasoning capabilities in a dynamic setting. This work
introduces a challenging and engaging benchmark that bridges the gap between AI
evaluation and interactive entertainment.


### [56] [FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation](https://arxiv.org/abs/2510.08945)
*Samuel Hildebrand, Curtis Taylor, Sean Oesch, James M Ghawaly Jr, Amir Sadovnik, Ryan Shivers, Brandon Schreiber, Kevin Kurian*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“çš„å¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å«93ä¸ªäººå·¥æ„å»ºçš„é—®é¢˜æ•°æ®é›†ã€çŸ­è¯­çº§å¬å›æŒ‡æ ‡å’Œæœ€è¿‘é‚»åµŒå…¥åˆ†ç±»å™¨ï¼Œå‘ç°é—­æºç®¡é“åœ¨æ­£ç¡®æ€§å’Œå¹»è§‰æ£€æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºå¼€æºç®¡é“ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨æ£€ç´¢ç­‰ç‰¹å®šæ–¹é¢ï¼Œç¼ºä¹å¯¹RAGç®¡é“æ•´ä½“èƒ½åŠ›çš„è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆæ–‡æœ¬ã€è¡¨æ ¼ã€å›¾åƒï¼‰å’Œè·¨æ–‡æ¡£ä¿¡æ¯çš„èƒ½åŠ›ï¼Œéœ€è¦å¼€å‘ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚

**Method:** æ„å»ºäº†åŒ…å«93ä¸ªå¤šæ¨¡æ€é—®é¢˜çš„äººå·¥æ•°æ®é›†ï¼Œå¼€å‘äº†çŸ­è¯­çº§å¬å›æ­£ç¡®æ€§æŒ‡æ ‡å’ŒåŸºäºæœ€è¿‘é‚»åµŒå…¥çš„å¹»è§‰åˆ†ç±»å™¨ï¼Œå¯¹2ä¸ªå¼€æºæ£€ç´¢æœºåˆ¶å’Œ4ä¸ªé—­æºåŸºç¡€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ï¼Œå¹¶è¿›è¡Œäº†ç¬¬ä¸‰æ–¹äººå·¥è¯„ä¼°éªŒè¯æŒ‡æ ‡ä¸€è‡´æ€§ã€‚

**Result:** é—­æºç®¡é“åœ¨æ­£ç¡®æ€§å’Œå¹»è§‰æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºå¼€æºç®¡é“ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¾èµ–å¤šæ¨¡æ€å’Œè·¨æ–‡æ¡£ä¿¡æ¯çš„é—®é¢˜ä¸Šæ€§èƒ½å·®è·æ›´å¤§ï¼Œäººå·¥è¯„ä¼°æ˜¾ç¤ºæ­£ç¡®æ€§æŒ‡æ ‡å¹³å‡ä¸€è‡´æ€§ä¸º4.62ï¼Œå¹»è§‰æ£€æµ‹ä¸º4.53ï¼ˆ5åˆ†åˆ¶ï¼‰ã€‚

**Conclusion:** è¯¥åŸºå‡†ä¸ºRAGç³»ç»Ÿæä¾›äº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ­ç¤ºäº†é—­æºæ¨¡å‹åœ¨å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†ä¸Šçš„ä¼˜åŠ¿ï¼Œæå‡ºçš„è¯„ä¼°æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼Œä¸ºæœªæ¥RAGç³»ç»Ÿå¼€å‘æä¾›äº†é‡è¦åŸºå‡†å’Œæ–¹å‘æŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
Retrieval-augmented generation (RAG) has emerged as a promising paradigm for
improving factual accuracy in large language models (LLMs). We introduce a
benchmark designed to evaluate RAG pipelines as a whole, evaluating a
pipeline's ability to ingest, retrieve, and reason about several modalities of
information, differentiating it from existing benchmarks that focus on
particular aspects such as retrieval. We present (1) a small, human-created
dataset of 93 questions designed to evaluate a pipeline's ability to ingest
textual data, tables, images, and data spread across these modalities in one or
more documents; (2) a phrase-level recall metric for correctness; (3) a
nearest-neighbor embedding classifier to identify potential pipeline
hallucinations; (4) a comparative evaluation of 2 pipelines built with
open-source retrieval mechanisms and 4 closed-source foundation models; and (5)
a third-party human evaluation of the alignment of our correctness and
hallucination metrics. We find that closed-source pipelines significantly
outperform open-source pipelines in both correctness and hallucination metrics,
with wider performance gaps in questions relying on multimodal and
cross-document information. Human evaluation of our metrics showed average
agreement of 4.62 for correctness and 4.53 for hallucination detection on a 1-5
Likert scale (5 indicating "strongly agree").


### [57] [Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging](https://arxiv.org/abs/2510.08987)
*Qixiang Yin, Huanjin Yao, Jianghao Chen, Jiaxing Huang, Zhicheng Zhao, Fei Su*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºTiny-R1Vï¼Œä¸€ç§è½»é‡çº§3Bå‚æ•°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ³•å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´é«˜çš„å‡†ç¡®ç‡ï¼Œç»Ÿä¸€äº†å¤šä»»åŠ¡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†åœ¨æ¨ç†æ•ˆç‡æ–¹é¢é¢ä¸´æ¨¡å‹è§„æ¨¡å¤§ã€è¿‡åº¦æ€è€ƒä»¥åŠåœ¨è½»é‡çº§åœºæ™¯ä¸‹å‡†ç¡®ç‡å—æŸç­‰æŒ‘æˆ˜ï¼Œè€Œé’ˆå¯¹è½»é‡çº§MLLMsæ¨ç†èƒ½åŠ›çš„ç ”ç©¶ç›¸å½“ç¼ºä¹ã€‚

**Method:** Tiny-R1Vé‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ³•ï¼šç¬¬ä¸€é˜¶æ®µå¼•å…¥é•¿åº¦æ„ŸçŸ¥ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆLIPOï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç»„å†…å“åº”ä¼˜åŠ¿æ¥ä¼˜å…ˆé€‰æ‹©ç®€æ´é«˜è´¨é‡å“åº”ï¼›ç¬¬äºŒé˜¶æ®µæå‡ºè‡ªé€‚åº”æ¨¡å‹èåˆï¼ˆAMMï¼‰ï¼Œé€šè¿‡æ¢¯åº¦æŠ•å½±æ­£åˆ™åŒ–æŸå¤±å‡½æ•°è‡ªé€‚åº”è°ƒæ•´ä»»åŠ¡å‘é‡æƒé‡å¹¶ä¼˜åŒ–åˆå¹¶å‘é‡ã€‚

**Result:** åœ¨æ¶µç›–æ•°å­¦ã€ç»“æ„åŒ–æ•°æ®ã€OCRå’Œé€šç”¨èƒ½åŠ›çš„åä¸ªå¹¿æ³›ä½¿ç”¨çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTiny-R1Vå±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½¿è½»é‡çº§æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šæ ·åŒ–å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡åˆ›æ–°çš„ä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ³•ï¼Œè½»é‡çº§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå®ç°é«˜æ•ˆæ¨ç†å’Œå‡†ç¡®æ€§èƒ½çš„ç»Ÿä¸€ï¼Œä¸ºè½»é‡çº§MLLMsçš„å‘å±•æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„å’Œè§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Although Multimodal Large Language Models (MLLMs) have demonstrated
remarkable capabilities across diverse tasks, they encounter numerous
challenges in terms of reasoning efficiency, such as large model size,
overthinking, and compromised accuracy in lightweight scenarios. However,
research on the reasoning capabilities of lightweight MLLMs is quite lacking.
To this end, we propose Tiny-R1V, a novel lightweight 3B model that achieves
faster inference and higher accuracy via a two-stage optimization, while
unifying multimodal reasoning across multiple tasks and using fewer tokens. In
the first stage, Tiny-R1V introduces Length-Informed Relative Policy
Optimization (LIPO), a novel reinforcement learning method, to train each
reasoning model. The LIPO is designed to dynamically adjusts advantages of
responses within groups, that is, by prioritizing concise yet high-quality
responses to encourage the generation of shorter and more accurate response. In
the second stage, we propose Adaptive Model Merging (AMM), a training-free
model merging method that merges multiple specialist models into a unified
architecture. Specifically, AMM adaptively adjusts the weights of task vectors
and robustly optimizes the merged vectors via a novel gradient projection
regularization loss function, thus mitigating redundant conflicts between them.
Extensive evaluations on ten widely-used reasoning benchmarks covering
mathematics, structured data (charts, tables, documents), OCR, and general
capabilities showcase the superior performance of Tiny-R1V, enabling
lightweight models to excel in diverse multimodal reasoning tasks.


### [58] [OSCAR: Orthogonal Stochastic Control for Alignment-Respecting Diversity in Flow Matching](https://arxiv.org/abs/2510.09060)
*Jingxuan Wu, Zhenglin Wan, Xingrui Yu, Yuzhe Yang, Bo An, Ivor Tsang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶æ§åˆ¶æœºåˆ¶ï¼Œé€šè¿‡ç‰¹å¾ç©ºé—´ç›®æ ‡å’Œæ­£äº¤éšæœºæ‰°åŠ¨ä½¿åŸºäºæµçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å…·å¤‡å¤šæ ·æ€§æ„ŸçŸ¥èƒ½åŠ›ï¼Œåœ¨ä¿æŒå›¾åƒè´¨é‡å’Œæç¤ºå¯¹é½çš„åŒæ—¶æ˜¾è‘—æå‡ç”Ÿæˆå¤šæ ·æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŸºäºæµçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹éµå¾ªç¡®å®šæ€§è½¨è¿¹ï¼Œè¿«ä½¿ç”¨æˆ·é‡å¤é‡‡æ ·ä»¥å‘ç°å¤šæ ·æ¨¡å¼ï¼Œè¿™ä¸€è¿‡ç¨‹æˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹åœ¨æ¨ç†æ—¶æœ‰æ•ˆæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§çš„æœºåˆ¶ã€‚

**Method:** è¯¥æ–¹æ³•é€šè¿‡ç‰¹å¾ç©ºé—´ç›®æ ‡åŒæ—¶ä¿ƒè¿›è½¨è¿¹é—´çš„æ¨ªå‘æ‰©å±•ï¼Œå¹¶å¼•å…¥æ—¶é—´è°ƒåº¦çš„éšæœºæ‰°åŠ¨æ¥é‡æ–°å¼•å…¥ä¸ç¡®å®šæ€§ï¼Œå…³é”®åˆ›æ–°åœ¨äºå°†æ‰°åŠ¨æŠ•å½±ä¸ºä¸ç”Ÿæˆæµæ­£äº¤çš„å‡ ä½•çº¦æŸï¼Œä»è€Œåœ¨ä¸é™ä½å›¾åƒç»†èŠ‚æˆ–æç¤ºä¿çœŸåº¦çš„æƒ…å†µä¸‹å¢å¼ºå˜åŒ–ã€‚

**Result:** åœ¨å›ºå®šé‡‡æ ·é¢„ç®—ä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒè®¾ç½®ä¸­æŒç»­æ”¹è¿›Vendi Scoreå’ŒBrisqueç­‰å¤šæ ·æ€§æŒ‡æ ‡ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡å’Œå¯¹é½åº¦ï¼Œä¼˜äºå¼ºåŸºçº¿æ–¹æ³•ã€‚

**Conclusion:** è¯¥æ–¹æ³•ç†è®ºä¸Šè¢«è¯æ˜èƒ½å•è°ƒå¢åŠ ä½“ç§¯ä»£ç†ï¼ŒåŒæ—¶ç”±äºå‡ ä½•çº¦æŸè¿‘ä¼¼ä¿æŒè¾¹ç¼˜åˆ†å¸ƒï¼Œä¸ºç”Ÿæˆè´¨é‡çš„é²æ£’ä¿æŒæä¾›äº†åŸç†æ€§è§£é‡Šï¼Œä¸ºæµåŒ¹é…æ±‚è§£å™¨æä¾›äº†æ— éœ€é‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹åŸºç¡€é‡‡æ ·å™¨çš„å¤šæ ·æ€§å¢å¼ºæ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Flow-based text-to-image models follow deterministic trajectories, forcing
users to repeatedly sample to discover diverse modes, which is a costly and
inefficient process. We present a training-free, inference-time control
mechanism that makes the flow itself diversity-aware. Our method simultaneously
encourages lateral spread among trajectories via a feature-space objective and
reintroduces uncertainty through a time-scheduled stochastic perturbation.
Crucially, this perturbation is projected to be orthogonal to the generation
flow, a geometric constraint that allows it to boost variation without
degrading image details or prompt fidelity. Our procedure requires no
retraining or modification to the base sampler and is compatible with common
flow-matching solvers. Theoretically, our method is shown to monotonically
increase a volume surrogate while, due to its geometric constraints,
approximately preserving the marginal distribution. This provides a principled
explanation for why generation quality is robustly maintained. Empirically,
across multiple text-to-image settings under fixed sampling budgets, our method
consistently improves diversity metrics such as the Vendi Score and Brisque
over strong baselines, while upholding image quality and alignment.


### [59] [Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges](https://arxiv.org/abs/2510.09404)
*Christian Bluethgen, Dave Van Veen, Daniel Truhn, Jakob Nikolas Kather, Michael Moor, Malgorzata Polacin, Akshay Chaudhari, Thomas Frauenfelder, Curtis P. Langlotz, Michael Krauthammer, Farhad Nooralahzadeh*

#### ğŸ§© TL;DR
æœ¬ç»¼è¿°æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä»£ç†ç³»ç»Ÿåœ¨æ”¾å°„å­¦ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹åˆ†æäº†å¦‚ä½•é€šè¿‡å¤–éƒ¨å·¥å…·å’Œåé¦ˆæœºåˆ¶å¢å¼ºLLMèƒ½åŠ›ä»¥æ”¯æŒå¤æ‚å¤šæ­¥éª¤å·¥ä½œæµï¼Œå¹¶è®¨è®ºäº†å…³é”®åº”ç”¨ã€è¯„ä¼°æ–¹æ³•å’ŒæŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ”¾å°„å­¦ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦åº”ç”¨äºä¿¡æ¯æå–å’ŒæŠ¥å‘Šæ€»ç»“ç­‰ç‹¬ç«‹ä»»åŠ¡ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å…¶åœ¨å¤æ‚å¤šæ­¥éª¤å·¥ä½œæµä¸­çš„æ½œåŠ›ï¼Œå…¶ä¸­å†³ç­–ä¾èµ–äºæ¥è‡ªå¤šä¸ªä¿¡æ¯æºçš„åŠ¨æ€ä¸Šä¸‹æ–‡ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿæ•´åˆå¤–éƒ¨å·¥å…·å’Œåé¦ˆæœºåˆ¶çš„æ™ºèƒ½ä»£ç†ç³»ç»Ÿã€‚

**Method:** ç ”ç©¶æå‡ºé€šè¿‡ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹é…å¤‡å¤–éƒ¨å·¥å…·å’Œåé¦ˆæœºåˆ¶æ¥æ„å»ºæ™ºèƒ½ä»£ç†ç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€æ•°æ®æµå’Œç¼–æ’å·¥ä½œæµï¼Œæ”¯æŒä»åŠè‡ªåŠ¨åŒ–å·¥ä½œæµåˆ°èƒ½å¤Ÿç®¡ç†å¤æ‚æµç¨‹çš„è‡ªé€‚åº”ä»£ç†çš„è‡ªä¸»æ€§è°±ç³»ã€‚

**Result:** ç»¼è¿°ç³»ç»Ÿæ€§åœ°åˆ†æäº†LLMé©±åŠ¨ä»£ç†ç³»ç»Ÿçš„è®¾è®¡åŸåˆ™ã€å…³é”®åº”ç”¨åœºæ™¯ã€è§„åˆ’ä¸å·¥å…·ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•ï¼Œå¹¶è¯†åˆ«äº†é”™è¯¯çº§è”ã€å·¥å…·ä½¿ç”¨æ•ˆç‡å’ŒåŒ»ç–—ITé›†æˆç­‰ä¸»è¦æŒ‘æˆ˜ã€‚

**Conclusion:** LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†ç³»ç»Ÿåœ¨æ”¾å°„å­¦ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å·¥ä½œæµæ•ˆç‡å’Œé€‚åº”æ€§ï¼Œä½†éœ€è¦è§£å†³é”™è¯¯ä¼ æ’­ã€å·¥å…·é›†æˆå’Œç³»ç»Ÿå¯é æ€§ç­‰å…³é”®æŒ‘æˆ˜æ‰èƒ½å®ç°ä¸´åºŠéƒ¨ç½²ã€‚

---

#### ğŸ“„ Abstract
Building agents, systems that perceive and act upon their environment with a
degree of autonomy, has long been a focus of AI research. This pursuit has
recently become vastly more practical with the emergence of large language
models (LLMs) capable of using natural language to integrate information,
follow instructions, and perform forms of "reasoning" and planning across a
wide range of tasks. With its multimodal data streams and orchestrated
workflows spanning multiple systems, radiology is uniquely suited to benefit
from agents that can adapt to context and automate repetitive yet complex
tasks. In radiology, LLMs and their multimodal variants have already
demonstrated promising performance for individual tasks such as information
extraction and report summarization. However, using LLMs in isolation
underutilizes their potential to support complex, multi-step workflows where
decisions depend on evolving context from multiple information sources.
Equipping LLMs with external tools and feedback mechanisms enables them to
drive systems that exhibit a spectrum of autonomy, ranging from semi-automated
workflows to more adaptive agents capable of managing complex processes. This
review examines the design of such LLM-driven agentic systems, highlights key
applications, discusses evaluation methods for planning and tool use, and
outlines challenges such as error cascades, tool-use efficiency, and health IT
integration.
