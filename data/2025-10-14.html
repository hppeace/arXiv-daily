<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-14.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-injecting-frame-event-complementary-fusion-into-diffusion-for-optical-flow-in-challenging-scenes">[1] <a href="https://arxiv.org/abs/2510.10577">Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes</a></h3>
<p><em>Haonan Wang, Hanyu Zhou, Haoyue Liu, Luxin Yan</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¸§-äº‹ä»¶å¤–è§‚è¾¹ç•Œèåˆå…‰æµä¼°è®¡æ¡†æ¶Diff-ABFlowï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å­¦ä¹ ä»å™ªå£°æµåˆ°æ¸…æ™°æµçš„æ˜ å°„ï¼Œæœ‰æ•ˆè§£å†³äº†é«˜é€Ÿå’Œä½å…‰åœºæ™¯ä¸‹ä¼ ç»Ÿå…‰æµä¼°è®¡æ–¹æ³•å› è¿åŠ¨æ¨¡ç³Šå’Œå…‰ç…§ä¸è¶³å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿå…‰æµä¼°è®¡æ–¹æ³•åœ¨é«˜é€Ÿå’Œä½å…‰åœºæ™¯ä¸‹é¢ä¸´ä¸¥é‡æŒ‘æˆ˜ï¼Œè¿™äº›åœºæ™¯ä¸­çš„è¿åŠ¨æ¨¡ç³Šå’Œå…‰ç…§ä¸è¶³å¯¼è‡´çº¹ç†ç‰¹å¾å‡å¼±ã€å™ªå£°æ”¾å¤§ï¼Œå¹¶ç ´åäº†å¸§ç›¸æœºçš„å¤–è§‚é¥±å’Œåº¦å’Œè¾¹ç•Œå®Œæ•´æ€§ã€‚è™½ç„¶äº‹ä»¶ç›¸æœºèƒ½å¤Ÿæä¾›å¯†é›†çš„è¾¹ç•Œå®Œæ•´æ€§ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•é€šè¿‡ç‰¹å¾èåˆæˆ–åŸŸé€‚åº”å¼•å…¥äº‹ä»¶æ•°æ®åï¼Œå¤–è§‚ç‰¹å¾ä»ç„¶ä¸¥é‡é€€åŒ–ï¼Œè¿™ä¸¥é‡å½±å“äº†åŸºäºè§†è§‰ç‰¹å¾åˆ°è¿åŠ¨åœºæ˜ å°„çš„åˆ¤åˆ«æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„Diff-ABFlowæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¸§-äº‹ä»¶å¤–è§‚è¾¹ç•Œèåˆç­–ç•¥ã€‚æ‰©æ•£æ¨¡å‹å­¦ä¹ ä»å™ªå£°æµåˆ°æ¸…æ™°æµçš„æ˜ å°„è¿‡ç¨‹ï¼Œä¸ä¾èµ–äºé€€åŒ–çš„è§†è§‰ç‰¹å¾ã€‚è¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨å¸§ç›¸æœºæä¾›çš„å¤–è§‚é¥±å’Œåº¦å’Œäº‹ä»¶ç›¸æœºæä¾›çš„è¾¹ç•Œå®Œæ•´æ€§ï¼Œé€šè¿‡æ‰©æ•£è¿‡ç¨‹çš„ç”Ÿæˆèƒ½åŠ›æ¥æ¢å¤é«˜è´¨é‡çš„å…‰æµåœºã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Diff-ABFlowæ¡†æ¶åœ¨é«˜é€Ÿå’Œä½å…‰åœºæ™¯ä¸‹æ˜¾è‘—æå‡äº†å…‰æµä¼°è®¡æ€§èƒ½ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨é€€åŒ–åœºæ™¯ä¸­çš„å±€é™æ€§ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å®ç°äº†å¯¹è¿åŠ¨æ¨¡ç³Šå’Œå…‰ç…§ä¸è¶³æ¡ä»¶ä¸‹å…‰æµåœºçš„å‡†ç¡®æ¢å¤ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨å…‰æµä¼°è®¡ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰ç‰¹å¾é€€åŒ–çš„æŒ‘æˆ˜æ€§åœºæ™¯ä¸­ã€‚é€šè¿‡å°†å¸§ç›¸æœºå’Œäº‹ä»¶ç›¸æœºçš„äº’è¡¥ä¼˜åŠ¿ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„è¿åŠ¨ä¼°è®¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºå¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆå’Œç”Ÿæˆæ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Optical flow estimation has achieved promising results in conventional scenes
but faces challenges in high-speed and low-light scenes, which suffer from
motion blur and insufficient illumination. These conditions lead to weakened
texture and amplified noise and deteriorate the appearance saturation and
boundary completeness of frame cameras, which are necessary for motion feature
matching. In degraded scenes, the frame camera provides dense appearance
saturation but sparse boundary completeness due to its long imaging time and
low dynamic range. In contrast, the event camera offers sparse appearance
saturation, while its short imaging time and high dynamic range gives rise to
dense boundary completeness. Traditionally, existing methods utilize feature
fusion or domain adaptation to introduce event to improve boundary
completeness. However, the appearance features are still deteriorated, which
severely affects the mostly adopted discriminative models that learn the
mapping from visual features to motion fields and generative models that
generate motion fields based on given visual features. So we introduce
diffusion models that learn the mapping from noising flow to clear flow, which
is not affected by the deteriorated visual features. Therefore, we propose a
novel optical flow estimation framework Diff-ABFlow based on diffusion models
with frame-event appearance-boundary fusion.</p>
<h3 id="2-ev4dgs-novel-view-rendering-of-non-rigid-objects-from-monocular-event-streams">[2] <a href="https://arxiv.org/abs/2510.11717">Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams</a></h3>
<p><em>Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Ev4DGSï¼Œè¿™æ˜¯é¦–ä¸ªä»å•ç›®äº‹ä»¶æµä¸­å®ç°éåˆšæ€§å˜å½¢ç‰©ä½“æ–°è§†è§’æ¸²æŸ“çš„æ–¹æ³•ï¼Œé€šè¿‡å›å½’å¯å˜å½¢3Dé«˜æ–¯æ³¼æº…è¡¨ç¤ºï¼Œä»…ä½¿ç”¨äº‹ä»¶æ•°æ®å³å¯ç”ŸæˆRGBæˆ–ç°åº¦å›¾åƒã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰äº‹ä»¶ç›¸æœºæ–¹æ³•åœ¨å¤„ç†éåˆšæ€§ç‰©ä½“æ—¶éœ€è¦é¢å¤–çš„ç¨€ç–RGBè¾“å…¥ï¼Œè¿™æ„æˆäº†å®é™…åº”ç”¨ä¸­çš„æ˜¾è‘—é™åˆ¶ï¼Œç›®å‰å°šä¸æ¸…æ¥šæ˜¯å¦èƒ½å¤Ÿä»…ä»äº‹ä»¶æµä¸­å­¦ä¹ ç±»ä¼¼æ¨¡å‹ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾æ€§é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é€šè¿‡ä¸¤ç§æœºåˆ¶å›å½’å¯å˜å½¢3Dé«˜æ–¯æ³¼æº…è¡¨ç¤ºï¼šä¸€æ˜¯å°†ä¼°è®¡æ¨¡å‹è¾“å‡ºä¸2Däº‹ä»¶è§‚æµ‹ç©ºé—´å…³è”çš„æŸå¤±å‡½æ•°ï¼ŒäºŒæ˜¯ä»äº‹ä»¶ç”Ÿæˆçš„äºŒå€¼æ©ç ä¸­è®­ç»ƒçš„ç²—ç³™3Då˜å½¢æ¨¡å‹ï¼Œå®ç°äº†ä»…åŸºäºäº‹ä»¶æ•°æ®çš„éåˆšæ€§åœºæ™¯å»ºæ¨¡ã€‚</p>
<p><strong>Result:</strong> åœ¨ç°æœ‰åˆæˆæ•°æ®é›†å’Œæ–°è®°å½•çš„åŒ…å«éåˆšæ€§ç‰©ä½“çš„çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒæ¯”è¾ƒè¡¨æ˜ï¼ŒEv4DGSæ–¹æ³•çš„æœ‰æ•ˆæ€§åŠå…¶ç›¸å¯¹äºå¤šç§å¯åº”ç”¨äºè¯¥è®¾ç½®çš„æœ´ç´ åŸºçº¿çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯æ˜äº†ä»…ä»äº‹ä»¶æµå­¦ä¹ éåˆšæ€§ç‰©ä½“æ–°è§†è§’æ¸²æŸ“æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä¸ºäº‹ä»¶ç›¸æœºåœ¨åŠ¨æ€åœºæ™¯å»ºæ¨¡ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼ŒåŒæ—¶å°†å‘å¸ƒè¯„ä¼°ä¸­ä½¿ç”¨çš„æ¨¡å‹å’Œæ•°æ®é›†ä»¥ä¿ƒè¿›ç›¸å…³ç ”ç©¶ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.</p>
  </article>
</body>
</html>
