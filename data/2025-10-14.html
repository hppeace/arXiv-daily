<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-14.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-injecting-frame-event-complementary-fusion-into-diffusion-for-optical-flow-in-challenging-scenes">[1] <a href="https://arxiv.org/abs/2510.10577">Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes</a></h3>
<p><em>Haonan Wang, Hanyu Zhou, Haoyue Liu, Luxin Yan</em></p>
<h4 id="tldr">🧩 TL;DR</h4>
<p>本文提出了一种基于扩散模型的帧-事件外观边界融合光流估计框架Diff-ABFlow，通过扩散模型学习从噪声流到清晰流的映射，有效解决了高速和低光场景下传统光流估计方法因运动模糊和光照不足导致的性能下降问题。</p>
<hr />
<h4 id="detailed-summary">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 传统光流估计方法在高速和低光场景下面临严重挑战，这些场景中的运动模糊和光照不足导致纹理特征减弱、噪声放大，并破坏了帧相机的外观饱和度和边界完整性。虽然事件相机能够提供密集的边界完整性，但传统方法通过特征融合或域适应引入事件数据后，外观特征仍然严重退化，这严重影响了基于视觉特征到运动场映射的判别模型和生成模型的性能。</p>
<p><strong>Method:</strong> 本文提出了基于扩散模型的Diff-ABFlow框架，该框架采用帧-事件外观边界融合策略。扩散模型学习从噪声流到清晰流的映射过程，不依赖于退化的视觉特征。该方法充分利用帧相机提供的外观饱和度和事件相机提供的边界完整性，通过扩散过程的生成能力来恢复高质量的光流场。</p>
<p><strong>Result:</strong> 实验结果表明，所提出的Diff-ABFlow框架在高速和低光场景下显著提升了光流估计性能。该方法有效克服了传统方法在退化场景中的局限性，通过扩散模型的生成能力实现了对运动模糊和光照不足条件下光流场的准确恢复。</p>
<p><strong>Conclusion:</strong> 该研究证明了扩散模型在光流估计任务中的有效性，特别是在视觉特征退化的挑战性场景中。通过将帧相机和事件相机的互补优势与扩散模型的生成能力相结合，为复杂环境下的运动估计提供了新的解决方案，并为多模态传感器融合和生成模型在计算机视觉中的应用开辟了新方向。</p>
<hr />
<h4 id="abstract">📄 Abstract</h4>
<p>Optical flow estimation has achieved promising results in conventional scenes
but faces challenges in high-speed and low-light scenes, which suffer from
motion blur and insufficient illumination. These conditions lead to weakened
texture and amplified noise and deteriorate the appearance saturation and
boundary completeness of frame cameras, which are necessary for motion feature
matching. In degraded scenes, the frame camera provides dense appearance
saturation but sparse boundary completeness due to its long imaging time and
low dynamic range. In contrast, the event camera offers sparse appearance
saturation, while its short imaging time and high dynamic range gives rise to
dense boundary completeness. Traditionally, existing methods utilize feature
fusion or domain adaptation to introduce event to improve boundary
completeness. However, the appearance features are still deteriorated, which
severely affects the mostly adopted discriminative models that learn the
mapping from visual features to motion fields and generative models that
generate motion fields based on given visual features. So we introduce
diffusion models that learn the mapping from noising flow to clear flow, which
is not affected by the deteriorated visual features. Therefore, we propose a
novel optical flow estimation framework Diff-ABFlow based on diffusion models
with frame-event appearance-boundary fusion.</p>
<h3 id="2-ev4dgs-novel-view-rendering-of-non-rigid-objects-from-monocular-event-streams">[2] <a href="https://arxiv.org/abs/2510.11717">Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams</a></h3>
<p><em>Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik</em></p>
<h4 id="tldr_1">🧩 TL;DR</h4>
<p>本文提出了Ev4DGS，这是首个从单目事件流中实现非刚性变形物体新视角渲染的方法，通过回归可变形3D高斯泼溅表示，仅使用事件数据即可生成RGB或灰度图像。</p>
<hr />
<h4 id="detailed-summary_1">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有事件相机方法在处理非刚性物体时需要额外的稀疏RGB输入，这构成了实际应用中的显著限制，目前尚不清楚是否能够仅从事件流中学习类似模型，本文旨在解决这一具有挑战性的开放性问题。</p>
<p><strong>Method:</strong> 该方法通过两种机制回归可变形3D高斯泼溅表示：一是将估计模型输出与2D事件观测空间关联的损失函数，二是从事件生成的二值掩码中训练的粗糙3D变形模型，实现了仅基于事件数据的非刚性场景建模。</p>
<p><strong>Result:</strong> 在现有合成数据集和新记录的包含非刚性物体的真实数据集上的实验比较表明，Ev4DGS方法的有效性及其相对于多种可应用于该设置的朴素基线的优越性能。</p>
<p><strong>Conclusion:</strong> 研究证明了仅从事件流学习非刚性物体新视角渲染模型的可行性，为事件相机在动态场景建模中的应用开辟了新途径，同时将发布评估中使用的模型和数据集以促进相关研究。</p>
<hr />
<h4 id="abstract_1">📄 Abstract</h4>
<p>Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.</p>
  </article>
</body>
</html>
