<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Hybrid SIFT-SNN for Efficient Anomaly Detection of Traffic Flow-Control Infrastructure](https://arxiv.org/abs/2511.21337)
*Munish Rathee, Boris BaÄiÄ‡, Maryam Doborjeh*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†SIFT-SNNæ¡†æ¶ï¼Œä¸€ç§ç”¨äºäº¤é€šåŸºç¡€è®¾æ–½ç»“æ„å¼‚å¸¸å®æ—¶æ£€æµ‹çš„ä½å»¶è¿Ÿç¥ç»å½¢æ€ä¿¡å·å¤„ç†ç®¡é“ï¼Œé€šè¿‡ç»“åˆå°ºåº¦ä¸å˜ç‰¹å¾å˜æ¢å’Œè„‰å†²ç¥ç»ç½‘ç»œå®ç°äº†92.3%çš„åˆ†ç±»å‡†ç¡®ç‡å’Œ9.5æ¯«ç§’çš„æ¨ç†æ—¶é—´ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºäºCNNçš„æ–¹æ³•åœ¨å®æ—¶ç»“æ„å®‰å…¨ç›‘æµ‹ä¸­çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬é«˜å»¶è¿Ÿã€é«˜åŠŸè€—ä»¥åŠç¼ºä¹å¯è§£é‡Šæ€§ç­‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²åœºæ™¯ä¸‹éœ€è¦ä½å»¶è¿Ÿå’Œé€æ˜å†³ç­–æ”¯æŒçš„éœ€æ±‚ã€‚

**Method:** æå‡ºçš„æ–¹æ³•æ•´åˆäº†å°ºåº¦ä¸å˜ç‰¹å¾å˜æ¢ç”¨äºç©ºé—´ç‰¹å¾ç¼–ç ï¼Œç»“åˆå»¶è¿Ÿé©±åŠ¨çš„è„‰å†²è½¬æ¢å±‚å’Œæ³„æ¼ç§¯åˆ†å‘æ”¾è„‰å†²ç¥ç»ç½‘ç»œè¿›è¡Œåˆ†ç±»ï¼Œåœ¨å¥¥å…‹å…°æµ·æ¸¯å¤§æ¡¥æ•°æ®é›†ä¸Šä½¿ç”¨6000ä¸ªæ ‡è®°å¸§è¿›è¡ŒéªŒè¯ï¼ŒåŒ…å«çœŸå®å’Œåˆæˆå¢å¼ºçš„ä¸å®‰å…¨æ¡ˆä¾‹ã€‚

**Result:** ç³»ç»Ÿå®ç°äº†92.3%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œæ¯å¸§æ¨ç†æ—¶é—´ä¸º9.5æ¯«ç§’ï¼Œç¨€ç–è„‰å†²æ´»åŠ¨ç‡ä¸º8.1%ï¼Œåˆæˆå¢å¼ºæé«˜äº†é²æ£’æ€§ï¼Œä½†åœ¨æœªè§ç°åœºæ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä»éœ€éªŒè¯ã€‚

**Conclusion:** SIFT-SNNæ¡†æ¶åœ¨ä¿æŒç©ºé—´ç‰¹å¾åŸºç¡€çš„åŒæ—¶å¢å¼ºäº†å¯è§£é‡Šæ€§ï¼Œæ”¯æŒé€æ˜å†³ç­–ï¼Œå¹¶åœ¨åµŒå…¥å¼ç¡¬ä»¶ä¸Šé«˜æ•ˆè¿è¡Œï¼Œä½œä¸ºå¯ç§»åŠ¨æ··å‡åœŸæŠ¤æ ç»“æ„å®‰å…¨ç›‘æµ‹çš„å¯æ¨å¹¿æ¡ˆä¾‹ç ”ç©¶ï¼Œå·²åœ¨å…¨çƒ20å¤šä¸ªåŸå¸‚éƒ¨ç½²çš„äº¤é€šæµæ§åˆ¶åŸºç¡€è®¾æ–½ä¸­å¾—åˆ°éªŒè¯ã€‚

---

#### ğŸ“„ Abstract
This paper presents the SIFT-SNN framework, a low-latency neuromorphic signal-processing pipeline for real-time detection of structural anomalies in transport infrastructure. The proposed approach integrates Scale-Invariant Feature Transform (SIFT) for spatial feature encoding with a latency-driven spike conversion layer and a Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) for classification. The Auckland Harbour Bridge dataset is recorded under various weather and lighting conditions, comprising 6,000 labelled frames that include both real and synthetically augmented unsafe cases. The presented system achieves a classification accuracy of 92.3% (+- 0.8%) with a per-frame inference time of 9.5 ms. Achieved sub-10 millisecond latency, combined with sparse spike activity (8.1%), enables real-time, low-power edge deployment. Unlike conventional CNN-based approaches, the hybrid SIFT-SNN pipeline explicitly preserves spatial feature grounding, enhances interpretability, supports transparent decision-making, and operates efficiently on embedded hardware. Although synthetic augmentation improved robustness, generalisation to unseen field conditions remains to be validated. The SIFT-SNN framework is validated through a working prototype deployed on a consumer-grade system and framed as a generalisable case study in structural safety monitoring for movable concrete barriers, which, as a traffic flow-control infrastructure, is deployed in over 20 cities worldwide.


### [2] [EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation](https://arxiv.org/abs/2511.21439)
*Futian Wang, Fan Zhang, Xiao Wang, Mengqi Wang, Dexing Huang, Jin Tang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¶…å›¾å¼•å¯¼çš„æ—¶ç©ºäº‹ä»¶æµè¡¥å…¨æœºåˆ¶ï¼Œé€šè¿‡è¶…å›¾è¿æ¥ä¸åŒæ—¶é—´å’Œç©ºé—´ä½ç½®çš„äº‹ä»¶æ ‡è®°ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ä¼ é€’æ¥è¡¥å…¨ç¨€ç–äº‹ä»¶ï¼Œæœ‰æ•ˆè§£å†³äº†äº‹ä»¶ç›¸æœºæ•°æ®ç©ºé—´ç¨€ç–æ€§å¯¼è‡´çš„æ¬ é‡‡æ ·é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¸»æµäº‹ä»¶è¡¨ç¤ºå­¦ä¹ æ–¹æ³•é€šå¸¸ä½¿ç”¨äº‹ä»¶å¸§ã€ä½“ç´ æˆ–å¼ é‡ä½œä¸ºè¾“å…¥ï¼Œè™½ç„¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†éš¾ä»¥è§£å†³ç”±ç©ºé—´ç¨€ç–æ€§å¯¼è‡´çš„æ¬ é‡‡æ ·é—®é¢˜ï¼Œé™åˆ¶äº†äº‹ä»¶æ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨å’Œç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚

**Method:** æå‡ºè¶…å›¾å¼•å¯¼çš„æ—¶ç©ºäº‹ä»¶æµè¡¥å…¨æœºåˆ¶ï¼Œé€šè¿‡è¶…å›¾è¿æ¥ä¸åŒæ—¶é—´å’Œç©ºé—´ä½ç½®çš„äº‹ä»¶æ ‡è®°ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ä¼ é€’å®Œæˆç¨€ç–äº‹ä»¶è¡¥å…¨ï¼›è¯¥æ¡†æ¶å¯çµæ´»é›†æˆRGBæ ‡è®°ä½œä¸ºè¶…å›¾èŠ‚ç‚¹ï¼Œå®ç°å¤šæ¨¡æ€è¶…å›¾ä¿¡æ¯è¡¥å…¨ï¼Œå¹¶é€šè¿‡è·¨æ—¶é—´æ­¥çš„è‡ªæ³¨æ„åŠ›èšåˆè¶…å›¾èŠ‚ç‚¹ä¿¡æ¯ã€‚

**Result:** åœ¨å•æ ‡ç­¾å’Œå¤šæ ‡ç­¾äº‹ä»¶åˆ†ç±»ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒå……åˆ†éªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡äº‹ä»¶æ•°æ®çš„è¡¨ç¤ºå­¦ä¹ æ€§èƒ½å’Œå¤„ç†æ•ˆç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºäº‹ä»¶ç›¸æœºæ•°æ®å¤„ç†æä¾›äº†æ–°çš„è¶…å›¾è¡¥å…¨èŒƒå¼ï¼Œé€šè¿‡æ—¶ç©ºä¸Šä¸‹æ–‡ä¿¡æ¯ä¼ é€’å’Œå¤šæ¨¡æ€èåˆï¼Œæœ‰æ•ˆå…‹æœäº†äº‹ä»¶æµç©ºé—´ç¨€ç–æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¸ºäº‹ä»¶è¡¨ç¤ºå­¦ä¹ å¼€è¾Ÿäº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.
