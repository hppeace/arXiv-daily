{"id": "2510.08589", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08589", "abs": "https://arxiv.org/abs/2510.08589", "authors": ["Nirmal Elamon", "Rouzbeh Davoudi"], "title": "Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes", "comment": null, "summary": "The field of object detection and understanding is rapidly evolving, driven\nby advances in both traditional CNN-based models and emerging multi-modal large\nlanguage models (LLMs). While CNNs like ResNet and YOLO remain highly effective\nfor image-based tasks, recent transformer-based LLMs introduce new capabilities\nsuch as dynamic context reasoning, language-guided prompts, and holistic scene\nunderstanding. However, when used out-of-the-box, the full potential of LLMs\nremains underexploited, often resulting in suboptimal performance on\nspecialized visual tasks. In this work, we conduct a comprehensive comparison\nof fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and\nfine-tuned multi-modal LLMs on the challenging task of artificial text overlay\ndetection in images. A key contribution of our study is demonstrating that LLMs\ncan be effectively fine-tuned on very limited data (fewer than 1,000 images) to\nachieve up to 36% accuracy improvement, matching or surpassing CNN-based\nbaselines that typically require orders of magnitude more data. By exploring\nhow language-guided models can be adapted for precise visual understanding with\nminimal supervision, our work contributes to the broader effort of bridging\nvision and language, offering novel insights into efficient cross-modal\nlearning strategies. These findings highlight the adaptability and data\nefficiency of LLM-based approaches for real-world object detection tasks and\nprovide actionable guidance for applying multi-modal transformers in\nlow-resource visual environments. To support continued progress in this area,\nwe have made the code used to fine-tune the models available in our GitHub,\nenabling future improvements and reuse in related applications."}
{"id": "2510.08625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08625", "abs": "https://arxiv.org/abs/2510.08625", "authors": ["Hyeonggeun Han", "Sehwan Kim", "Hyungjun Joo", "Sangwoo Hong", "Jungwoo Lee"], "title": "Adjusting Initial Noise to Mitigate Memorization in Text-to-Image Diffusion Models", "comment": null, "summary": "Despite their impressive generative capabilities, text-to-image diffusion\nmodels often memorize and replicate training data, prompting serious concerns\nover privacy and copyright. Recent work has attributed this memorization to an\nattraction basin-a region where applying classifier-free guidance (CFG) steers\nthe denoising trajectory toward memorized outputs-and has proposed deferring\nCFG application until the denoising trajectory escapes this basin. However,\nsuch delays often result in non-memorized images that are poorly aligned with\nthe input prompts, highlighting the need to promote earlier escape so that CFG\ncan be applied sooner in the denoising process. In this work, we show that the\ninitial noise sample plays a crucial role in determining when this escape\noccurs. We empirically observe that different initial samples lead to varying\nescape times. Building on this insight, we propose two mitigation strategies\nthat adjust the initial noise-either collectively or individually-to find and\nutilize initial samples that encourage earlier basin escape. These approaches\nsignificantly reduce memorization while preserving image-text alignment."}
{"id": "2510.08668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08668", "abs": "https://arxiv.org/abs/2510.08668", "authors": ["Songtao Jiang", "Yuan Wang", "Sibo Song", "Tianxiang Hu", "Chenyi Zhou", "Bin Pu", "Yan Zhang", "Zhibo Yang", "Yang Feng", "Joey Tianyi Zhou", "Jin Hao", "Zijian Chen", "Ruijia Wu", "Tao Tang", "Junhui Lv", "Hongxia Xu", "Hongwei Wang", "Jun Xiao", "Bin Feng", "Fudong Zhu", "Kenli Li", "Weidi Xie", "Jimeng Sun", "Jian Wu", "Zuozhu Liu"], "title": "Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding", "comment": null, "summary": "Real-world clinical decision-making grapples with integrating information\nfrom diverse data modalities, including medical text, 2D/3D images, and video,\nleading to inefficiencies and potential diagnostic oversights. While generalist\nvision-language models (VLMs) offer promise, their medical development faces\nchallenges of opaque pipelines, data scarcity, and architectural inflexibility.\nHere we present Hulu-Med, a transparent medical VLM that unifies understanding\nacross all these modalities. Built upon a unified patch-based vision encoder\nand an LLM decoder, Hulu-Med was progressively trained on 16.7 million (M)\nsamples to scale from 2D to 3D and video comprehension. The medical-aware token\nreduction enables efficient training, requiring only 4,000 to 40,000 GPU hours\nfor 7B to 32B parameter variants. Extensive evaluation across 30 benchmarks\nexhibits state-of-the-art performance, surpassing leading open-source models\nand competing with proprietary systems in tasks spanning visual\nquestion-answering, medical report generation, and complex reasoning in\nmultilingual and rare disease scenarios. By open-sourcing our complete\npipeline, we establish that high-performance medical VLM can be achieved\ntransparently, providing a foundational tool for accessible and impactful\nclinical AI. Code is released on\n\\href{https://github.com/ZJUI-AI4H/Hulu-Med}{https://github.com/ZJUI-AI4H/Hulu-Med}."}
{"id": "2510.08673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08673", "abs": "https://arxiv.org/abs/2510.08673", "authors": ["Kang Liao", "Size Wu", "Zhonghua Wu", "Linyi Jin", "Chao Wang", "Yikai Wang", "Fei Wang", "Wei Li", "Chen Change Loy"], "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation", "comment": "Project Page: https://kangliao929.github.io/projects/puffin/", "summary": "Camera-centric understanding and generation are two cornerstones of spatial\nintelligence, yet they are typically studied in isolation. We present Puffin, a\nunified camera-centric multimodal model that extends spatial awareness along\nthe camera dimension. Puffin integrates language regression and diffusion-based\ngeneration to interpret and create scenes from arbitrary viewpoints. To bridge\nthe modality gap between cameras and vision-language, we introduce a novel\nparadigm that treats camera as language, enabling thinking with camera. This\nguides the model to align spatially grounded visual cues with photographic\nterminology while reasoning across geometric context. Puffin is trained on\nPuffin-4M, a large-scale dataset of 4 million vision-language-camera triplets.\nWe incorporate both global camera parameters and pixel-wise camera maps,\nyielding flexible and reliable spatial generation. Experiments demonstrate\nPuffin superior performance over specialized models for camera-centric\ngeneration and understanding. With instruction tuning, Puffin generalizes to\ndiverse cross-view tasks such as spatial imagination, world exploration, and\nphotography guidance. We will release the code, models, dataset pipeline, and\nbenchmark to advance multimodal spatial intelligence research."}
{"id": "2510.08713", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08713", "abs": "https://arxiv.org/abs/2510.08713", "authors": ["Yifei Dong", "Fengyi Wu", "Guangyu Chen", "Zhi-Qi Cheng", "Qiyu Hu", "Yuxuan Zhou", "Jingdong Sun", "Jun-Yan He", "Qi Dai", "Alexander G Hauptmann"], "title": "Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation", "comment": "18 pages, 11 figures, code: https://github.com/F1y1113/UniWM", "summary": "Enabling embodied agents to effectively imagine future states is critical for\nrobust and generalizable visual navigation. Current state-of-the-art\napproaches, however, adopt modular architectures that separate navigation\nplanning from visual world modeling, leading to state-action misalignment and\nlimited adaptability in novel or dynamic scenarios. To overcome this\nfundamental limitation, we propose UniWM, a unified, memory-augmented world\nmodel integrating egocentric visual foresight and planning within a single\nmultimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly\ngrounds action decisions in visually imagined outcomes, ensuring tight\nalignment between prediction and control. A hierarchical memory mechanism\nfurther integrates detailed short-term perceptual cues with longer-term\ntrajectory context, enabling stable, coherent reasoning over extended horizons.\nExtensive experiments across four challenging benchmarks (Go Stanford, ReCon,\nSCAND, HuRoN) demonstrate that UniWM substantially improves navigation success\nrates by up to 30%, significantly reduces trajectory errors compared to strong\nbaselines, and exhibits impressive zero-shot generalization on the unseen\nTartanDrive dataset. These results highlight UniWM as a principled step toward\nunified, imagination-driven embodied navigation."}
{"id": "2510.08593", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08593", "abs": "https://arxiv.org/abs/2510.08593", "authors": ["Yuxin Li", "Eng Siong Chng", "Cuntai Guan"], "title": "Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech", "comment": null, "summary": "Speech-based depression detection (SDD) is a promising, non-invasive\nalternative to traditional clinical assessments. However, it remains limited by\nthe difficulty of extracting meaningful features and capturing sparse,\nheterogeneous depressive cues over time. Pretrained self-supervised learning\n(SSL) models such as WavLM provide rich, multi-layer speech representations,\nyet most existing SDD methods rely only on the final layer or search for a\nsingle best-performing one. These approaches often overfit to specific datasets\nand fail to leverage the full hierarchical structure needed to detect subtle\nand persistent depression signals.\n  To address this challenge, we propose HAREN-CTC, a novel architecture that\nintegrates multi-layer SSL features using cross-attention within a multitask\nlearning framework, combined with Connectionist Temporal Classification loss to\nhandle sparse temporal supervision. HAREN-CTC comprises two key modules: a\nHierarchical Adaptive Clustering module that reorganizes SSL features into\ncomplementary embeddings, and a Cross-Modal Fusion module that models\ninter-layer dependencies through cross-attention. The CTC objective enables\nalignment-aware training, allowing the model to track irregular temporal\npatterns of depressive speech cues.\n  We evaluate HAREN-CTC under both an upper-bound setting with standard data\nsplits and a generalization setting using five-fold cross-validation. The model\nachieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on\nMODMA, outperforming prior methods across both evaluation scenarios."}
{"id": "2510.08759", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08759", "abs": "https://arxiv.org/abs/2510.08759", "authors": ["Yu Qi", "Haibo Zhao", "Ziyu Guo", "Siyuan Ma", "Ziyan Chen", "Yaokun Han", "Renrui Zhang", "Zitiantao Lin", "Shiji Xin", "Yijian Huang", "Kai Cheng", "Peiheng Wang", "Jiazheng Liu", "Jiayi Zhang", "Yizhe Zhu", "Wenqing Wang", "Yiran Qin", "Xupeng Zhu", "Haojie Huang", "Lawson L. S. Wong"], "title": "BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities", "comment": null, "summary": "Embodied capabilities refer to a suite of fundamental abilities for an agent\nto perceive, comprehend, and interact with the physical world. While multimodal\nlarge language models (MLLMs) show promise as embodied agents, a thorough and\nsystematic evaluation of their embodied capabilities remains underexplored, as\nexisting benchmarks primarily focus on specific domains such as planning or\nspatial understanding. To bridge this gap, we introduce BEAR, a comprehensive\nand fine-grained benchmark that evaluates MLLMs on atomic embodied\ncapabilities. BEAR comprises 4,469 interleaved image-video-text entries across\n14 domains in 6 categories, including tasks from low-level pointing, trajectory\nunderstanding, spatial reasoning, to high-level planning. Extensive evaluation\nresults of 20 representative MLLMs reveal their persistent limitations across\nall domains of embodied capabilities. To tackle the shortfall, we propose\nBEAR-Agent, a multimodal conversable agent that integrates pretrained vision\nmodels to strengthen MLLM perception, 3D understanding, and planning\ncapabilities. It substantially enhances MLLM performance across diverse\nembodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative\nimprovement of 17.5% on GPT-5. Furthermore, our experiments indicate that\nimproving MLLM embodied capabilities can benefit embodied tasks in simulated\nenvironments. Project website: https://bear-official66.github.io/"}
{"id": "2510.08928", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08928", "abs": "https://arxiv.org/abs/2510.08928", "authors": ["Yushuo Zheng", "Zicheng Zhang", "Xiongkuo Min", "Huiyu Duan", "Guangtao Zhai"], "title": "LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition", "comment": null, "summary": "Existing benchmarks for large multimodal models (LMMs) often fail to capture\ntheir performance in real-time, adversarial environments. We introduce LM Fight\nArena (Large Model Fight Arena), a novel framework that evaluates LMMs by\npitting them against each other in the classic fighting game Mortal Kombat II,\na task requiring rapid visual understanding and tactical, sequential\ndecision-making. In a controlled tournament, we test six leading open- and\nclosed-source models, where each agent operates controlling the same character\nto ensure a fair comparison. The models are prompted to interpret game frames\nand state data to select their next actions. Unlike static evaluations, LM\nFight Arena provides a fully automated, reproducible, and objective assessment\nof an LMM's strategic reasoning capabilities in a dynamic setting. This work\nintroduces a challenging and engaging benchmark that bridges the gap between AI\nevaluation and interactive entertainment."}
{"id": "2510.08602", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08602", "abs": "https://arxiv.org/abs/2510.08602", "authors": ["Cong Zeng", "Shengkun Tang", "Yuanzhou Chen", "Zhiqiang Shen", "Wenchao Yu", "Xujiang Zhao", "Haifeng Chen", "Wei Cheng", "Zhiqiang Xu"], "title": "Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection", "comment": null, "summary": "The rapid advancement of large language models (LLMs) such as ChatGPT,\nDeepSeek, and Claude has significantly increased the presence of AI-generated\ntext in digital communication. This trend has heightened the need for reliable\ndetection methods to distinguish between human-authored and machine-generated\ncontent. Existing approaches both zero-shot methods and supervised classifiers\nlargely conceptualize this task as a binary classification problem, often\nleading to poor generalization across domains and models. In this paper, we\nargue that such a binary formulation fundamentally mischaracterizes the\ndetection task by assuming a coherent representation of human-written texts. In\nreality, human texts do not constitute a unified distribution, and their\ndiversity cannot be effectively captured through limited sampling. This causes\nprevious classifiers to memorize observed OOD characteristics rather than learn\nthe essence of `non-ID' behavior, limiting generalization to unseen\nhuman-authored inputs. Based on this observation, we propose reframing the\ndetection task as an out-of-distribution (OOD) detection problem, treating\nhuman-written texts as distributional outliers while machine-generated texts\nare in-distribution (ID) samples. To this end, we develop a detection framework\nusing one-class learning method including DeepSVDD and HRN, and score-based\nlearning techniques such as energy-based method, enabling robust and\ngeneralizable performance. Extensive experiments across multiple datasets\nvalidate the effectiveness of our OOD-based approach. Specifically, the\nOOD-based method achieves 98.3% AUROC and AUPR with only 8.9% FPR95 on DeepFake\ndataset. Moreover, we test our detection framework on multilingual, attacked,\nand unseen-model and -domain text settings, demonstrating the robustness and\ngeneralizability of our framework. Code, pretrained weights, and demo will be\nreleased."}
{"id": "2510.08789", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08789", "abs": "https://arxiv.org/abs/2510.08789", "authors": ["Shuo Xing", "Soumik Dey", "Mingyang Wu", "Ashirbad Mishra", "Hansi Wu", "Binbin Li", "Zhengzhong Tu"], "title": "Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization", "comment": null, "summary": "Video quality assessment (VQA) is a fundamental computer vision task that\naims to predict the perceptual quality of a given video in alignment with human\njudgments. Existing performant VQA models trained with direct score supervision\nsuffer from (1) poor generalization across diverse content and tasks, ranging\nfrom user-generated content (UGC), short-form videos, to AI-generated content\n(AIGC), (2) limited interpretability, and (3) lack of extensibility to novel\nuse cases or content types. We propose Q-Router, an agentic framework for\nuniversal VQA with a multi-tier model routing system. Q-Router integrates a\ndiverse set of expert models and employs vision--language models (VLMs) as\nreal-time routers that dynamically reason and then ensemble the most\nappropriate experts conditioned on the input video semantics. We build a\nmulti-tiered routing system based on the computing budget, with the heaviest\ntier involving a specific spatiotemporal artifacts localization for\ninterpretability. This agentic design enables Q-Router to combine the\ncomplementary strengths of specialized experts, achieving both flexibility and\nrobustness in delivering consistent performance across heterogeneous video\nsources and tasks. Extensive experiments demonstrate that Q-Router matches or\nsurpasses state-of-the-art VQA models on a variety of benchmarks, while\nsubstantially improving generalization and interpretability. Moreover, Q-Router\nexcels on the quality-based question answering benchmark, Q-Bench-Video,\nhighlighting its promise as a foundation for next-generation VQA systems.\nFinally, we show that Q-Router capably localizes spatiotemporal artifacts,\nshowing potential as a reward function for post-training video generation\nmodels."}
{"id": "2510.08945", "categories": ["cs.AI", "I.7.5; I.2.1; I.2.8; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.08945", "abs": "https://arxiv.org/abs/2510.08945", "authors": ["Samuel Hildebrand", "Curtis Taylor", "Sean Oesch", "James M Ghawaly Jr", "Amir Sadovnik", "Ryan Shivers", "Brandon Schreiber", "Kevin Kurian"], "title": "FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a promising paradigm for\nimproving factual accuracy in large language models (LLMs). We introduce a\nbenchmark designed to evaluate RAG pipelines as a whole, evaluating a\npipeline's ability to ingest, retrieve, and reason about several modalities of\ninformation, differentiating it from existing benchmarks that focus on\nparticular aspects such as retrieval. We present (1) a small, human-created\ndataset of 93 questions designed to evaluate a pipeline's ability to ingest\ntextual data, tables, images, and data spread across these modalities in one or\nmore documents; (2) a phrase-level recall metric for correctness; (3) a\nnearest-neighbor embedding classifier to identify potential pipeline\nhallucinations; (4) a comparative evaluation of 2 pipelines built with\nopen-source retrieval mechanisms and 4 closed-source foundation models; and (5)\na third-party human evaluation of the alignment of our correctness and\nhallucination metrics. We find that closed-source pipelines significantly\noutperform open-source pipelines in both correctness and hallucination metrics,\nwith wider performance gaps in questions relying on multimodal and\ncross-document information. Human evaluation of our metrics showed average\nagreement of 4.62 for correctness and 4.53 for hallucination detection on a 1-5\nLikert scale (5 indicating \"strongly agree\")."}
{"id": "2510.08606", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08606", "abs": "https://arxiv.org/abs/2510.08606", "authors": ["Yu Liu", "Hanlei Shi", "Haoxun Li", "Yuqing Sun", "Yuxuan Ding", "Linlin Gong", "Leyuan Qu", "Taihao Li"], "title": "Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations", "comment": "Under review for ICASSP 2026", "summary": "Emotion Recognition in Conversations (ERC) is hard because discriminative\nevidence is sparse, localized, and often asynchronous across modalities. We\ncenter ERC on emotion hotspots and present a unified model that detects\nper-utterance hotspots in text, audio, and video, fuses them with global\nfeatures via Hotspot-Gated Fusion, and aligns modalities using a routed\nMixture-of-Aligners; a cross-modal graph encodes conversational structure. This\ndesign focuses modeling on salient spans, mitigates misalignment, and preserves\ncontext. Experiments on standard ERC benchmarks show consistent gains over\nstrong baselines, with ablations confirming the contributions of HGF and MoA.\nOur results point to a hotspot-centric view that can inform future multimodal\nlearning, offering a new perspective on modality fusion in ERC."}
{"id": "2510.08791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08791", "abs": "https://arxiv.org/abs/2510.08791", "authors": ["Yuanhao Zou", "Zhaozheng Yin"], "title": "Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering", "comment": "CVPR2025 Paper", "summary": "Medical Visual Question Answering (Med-VQA) is a challenging task that\nrequires a deep understanding of both medical images and textual questions.\nAlthough recent works leveraging Medical Vision-Language Pre-training (Med-VLP)\nhave shown strong performance on the Med-VQA task, there is still no unified\nsolution for modality alignment, and the issue of hard negatives remains\nunder-explored. Additionally, commonly used knowledge fusion techniques for\nMed-VQA may introduce irrelevant information. In this work, we propose a\nframework to address these challenges through three key contributions: (1) a\nunified solution for heterogeneous modality alignments across multiple levels,\nmodalities, views, and stages, leveraging methods like contrastive learning and\noptimal transport theory; (2) a hard negative mining method that employs soft\nlabels for multi-modality alignments and enforces the hard negative pair\ndiscrimination; and (3) a Gated Cross-Attention Module for Med-VQA that\nintegrates the answer vocabulary as prior knowledge and selects relevant\ninformation from it. Our framework outperforms the previous state-of-the-art on\nwidely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019."}
{"id": "2510.08987", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08987", "abs": "https://arxiv.org/abs/2510.08987", "authors": ["Qixiang Yin", "Huanjin Yao", "Jianghao Chen", "Jiaxing Huang", "Zhicheng Zhao", "Fei Su"], "title": "Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging", "comment": "Technical report, Code will be available at\n  https://github.com/buptyqx/Tiny-R1V", "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable capabilities across diverse tasks, they encounter numerous\nchallenges in terms of reasoning efficiency, such as large model size,\noverthinking, and compromised accuracy in lightweight scenarios. However,\nresearch on the reasoning capabilities of lightweight MLLMs is quite lacking.\nTo this end, we propose Tiny-R1V, a novel lightweight 3B model that achieves\nfaster inference and higher accuracy via a two-stage optimization, while\nunifying multimodal reasoning across multiple tasks and using fewer tokens. In\nthe first stage, Tiny-R1V introduces Length-Informed Relative Policy\nOptimization (LIPO), a novel reinforcement learning method, to train each\nreasoning model. The LIPO is designed to dynamically adjusts advantages of\nresponses within groups, that is, by prioritizing concise yet high-quality\nresponses to encourage the generation of shorter and more accurate response. In\nthe second stage, we propose Adaptive Model Merging (AMM), a training-free\nmodel merging method that merges multiple specialist models into a unified\narchitecture. Specifically, AMM adaptively adjusts the weights of task vectors\nand robustly optimizes the merged vectors via a novel gradient projection\nregularization loss function, thus mitigating redundant conflicts between them.\nExtensive evaluations on ten widely-used reasoning benchmarks covering\nmathematics, structured data (charts, tables, documents), OCR, and general\ncapabilities showcase the superior performance of Tiny-R1V, enabling\nlightweight models to excel in diverse multimodal reasoning tasks."}
{"id": "2510.08608", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08608", "abs": "https://arxiv.org/abs/2510.08608", "authors": ["Weihua Zheng", "Zhengyuan Liu", "Tanmoy Chakraborty", "Weiwen Xu", "Xiaoxue Gao", "Bryan Chen Zhengyu Tan", "Bowei Zou", "Chang Liu", "Yujia Hu", "Xing Xie", "Xiaoyuan Yi", "Jing Yao", "Chaojun Wang", "Long Li", "Rui Liu", "Huiyao Liu", "Koji Inoue", "Ryuichi Sumida", "Tatsuya Kawahara", "Fan Xu", "Lingyu Ye", "Wei Tian", "Dongjun Kim", "Jimin Jung", "Jaehyung Seo", "Nadya Yuki Wangsajaya", "Pham Minh Duc", "Ojasva Saxena", "Palash Nandi", "Xiyan Tao", "Wiwik Karlina", "Tuan Luong", "Keertana Arun Vasan", "Roy Ka-Wei Lee", "Nancy F. Chen"], "title": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation", "comment": null, "summary": "Large language models (LLMs) are now used worldwide, yet their multimodal\nunderstanding and reasoning often degrade outside Western, high-resource\nsettings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs'\ncultural awareness with a focus on Asian contexts. MMA-ASIA centers on a\nhuman-curated, multilingual, and multimodally aligned multiple-choice benchmark\ncovering 8 Asian countries and 10 languages, comprising 27,000 questions; over\n79 percent require multi-step reasoning grounded in cultural context, moving\nbeyond simple memorization. To our knowledge, this is the first dataset aligned\nat the input level across three modalities: text, image (visual question\nanswering), and speech. This enables direct tests of cross-modal transfer.\nBuilding on this benchmark, we propose a five-dimensional evaluation protocol\nthat measures: (i) cultural-awareness disparities across countries, (ii)\ncross-lingual consistency, (iii) cross-modal consistency, (iv) cultural\nknowledge generalization, and (v) grounding validity. To ensure rigorous\nassessment, a Cultural Awareness Grounding Validation Module detects \"shortcut\nlearning\" by checking whether the requisite cultural knowledge supports correct\nanswers. Finally, through comparative model analysis, attention tracing, and an\ninnovative Vision-ablated Prefix Replay (VPR) method, we probe why models\ndiverge across languages and modalities, offering actionable insights for\nbuilding culturally reliable multimodal LLMs."}
{"id": "2510.08818", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08818", "abs": "https://arxiv.org/abs/2510.08818", "authors": ["Yiyang Huang", "Yizhou Wang", "Yun Fu"], "title": "D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition", "comment": "This paper has been accepted to EMNLP 2025", "summary": "Video large language models (Vid-LLMs), which excel in diverse video-language\ntasks, can be effectively constructed by adapting image-pretrained\nvision-language models (VLMs). However, this adaptation remains challenging, as\nit requires processing dense and temporally extended visual inputs that exceed\nthe capacity of image-based models. This paper identifies the perception\nbottleneck and token overload as key challenges in extending image-based VLMs\nto the video domain. To address these issues, we propose D-CoDe, a\ntraining-free adaptation framework that incorporates dynamic compression and\nquestion decomposition. Specifically, dynamic compression alleviates the\nperception bottleneck through adaptive selection of representative frames and\ncontent-aware aggregation of spatial tokens, thereby reducing redundancy while\npreserving informative content. In parallel, question decomposition mitigates\ntoken overload by reformulating the original query into sub-questions, guiding\nthe model to focus on distinct aspects of the video and enabling more\ncomprehensive understanding. Experiments demonstrate that D-CoDe effectively\nimproves video understanding across various benchmarks. Furthermore, strong\nperformance on the challenging long-video benchmark highlights the potential of\nD-CoDe in handling complex video-language tasks. Code is available at\nhttps://github.com/hukcc/D-CoDe."}
{"id": "2510.09060", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09060", "abs": "https://arxiv.org/abs/2510.09060", "authors": ["Jingxuan Wu", "Zhenglin Wan", "Xingrui Yu", "Yuzhe Yang", "Bo An", "Ivor Tsang"], "title": "OSCAR: Orthogonal Stochastic Control for Alignment-Respecting Diversity in Flow Matching", "comment": null, "summary": "Flow-based text-to-image models follow deterministic trajectories, forcing\nusers to repeatedly sample to discover diverse modes, which is a costly and\ninefficient process. We present a training-free, inference-time control\nmechanism that makes the flow itself diversity-aware. Our method simultaneously\nencourages lateral spread among trajectories via a feature-space objective and\nreintroduces uncertainty through a time-scheduled stochastic perturbation.\nCrucially, this perturbation is projected to be orthogonal to the generation\nflow, a geometric constraint that allows it to boost variation without\ndegrading image details or prompt fidelity. Our procedure requires no\nretraining or modification to the base sampler and is compatible with common\nflow-matching solvers. Theoretically, our method is shown to monotonically\nincrease a volume surrogate while, due to its geometric constraints,\napproximately preserving the marginal distribution. This provides a principled\nexplanation for why generation quality is robustly maintained. Empirically,\nacross multiple text-to-image settings under fixed sampling budgets, our method\nconsistently improves diversity metrics such as the Vendi Score and Brisque\nover strong baselines, while upholding image quality and alignment."}
{"id": "2510.08886", "categories": ["cs.CL", "cs.CE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08886", "abs": "https://arxiv.org/abs/2510.08886", "authors": ["Yan Wang", "Keyi Wang", "Shanshan Yang", "Jaisal Patel", "Jeff Zhao", "Fengran Mo", "Xueqing Peng", "Lingfei Qian", "Jimin Huang", "Guojun Xiong", "Xiao-Yang Liu", "Jian-Yun Nie"], "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs", "comment": null, "summary": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the\nhierarchical structure of eXtensible Business Reporting Language (XBRL) filings\nmake financial auditing increasingly difficult to automate and verify. While\nlarge language models (LLMs) have demonstrated strong capabilities in\nunstructured text understanding, their ability to reason over structured,\ninterdependent, and taxonomy-driven financial documents remains largely\nunexplored. To fill this gap, we introduce FinAuditing, the first\ntaxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs\non financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,\nFinAuditing defines three complementary subtasks, FinSM for semantic\nconsistency, FinRE for relational consistency, and FinMR for numerical\nconsistency, each targeting a distinct aspect of structured auditing reasoning.\nWe further propose a unified evaluation framework integrating retrieval,\nclassification, and reasoning metrics across these subtasks. Extensive\nzero-shot experiments on 13 state-of-the-art LLMs reveal that current models\nperform inconsistently across semantic, relational, and mathematical\ndimensions, with accuracy drops of up to 60-90% when reasoning over\nhierarchical multi-document structures. Our findings expose the systematic\nlimitations of modern LLMs in taxonomy-grounded financial reasoning and\nestablish FinAuditing as a foundation for developing trustworthy,\nstructure-aware, and regulation-aligned financial intelligence systems. The\nbenchmark dataset is available at Hugging Face."}
{"id": "2510.08849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08849", "abs": "https://arxiv.org/abs/2510.08849", "authors": ["Hongrui Wu", "Zhicheng Gao", "Jin Cao", "Kelu Yao", "Wen Shen", "Zhihua Wei"], "title": "FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided Knowledge Distillation", "comment": null, "summary": "Open-vocabulary 3D instance segmentation seeks to segment and classify\ninstances beyond the annotated label space. Existing methods typically map 3D\ninstances to 2D RGB-D images, and then employ vision-language models (VLMs) for\nclassification. However, such a mapping strategy usually introduces noise from\n2D occlusions and incurs substantial computational and memory costs during\ninference, slowing down the inference speed. To address the above problems, we\npropose a Fast Open-vocabulary 3D instance segmentation method via Label-guided\nKnowledge distillation (FOLK). Our core idea is to design a teacher model that\nextracts high-quality instance embeddings and distills its open-vocabulary\nknowledge into a 3D student model. In this way, during inference, the distilled\n3D model can directly classify instances from the 3D point cloud, avoiding\nnoise caused by occlusions and significantly accelerating the inference\nprocess. Specifically, we first design a teacher model to generate a 2D CLIP\nembedding for each 3D instance, incorporating both visibility and viewpoint\ndiversity, which serves as the learning target for distillation. We then\ndevelop a 3D student model that directly produces a 3D embedding for each 3D\ninstance. During training, we propose a label-guided distillation algorithm to\ndistill open-vocabulary knowledge from label-consistent 2D embeddings into the\nstudent model. FOLK conducted experiments on the ScanNet200 and Replica\ndatasets, achieving state-of-the-art performance on the ScanNet200 dataset with\nan AP50 score of 35.7, while running approximately 6.0x to 152.2x faster than\nprevious methods. All codes will be released after the paper is accepted."}
{"id": "2510.09404", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09404", "abs": "https://arxiv.org/abs/2510.09404", "authors": ["Christian Bluethgen", "Dave Van Veen", "Daniel Truhn", "Jakob Nikolas Kather", "Michael Moor", "Malgorzata Polacin", "Akshay Chaudhari", "Thomas Frauenfelder", "Curtis P. Langlotz", "Michael Krauthammer", "Farhad Nooralahzadeh"], "title": "Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges", "comment": null, "summary": "Building agents, systems that perceive and act upon their environment with a\ndegree of autonomy, has long been a focus of AI research. This pursuit has\nrecently become vastly more practical with the emergence of large language\nmodels (LLMs) capable of using natural language to integrate information,\nfollow instructions, and perform forms of \"reasoning\" and planning across a\nwide range of tasks. With its multimodal data streams and orchestrated\nworkflows spanning multiple systems, radiology is uniquely suited to benefit\nfrom agents that can adapt to context and automate repetitive yet complex\ntasks. In radiology, LLMs and their multimodal variants have already\ndemonstrated promising performance for individual tasks such as information\nextraction and report summarization. However, using LLMs in isolation\nunderutilizes their potential to support complex, multi-step workflows where\ndecisions depend on evolving context from multiple information sources.\nEquipping LLMs with external tools and feedback mechanisms enables them to\ndrive systems that exhibit a spectrum of autonomy, ranging from semi-automated\nworkflows to more adaptive agents capable of managing complex processes. This\nreview examines the design of such LLM-driven agentic systems, highlights key\napplications, discusses evaluation methods for planning and tool use, and\noutlines challenges such as error cascades, tool-use efficiency, and health IT\nintegration."}
{"id": "2510.08902", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08902", "abs": "https://arxiv.org/abs/2510.08902", "authors": ["Tengxiao Lv", "Ling Luo", "Juntao Li", "Yanhua Wang", "Yuchen Pan", "Chao Liu", "Yanan Wang", "Yan Jiang", "Huiyi Lv", "Yuanyuan Sun", "Jian Wang", "Hongfei Lin"], "title": "A Unified Biomedical Named Entity Recognition Framework with Large Language Models", "comment": "Accepted as a short paper at BIBM2025", "summary": "Accurate recognition of biomedical named entities is critical for medical\ninformation extraction and knowledge discovery. However, existing methods often\nstruggle with nested entities, entity boundary ambiguity, and cross-lingual\ngeneralization. In this paper, we propose a unified Biomedical Named Entity\nRecognition (BioNER) framework based on Large Language Models (LLMs). We first\nreformulate BioNER as a text generation task and design a symbolic tagging\nstrategy to jointly handle both flat and nested entities with explicit boundary\nannotation. To enhance multilingual and multi-task generalization, we perform\nbilingual joint fine-tuning across multiple Chinese and English datasets.\nAdditionally, we introduce a contrastive learning-based entity selector that\nfilters incorrect or spurious predictions by leveraging boundary-sensitive\npositive and negative samples. Experimental results on four benchmark datasets\nand two unseen corpora show that our method achieves state-of-the-art\nperformance and robust zero-shot generalization across languages. The source\ncodes are freely available at https://github.com/dreamer-tx/LLMNER."}
{"id": "2510.08919", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08919", "abs": "https://arxiv.org/abs/2510.08919", "authors": ["Daiki Yoshikawa", "Takashi Matsubara"], "title": "PHyCLIP: $\\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning", "comment": "23 pages", "summary": "Vision-language models have achieved remarkable success in multi-modal\nrepresentation learning from large-scale pairs of visual scenes and linguistic\ndescriptions. However, they still struggle to simultaneously express two\ndistinct types of semantic structures: the hierarchy within a concept family\n(e.g., dog $\\preceq$ mammal $\\preceq$ animal) and the compositionality across\ndifferent concept families (e.g., \"a dog in a car\" $\\preceq$ dog, car). Recent\nworks have addressed this challenge by employing hyperbolic space, which\nefficiently captures tree-like hierarchy, yet its suitability for representing\ncompositionality remains unclear. To resolve this dilemma, we propose PHyCLIP,\nwhich employs an $\\ell_1$-Product metric on a Cartesian product of Hyperbolic\nfactors. With our design, intra-family hierarchies emerge within individual\nhyperbolic factors, and cross-family composition is captured by the\n$\\ell_1$-product metric, analogous to a Boolean algebra. Experiments on\nzero-shot classification, retrieval, hierarchical classification, and\ncompositional understanding tasks demonstrate that PHyCLIP outperforms existing\nsingle-space approaches and offers more interpretable structures in the\nembedding space."}
{"id": "1812.06145", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "stat.ML", "68T45, 62H30", "I.5.3; I.2.10"], "pdf": "https://arxiv.org/pdf/1812.06145", "abs": "https://arxiv.org/abs/1812.06145", "authors": ["Mahdi Abavisani", "Hamid Reza Vaezi Joze", "Vishal M. Patel"], "title": "Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition with Multimodal Training", "comment": null, "summary": "We present an efficient approach for leveraging the knowledge from multiple\nmodalities in training unimodal 3D convolutional neural networks (3D-CNNs) for\nthe task of dynamic hand gesture recognition. Instead of explicitly combining\nmultimodal information, which is commonplace in many state-of-the-art methods,\nwe propose a different framework in which we embed the knowledge of multiple\nmodalities in individual networks so that each unimodal network can achieve an\nimproved performance. In particular, we dedicate separate networks per\navailable modality and enforce them to collaborate and learn to develop\nnetworks with common semantics and better representations. We introduce a\n\"spatiotemporal semantic alignment\" loss (SSA) to align the content of the\nfeatures from different networks. In addition, we regularize this loss with our\nproposed \"focal regularization parameter\" to avoid negative knowledge transfer.\nExperimental results show that our framework improves the test time recognition\naccuracy of unimodal networks, and provides the state-of-the-art performance on\nvarious dynamic hand gesture recognition datasets."}
{"id": "2510.09243", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09243", "abs": "https://arxiv.org/abs/2510.09243", "authors": ["Giacomo Gonella", "Gian Maria Campedelli", "Stefano Menini", "Marco Guerini"], "title": "CrisiText: A dataset of warning messages for LLM training in emergency communication", "comment": null, "summary": "Effectively identifying threats and mitigating their potential damage during\ncrisis situations, such as natural disasters or violent attacks, is paramount\nfor safeguarding endangered individuals. To tackle these challenges, AI has\nbeen used in assisting humans in emergency situations. Still, the use of NLP\ntechniques remains limited and mostly focuses on classification tasks. The\nsignificant potential of timely warning message generation using NLG\narchitectures, however, has been largely overlooked. In this paper we present\nCrisiText, the first large-scale dataset for the generation of warning messages\nacross 13 different types of crisis scenarios. The dataset contains more than\n400,000 warning messages (spanning almost 18,000 crisis situations) aimed at\nassisting civilians during and after such events. To generate the dataset, we\nstarted from existing crisis descriptions and created chains of events related\nto the scenarios. Each event was then paired with a warning message. The\ngenerations follow experts' written guidelines to ensure correct terminology\nand factuality of their suggestions. Additionally, each message is accompanied\nby three suboptimal warning types to allow for the study of different NLG\napproaches. To this end, we conducted a series of experiments comparing\nsupervised fine-tuning setups with preference alignment, zero-shot, and\nfew-shot approaches. We further assessed model performance in\nout-of-distribution scenarios and evaluated the effectiveness of an automatic\npost-editor."}
{"id": "2510.08936", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08936", "abs": "https://arxiv.org/abs/2510.08936", "authors": ["Zixi Yang", "Jiapeng Li", "Muxi Diao", "Yinuo Jing", "Kongming Liang"], "title": "RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos", "comment": null, "summary": "Recently, Multi-modal Large Language Models (MLLMs) have demonstrated\nsignificant performance across various video understanding tasks. However,\ntheir robustness, particularly when faced with manipulated video content,\nremains largely unexplored. In this paper, we introduce Ro-Bench, the first\nbenchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)\ncounterfactual video test sets. Ro-Bench incorporates high-quality, diverse and\ntemporally relevant video data, by editing Style, Object, Background and their\ncompositions. We evaluated eight recent video MLLMs and found that current\nmodels exhibit substantial performance degradation on Ro-Bench when exposed to\ncounterfactual video content. Furthermore, we demonstrate that fine-tuning\nMLLMs with counterfactual data enhances robustness, achieving a 21.73%\nperformance increase on Ro-Bench and a 12.78% improvement across 20 tasks in\nthe MVBench dataset. These findings underscore the effectiveness of\ncounterfactual data in enhancing the video understanding ability of MLLMs. The\ncode and data will be released shortly."}
{"id": "2510.08589", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08589", "abs": "https://arxiv.org/abs/2510.08589", "authors": ["Nirmal Elamon", "Rouzbeh Davoudi"], "title": "Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes", "comment": null, "summary": "The field of object detection and understanding is rapidly evolving, driven\nby advances in both traditional CNN-based models and emerging multi-modal large\nlanguage models (LLMs). While CNNs like ResNet and YOLO remain highly effective\nfor image-based tasks, recent transformer-based LLMs introduce new capabilities\nsuch as dynamic context reasoning, language-guided prompts, and holistic scene\nunderstanding. However, when used out-of-the-box, the full potential of LLMs\nremains underexploited, often resulting in suboptimal performance on\nspecialized visual tasks. In this work, we conduct a comprehensive comparison\nof fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and\nfine-tuned multi-modal LLMs on the challenging task of artificial text overlay\ndetection in images. A key contribution of our study is demonstrating that LLMs\ncan be effectively fine-tuned on very limited data (fewer than 1,000 images) to\nachieve up to 36% accuracy improvement, matching or surpassing CNN-based\nbaselines that typically require orders of magnitude more data. By exploring\nhow language-guided models can be adapted for precise visual understanding with\nminimal supervision, our work contributes to the broader effort of bridging\nvision and language, offering novel insights into efficient cross-modal\nlearning strategies. These findings highlight the adaptability and data\nefficiency of LLM-based approaches for real-world object detection tasks and\nprovide actionable guidance for applying multi-modal transformers in\nlow-resource visual environments. To support continued progress in this area,\nwe have made the code used to fine-tune the models available in our GitHub,\nenabling future improvements and reuse in related applications."}
{"id": "2510.09266", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09266", "abs": "https://arxiv.org/abs/2510.09266", "authors": ["Kaiwen Wei", "Xiao Liu", "Jie Zhang", "Zijian Wang", "Ruida Liu", "Yuming Yang", "Xin Xiao", "Xiao Sun", "Haoyang Zeng", "Changzai Pan", "Yidan Zhang", "Jiang Zhong", "Peijin Wang", "Yingchao Feng"], "title": "CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation", "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) enables Multimodal Large\nLanguage Models (MLLMs) to generate responses with external multimodal\nevidence, and numerous video-based MRAG benchmarks have been proposed to\nevaluate model capabilities across retrieval and generation stages. However,\nexisting benchmarks remain limited in modality coverage and format diversity,\noften focusing on single- or limited-modality tasks, or coarse-grained scene\nunderstanding. To address these gaps, we introduce CFVBench, a large-scale,\nmanually verified benchmark constructed from 599 publicly available videos,\nyielding 5,360 open-ended QA pairs. CFVBench spans high-density formats and\ndomains such as chart-heavy reports, news broadcasts, and software tutorials,\nrequiring models to retrieve and reason over long temporal video spans while\nmaintaining fine-grained multimodal information. Using CFVBench, we\nsystematically evaluate 7 retrieval methods and 14 widely-used MLLMs, revealing\na critical bottleneck: current models (even GPT5 or Gemini) struggle to capture\ntransient yet essential fine-grained multimodal details. To mitigate this, we\npropose Adaptive Visual Refinement (AVR), a simple yet effective framework that\nadaptively increases frame sampling density and selectively invokes external\ntools when necessary. Experiments show that AVR consistently enhances\nfine-grained multimodal comprehension and improves performance across all\nevaluated MLLMs"}
{"id": "2510.08964", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08964", "abs": "https://arxiv.org/abs/2510.08964", "authors": ["Yifan Li", "Zhenghao Chen", "Ziheng Wu", "Kun Zhou", "Ruipu Luo", "Can Zhang", "Zhentao He", "Yufei Zhan", "Wayne Xin Zhao", "Minghui Qiu"], "title": "Unleashing Perception-Time Scaling to Multimodal Reasoning Models", "comment": null, "summary": "Recent advances in inference-time scaling, particularly those leveraging\nreinforcement learning with verifiable rewards, have substantially enhanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs). Inspired by\nthis success, similar strategies have been applied to multimodal reasoning, yet\ntheir impact on visual perception remains unclear. To investigate this gap, we\nintroduce DisTANCE, a perception-centric benchmark for visual estimation tasks.\nEvaluation results show that LVLMs exhibit limited estimation precision, and\ninference-time scaling offers only marginal gains. We attribute this to the\nfast perception paradigm of current LVLMs, where visual understanding is\ntreated as a one-shot output without modeling the underlying perceptual\nprocess. To address this, we propose Perception-Time Scaling (PTS), a novel\nparadigm that encourages token-rich perception and decomposes complex\nperception problems into intermediate tractable sub-problems, thereby enabling\nperception to align with and benefit from inference-time scaling. Combined with\nreinforcement learning techniques, PTS significantly improves perception\naccuracy, raising high-precision performance on DisTANCE from 8.0% to 64.7%,\nand generalizes well to out-of-domain tasks. Surprisingly, even though PTS data\nare purely synthetic, combining them with math reasoning data yields consistent\ngains in both reasoning and real-world perception benchmarks. Further analysis\nreveals that PTS introduces more perception-related tokens and increases the\nmodel's attention to image tokens. Our code and data will be publicly released."}
{"id": "2510.08593", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08593", "abs": "https://arxiv.org/abs/2510.08593", "authors": ["Yuxin Li", "Eng Siong Chng", "Cuntai Guan"], "title": "Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech", "comment": null, "summary": "Speech-based depression detection (SDD) is a promising, non-invasive\nalternative to traditional clinical assessments. However, it remains limited by\nthe difficulty of extracting meaningful features and capturing sparse,\nheterogeneous depressive cues over time. Pretrained self-supervised learning\n(SSL) models such as WavLM provide rich, multi-layer speech representations,\nyet most existing SDD methods rely only on the final layer or search for a\nsingle best-performing one. These approaches often overfit to specific datasets\nand fail to leverage the full hierarchical structure needed to detect subtle\nand persistent depression signals.\n  To address this challenge, we propose HAREN-CTC, a novel architecture that\nintegrates multi-layer SSL features using cross-attention within a multitask\nlearning framework, combined with Connectionist Temporal Classification loss to\nhandle sparse temporal supervision. HAREN-CTC comprises two key modules: a\nHierarchical Adaptive Clustering module that reorganizes SSL features into\ncomplementary embeddings, and a Cross-Modal Fusion module that models\ninter-layer dependencies through cross-attention. The CTC objective enables\nalignment-aware training, allowing the model to track irregular temporal\npatterns of depressive speech cues.\n  We evaluate HAREN-CTC under both an upper-bound setting with standard data\nsplits and a generalization setting using five-fold cross-validation. The model\nachieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on\nMODMA, outperforming prior methods across both evaluation scenarios."}
{"id": "2510.09293", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09293", "abs": "https://arxiv.org/abs/2510.09293", "authors": ["Kohei Oda", "Po-Min Chuang", "Kiyoaki Shirai", "Natthawut Kertkeidkachorn"], "title": "One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations", "comment": null, "summary": "Sentence embedding methods have made remarkable progress, yet they still\nstruggle to capture the implicit semantics within sentences. This can be\nattributed to the inherent limitations of conventional sentence embedding\nmethods that assign only a single vector per sentence. To overcome this\nlimitation, we propose DualCSE, a sentence embedding method that assigns two\nembeddings to each sentence: one representing the explicit semantics and the\nother representing the implicit semantics. These embeddings coexist in the\nshared space, enabling the selection of the desired semantics for specific\npurposes such as information retrieval and text classification. Experimental\nresults demonstrate that DualCSE can effectively encode both explicit and\nimplicit meanings and improve the performance of the downstream task."}
{"id": "2510.08976", "categories": ["cs.CV", "cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08976", "abs": "https://arxiv.org/abs/2510.08976", "authors": ["Maoliang Li", "Ke Li", "Yaoyang Liu", "Jiayu Chen", "Zihao Zheng", "Yinjun Wu", "Xiang Chen"], "title": "Hierarchical Scheduling for Multi-Vector Image Retrieval", "comment": "Under Review", "summary": "To effectively leverage user-specific data, retrieval augmented generation\n(RAG) is employed in multimodal large language model (MLLM) applications.\nHowever, conventional retrieval approaches often suffer from limited retrieval\naccuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by\ndecomposing queries and matching against segmented images. They still suffer\nfrom sub-optimal accuracy and efficiency, overlooking alignment between the\nquery and varying image objects and redundant fine-grained image segments. In\nthis work, we present an efficient scheduling framework for image retrieval -\nHiMIR. First, we introduce a novel hierarchical paradigm, employing multiple\nintermediate granularities for varying image objects to enhance alignment.\nSecond, we minimize redundancy in retrieval by leveraging cross-hierarchy\nsimilarity consistency and hierarchy sparsity to minimize unnecessary matching\ncomputation. Furthermore, we configure parameters for each dataset\nautomatically for practicality across diverse scenarios. Our empirical study\nshows that, HiMIR not only achieves substantial accuracy improvements but also\nreduces computation by up to 3.5 times over the existing MVR system."}
{"id": "2510.08606", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08606", "abs": "https://arxiv.org/abs/2510.08606", "authors": ["Yu Liu", "Hanlei Shi", "Haoxun Li", "Yuqing Sun", "Yuxuan Ding", "Linlin Gong", "Leyuan Qu", "Taihao Li"], "title": "Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations", "comment": "Under review for ICASSP 2026", "summary": "Emotion Recognition in Conversations (ERC) is hard because discriminative\nevidence is sparse, localized, and often asynchronous across modalities. We\ncenter ERC on emotion hotspots and present a unified model that detects\nper-utterance hotspots in text, audio, and video, fuses them with global\nfeatures via Hotspot-Gated Fusion, and aligns modalities using a routed\nMixture-of-Aligners; a cross-modal graph encodes conversational structure. This\ndesign focuses modeling on salient spans, mitigates misalignment, and preserves\ncontext. Experiments on standard ERC benchmarks show consistent gains over\nstrong baselines, with ablations confirming the contributions of HGF and MoA.\nOur results point to a hotspot-centric view that can inform future multimodal\nlearning, offering a new perspective on modality fusion in ERC."}
{"id": "2510.09424", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09424", "abs": "https://arxiv.org/abs/2510.09424", "authors": ["Nizar El Ghazal", "Antoine Caubrire", "Valentin Vielzeuf"], "title": "The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach", "comment": null, "summary": "This paper presents a comparative study of context management strategies for\nend-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically\nevaluate traditional multimodal context (combining text history and spoken\ncurrent turn), full spoken history, and compressed spoken history approaches.\nOur experiments on the SpokenWOZ corpus demonstrate that providing the full\nspoken conversation as input yields the highest performance among models of\nsimilar size, significantly surpassing prior methods. Furthermore, we show that\nattention-pooling-based compression of the spoken history offers a strong\ntrade-off, maintaining competitive accuracy with reduced context size. Detailed\nanalysis confirms that improvements stem from more effective context\nutilization."}
{"id": "2510.08978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08978", "abs": "https://arxiv.org/abs/2510.08978", "authors": ["Zichuan Wang", "Bo Peng", "Songlin Yang", "Zhenchen Tang", "Jing Dong"], "title": "HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images", "comment": null, "summary": "Although recent text-to-image (T2I) models have significantly improved the\noverall visual quality of generated images, they still struggle in the\ngeneration of accurate details in complex local regions, especially human\nhands. Generated hands often exhibit structural distortions and unrealistic\ntextures, which can be very noticeable even when the rest of the body is\nwell-generated. However, the quality assessment of hand regions remains largely\nneglected, limiting downstream task performance like human-centric generation\nquality optimization and AIGC detection. To address this, we propose the first\nquality assessment task targeting generated hand regions and showcase its\nabundant downstream applications. We first introduce the HandPair dataset for\ntraining hand quality assessment models. It consists of 48k images formed by\nhigh- and low-quality hand pairs, enabling low-cost, efficient supervision\nwithout manual annotation. Based on it, we develop HandEval, a carefully\ndesigned hand-specific quality assessment model. It leverages the powerful\nvisual understanding capability of Multimodal Large Language Model (MLLM) and\nincorporates prior knowledge of hand keypoints, gaining strong perception of\nhand quality. We further construct a human-annotated test set with hand images\nfrom various state-of-the-art (SOTA) T2I models to validate its quality\nevaluation capability. Results show that HandEval aligns better with human\njudgments than existing SOTA methods. Furthermore, we integrate HandEval into\nimage generation and AIGC detection pipelines, prominently enhancing generated\nhand realism and detection accuracy, respectively, confirming its universal\neffectiveness in downstream applications. Code and dataset will be available."}
{"id": "2510.08608", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08608", "abs": "https://arxiv.org/abs/2510.08608", "authors": ["Weihua Zheng", "Zhengyuan Liu", "Tanmoy Chakraborty", "Weiwen Xu", "Xiaoxue Gao", "Bryan Chen Zhengyu Tan", "Bowei Zou", "Chang Liu", "Yujia Hu", "Xing Xie", "Xiaoyuan Yi", "Jing Yao", "Chaojun Wang", "Long Li", "Rui Liu", "Huiyao Liu", "Koji Inoue", "Ryuichi Sumida", "Tatsuya Kawahara", "Fan Xu", "Lingyu Ye", "Wei Tian", "Dongjun Kim", "Jimin Jung", "Jaehyung Seo", "Nadya Yuki Wangsajaya", "Pham Minh Duc", "Ojasva Saxena", "Palash Nandi", "Xiyan Tao", "Wiwik Karlina", "Tuan Luong", "Keertana Arun Vasan", "Roy Ka-Wei Lee", "Nancy F. Chen"], "title": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation", "comment": null, "summary": "Large language models (LLMs) are now used worldwide, yet their multimodal\nunderstanding and reasoning often degrade outside Western, high-resource\nsettings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs'\ncultural awareness with a focus on Asian contexts. MMA-ASIA centers on a\nhuman-curated, multilingual, and multimodally aligned multiple-choice benchmark\ncovering 8 Asian countries and 10 languages, comprising 27,000 questions; over\n79 percent require multi-step reasoning grounded in cultural context, moving\nbeyond simple memorization. To our knowledge, this is the first dataset aligned\nat the input level across three modalities: text, image (visual question\nanswering), and speech. This enables direct tests of cross-modal transfer.\nBuilding on this benchmark, we propose a five-dimensional evaluation protocol\nthat measures: (i) cultural-awareness disparities across countries, (ii)\ncross-lingual consistency, (iii) cross-modal consistency, (iv) cultural\nknowledge generalization, and (v) grounding validity. To ensure rigorous\nassessment, a Cultural Awareness Grounding Validation Module detects \"shortcut\nlearning\" by checking whether the requisite cultural knowledge supports correct\nanswers. Finally, through comparative model analysis, attention tracing, and an\ninnovative Vision-ablated Prefix Replay (VPR) method, we probe why models\ndiverge across languages and modalities, offering actionable insights for\nbuilding culturally reliable multimodal LLMs."}
{"id": "2510.09474", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09474", "abs": "https://arxiv.org/abs/2510.09474", "authors": ["Zhenhailong Wang", "Jiateng Liu", "Amin Fazel", "Ritesh Sarkhel", "Xing Fan", "Xiang Li", "Chenlei Guo", "Heng Ji", "Ruhi Sarikaya"], "title": "Multimodal Policy Internalization for Conversational Agents", "comment": null, "summary": "Modern conversational agents like ChatGPT and Alexa+ rely on predefined\npolicies specifying metadata, response styles, and tool-usage rules. As these\nLLM-based systems expand to support diverse business and user queries, such\npolicies, often implemented as in-context prompts, are becoming increasingly\ncomplex and lengthy, making faithful adherence difficult and imposing large\nfixed computational costs. With the rise of multimodal agents, policies that\ngovern visual and multimodal behaviors are critical but remain understudied.\nPrior prompt-compression work mainly shortens task templates and\ndemonstrations, while existing policy-alignment studies focus only on\ntext-based safety rules. We introduce Multimodal Policy Internalization (MPI),\na new task that internalizes reasoning-intensive multimodal policies into model\nparameters, enabling stronger policy-following without including the policy\nduring inference. MPI poses unique data and algorithmic challenges. We build\ntwo datasets spanning synthetic and real-world decision-making and tool-using\ntasks and propose TriMPI, a three-stage training framework. TriMPI first\ninjects policy knowledge via continual pretraining, then performs supervised\nfinetuning, and finally applies PolicyRollout, a GRPO-style reinforcement\nlearning extension that augments rollouts with policy-aware responses for\ngrounded exploration. TriMPI achieves notable gains in end-to-end accuracy,\ngeneralization, and robustness to forgetting. As the first work on multimodal\npolicy internalization, we provide datasets, training recipes, and\ncomprehensive evaluations to foster future research. Project page:\nhttps://mikewangwzhl.github.io/TriMPI."}
{"id": "2510.08994", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08994", "abs": "https://arxiv.org/abs/2510.08994", "authors": ["Yao Teng", "Fuyun Wang", "Xian Liu", "Zhekai Chen", "Han Shi", "Yu Wang", "Zhenguo Li", "Weiyang Liu", "Difan Zou", "Xihui Liu"], "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation", "comment": null, "summary": "As a new paradigm of visual content generation, autoregressive text-to-image\nmodels suffer from slow inference due to their sequential token-by-token\ndecoding process, often requiring thousands of model forward passes to generate\na single image. To address this inefficiency, we propose Speculative\nJacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising\nprocess into Jacobi iterations to enable parallel token generation in\nautoregressive models. Our method introduces a next-clean-token prediction\nparadigm that enables the pre-trained autoregressive models to accept\nnoise-perturbed token embeddings and predict the next clean tokens through\nlow-cost fine-tuning. This denoising paradigm guides the model towards more\nstable Jacobi trajectories. During inference, our method initializes token\nsequences with Gaussian noise and performs iterative\nnext-clean-token-prediction in the embedding space. We employ a probabilistic\ncriterion to verify and accept multiple tokens in parallel, and refine the\nunaccepted tokens for the next iteration with the denoising trajectory.\nExperiments show that our method can accelerate generation by reducing model\nforward passes while maintaining the visual quality of generated images."}
{"id": "2510.08818", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08818", "abs": "https://arxiv.org/abs/2510.08818", "authors": ["Yiyang Huang", "Yizhou Wang", "Yun Fu"], "title": "D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition", "comment": "This paper has been accepted to EMNLP 2025", "summary": "Video large language models (Vid-LLMs), which excel in diverse video-language\ntasks, can be effectively constructed by adapting image-pretrained\nvision-language models (VLMs). However, this adaptation remains challenging, as\nit requires processing dense and temporally extended visual inputs that exceed\nthe capacity of image-based models. This paper identifies the perception\nbottleneck and token overload as key challenges in extending image-based VLMs\nto the video domain. To address these issues, we propose D-CoDe, a\ntraining-free adaptation framework that incorporates dynamic compression and\nquestion decomposition. Specifically, dynamic compression alleviates the\nperception bottleneck through adaptive selection of representative frames and\ncontent-aware aggregation of spatial tokens, thereby reducing redundancy while\npreserving informative content. In parallel, question decomposition mitigates\ntoken overload by reformulating the original query into sub-questions, guiding\nthe model to focus on distinct aspects of the video and enabling more\ncomprehensive understanding. Experiments demonstrate that D-CoDe effectively\nimproves video understanding across various benchmarks. Furthermore, strong\nperformance on the challenging long-video benchmark highlights the potential of\nD-CoDe in handling complex video-language tasks. Code is available at\nhttps://github.com/hukcc/D-CoDe."}
{"id": "2510.09519", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09519", "abs": "https://arxiv.org/abs/2510.09519", "authors": ["Veronica Rammouz", "Aaron Gonzalez", "Carlos Cruzportillo", "Adrian Tan", "Nicole Beebe", "Anthony Rios"], "title": "Can We Reliably Rank Model Performance across Domains without Labeled Data?", "comment": "8 pages + references and Appendix", "summary": "Estimating model performance without labels is an important goal for\nunderstanding how NLP models generalize. While prior work has proposed measures\nbased on dataset similarity or predicted correctness, it remains unclear when\nthese estimates produce reliable performance rankings across domains. In this\npaper, we analyze the factors that affect ranking reliability using a two-step\nevaluation setup with four base classifiers and several large language models\nas error predictors. Experiments on the GeoOLID and Amazon Reviews datasets,\nspanning 15 domains, show that large language model-based error predictors\nproduce stronger and more consistent rank correlations with true accuracy than\ndrift-based or zero-shot baselines. Our analysis reveals two key findings:\nranking is more reliable when performance differences across domains are\nlarger, and when the error model's predictions align with the base model's true\nfailure patterns. These results clarify when performance estimation methods can\nbe trusted and provide guidance for their use in cross-domain model evaluation."}
{"id": "2510.09008", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09008", "abs": "https://arxiv.org/abs/2510.09008", "authors": ["Hoigi Seo", "Dong Un Kang", "Hyunjin Cho", "Joohoon Lee", "Se Young Chun"], "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models", "comment": null, "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts."}
{"id": "2510.08902", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08902", "abs": "https://arxiv.org/abs/2510.08902", "authors": ["Tengxiao Lv", "Ling Luo", "Juntao Li", "Yanhua Wang", "Yuchen Pan", "Chao Liu", "Yanan Wang", "Yan Jiang", "Huiyi Lv", "Yuanyuan Sun", "Jian Wang", "Hongfei Lin"], "title": "A Unified Biomedical Named Entity Recognition Framework with Large Language Models", "comment": "Accepted as a short paper at BIBM2025", "summary": "Accurate recognition of biomedical named entities is critical for medical\ninformation extraction and knowledge discovery. However, existing methods often\nstruggle with nested entities, entity boundary ambiguity, and cross-lingual\ngeneralization. In this paper, we propose a unified Biomedical Named Entity\nRecognition (BioNER) framework based on Large Language Models (LLMs). We first\nreformulate BioNER as a text generation task and design a symbolic tagging\nstrategy to jointly handle both flat and nested entities with explicit boundary\nannotation. To enhance multilingual and multi-task generalization, we perform\nbilingual joint fine-tuning across multiple Chinese and English datasets.\nAdditionally, we introduce a contrastive learning-based entity selector that\nfilters incorrect or spurious predictions by leveraging boundary-sensitive\npositive and negative samples. Experimental results on four benchmark datasets\nand two unseen corpora show that our method achieves state-of-the-art\nperformance and robust zero-shot generalization across languages. The source\ncodes are freely available at https://github.com/dreamer-tx/LLMNER."}
{"id": "2510.09553", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09553", "abs": "https://arxiv.org/abs/2510.09553", "authors": ["Yu Wang", "Tianhao Tan", "Yifei Wang"], "title": "Hierarchical Indexing with Knowledge Enrichment for Multilingual Video Corpus Retrieval", "comment": "Accepted to NLPCC 2025 (Springer), to appear November 2025", "summary": "Retrieving relevant instructional videos from multilingual medical archives\nis crucial for answering complex, multi-hop questions across language\nboundaries. However, existing systems either compress hour-long videos into\ncoarse embeddings or incur prohibitive costs for fine-grained matching. We\ntackle the Multilingual Video Corpus Retrieval (mVCR) task in the NLPCC-2025\nM4IVQA challenge with a multi-stage framework that integrates multilingual\nsemantics, domain terminology, and efficient long-form processing. Video\nsubtitles are divided into semantically coherent chunks, enriched with concise\nknowledge-graph (KG) facts, and organized into a hierarchical tree whose node\nembeddings are generated by a language-agnostic multilingual encoder. At query\ntime, the same encoder embeds the input question; a coarse-to-fine tree search\nprunes irrelevant branches, and only the top-ranked chunks are re-scored by a\nlightweight large language model (LLM). This design avoids exhaustive\ncross-encoder scoring while preserving chunk-level precision. Experiments on\nthe mVCR test set demonstrate state-of-the-art performance, and ablation\nstudies confirm the complementary contributions of KG enrichment, hierarchical\nindexing, and targeted LLM re-ranking. The proposed method offers an accurate\nand scalable solution for multilingual retrieval in specialized medical video\ncollections."}
{"id": "2510.09094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09094", "abs": "https://arxiv.org/abs/2510.09094", "authors": ["Youwei Zheng", "Yuxi Ren", "Xin Xia", "Xuefeng Xiao", "Xiaohua Xie"], "title": "Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation", "comment": "Accepted by ICCV 2025", "summary": "Diffusion Transformer (DiT) has demonstrated remarkable performance in\ntext-to-image generation; however, its large parameter size results in\nsubstantial inference overhead. Existing parameter compression methods\nprimarily focus on pruning, but aggressive pruning often leads to severe\nperformance degradation due to reduced model capacity. To address this\nlimitation, we pioneer the transformation of a dense DiT into a Mixture of\nExperts (MoE) for structured sparsification, reducing the number of activated\nparameters while preserving model capacity. Specifically, we replace the\nFeed-Forward Networks (FFNs) in DiT Blocks with MoE layers, reducing the number\nof activated parameters in the FFNs by 62.5\\%. Furthermore, we propose the\nMixture of Blocks (MoB) to selectively activate DiT blocks, thereby further\nenhancing sparsity. To ensure an effective dense-to-MoE conversion, we design a\nmulti-step distillation pipeline, incorporating Taylor metric-based expert\ninitialization, knowledge distillation with load balancing, and group feature\nloss for MoB optimization. We transform large diffusion transformers (e.g.,\nFLUX.1 [dev]) into an MoE structure, reducing activated parameters by 60\\%\nwhile maintaining original performance and surpassing pruning-based approaches\nin extensive experiments. Overall, Dense2MoE establishes a new paradigm for\nefficient text-to-image generation."}
{"id": "2510.08936", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08936", "abs": "https://arxiv.org/abs/2510.08936", "authors": ["Zixi Yang", "Jiapeng Li", "Muxi Diao", "Yinuo Jing", "Kongming Liang"], "title": "RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos", "comment": null, "summary": "Recently, Multi-modal Large Language Models (MLLMs) have demonstrated\nsignificant performance across various video understanding tasks. However,\ntheir robustness, particularly when faced with manipulated video content,\nremains largely unexplored. In this paper, we introduce Ro-Bench, the first\nbenchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)\ncounterfactual video test sets. Ro-Bench incorporates high-quality, diverse and\ntemporally relevant video data, by editing Style, Object, Background and their\ncompositions. We evaluated eight recent video MLLMs and found that current\nmodels exhibit substantial performance degradation on Ro-Bench when exposed to\ncounterfactual video content. Furthermore, we demonstrate that fine-tuning\nMLLMs with counterfactual data enhances robustness, achieving a 21.73%\nperformance increase on Ro-Bench and a 12.78% improvement across 20 tasks in\nthe MVBench dataset. These findings underscore the effectiveness of\ncounterfactual data in enhancing the video understanding ability of MLLMs. The\ncode and data will be released shortly."}
{"id": "2510.09558", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09558", "abs": "https://arxiv.org/abs/2510.09558", "authors": ["Qiguang Chen", "Zheng Yan", "Mingda Yang", "Libo Qin", "Yixin Yuan", "Hanjing Li", "Jinhao Liu", "Yiyan Ji", "Dengyun Peng", "Jiannan Guan", "Mengkang Hu", "Yantao Du", "Wanxiang Che"], "title": "AutoPR: Let's Automate Your Academic Promotion!", "comment": "Preprint. Code: https://github.com/LightChen2333/AutoPR . Benchmark:\n  https://huggingface.co/datasets/yzweak/PRBench", "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication."}
{"id": "2510.09110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09110", "abs": "https://arxiv.org/abs/2510.09110", "authors": ["Weikai Huang", "Jieyu Zhang", "Taoyang Jia", "Chenhao Zheng", "Ziqi Gao", "Jae Sung Park", "Ranjay Krishna"], "title": "SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding", "comment": "Project website: https://github.com/weikaih04/SOS", "summary": "Visual grouping -- operationalized via instance segmentation, visual\ngrounding, and object detection -- underpins applications from robotic\nperception to photo editing. Large annotated datasets are costly, biased in\ncoverage, and hard to scale. Synthetic data are promising but often lack\nflexibility, accuracy, and compositional diversity.\n  We present SOS, a simple and scalable data synthesis pipeline based on an\nobject-centric composition strategy. It pastes high-quality synthetic object\nsegments into new images using structured layout priors and generative\nrelighting, producing accurate and diverse masks, boxes, and referring\nexpressions. Models trained on 100000 synthetic images from SOS outperform\nthose trained on larger real-image datasets such as GRIT (20M) and V3Det (200K)\non detection and grounding tasks, achieving +10.9 AP on LVIS detection and +8.4\n$N_{\\text{Acc}}$ on gRefCOCO grounding. SOS enables controllable dataset\nconstruction and improves generalization in both low-data and closed-vocabulary\nsettings. Augmenting LVIS and COCO with synthetic object segments yields strong\nperformance across real-data scales and even larger gains under extremely\nlimited real data (for example, +3.83 $AP_{\\text{rare}}$ on LVIS instance\nsegmentation and +6.59 AP with a 1 percent COCO setup). This controllability\nalso supports targeted data generation for challenging intra-class referring in\nvisual grounding."}
{"id": "2510.09008", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09008", "abs": "https://arxiv.org/abs/2510.09008", "authors": ["Hoigi Seo", "Dong Un Kang", "Hyunjin Cho", "Joohoon Lee", "Se Young Chun"], "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models", "comment": null, "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts."}
{"id": "2510.09599", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09599", "abs": "https://arxiv.org/abs/2510.09599", "authors": ["Sondos Mahmoud Bsharat", "Zhiqiang Shen"], "title": "Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation", "comment": "Our code and data are available at https://github.com/VILA-Lab/PTTS", "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities when provided with chain-of-thought exemplars, but curating large\nreasoning datasets remains laborious and resource-intensive. In this work, we\nintroduce Prompting Test-Time Scaling (P-TTS), a simple yet effective\ninference-time data augmentation strategy for enhancing LLM reasoning through\nfinetuning. Rather than collecting thousands or even millions of examples,\nP-TTS leverages a small pool of only 90 manually selected reasoning instances\nand systematically varies exemplar augmentation through principled instruction\nprompting intensities at test time to synthesize diverse reasoning trajectory\ncontexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.\nAcross a suite of mathematical reasoning AIME2024 & 25, MATH500, and\nGPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive\nbaselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of\n+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);\nP-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and\n+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better\nperformance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances\nzero-shot generalization accuracy on out-of-domain reasoning benchmarks of\nGaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our\nanalysis suggests that test-time scaling effectively explores the latent space\nof reasoning patterns, amplifying LLM problem-solving with minimal annotation\noverhead, and further unlocking the reasoning potential and capabilities of\nLLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit\nLLM reasoning in resource-constrained or rapidly evolving domains."}
{"id": "2510.09121", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09121", "abs": "https://arxiv.org/abs/2510.09121", "authors": ["Dominik Winter", "Mai Bui", "Monica Azqueta Gavaldon", "Nicolas Triltsch", "Marco Rosati", "Nicolas Brieu"], "title": "MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation", "comment": null, "summary": "Scarcity of annotated data, particularly for rare or atypical morphologies,\npresent significant challenges for cell and nuclei segmentation in\ncomputational pathology. While manual annotation is labor-intensive and costly,\nsynthetic data offers a cost-effective alternative. We introduce a Multimodal\nSemantic Diffusion Model (MSDM) for generating realistic pixel-precise\nimage-mask pairs for cell and nuclei segmentation. By conditioning the\ngenerative process with cellular/nuclear morphologies (using horizontal and\nvertical maps), RGB color characteristics, and BERT-encoded assay/indication\nmetadata, MSDM generates datasests with desired morphological properties. These\nheterogeneous modalities are integrated via multi-head cross-attention,\nenabling fine-grained control over the generated images. Quantitative analysis\ndemonstrates that synthetic images closely match real data, with low\nWasserstein distances between embeddings of generated and real images under\nmatching biological conditions. The incorporation of these synthetic samples,\nexemplified by columnar cells, significantly improves segmentation model\naccuracy on columnar cells. This strategy systematically enriches data sets,\ndirectly targeting model deficiencies. We highlight the effectiveness of\nmultimodal diffusion-based augmentation for advancing the robustness and\ngeneralizability of cell and nuclei segmentation models. Thereby, we pave the\nway for broader application of generative models in computational pathology."}
{"id": "2510.09110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09110", "abs": "https://arxiv.org/abs/2510.09110", "authors": ["Weikai Huang", "Jieyu Zhang", "Taoyang Jia", "Chenhao Zheng", "Ziqi Gao", "Jae Sung Park", "Ranjay Krishna"], "title": "SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding", "comment": "Project website: https://github.com/weikaih04/SOS", "summary": "Visual grouping -- operationalized via instance segmentation, visual\ngrounding, and object detection -- underpins applications from robotic\nperception to photo editing. Large annotated datasets are costly, biased in\ncoverage, and hard to scale. Synthetic data are promising but often lack\nflexibility, accuracy, and compositional diversity.\n  We present SOS, a simple and scalable data synthesis pipeline based on an\nobject-centric composition strategy. It pastes high-quality synthetic object\nsegments into new images using structured layout priors and generative\nrelighting, producing accurate and diverse masks, boxes, and referring\nexpressions. Models trained on 100000 synthetic images from SOS outperform\nthose trained on larger real-image datasets such as GRIT (20M) and V3Det (200K)\non detection and grounding tasks, achieving +10.9 AP on LVIS detection and +8.4\n$N_{\\text{Acc}}$ on gRefCOCO grounding. SOS enables controllable dataset\nconstruction and improves generalization in both low-data and closed-vocabulary\nsettings. Augmenting LVIS and COCO with synthetic object segments yields strong\nperformance across real-data scales and even larger gains under extremely\nlimited real data (for example, +3.83 $AP_{\\text{rare}}$ on LVIS instance\nsegmentation and +6.59 AP with a 1 percent COCO setup). This controllability\nalso supports targeted data generation for challenging intra-class referring in\nvisual grounding."}
{"id": "2510.08964", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08964", "abs": "https://arxiv.org/abs/2510.08964", "authors": ["Yifan Li", "Zhenghao Chen", "Ziheng Wu", "Kun Zhou", "Ruipu Luo", "Can Zhang", "Zhentao He", "Yufei Zhan", "Wayne Xin Zhao", "Minghui Qiu"], "title": "Unleashing Perception-Time Scaling to Multimodal Reasoning Models", "comment": null, "summary": "Recent advances in inference-time scaling, particularly those leveraging\nreinforcement learning with verifiable rewards, have substantially enhanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs). Inspired by\nthis success, similar strategies have been applied to multimodal reasoning, yet\ntheir impact on visual perception remains unclear. To investigate this gap, we\nintroduce DisTANCE, a perception-centric benchmark for visual estimation tasks.\nEvaluation results show that LVLMs exhibit limited estimation precision, and\ninference-time scaling offers only marginal gains. We attribute this to the\nfast perception paradigm of current LVLMs, where visual understanding is\ntreated as a one-shot output without modeling the underlying perceptual\nprocess. To address this, we propose Perception-Time Scaling (PTS), a novel\nparadigm that encourages token-rich perception and decomposes complex\nperception problems into intermediate tractable sub-problems, thereby enabling\nperception to align with and benefit from inference-time scaling. Combined with\nreinforcement learning techniques, PTS significantly improves perception\naccuracy, raising high-precision performance on DisTANCE from 8.0% to 64.7%,\nand generalizes well to out-of-domain tasks. Surprisingly, even though PTS data\nare purely synthetic, combining them with math reasoning data yields consistent\ngains in both reasoning and real-world perception benchmarks. Further analysis\nreveals that PTS introduces more perception-related tokens and increases the\nmodel's attention to image tokens. Our code and data will be publicly released."}
{"id": "2510.09200", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09200", "abs": "https://arxiv.org/abs/2510.09200", "authors": ["Mukilan Karuppasamy", "Shankar Gangisetty", "Shyam Nandan Rai", "Carlo Masone", "C V Jawahar"], "title": "Towards Safer and Understandable Driver Intention Prediction", "comment": "10 pages", "summary": "Autonomous driving (AD) systems are becoming increasingly capable of handling\ncomplex tasks, mainly due to recent advances in deep learning and AI. As\ninteractions between autonomous systems and humans increase, the\ninterpretability of decision-making processes in driving systems becomes\nincreasingly crucial for ensuring safe driving operations. Successful\nhuman-machine interaction requires understanding the underlying representations\nof the environment and the driving task, which remains a significant challenge\nin deep learning-based systems. To address this, we introduce the task of\ninterpretability in maneuver prediction before they occur for driver safety,\ni.e., driver intent prediction (DIP), which plays a critical role in AD\nsystems. To foster research in interpretable DIP, we curate the eXplainable\nDriving Action Anticipation Dataset (DAAD-X), a new multimodal, ego-centric\nvideo dataset to provide hierarchical, high-level textual explanations as\ncausal reasoning for the driver's decisions. These explanations are derived\nfrom both the driver's eye-gaze and the ego-vehicle's perspective. Next, we\npropose Video Concept Bottleneck Model (VCBM), a framework that generates\nspatio-temporally coherent explanations inherently, without relying on post-hoc\ntechniques. Finally, through extensive evaluations of the proposed VCBM on the\nDAAD-X dataset, we demonstrate that transformer-based models exhibit greater\ninterpretability than conventional CNN-based models. Additionally, we introduce\na multilabel t-SNE visualization technique to illustrate the disentanglement\nand causal correlation among multiple explanations. Our data, code and models\nare available at: https://mukil07.github.io/VCBM.github.io/"}
{"id": "2510.09121", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09121", "abs": "https://arxiv.org/abs/2510.09121", "authors": ["Dominik Winter", "Mai Bui", "Monica Azqueta Gavaldon", "Nicolas Triltsch", "Marco Rosati", "Nicolas Brieu"], "title": "MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation", "comment": null, "summary": "Scarcity of annotated data, particularly for rare or atypical morphologies,\npresent significant challenges for cell and nuclei segmentation in\ncomputational pathology. While manual annotation is labor-intensive and costly,\nsynthetic data offers a cost-effective alternative. We introduce a Multimodal\nSemantic Diffusion Model (MSDM) for generating realistic pixel-precise\nimage-mask pairs for cell and nuclei segmentation. By conditioning the\ngenerative process with cellular/nuclear morphologies (using horizontal and\nvertical maps), RGB color characteristics, and BERT-encoded assay/indication\nmetadata, MSDM generates datasests with desired morphological properties. These\nheterogeneous modalities are integrated via multi-head cross-attention,\nenabling fine-grained control over the generated images. Quantitative analysis\ndemonstrates that synthetic images closely match real data, with low\nWasserstein distances between embeddings of generated and real images under\nmatching biological conditions. The incorporation of these synthetic samples,\nexemplified by columnar cells, significantly improves segmentation model\naccuracy on columnar cells. This strategy systematically enriches data sets,\ndirectly targeting model deficiencies. We highlight the effectiveness of\nmultimodal diffusion-based augmentation for advancing the robustness and\ngeneralizability of cell and nuclei segmentation models. Thereby, we pave the\nway for broader application of generative models in computational pathology."}
{"id": "2510.09008", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09008", "abs": "https://arxiv.org/abs/2510.09008", "authors": ["Hoigi Seo", "Dong Un Kang", "Hyunjin Cho", "Joohoon Lee", "Se Young Chun"], "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models", "comment": null, "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts."}
{"id": "2510.09203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09203", "abs": "https://arxiv.org/abs/2510.09203", "authors": ["Huimin Liu", "Jing Gao", "Daria Baran", "AxelX Montout", "Neill W Campbell", "Andrew W Dowsey"], "title": "Cattle-CLIP: A Multimodal Framework for Cattle Behaviour Recognition", "comment": "16 pages, 10 figures, submitted to Computers and Electronics in\n  Agriculture", "summary": "Cattle behaviour is a crucial indicator of an individual animal health,\nproductivity and overall well-being. Video-based monitoring, combined with deep\nlearning techniques, has become a mainstream approach in animal biometrics, and\nit can offer high accuracy in some behaviour recognition tasks. We present\nCattle-CLIP, a multimodal deep learning framework for cattle behaviour\nrecognition, using semantic cues to improve the performance of video-based\nvisual feature recognition. It is adapted from the large-scale image-language\nmodel CLIP by adding a temporal integration module. To address the domain gap\nbetween web data used for the pre-trained model and real-world cattle\nsurveillance footage, we introduce tailored data augmentation strategies and\nspecialised text prompts. Cattle-CLIP is evaluated under both fully-supervised\nand few-shot learning scenarios, with a particular focus on data-scarce\nbehaviour recognition - an important yet under-explored goal in livestock\nmonitoring. To evaluate the proposed method, we release the CattleBehaviours6\ndataset, which comprises six types of indoor behaviours: feeding, drinking,\nstanding-self-grooming, standing-ruminating, lying-self-grooming and\nlying-ruminating. The dataset consists of 1905 clips collected from our John\nOldacre Centre dairy farm research platform housing 200 Holstein-Friesian cows.\nExperiments show that Cattle-CLIP achieves 96.1% overall accuracy across six\nbehaviours in a supervised setting, with nearly 100% recall for feeding,\ndrinking and standing-ruminating behaviours, and demonstrates robust\ngeneralisation with limited data in few-shot scenarios, highlighting the\npotential of multimodal learning in agricultural and animal behaviour analysis."}
{"id": "2510.09200", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09200", "abs": "https://arxiv.org/abs/2510.09200", "authors": ["Mukilan Karuppasamy", "Shankar Gangisetty", "Shyam Nandan Rai", "Carlo Masone", "C V Jawahar"], "title": "Towards Safer and Understandable Driver Intention Prediction", "comment": "10 pages", "summary": "Autonomous driving (AD) systems are becoming increasingly capable of handling\ncomplex tasks, mainly due to recent advances in deep learning and AI. As\ninteractions between autonomous systems and humans increase, the\ninterpretability of decision-making processes in driving systems becomes\nincreasingly crucial for ensuring safe driving operations. Successful\nhuman-machine interaction requires understanding the underlying representations\nof the environment and the driving task, which remains a significant challenge\nin deep learning-based systems. To address this, we introduce the task of\ninterpretability in maneuver prediction before they occur for driver safety,\ni.e., driver intent prediction (DIP), which plays a critical role in AD\nsystems. To foster research in interpretable DIP, we curate the eXplainable\nDriving Action Anticipation Dataset (DAAD-X), a new multimodal, ego-centric\nvideo dataset to provide hierarchical, high-level textual explanations as\ncausal reasoning for the driver's decisions. These explanations are derived\nfrom both the driver's eye-gaze and the ego-vehicle's perspective. Next, we\npropose Video Concept Bottleneck Model (VCBM), a framework that generates\nspatio-temporally coherent explanations inherently, without relying on post-hoc\ntechniques. Finally, through extensive evaluations of the proposed VCBM on the\nDAAD-X dataset, we demonstrate that transformer-based models exhibit greater\ninterpretability than conventional CNN-based models. Additionally, we introduce\na multilabel t-SNE visualization technique to illustrate the disentanglement\nand causal correlation among multiple explanations. Our data, code and models\nare available at: https://mukil07.github.io/VCBM.github.io/"}
{"id": "2510.09230", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09230", "abs": "https://arxiv.org/abs/2510.09230", "authors": ["Jindong Hong", "Wencheng Zhang", "Shiqin Qiao", "Jianhai Chen", "Jianing Qiu", "Chuanyang Zheng", "Qian Xu", "Yun Ji", "Qianyue Wen", "Weiwei Sun", "Hao Li", "Huizhen Li", "Huichao Wang", "Kai Wu", "Meng Li", "Yijun He", "Lingjie Luo", "Jiankai Sun"], "title": "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras", "comment": null, "summary": "Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),\nare common conditions affecting the health of people worldwide, and have a high\nincidence rate among the elderly and workers engaged in repetitive shoulder\ntasks. In regions with scarce medical resources, achieving early and accurate\ndiagnosis poses significant challenges, and there is an urgent need for\nlow-cost and easily scalable auxiliary diagnostic solutions. This research\nintroduces videos captured by consumer-grade devices as the basis for\ndiagnosis, reducing the cost for users. We focus on the innovative application\nof Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of\nshoulder disorders and propose a Hybrid Motion Video Diagnosis framework\n(HMVDx). This framework divides the two tasks of action understanding and\ndisease diagnosis, which are respectively completed by two MLLMs. In addition\nto traditional evaluation indicators, this work proposes a novel metric called\nUsability Index by the logical process of medical decision-making (action\nrecognition, movement diagnosis, and final diagnosis). This index evaluates the\neffectiveness of MLLMs in the medical field from the perspective of the entire\nmedical diagnostic pathway, revealing the potential value of low-cost MLLMs in\nmedical applications for medical practitioners. In experimental comparisons,\nthe accuracy of HMVDx in diagnosing shoulder joint injuries has increased by\n79.6\\% compared with direct video diagnosis, a significant technical\ncontribution to future research on the application of MLLMs for video\nunderstanding in the medical field."}
{"id": "2510.09228", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09228", "abs": "https://arxiv.org/abs/2510.09228", "authors": ["Vijay M. Galshetwar", "Praful Hambarde", "Prashant W. Patil", "Akshay Dudhane", "Sachin Chaudhary", "Santosh Kumar Vipparathi", "Subrahmanyam Murala"], "title": "Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation", "comment": "This work has been submitted to IEEE for possible publication", "summary": "Adverse weather conditions such as haze, rain, and snow significantly degrade\nthe quality of images and videos, posing serious challenges to intelligent\ntransportation systems (ITS) that rely on visual input. These degradations\naffect critical applications including autonomous driving, traffic monitoring,\nand surveillance. This survey presents a comprehensive review of image and\nvideo restoration techniques developed to mitigate weather-induced visual\nimpairments. We categorize existing approaches into traditional prior-based\nmethods and modern data-driven models, including CNNs, transformers, diffusion\nmodels, and emerging vision-language models (VLMs). Restoration strategies are\nfurther classified based on their scope: single-task models,\nmulti-task/multi-weather systems, and all-in-one frameworks capable of handling\ndiverse degradations. In addition, we discuss day and night time restoration\nchallenges, benchmark datasets, and evaluation protocols. The survey concludes\nwith an in-depth discussion on limitations in current research and outlines\nfuture directions such as mixed/compound-degradation restoration, real-time\ndeployment, and agentic AI frameworks. This work aims to serve as a valuable\nreference for advancing weather-resilient vision systems in smart\ntransportation environments. Lastly, to stay current with rapid advancements in\nthis field, we will maintain regular updates of the latest relevant papers and\ntheir open-source implementations at\nhttps://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration"}
{"id": "2510.09228", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09228", "abs": "https://arxiv.org/abs/2510.09228", "authors": ["Vijay M. Galshetwar", "Praful Hambarde", "Prashant W. Patil", "Akshay Dudhane", "Sachin Chaudhary", "Santosh Kumar Vipparathi", "Subrahmanyam Murala"], "title": "Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation", "comment": "This work has been submitted to IEEE for possible publication", "summary": "Adverse weather conditions such as haze, rain, and snow significantly degrade\nthe quality of images and videos, posing serious challenges to intelligent\ntransportation systems (ITS) that rely on visual input. These degradations\naffect critical applications including autonomous driving, traffic monitoring,\nand surveillance. This survey presents a comprehensive review of image and\nvideo restoration techniques developed to mitigate weather-induced visual\nimpairments. We categorize existing approaches into traditional prior-based\nmethods and modern data-driven models, including CNNs, transformers, diffusion\nmodels, and emerging vision-language models (VLMs). Restoration strategies are\nfurther classified based on their scope: single-task models,\nmulti-task/multi-weather systems, and all-in-one frameworks capable of handling\ndiverse degradations. In addition, we discuss day and night time restoration\nchallenges, benchmark datasets, and evaluation protocols. The survey concludes\nwith an in-depth discussion on limitations in current research and outlines\nfuture directions such as mixed/compound-degradation restoration, real-time\ndeployment, and agentic AI frameworks. This work aims to serve as a valuable\nreference for advancing weather-resilient vision systems in smart\ntransportation environments. Lastly, to stay current with rapid advancements in\nthis field, we will maintain regular updates of the latest relevant papers and\ntheir open-source implementations at\nhttps://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration"}
{"id": "2510.09302", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09302", "abs": "https://arxiv.org/abs/2510.09302", "authors": ["Yuying Li", "Siyi Qian", "Hao Liang", "Leqi Zheng", "Ruichuan An", "Yongzhen Guo", "Wentao Zhang"], "title": "CapGeo: A Caption-Assisted Approach to Geometric Reasoning", "comment": "preprint, under review", "summary": "Geometric reasoning remains a core challenge for Multimodal Large Language\nModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3\nand Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite\nexhibiting strong textual reasoning abilities on tasks like the International\nMathematical Olympiad (IMO). This gap suggests that the bottleneck lies in\nunderstanding geometric diagrams rather than reasoning itself. Since geometric\nfigures can often be faithfully described in concise textual form, converting\nvisual content into captions offers a promising direction. Motivated by this\ninsight, we introduce CapGeo, a caption-assisted reasoning framework that\nbridges visual and textual modalities. Experiments show substantial\nimprovements when models are equipped with captions: Qwen2.5-VL-72B improves\nfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to\n73.0%. To systematically evaluate and identify high-quality geometric\ncaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated\nfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based\nevaluation metric that correlates strongly with downstream CapGeo performance,\nenabling reliable assessment of geometric captioning ability. Together, our\nframework and benchmark highlight a new pathway toward advancing geometric\nreasoning in MLLMs."}
{"id": "2510.09230", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09230", "abs": "https://arxiv.org/abs/2510.09230", "authors": ["Jindong Hong", "Wencheng Zhang", "Shiqin Qiao", "Jianhai Chen", "Jianing Qiu", "Chuanyang Zheng", "Qian Xu", "Yun Ji", "Qianyue Wen", "Weiwei Sun", "Hao Li", "Huizhen Li", "Huichao Wang", "Kai Wu", "Meng Li", "Yijun He", "Lingjie Luo", "Jiankai Sun"], "title": "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras", "comment": null, "summary": "Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),\nare common conditions affecting the health of people worldwide, and have a high\nincidence rate among the elderly and workers engaged in repetitive shoulder\ntasks. In regions with scarce medical resources, achieving early and accurate\ndiagnosis poses significant challenges, and there is an urgent need for\nlow-cost and easily scalable auxiliary diagnostic solutions. This research\nintroduces videos captured by consumer-grade devices as the basis for\ndiagnosis, reducing the cost for users. We focus on the innovative application\nof Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of\nshoulder disorders and propose a Hybrid Motion Video Diagnosis framework\n(HMVDx). This framework divides the two tasks of action understanding and\ndisease diagnosis, which are respectively completed by two MLLMs. In addition\nto traditional evaluation indicators, this work proposes a novel metric called\nUsability Index by the logical process of medical decision-making (action\nrecognition, movement diagnosis, and final diagnosis). This index evaluates the\neffectiveness of MLLMs in the medical field from the perspective of the entire\nmedical diagnostic pathway, revealing the potential value of low-cost MLLMs in\nmedical applications for medical practitioners. In experimental comparisons,\nthe accuracy of HMVDx in diagnosing shoulder joint injuries has increased by\n79.6\\% compared with direct video diagnosis, a significant technical\ncontribution to future research on the application of MLLMs for video\nunderstanding in the medical field."}
{"id": "2510.09230", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09230", "abs": "https://arxiv.org/abs/2510.09230", "authors": ["Jindong Hong", "Wencheng Zhang", "Shiqin Qiao", "Jianhai Chen", "Jianing Qiu", "Chuanyang Zheng", "Qian Xu", "Yun Ji", "Qianyue Wen", "Weiwei Sun", "Hao Li", "Huizhen Li", "Huichao Wang", "Kai Wu", "Meng Li", "Yijun He", "Lingjie Luo", "Jiankai Sun"], "title": "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras", "comment": null, "summary": "Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),\nare common conditions affecting the health of people worldwide, and have a high\nincidence rate among the elderly and workers engaged in repetitive shoulder\ntasks. In regions with scarce medical resources, achieving early and accurate\ndiagnosis poses significant challenges, and there is an urgent need for\nlow-cost and easily scalable auxiliary diagnostic solutions. This research\nintroduces videos captured by consumer-grade devices as the basis for\ndiagnosis, reducing the cost for users. We focus on the innovative application\nof Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of\nshoulder disorders and propose a Hybrid Motion Video Diagnosis framework\n(HMVDx). This framework divides the two tasks of action understanding and\ndisease diagnosis, which are respectively completed by two MLLMs. In addition\nto traditional evaluation indicators, this work proposes a novel metric called\nUsability Index by the logical process of medical decision-making (action\nrecognition, movement diagnosis, and final diagnosis). This index evaluates the\neffectiveness of MLLMs in the medical field from the perspective of the entire\nmedical diagnostic pathway, revealing the potential value of low-cost MLLMs in\nmedical applications for medical practitioners. In experimental comparisons,\nthe accuracy of HMVDx in diagnosing shoulder joint injuries has increased by\n79.6\\% compared with direct video diagnosis, a significant technical\ncontribution to future research on the application of MLLMs for video\nunderstanding in the medical field."}
{"id": "2510.09608", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09608", "abs": "https://arxiv.org/abs/2510.09608", "authors": ["Ruyi Xu", "Guangxuan Xiao", "Yukang Chen", "Liuning He", "Kelly Peng", "Yao Lu", "Song Han"], "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams", "comment": "The first two authors contributed equally to this work", "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."}
{"id": "2510.09253", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.09253", "abs": "https://arxiv.org/abs/2510.09253", "authors": ["Alina Elena Baia", "Alessio Xompero", "Andrea Cavallaro"], "title": "Zero-shot image privacy classification with Vision-Language Models", "comment": "5 pages, 3 figures, 3 tables. This work has been submitted to the\n  ICASSP 2026", "summary": "While specialized learning-based models have historically dominated image\nprivacy prediction, the current literature increasingly favours adopting large\nVision-Language Models (VLMs) designed for generic tasks. This trend risks\noverlooking the performance ceiling set by purpose-built models due to a lack\nof systematic evaluation. To address this problem, we establish a zero-shot\nbenchmark for image privacy classification, enabling a fair comparison. We\nevaluate the top-3 open-source VLMs, according to a privacy benchmark, using\ntask-aligned prompts and we contrast their performance, efficiency, and\nrobustness against established vision-only and multi-modal methods.\nCounter-intuitively, our results show that VLMs, despite their\nresource-intensive nature in terms of high parameter count and slower\ninference, currently lag behind specialized, smaller models in privacy\nprediction accuracy. We also find that VLMs exhibit higher robustness to image\nperturbations."}
{"id": "2510.09243", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09243", "abs": "https://arxiv.org/abs/2510.09243", "authors": ["Giacomo Gonella", "Gian Maria Campedelli", "Stefano Menini", "Marco Guerini"], "title": "CrisiText: A dataset of warning messages for LLM training in emergency communication", "comment": null, "summary": "Effectively identifying threats and mitigating their potential damage during\ncrisis situations, such as natural disasters or violent attacks, is paramount\nfor safeguarding endangered individuals. To tackle these challenges, AI has\nbeen used in assisting humans in emergency situations. Still, the use of NLP\ntechniques remains limited and mostly focuses on classification tasks. The\nsignificant potential of timely warning message generation using NLG\narchitectures, however, has been largely overlooked. In this paper we present\nCrisiText, the first large-scale dataset for the generation of warning messages\nacross 13 different types of crisis scenarios. The dataset contains more than\n400,000 warning messages (spanning almost 18,000 crisis situations) aimed at\nassisting civilians during and after such events. To generate the dataset, we\nstarted from existing crisis descriptions and created chains of events related\nto the scenarios. Each event was then paired with a warning message. The\ngenerations follow experts' written guidelines to ensure correct terminology\nand factuality of their suggestions. Additionally, each message is accompanied\nby three suboptimal warning types to allow for the study of different NLG\napproaches. To this end, we conducted a series of experiments comparing\nsupervised fine-tuning setups with preference alignment, zero-shot, and\nfew-shot approaches. We further assessed model performance in\nout-of-distribution scenarios and evaluated the effectiveness of an automatic\npost-editor."}
{"id": "2510.09256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09256", "abs": "https://arxiv.org/abs/2510.09256", "authors": ["Patrick Wienholt", "Sophie Caselitz", "Robert Siepmann", "Philipp Bruners", "Keno Bressem", "Christiane Kuhl", "Jakob Nikolas Kather", "Sven Nebelung", "Daniel Truhn"], "title": "Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy", "comment": "Code is available: https://github.com/TruhnLab/VisionSemanticEntropy", "summary": "To determine whether using discrete semantic entropy (DSE) to reject\nquestions likely to generate hallucinations can improve the accuracy of\nblack-box vision-language models (VLMs) in radiologic image based visual\nquestion answering (VQA). This retrospective study evaluated DSE using two\npublicly available, de-identified datasets: (i) the VQA-Med 2019 benchmark (500\nimages with clinical questions and short-text answers) and (ii) a diagnostic\nradiology dataset (206 cases: 60 computed tomography scans, 60 magnetic\nresonance images, 60 radiographs, 26 angiograms) with corresponding\nground-truth diagnoses. GPT-4o and GPT-4.1 answered each question 15 times\nusing a temperature of 1.0. Baseline accuracy was determined using\nlow-temperature answers (temperature 0.1). Meaning-equivalent responses were\ngrouped using bidirectional entailment checks, and DSE was computed from the\nrelative frequencies of the resulting semantic clusters. Accuracy was\nrecalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and\n95% confidence intervals were obtained using bootstrap resampling and a\nBonferroni-corrected threshold of p < .004 for statistical significance. Across\n706 image-question pairs, baseline accuracy was 51.7% for GPT-4o and 54.8% for\nGPT-4.1. After filtering out high-entropy questions (DSE > 0.3), accuracy on\nthe remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and\n63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains\nwere observed across both datasets and largely remained statistically\nsignificant after Bonferroni correction. DSE enables reliable hallucination\ndetection in black-box VLMs by quantifying semantic inconsistency. This method\nsignificantly improves diagnostic answer accuracy and offers a filtering\nstrategy for clinical VLM applications."}
{"id": "2510.09302", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09302", "abs": "https://arxiv.org/abs/2510.09302", "authors": ["Yuying Li", "Siyi Qian", "Hao Liang", "Leqi Zheng", "Ruichuan An", "Yongzhen Guo", "Wentao Zhang"], "title": "CapGeo: A Caption-Assisted Approach to Geometric Reasoning", "comment": "preprint, under review", "summary": "Geometric reasoning remains a core challenge for Multimodal Large Language\nModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3\nand Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite\nexhibiting strong textual reasoning abilities on tasks like the International\nMathematical Olympiad (IMO). This gap suggests that the bottleneck lies in\nunderstanding geometric diagrams rather than reasoning itself. Since geometric\nfigures can often be faithfully described in concise textual form, converting\nvisual content into captions offers a promising direction. Motivated by this\ninsight, we introduce CapGeo, a caption-assisted reasoning framework that\nbridges visual and textual modalities. Experiments show substantial\nimprovements when models are equipped with captions: Qwen2.5-VL-72B improves\nfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to\n73.0%. To systematically evaluate and identify high-quality geometric\ncaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated\nfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based\nevaluation metric that correlates strongly with downstream CapGeo performance,\nenabling reliable assessment of geometric captioning ability. Together, our\nframework and benchmark highlight a new pathway toward advancing geometric\nreasoning in MLLMs."}
{"id": "2510.09285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09285", "abs": "https://arxiv.org/abs/2510.09285", "authors": ["Siyuan Huang", "Xiaoye Qu", "Yafu Li", "Yun Luo", "Zefeng He", "Daizong Liu", "Yu Cheng"], "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning", "comment": "31 pages, 10 figures, project page:\n  https://github.com/huaixuheqing/VPPO-RL", "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs."}
{"id": "2510.09424", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09424", "abs": "https://arxiv.org/abs/2510.09424", "authors": ["Nizar El Ghazal", "Antoine Caubrire", "Valentin Vielzeuf"], "title": "The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach", "comment": null, "summary": "This paper presents a comparative study of context management strategies for\nend-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically\nevaluate traditional multimodal context (combining text history and spoken\ncurrent turn), full spoken history, and compressed spoken history approaches.\nOur experiments on the SpokenWOZ corpus demonstrate that providing the full\nspoken conversation as input yields the highest performance among models of\nsimilar size, significantly surpassing prior methods. Furthermore, we show that\nattention-pooling-based compression of the spoken history offers a strong\ntrade-off, maintaining competitive accuracy with reduced context size. Detailed\nanalysis confirms that improvements stem from more effective context\nutilization."}
{"id": "2510.09302", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09302", "abs": "https://arxiv.org/abs/2510.09302", "authors": ["Yuying Li", "Siyi Qian", "Hao Liang", "Leqi Zheng", "Ruichuan An", "Yongzhen Guo", "Wentao Zhang"], "title": "CapGeo: A Caption-Assisted Approach to Geometric Reasoning", "comment": "preprint, under review", "summary": "Geometric reasoning remains a core challenge for Multimodal Large Language\nModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3\nand Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite\nexhibiting strong textual reasoning abilities on tasks like the International\nMathematical Olympiad (IMO). This gap suggests that the bottleneck lies in\nunderstanding geometric diagrams rather than reasoning itself. Since geometric\nfigures can often be faithfully described in concise textual form, converting\nvisual content into captions offers a promising direction. Motivated by this\ninsight, we introduce CapGeo, a caption-assisted reasoning framework that\nbridges visual and textual modalities. Experiments show substantial\nimprovements when models are equipped with captions: Qwen2.5-VL-72B improves\nfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to\n73.0%. To systematically evaluate and identify high-quality geometric\ncaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated\nfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based\nevaluation metric that correlates strongly with downstream CapGeo performance,\nenabling reliable assessment of geometric captioning ability. Together, our\nframework and benchmark highlight a new pathway toward advancing geometric\nreasoning in MLLMs."}
{"id": "2510.09474", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09474", "abs": "https://arxiv.org/abs/2510.09474", "authors": ["Zhenhailong Wang", "Jiateng Liu", "Amin Fazel", "Ritesh Sarkhel", "Xing Fan", "Xiang Li", "Chenlei Guo", "Heng Ji", "Ruhi Sarikaya"], "title": "Multimodal Policy Internalization for Conversational Agents", "comment": null, "summary": "Modern conversational agents like ChatGPT and Alexa+ rely on predefined\npolicies specifying metadata, response styles, and tool-usage rules. As these\nLLM-based systems expand to support diverse business and user queries, such\npolicies, often implemented as in-context prompts, are becoming increasingly\ncomplex and lengthy, making faithful adherence difficult and imposing large\nfixed computational costs. With the rise of multimodal agents, policies that\ngovern visual and multimodal behaviors are critical but remain understudied.\nPrior prompt-compression work mainly shortens task templates and\ndemonstrations, while existing policy-alignment studies focus only on\ntext-based safety rules. We introduce Multimodal Policy Internalization (MPI),\na new task that internalizes reasoning-intensive multimodal policies into model\nparameters, enabling stronger policy-following without including the policy\nduring inference. MPI poses unique data and algorithmic challenges. We build\ntwo datasets spanning synthetic and real-world decision-making and tool-using\ntasks and propose TriMPI, a three-stage training framework. TriMPI first\ninjects policy knowledge via continual pretraining, then performs supervised\nfinetuning, and finally applies PolicyRollout, a GRPO-style reinforcement\nlearning extension that augments rollouts with policy-aware responses for\ngrounded exploration. TriMPI achieves notable gains in end-to-end accuracy,\ngeneralization, and robustness to forgetting. As the first work on multimodal\npolicy internalization, we provide datasets, training recipes, and\ncomprehensive evaluations to foster future research. Project page:\nhttps://mikewangwzhl.github.io/TriMPI."}
{"id": "2510.09320", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09320", "abs": "https://arxiv.org/abs/2510.09320", "authors": ["Wenyao Zhang", "Hongsi Liu", "Bohan Li", "Jiawei He", "Zekun Qi", "Yunnan Wang", "Shengyang Zhao", "Xinqiang Yu", "Wenjun Zeng", "Xin Jin"], "title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation", "comment": "ICCV 2025", "summary": "Current self-supervised monocular depth estimation (MDE) approaches encounter\nperformance limitations due to insufficient semantic-spatial knowledge\nextraction. To address this challenge, we propose Hybrid-depth, a novel\nframework that systematically integrates foundation models (e.g., CLIP and\nDINO) to extract visual priors and acquire sufficient contextual information\nfor MDE. Our approach introduces a coarse-to-fine progressive learning\nframework: 1) Firstly, we aggregate multi-grained features from CLIP (global\nsemantics) and DINO (local spatial details) under contrastive language\nguidance. A proxy task comparing close-distant image patches is designed to\nenforce depth-aware feature alignment using text prompts; 2) Next, building on\nthe coarse features, we integrate camera pose information and pixel-wise\nlanguage alignment to refine depth predictions. This module seamlessly\nintegrates with existing self-supervised MDE pipelines (e.g., Monodepth2,\nManyDepth) as a plug-and-play depth encoder, enhancing continuous depth\nestimation. By aggregating CLIP's semantic context and DINO's spatial details\nthrough language guidance, our method effectively addresses feature granularity\nmismatches. Extensive experiments on the KITTI benchmark demonstrate that our\nmethod significantly outperforms SOTA methods across all metrics, which also\nindeed benefits downstream tasks like BEV perception. Code is available at\nhttps://github.com/Zhangwenyao1/Hybrid-depth."}
{"id": "2510.09599", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09599", "abs": "https://arxiv.org/abs/2510.09599", "authors": ["Sondos Mahmoud Bsharat", "Zhiqiang Shen"], "title": "Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation", "comment": "Our code and data are available at https://github.com/VILA-Lab/PTTS", "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities when provided with chain-of-thought exemplars, but curating large\nreasoning datasets remains laborious and resource-intensive. In this work, we\nintroduce Prompting Test-Time Scaling (P-TTS), a simple yet effective\ninference-time data augmentation strategy for enhancing LLM reasoning through\nfinetuning. Rather than collecting thousands or even millions of examples,\nP-TTS leverages a small pool of only 90 manually selected reasoning instances\nand systematically varies exemplar augmentation through principled instruction\nprompting intensities at test time to synthesize diverse reasoning trajectory\ncontexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.\nAcross a suite of mathematical reasoning AIME2024 & 25, MATH500, and\nGPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive\nbaselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of\n+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);\nP-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and\n+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better\nperformance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances\nzero-shot generalization accuracy on out-of-domain reasoning benchmarks of\nGaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our\nanalysis suggests that test-time scaling effectively explores the latent space\nof reasoning patterns, amplifying LLM problem-solving with minimal annotation\noverhead, and further unlocking the reasoning potential and capabilities of\nLLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit\nLLM reasoning in resource-constrained or rapidly evolving domains."}
{"id": "2510.09358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09358", "abs": "https://arxiv.org/abs/2510.09358", "authors": ["Qihang Ma", "Shengyu Li", "Jie Tang", "Dingkang Yang", "Shaodong Chen", "Yingyi Zhang", "Chao Feng", "Jiao Ran"], "title": "Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models", "comment": "EMNLP2025. Code is avaible at https://github.com/bytedance/DynamicCoT", "summary": "Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only\nmethods by incorporating multiple modalities of input information to produce a\nset of conclusive phrases. Traditional multi-modal approaches have been proven\nto have significant limitations in handling the challenging absence and unseen\nscenarios. Additionally, we identify shortcomings in existing benchmarks that\noverestimate model capability due to significant overlap in training tests. In\nthis work, we propose leveraging vision-language models (VLMs) for the MMKP\ntask. Firstly, we use two widely-used strategies, e.g., zero-shot and\nsupervised fine-tuning (SFT) to assess the lower bound performance of VLMs.\nNext, to improve the complex reasoning capabilities of VLMs, we adopt\nFine-tune-CoT, which leverages high-quality CoT reasoning data generated by a\nteacher model to finetune smaller models. Finally, to address the\n\"overthinking\" phenomenon, we propose a dynamic CoT strategy which adaptively\ninjects CoT data during training, allowing the model to flexibly leverage its\nreasoning capabilities during the inference stage. We evaluate the proposed\nstrategies on various datasets and the experimental results demonstrate the\neffectiveness of the proposed approaches. The code is available at\nhttps://github.com/bytedance/DynamicCoT."}
{"id": "2510.09608", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09608", "abs": "https://arxiv.org/abs/2510.09608", "authors": ["Ruyi Xu", "Guangxuan Xiao", "Yukang Chen", "Liuning He", "Kelly Peng", "Yao Lu", "Song Han"], "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams", "comment": "The first two authors contributed equally to this work", "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."}
{"id": "2510.09361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09361", "abs": "https://arxiv.org/abs/2510.09361", "authors": ["Junyan Ye", "Dongzhi Jiang", "Jun He", "Baichuan Zhou", "Zilong Huang", "Zhiyuan Yan", "Hongsheng Li", "Conghui He", "Weijia Li"], "title": "BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception", "comment": "Accepted to 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Track on Datasets and Benchmarks", "summary": "Recently, Multimodal Large Language Models (MLLMs) have made rapid progress,\nparticularly in enhancing their reasoning capabilities. However, existing\nreasoning benchmarks still primarily assess language-based reasoning, often\ntreating visual input as replaceable context. To address this gap, we introduce\nBLINK-Twice, a vision-centric reasoning benchmark grounded in challenging\nperceptual tasks. Instead of relying on external knowledge, our tasks require\nmodels to reason from visual content alone, shifting the focus from\nlanguage-based to image-grounded reasoning. Compared to prior perception\nbenchmarks, it moves beyond shallow perception (\"see\") and requires\nfine-grained observation and analytical reasoning (\"observe\"). BLINK-Twice\nintegrates three core components: seven types of visual challenges for testing\nvisual reasoning, natural adversarial image pairs that enforce reliance on\nvisual content, and annotated reasoning chains for fine-grained evaluation of\nthe reasoning process rather than final answers alone. We evaluate 20 leading\nMLLMs, including 12 foundation models and 8 reasoning-enhanced models.\nBLINK-Twice poses a significant challenge to current models. While existing\nreasoning strategies in the language space-such as chain-of-thought or\nself-criticism can improve performance, they often result in unstable and\nredundant reasoning. We observe that repeated image observation improves\nperformance across models, and active visual interaction, as demonstrated by\nmodels like o3, highlights the need for a new paradigm for vision reasoning.\nThe dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice"}
{"id": "2510.09438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09438", "abs": "https://arxiv.org/abs/2510.09438", "authors": ["Jin-Chuan Shi", "Chengye Su", "Jiajun Wang", "Ariel Shamir", "Miao Wang"], "title": "Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians", "comment": "19 pages, 9 figures", "summary": "Editing 4D scenes reconstructed from monocular videos based on text prompts\nis a valuable yet challenging task with broad applications in content creation\nand virtual environments. The key difficulty lies in achieving semantically\nprecise edits in localized regions of complex, dynamic scenes, while preserving\nthe integrity of unedited content. To address this, we introduce Mono4DEditor,\na novel framework for flexible and accurate text-driven 4D scene editing. Our\nmethod augments 3D Gaussians with quantized CLIP features to form a\nlanguage-embedded dynamic representation, enabling efficient semantic querying\nof arbitrary spatial regions. We further propose a two-stage point-level\nlocalization strategy that first selects candidate Gaussians via CLIP\nsimilarity and then refines their spatial extent to improve accuracy. Finally,\ntargeted edits are performed on localized regions using a diffusion-based video\nediting model, with flow and scribble guidance ensuring spatial fidelity and\ntemporal coherence. Extensive experiments demonstrate that Mono4DEditor enables\nhigh-quality, text-driven edits across diverse scenes and object types, while\npreserving the appearance and geometry of unedited areas and surpassing prior\napproaches in both flexibility and visual fidelity."}
{"id": "2510.09473", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09473", "abs": "https://arxiv.org/abs/2510.09473", "authors": ["Jisu Han", "Wonjun Hwang"], "title": "D-TPT: Dimensional Entropy Maximization for Calibrating Test-Time Prompt Tuning in Vision-Language Models", "comment": null, "summary": "Test-time adaptation paradigm provides flexibility towards domain shifts by\nperforming immediate adaptation on unlabeled target data from the source model.\nVision-Language Models (VLMs) leverage their generalization capabilities for\ndiverse downstream tasks, and test-time prompt tuning has emerged as a\nprominent solution for adapting VLMs. In this work, we explore contrastive VLMs\nand identify the modality gap caused by a single dominant feature dimension\nacross modalities. We observe that the dominant dimensions in both text and\nimage modalities exhibit high predictive sensitivity, and that constraining\ntheir influence can improve calibration error. Building on this insight, we\npropose dimensional entropy maximization that regularizes the distribution of\ntextual features toward uniformity to mitigate the dependency of dominant\ndimensions. Our method alleviates the degradation of calibration performance in\ntest-time prompt tuning, offering a simple yet effective solution to enhance\nthe reliability of VLMs in real-world deployment scenarios."}
{"id": "2510.09475", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09475", "abs": "https://arxiv.org/abs/2510.09475", "authors": ["Ruben Pascual", "Mikel Sesma-Sara", "Aranzazu Jurio", "Daniel Paternain", "Mikel Galar"], "title": "Few-shot multi-token DreamBooth with LoRa for style-consistent character generation", "comment": null, "summary": "The audiovisual industry is undergoing a profound transformation as it is\nintegrating AI developments not only to automate routine tasks but also to\ninspire new forms of art. This paper addresses the problem of producing a\nvirtually unlimited number of novel characters that preserve the artistic style\nand shared visual traits of a small set of human-designed reference characters,\nthus broadening creative possibilities in animation, gaming, and related\ndomains. Our solution builds upon DreamBooth, a well-established fine-tuning\ntechnique for text-to-image diffusion models, and adapts it to tackle two core\nchallenges: capturing intricate character details beyond textual prompts and\nthe few-shot nature of the training data. To achieve this, we propose a\nmulti-token strategy, using clustering to assign separate tokens to individual\ncharacters and their collective style, combined with LoRA-based\nparameter-efficient fine-tuning. By removing the class-specific regularization\nset and introducing random tokens and embeddings during generation, our\napproach allows for unlimited character creation while preserving the learned\nstyle. We evaluate our method on five small specialized datasets, comparing it\nto relevant baselines using both quantitative metrics and a human evaluation\nstudy. Our results demonstrate that our approach produces high-quality, diverse\ncharacters while preserving the distinctive aesthetic features of the reference\ncharacters, with human evaluation further reinforcing its effectiveness and\nhighlighting the potential of our method."}
{"id": "2510.09507", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09507", "abs": "https://arxiv.org/abs/2510.09507", "authors": ["Zixin Zhang", "Kanghao Chen", "Xingwang Lin", "Lutao Jiang", "Xu Zheng", "Yuanhuiyi Lyu", "Litao Guo", "Yinchuan Li", "Ying-Cong Chen"], "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs", "comment": null, "summary": "The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available."}
{"id": "2510.09586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09586", "abs": "https://arxiv.org/abs/2510.09586", "authors": ["Fengming Lin"], "title": "Vision Language Models: A Survey of 26K Papers", "comment": "VLM/LLM Learning Notes", "summary": "We present a transparent, reproducible measurement of research trends across\n26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles\nand abstracts are normalized, phrase-protected, and matched against a\nhand-crafted lexicon to assign up to 35 topical labels and mine fine-grained\ncues about tasks, architectures, training regimes, objectives, datasets, and\nco-mentioned modalities. The analysis quantifies three macro shifts: (1) a\nsharp rise of multimodal vision-language-LLM work, which increasingly reframes\nclassic perception as instruction following and multi-step reasoning; (2)\nsteady expansion of generative methods, with diffusion research consolidating\naround controllability, distillation, and speed; and (3) resilient 3D and video\nactivity, with composition moving from NeRFs to Gaussian splatting and a\ngrowing emphasis on human- and agent-centric understanding. Within VLMs,\nparameter-efficient adaptation like prompting/adapters/LoRA and lightweight\nvision-language bridges dominate; training practice shifts from building\nencoders from scratch to instruction tuning and finetuning strong backbones;\ncontrastive objectives recede relative to cross-entropy/ranking and\ndistillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and\nICLR the highest VLM share, while reliability themes such as efficiency or\nrobustness diffuse across areas. We release the lexicon and methodology to\nenable auditing and extension. Limitations include lexicon recall and\nabstract-only scope, but the longitudinal signals are consistent across venues\nand years."}
{"id": "2510.09607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09607", "abs": "https://arxiv.org/abs/2510.09607", "authors": ["Shaoqi Dong", "Chaoyou Fu", "Haihan Gao", "Yi-Fan Zhang", "Chi Yan", "Chu Wu", "Xiaoyu Liu", "Yunhang Shen", "Jing Huo", "Deqiang Jiang", "Haoyu Cao", "Yang Gao", "Xing Sun", "Ran He", "Caifeng Shan"], "title": "VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation", "comment": "Homepage: https://ltbai.github.io/VITA-VLA/", "summary": "Vision-Language Action (VLA) models significantly advance robotic\nmanipulation by leveraging the strong perception capabilities of pretrained\nvision-language models (VLMs). By integrating action modules into these\npretrained models, VLA methods exhibit improved generalization. However,\ntraining them from scratch is costly. In this work, we propose a simple yet\neffective distillation-based framework that equips VLMs with action-execution\ncapability by transferring knowledge from pretrained small action models. Our\narchitecture retains the original VLM structure, adding only an action token\nand a state encoder to incorporate physical inputs. To distill action\nknowledge, we adopt a two-stage training strategy. First, we perform\nlightweight alignment by mapping VLM hidden states into the action space of the\nsmall action model, enabling effective reuse of its pretrained action decoder\nand avoiding expensive pretraining. Second, we selectively fine-tune the\nlanguage model, state encoder, and action modules, enabling the system to\nintegrate multimodal inputs with precise action generation. Specifically, the\naction token provides the VLM with a direct handle for predicting future\nactions, while the state encoder allows the model to incorporate robot dynamics\nnot captured by vision alone. This design yields substantial efficiency gains\nover training large VLA models from scratch. Compared with previous\nstate-of-the-art methods, our method achieves 97.3% average success rate on\nLIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In\nreal-world experiments across five manipulation tasks, our method consistently\noutperforms the teacher model, achieving 82.0% success rate (17% improvement),\nwhich demonstrate that action distillation effectively enables VLMs to generate\nprecise actions while substantially reducing training costs."}
{"id": "2510.09608", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09608", "abs": "https://arxiv.org/abs/2510.09608", "authors": ["Ruyi Xu", "Guangxuan Xiao", "Yukang Chen", "Liuning He", "Kelly Peng", "Yao Lu", "Song Han"], "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams", "comment": "The first two authors contributed equally to this work", "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."}
{"id": "2510.08713", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08713", "abs": "https://arxiv.org/abs/2510.08713", "authors": ["Yifei Dong", "Fengyi Wu", "Guangyu Chen", "Zhi-Qi Cheng", "Qiyu Hu", "Yuxuan Zhou", "Jingdong Sun", "Jun-Yan He", "Qi Dai", "Alexander G Hauptmann"], "title": "Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation", "comment": "18 pages, 11 figures, code: https://github.com/F1y1113/UniWM", "summary": "Enabling embodied agents to effectively imagine future states is critical for\nrobust and generalizable visual navigation. Current state-of-the-art\napproaches, however, adopt modular architectures that separate navigation\nplanning from visual world modeling, leading to state-action misalignment and\nlimited adaptability in novel or dynamic scenarios. To overcome this\nfundamental limitation, we propose UniWM, a unified, memory-augmented world\nmodel integrating egocentric visual foresight and planning within a single\nmultimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly\ngrounds action decisions in visually imagined outcomes, ensuring tight\nalignment between prediction and control. A hierarchical memory mechanism\nfurther integrates detailed short-term perceptual cues with longer-term\ntrajectory context, enabling stable, coherent reasoning over extended horizons.\nExtensive experiments across four challenging benchmarks (Go Stanford, ReCon,\nSCAND, HuRoN) demonstrate that UniWM substantially improves navigation success\nrates by up to 30%, significantly reduces trajectory errors compared to strong\nbaselines, and exhibits impressive zero-shot generalization on the unseen\nTartanDrive dataset. These results highlight UniWM as a principled step toward\nunified, imagination-driven embodied navigation."}
{"id": "2510.09060", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09060", "abs": "https://arxiv.org/abs/2510.09060", "authors": ["Jingxuan Wu", "Zhenglin Wan", "Xingrui Yu", "Yuzhe Yang", "Bo An", "Ivor Tsang"], "title": "OSCAR: Orthogonal Stochastic Control for Alignment-Respecting Diversity in Flow Matching", "comment": null, "summary": "Flow-based text-to-image models follow deterministic trajectories, forcing\nusers to repeatedly sample to discover diverse modes, which is a costly and\ninefficient process. We present a training-free, inference-time control\nmechanism that makes the flow itself diversity-aware. Our method simultaneously\nencourages lateral spread among trajectories via a feature-space objective and\nreintroduces uncertainty through a time-scheduled stochastic perturbation.\nCrucially, this perturbation is projected to be orthogonal to the generation\nflow, a geometric constraint that allows it to boost variation without\ndegrading image details or prompt fidelity. Our procedure requires no\nretraining or modification to the base sampler and is compatible with common\nflow-matching solvers. Theoretically, our method is shown to monotonically\nincrease a volume surrogate while, due to its geometric constraints,\napproximately preserving the marginal distribution. This provides a principled\nexplanation for why generation quality is robustly maintained. Empirically,\nacross multiple text-to-image settings under fixed sampling budgets, our method\nconsistently improves diversity metrics such as the Vendi Score and Brisque\nover strong baselines, while upholding image quality and alignment."}
