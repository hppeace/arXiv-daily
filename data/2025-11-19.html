<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-19.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-compevent-complex-valued-event-rgb-fusion-for-low-light-video-enhancement-and-deblurring">[1] <a href="https://arxiv.org/abs/2511.14469">CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring</a></h3>
<p><em>Mingchen Zhong, Xin Lu, Dong Li, Senyan Xu, Ruixuan Jiang, Xueyang Fu, Baocai Yin</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCompEventï¼Œä¸€ç§åŸºäºå¤æ•°ç¥ç»ç½‘ç»œçš„æ¡†æ¶ï¼Œå®ç°äº‹ä»¶æ•°æ®å’ŒRGBè§†é¢‘å¸§çš„å…¨æµç¨‹èåˆï¼Œæ˜¾è‘—æå‡äº†ä½å…‰è§†é¢‘å»æ¨¡ç³Šæ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤æ•°æ—¶é—´å¯¹é½GRUå’Œå¤æ•°ç©ºé—´-é¢‘ç‡å­¦ä¹ æ¨¡å—ï¼Œåœ¨æ—¶ç©ºç»´åº¦ä¸Šå®ç°æ·±åº¦æ¨¡æ€äº’è¡¥å­¦ä¹ ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä½å…‰è§†é¢‘å»æ¨¡ç³Šåœ¨å¤œé—´ç›‘æ§å’Œè‡ªåŠ¨é©¾é©¶ç­‰åº”ç”¨ä¸­é¢ä¸´ä¸¥é‡æŒ‘æˆ˜ï¼Œç°æœ‰åŸºäºäº‹ä»¶ç›¸æœºçš„èåˆæ–¹æ³•é€šå¸¸é‡‡ç”¨åˆ†é˜¶æ®µç­–ç•¥ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹ä½å…‰ç…§å’Œè¿åŠ¨æ¨¡ç³Šçš„è”åˆé€€åŒ–é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> CompEventæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤æ•°æ—¶é—´å¯¹é½GRUåˆ©ç”¨å¤æ•°å·ç§¯å’ŒGRUè¿­ä»£å¤„ç†è§†é¢‘å’Œäº‹ä»¶æµå®ç°æ—¶é—´å¯¹é½å’Œè¿ç»­èåˆï¼›å¤æ•°ç©ºé—´-é¢‘ç‡å­¦ä¹ æ¨¡å—åœ¨ç©ºé—´åŸŸå’Œé¢‘åŸŸæ‰§è¡Œç»Ÿä¸€çš„å¤æ•°ä¿¡å·å¤„ç†ï¼Œé€šè¿‡ç©ºé—´ç»“æ„å’Œç³»ç»Ÿçº§ç‰¹æ€§ä¿ƒè¿›æ·±åº¦èåˆã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCompEventåœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜æ€§ä»»åŠ¡æ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨ä½å…‰è§†é¢‘å»æ¨¡ç³Šæ–¹é¢çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡å¤æ•°ç¥ç»ç½‘ç»œçš„æ•´ä½“è¡¨ç¤ºèƒ½åŠ›ï¼ŒCompEventå®ç°äº†å…¨æµç¨‹æ—¶ç©ºèåˆï¼Œæœ€å¤§åŒ–æ¨¡æ€é—´çš„äº’è¡¥å­¦ä¹ ï¼Œä¸ºä½å…‰è§†é¢‘æ¢å¤æä¾›äº†æ–°çš„æŠ€æœ¯é€”å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task. The code is available at https://github.com/YuXie1/CompEvent.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="2-ms2edge-towards-energy-efficient-and-crisp-edge-detection-with-multi-scale-residual-learning-in-snns">[2] <a href="https://arxiv.org/abs/2511.13735">MS2Edge: Towards Energy-Efficient and Crisp Edge Detection with Multi-Scale Residual Learning in SNNs</a></h3>
<p><em>Yimeng Fan, Changsong Liu, Mingyang Li, Yuzhou Dai, Yanyan Liu, Wei Zhang</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MS2Edgeï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰çš„è¾¹ç¼˜æ£€æµ‹æ¨¡å‹ï¼Œé€šè¿‡å¤šå°ºåº¦æ®‹å·®å­¦ä¹ å’Œæ–°å‹ç¥ç»å…ƒè®¾è®¡è§£å†³äº†SNNé‡åŒ–è¯¯å·®å¯¼è‡´çš„è¾¹ç¼˜ä¸è¿ç»­é—®é¢˜ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæä½èƒ½è€—ä¸”æ— éœ€åå¤„ç†ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰çš„è¾¹ç¼˜æ£€æµ‹æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šéœ€è¦å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®å’Œå¤æ‚å…ˆéªŒçŸ¥è¯†è®¾è®¡å¯¼è‡´é«˜èƒ½è€—ï¼Œä»¥åŠé¢„æµ‹è¾¹ç¼˜æ¸…æ™°åº¦å·®ä¸”ä¸¥é‡ä¾èµ–åå¤„ç†ã€‚è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰è™½ç„¶å…·æœ‰é‡åŒ–ç‰¹æ€§å’Œè„‰å†²é©±åŠ¨è®¡ç®—æœºåˆ¶ï¼Œèƒ½å¤Ÿæä¾›è¾¹ç¼˜æ£€æµ‹çš„å¼ºå…ˆéªŒå¹¶æŠ‘åˆ¶çº¹ç†ä¼ªå½±ï¼Œä½†å…¶é‡åŒ–è¯¯å·®ä¼šå¼•å…¥ç¨€ç–è¾¹ç¼˜ä¸è¿ç»­æ€§ï¼Œé™åˆ¶äº†æ¸…æ™°åº¦çš„è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„MS2Edgeæ¨¡å‹æ ¸å¿ƒæ˜¯æ„å»ºäº†åä¸ºMS2ResNetçš„æ–°å‹è„‰å†²éª¨å¹²ç½‘ç»œï¼Œè¯¥ç½‘ç»œé›†æˆäº†å¤šå°ºåº¦æ®‹å·®å­¦ä¹ ä»¥æ¢å¤ç¼ºå¤±çš„è¾¹ç•Œçº¿å¹¶ç”Ÿæˆæ¸…æ™°è¾¹ç¼˜ï¼ŒåŒæ—¶ç»“åˆI-LIFç¥ç»å…ƒå’ŒåŸºäºè†œç”µä½çš„å˜å½¢æ·å¾„ï¼ˆMDSï¼‰æ¥å‡è½»é‡åŒ–è¯¯å·®ã€‚æ¨¡å‹è¿˜åŒ…å«ç”¨äºä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­ç»†èŠ‚é‡å»ºçš„è„‰å†²å¤šå°ºåº¦ä¸Šé‡‡æ ·å—ï¼ˆSMSUBï¼‰ï¼Œä»¥åŠç”¨äºæœ‰æ•ˆæ•´åˆå¤šæ—¶é—´æ­¥è¾¹ç¼˜å›¾çš„è†œå¹³å‡è§£ç ï¼ˆMADï¼‰æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒMS2Edgeåœ¨BSDS500ã€NYUDv2ã€BIPEDã€PLDUå’ŒPLDMæ•°æ®é›†ä¸Šè¶…è¶Šäº†åŸºäºANNçš„æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”æ— éœ€é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œã€‚è¯¥æ¨¡å‹åŒæ—¶ä¿æŒäº†æä½çš„èƒ½è€—ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆæ— éœ€åå¤„ç†çš„æ¸…æ™°è¾¹ç¼˜å›¾ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹ç ”ç©¶è¯æ˜äº†SNNåœ¨è¾¹ç¼˜æ£€æµ‹ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„æœ‰æ•ˆå…‹æœäº†é‡åŒ–è¯¯å·®å¸¦æ¥çš„æŒ‘æˆ˜ã€‚MS2Edgeçš„æˆåŠŸè¡¨æ˜SNNä¸ä»…èƒ½å¤Ÿå®ç°èƒ½æºé«˜æ•ˆçš„è¾¹ç¼˜æ£€æµ‹ï¼Œè¿˜èƒ½åœ¨æ€§èƒ½ä¸Šè¶…è¶Šä¼ ç»ŸANNæ–¹æ³•ï¼Œä¸ºæœªæ¥ä½åŠŸè€—è®¡ç®—æœºè§†è§‰åº”ç”¨æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Edge detection with Artificial Neural Networks (ANNs) has achieved remarkable prog-ress but faces two major challenges. First, it requires pre-training on large-scale extra data and complex designs for prior knowledge, leading to high energy consumption. Second, the predicted edges perform poorly in crispness and heavily rely on post-processing. Spiking Neural Networks (SNNs), as third generation neural networks, feature quantization and spike-driven computation mechanisms. They inherently provide a strong prior for edge detection in an energy-efficient manner, while its quantization mechanism helps suppress texture artifact interference around true edges, improving prediction crispness. However, the resulting quantization error inevitably introduces sparse edge discontinuities, compromising further enhancement of crispness. To address these challenges, we propose MS2Edge, the first SNN-based model for edge detection. At its core, we build a novel spiking backbone named MS2ResNet that integrates multi-scale residual learning to recover missing boundary lines and generate crisp edges, while combining I-LIF neurons with Membrane-based Deformed Shortcut (MDS) to mitigate quantization errors. The model is complemented by a Spiking Multi-Scale Upsample Block (SMSUB) for detail reconstruction during upsampling and a Membrane Average Decoding (MAD) method for effective integration of edge maps across multiple time steps. Experimental results demonstrate that MS2Edge outperforms ANN-based methods and achieves state-of-the-art performance on the BSDS500, NYUDv2, BIPED, PLDU, and PLDM datasets without pre-trained backbones, while maintaining ultralow energy consumption and generating crisp edge maps without post-processing.</p>
  </article>
</body>
</html>
