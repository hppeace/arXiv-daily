<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-26.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-realizing-fully-integrated-low-power-event-based-pupil-tracking-with-neuromorphic-hardware">[1] <a href="https://arxiv.org/abs/2511.20175">Realizing Fully-Integrated, Low-Power, Event-Based Pupil Tracking with Neuromorphic Hardware</a></h3>
<p><em>Federico Paredes-Valles, Yoshitaka Miyatani, Kirk Y. W. Scheper</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªç”µæ± ä¾›ç”µçš„å¯ç©¿æˆ´ç³å­”ä¸­å¿ƒè¿½è¸ªç³»ç»Ÿï¼Œé€šè¿‡äº‹ä»¶è§†è§‰ä¼ æ„Ÿä¸ç¥ç»å½¢æ€å¤„ç†çš„ç«¯åˆ°ç«¯é›†æˆï¼Œåœ¨Speck2fç‰‡ä¸Šç³»ç»Ÿä¸Šå®ç°äº†100HzåŒç›®å…‰å­¦è¿½è¸ªï¼Œæ¯çœ¼å¹³å‡åŠŸè€—ä½äº5mWã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¯ç©¿æˆ´å¹³å°åœ¨å®ç°ç¨³å¥ã€é«˜é¢‘ç‡çœ¼åŠ¨è¿½è¸ªçš„åŒæ—¶ä¿æŒè¶…ä½åŠŸè€—é¢ä¸´æŒ‘æˆ˜ï¼Œäº‹ä»¶è§†è§‰ä¼ æ„Ÿå™¨è™½ç„¶æä¾›å¾®ç§’çº§åˆ†è¾¨ç‡å’Œç¨€ç–æ•°æ®æµï¼Œä½†ç¼ºä¹å®Œå…¨é›†æˆçš„ä½åŠŸè€—å®æ—¶æ¨ç†è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨å•†ç”¨Speck2fç‰‡ä¸Šç³»ç»Ÿé›†æˆäº‹ä»¶ä¼ æ„Ÿä¸ç¥ç»å½¢æ€å¤„ç†ï¼Œç»“åˆä½åŠŸè€—å¾®æ§åˆ¶å™¨è¿›è¡Œè½»é‡çº§åæ ‡è§£ç ï¼›æå‡ºæ–°å‹ä¸ç¡®å®šæ€§é‡åŒ–è„‰å†²ç¥ç»ç½‘ç»œï¼Œé…å¤‡é—¨æ§æ—¶åºè§£ç æœºåˆ¶ï¼Œé’ˆå¯¹ä¸¥æ ¼å†…å­˜å’Œå¸¦å®½çº¦æŸä¼˜åŒ–ï¼Œå¹¶é€šè¿‡ç³»ç»ŸåŒ–éƒ¨ç½²æœºåˆ¶å¼¥åˆç°å®å·®è·ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šç”¨æˆ·æ•°æ®é›†ä¸ŠéªŒè¯äº†ç³»ç»Ÿæ€§èƒ½ï¼Œå¯ç©¿æˆ´åŸå‹é…å¤‡åŒç¥ç»å½¢æ€è®¾å¤‡ï¼Œå®ç°100Hzç¨³å¥åŒç›®å…‰å­¦è¿½è¸ªï¼Œæ¯çœ¼å¹³å‡åŠŸè€—ä½äº5mWã€‚</p>
<p><strong>Conclusion:</strong> ç«¯åˆ°ç«¯ç¥ç»å½¢æ€è®¡ç®—ä¸ºä¸‹ä¸€ä»£é«˜èƒ½æ•ˆå¯ç©¿æˆ´ç³»ç»Ÿå®ç°äº†å®ç”¨çš„å¸¸å¼€çœ¼åŠ¨è¿½è¸ªï¼Œè¯æ˜äº†å®Œå…¨é›†æˆçš„äº‹ä»¶é©±åŠ¨ç³»ç»Ÿåœ¨è¶…ä½åŠŸè€—ä¸‹çš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Eye tracking is fundamental to numerous applications, yet achieving robust, high-frequency tracking with ultra-low power consumption remains challenging for wearable platforms. While event-based vision sensors offer microsecond resolution and sparse data streams, they have lacked fully integrated, low-power processing solutions capable of real-time inference. In this work, we present the first battery-powered, wearable pupil-center-tracking system with complete on-device integration, combining event-based sensing and neuromorphic processing on the commercially available Speck2f system-on-chip with lightweight coordinate decoding on a low-power microcontroller. Our solution features a novel uncertainty-quantifying spiking neural network with gated temporal decoding, optimized for strict memory and bandwidth constraints, complemented by systematic deployment mechanisms that bridge the reality gap. We validate our system on a new multi-user dataset and demonstrate a wearable prototype with dual neuromorphic devices achieving robust binocular pupil tracking at 100 Hz with an average power consumption below 5 mW per eye. Our work demonstrates that end-to-end neuromorphic computing enables practical, always-on eye tracking for next-generation energy-efficient wearable systems.</p>
  </article>
</body>
</html>
