<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-17.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-tumtraf-emot-event-based-multi-object-tracking-dataset-and-baseline-for-traffic-scenarios">[1] <a href="https://arxiv.org/abs/2512.14595">TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios</a></h3>
<p><em>Mengyu Li, Xingcheng Zhou, Guang Chen, Alois Knoll, Hu Cao</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é’ˆå¯¹æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­ä¼ ç»Ÿå¸§ç›¸æœºåœ¨å¼±å…‰å’Œé«˜åŠ¨æ€åœºæ™¯ä¸‹çš„æ€§èƒ½é™åˆ¶ï¼Œæå‡ºäº†é¦–ä¸ªé¢å‘äº‹ä»¶ç›¸æœºçš„ITSæ•°æ®é›†ï¼Œå¹¶å»ºç«‹äº†åŸºäºæ£€æµ‹çš„è·Ÿè¸ªåŸºå‡†ï¼Œå–å¾—äº†ä¼˜å¼‚çš„è·Ÿè¸ªæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„å¤šç›®æ ‡è·Ÿè¸ªä¸»è¦ä¾èµ–å¸§ç›¸æœºï¼Œä½†å¸§ç›¸æœºåœ¨å¼±å…‰ç…§å’Œé«˜é€Ÿè¿åŠ¨æ¡ä»¶ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚äº‹ä»¶ç›¸æœºå…·æœ‰ä½å»¶è¿Ÿã€é«˜åŠ¨æ€èŒƒå›´å’Œé«˜æ—¶é—´åˆ†è¾¨ç‡çš„ç‰¹æ€§ï¼Œæœ‰æœ›è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†ç›®å‰äº‹ä»¶è§†è§‰åœ¨ITSé¢†åŸŸçš„ç ”ç©¶ç›¸å¯¹åŒ®ä¹ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æ„å»ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹äº‹ä»¶ç›¸æœºçš„æ™ºèƒ½äº¤é€šç³»ç»Ÿæ•°æ®é›†ï¼Œæ¶µç›–è½¦è¾†å’Œè¡Œäººçš„æ£€æµ‹ä¸è·Ÿè¸ªä»»åŠ¡ã€‚åŸºäºè¯¥æ•°æ®é›†å»ºç«‹äº†è·Ÿè¸ª-æ£€æµ‹åŸºå‡†ï¼Œå¹¶è®¾è®¡äº†ä¸“é—¨çš„ç‰¹å¾æå–å™¨æ¥å®ç°å¤šç›®æ ‡è·Ÿè¸ªã€‚</p>
<p><strong>Result:</strong> åœ¨è¯¥æ•°æ®é›†ä¸Šå»ºç«‹çš„è·Ÿè¸ªåŸºå‡†å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†äº‹ä»¶ç›¸æœºåœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­å¤šç›®æ ‡è·Ÿè¸ªä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†å¯é çš„è¯„ä¼°æ ‡å‡†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¡«è¡¥äº†äº‹ä»¶è§†è§‰åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿé¢†åŸŸçš„ç©ºç™½ï¼Œè¯æ˜äº†äº‹ä»¶ç›¸æœºåœ¨å¤æ‚äº¤é€šåœºæ™¯ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥åŸºäºäº‹ä»¶ç›¸æœºçš„ITSç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®é›†å’ŒåŸºå‡†æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.</p>
  </article>
</body>
</html>
