<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-12.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 1]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-neuromorphic-eye-tracking-for-low-latency-pupil-detection">[1] <a href="https://arxiv.org/abs/2512.09969">Neuromorphic Eye Tracking for Low-Latency Pupil Detection</a></h3>
<p><em>Paul Hueber, Luca Peres, Florian Pitters, Alejandro Gloriani, Oliver Rhodes</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¯ç©¿æˆ´çœ¼åŠ¨è¿½è¸ªçš„ç¥ç»å½¢æ€æ¨¡å‹ï¼Œé€šè¿‡å°†é«˜æ€§èƒ½äº‹ä»¶é©±åŠ¨çœ¼åŠ¨è¿½è¸ªæ¶æ„è½¬æ¢ä¸ºè„‰å†²ç¥ç»ç½‘ç»œï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶å®ç°äº†20å€æ¨¡å‹å‹ç¼©å’Œ850å€è®¡ç®—æ•ˆç‡æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»ŸåŸºäºå¸§çš„çœ¼åŠ¨è¿½è¸ªç³»ç»Ÿå­˜åœ¨è¿åŠ¨æ¨¡ç³Šã€è®¡ç®—æˆæœ¬é«˜å’Œæ—¶é—´åˆ†è¾¨ç‡æœ‰é™çš„é—®é¢˜ï¼Œéš¾ä»¥æ»¡è¶³å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰å¯ç©¿æˆ´è®¾å¤‡å¯¹ä½å»¶è¿Ÿå’Œæ¯«ç“¦çº§åŠŸè€—çš„éœ€æ±‚ï¼Œè€Œç°æœ‰è„‰å†²ç¥ç»ç½‘ç»œæ–¹æ³•è¦ä¹ˆè¿‡äºä¸“ç”¨åŒ–ï¼Œè¦ä¹ˆæ€§èƒ½ä¸åŠç°ä»£äººå·¥ç¥ç»ç½‘ç»œæ¶æ„ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶å°†é«˜æ€§èƒ½äº‹ä»¶é©±åŠ¨çœ¼åŠ¨è¿½è¸ªæ¨¡å‹è½¬æ¢ä¸ºç¥ç»å½¢æ€ç‰ˆæœ¬ï¼Œç”¨è½»é‡çº§LIFå±‚æ›¿æ¢åŸæœ‰çš„å¾ªç¯å’Œæ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶åˆ©ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯é™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œæ„å»ºäº†é«˜æ•ˆçš„è„‰å†²ç¥ç»ç½‘ç»œæ¶æ„ã€‚</p>
<p><strong>Result:</strong> æ‰€ææ¨¡å‹å®ç°äº†3.7-4.1åƒç´ çš„å¹³å‡è¯¯å·®ï¼Œæ¥è¿‘ä¸“ç”¨ç¥ç»å½¢æ€ç³»ç»ŸRetinaçš„3.24åƒç´ ç²¾åº¦ï¼ŒåŒæ—¶ç›¸æ¯”æœ€æ¥è¿‘çš„äººå·¥ç¥ç»ç½‘ç»œå˜ä½“ï¼Œæ¨¡å‹å¤§å°å‡å°‘20å€ï¼Œç†è®ºè®¡ç®—é‡é™ä½850å€ï¼Œé¢„è®¡å¯åœ¨3.9-4.9æ¯«ç“¦åŠŸè€—å’Œ3æ¯«ç§’å»¶è¿Ÿä¸‹ä»¥1åƒèµ«å…¹é¢‘ç‡è¿è¡Œã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜é«˜æ€§èƒ½äº‹ä»¶é©±åŠ¨çœ¼åŠ¨è¿½è¸ªæ¶æ„å¯ä»¥é‡æ–°è®¾è®¡ä¸ºè„‰å†²ç¥ç»ç½‘ç»œï¼Œåœ¨ä¿æŒå®æ—¶å¯ç©¿æˆ´éƒ¨ç½²æ‰€éœ€ç²¾åº¦çš„åŒæ—¶è·å¾—æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œä¸ºä½åŠŸè€—å¯ç©¿æˆ´çœ¼åŠ¨è¿½è¸ªç³»ç»Ÿæä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.</p>
<div id='cs.NE'></div>

<h1 id="csne-back">cs.NE <a href="#toc">[Back]</a></h1>
<h3 id="2-spatial-spiking-neural-networks-enable-efficient-and-robust-temporal-computation">[2] <a href="https://arxiv.org/abs/2512.10011">Spatial Spiking Neural Networks Enable Efficient and Robust Temporal Computation</a></h3>
<p><em>Lennart P. L. Landsmeer, Amirreza Movahedin, Mario Negrello, Said Hamdioui, Christos Strydis</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ç©ºé—´è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSpSNNï¼‰ï¼Œé€šè¿‡è®©ç¥ç»å…ƒå­¦ä¹ æœ‰é™ç»´æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­çš„åæ ‡ï¼Œä½¿çªè§¦å»¶è¿Ÿä»ç¥ç»å…ƒé—´çš„ç‰©ç†è·ç¦»ä¸­è‡ªç„¶äº§ç”Ÿï¼Œä»è€Œåœ¨ä¿æŒæ—¶é—´è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶å¤§å¹…å‡å°‘å‚æ•°æ•°é‡ï¼Œå¹¶ä¸ºé«˜æ•ˆç¥ç»å½¢æ€å®ç°æä¾›äº†ç¡¬ä»¶å‹å¥½çš„åŸºç¡€ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰å°†çªè§¦å»¶è¿Ÿè§†ä¸ºå®Œå…¨å¯è®­ç»ƒä¸”æ— çº¦æŸçš„å‚æ•°ï¼Œè¿™å¯¼è‡´å†…å­˜å ç”¨å¤§ã€è®¡ç®—éœ€æ±‚é«˜ï¼Œå¹¶åç¦»äº†ç”Ÿç‰©åˆç†æ€§ã€‚åœ¨å¤§è„‘ä¸­ï¼Œå»¶è¿Ÿæºäºç¥ç»å…ƒåœ¨ç©ºé—´ä¸­çš„ç‰©ç†è·ç¦»ï¼Œè€Œç°æœ‰æ¨¡å‹æœªèƒ½åˆ©ç”¨è¿™ä¸€ç©ºé—´ç»“æ„åŸç†ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ç©ºé—´è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSpSNNï¼‰æ¡†æ¶ï¼Œå…¶ä¸­ç¥ç»å…ƒå­¦ä¹ æœ‰é™ç»´æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­çš„åæ ‡ï¼Œçªè§¦å»¶è¿Ÿä»ç¥ç»å…ƒé—´çš„è·ç¦»è‡ªç„¶äº§ç”Ÿï¼Œä»è€Œç”¨ä½ç½®å­¦ä¹ æ›¿ä»£äº†æ¯ä¸ªçªè§¦çš„å»¶è¿Ÿå­¦ä¹ ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªåŠ¨å¾®åˆ†å’Œè‡ªå®šä¹‰æ¨å¯¼è§„åˆ™è®¡ç®—ç²¾ç¡®çš„å»¶è¿Ÿæ¢¯åº¦ï¼Œæ”¯æŒä»»æ„ç¥ç»å…ƒæ¨¡å‹å’Œæ¶æ„ã€‚</p>
<p><strong>Result:</strong> åœ¨Yin-Yangå’ŒSpiking Heidelberg DigitsåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSpSNNåœ¨å‚æ•°æ•°é‡å¤§å¹…å‡å°‘çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºå…·æœ‰æ— çº¦æŸå»¶è¿Ÿçš„SNNã€‚æ€§èƒ½åœ¨2Då’Œ3Dç½‘ç»œä¸­è¾¾åˆ°å³°å€¼ï¼Œæ˜¾ç¤ºå‡ºå‡ ä½•æ­£åˆ™åŒ–æ•ˆåº”ã€‚åŠ¨æ€ç¨€ç–åŒ–çš„SpSNNåœ¨90%ç¨€ç–åº¦ä¸‹ä»ä¿æŒå®Œå…¨ç²¾åº¦ï¼Œä¸æ ‡å‡†å»¶è¿Ÿè®­ç»ƒSNNç›¸å½“ï¼ŒåŒæ—¶å‚æ•°æ•°é‡å‡å°‘é«˜è¾¾18å€ã€‚</p>
<p><strong>Conclusion:</strong> SpSNNä¸ºæ¢ç´¢æ—¶é—´è®¡ç®—ä¸­çš„ç©ºé—´ç»“æ„æä¾›äº†åŸåˆ™æ€§å¹³å°ï¼Œå¹¶ä¸ºå¯æ‰©å±•ã€é«˜èƒ½æ•ˆçš„ç¥ç»å½¢æ€æ™ºèƒ½æä¾›äº†ç¡¬ä»¶å‹å¥½çš„åŸºç¡€ã€‚å­¦ä¹ åˆ°çš„ç©ºé—´å¸ƒå±€è‡ªç„¶åœ°æ˜ å°„åˆ°ç¡¬ä»¶å‡ ä½•ç»“æ„ä¸Šï¼Œæœ‰åˆ©äºé«˜æ•ˆçš„ç¥ç»å½¢æ€å®ç°ï¼Œæ­ç¤ºäº†ç©ºé—´åµŒå…¥ä½œä¸ºå»¶è¿Ÿå­¦ä¹ çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>The efficiency of modern machine intelligence depends on high accuracy with minimal computational cost. In spiking neural networks (SNNs), synaptic delays are crucial for encoding temporal structure, yet existing models treat them as fully trainable, unconstrained parameters, leading to large memory footprints, higher computational demand, and a departure from biological plausibility. In the brain, however, delays arise from physical distances between neurons embedded in space. Building on this principle, we introduce Spatial Spiking Neural Networks (SpSNNs), a framework in which neurons learn coordinates in a finite-dimensional Euclidean space and delays emerge from inter-neuron distances. This replaces per-synapse delay learning with position learning, substantially reducing parameter count while retaining temporal expressiveness. Across the Yin-Yang and Spiking Heidelberg Digits benchmarks, SpSNNs outperform SNNs with unconstrained delays despite using far fewer parameters. Performance consistently peaks in 2D and 3D networks rather than infinite-dimensional delay spaces, revealing a geometric regularization effect. Moreover, dynamically sparsified SpSNNs maintain full accuracy even at 90% sparsity, matching standard delay-trained SNNs while using up to 18x fewer parameters. Because learned spatial layouts map naturally onto hardware geometries, SpSNNs lend themselves to efficient neuromorphic implementation. Methodologically, SpSNNs compute exact delay gradients via automatic differentiation with custom-derived rules, supporting arbitrary neuron models and architectures. Altogether, SpSNNs provide a principled platform for exploring spatial structure in temporal computation and offer a hardware-friendly substrate for scalable, energy-efficient neuromorphic intelligence.</p>
<h3 id="3-a-spiking-neural-network-implementation-of-gaussian-belief-propagation">[3] <a href="https://arxiv.org/abs/2512.10638">A Spiking Neural Network Implementation of Gaussian Belief Propagation</a></h3>
<p><em>Sepideh Adamiat, Wouter M. Kouw, Bert de Vries</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ³„æ¼ç§¯åˆ†å‘æ”¾ç¥ç»å…ƒçš„è„‰å†²ç¥ç»ç½‘ç»œå®ç°é«˜æ–¯ç½®ä¿¡åº¦ä¼ æ’­çš„æ–¹æ³•ï¼Œä¸ºç”Ÿç‰©åˆç†çš„æ¦‚ç‡æ¨ç†æä¾›äº†ç¥ç»æœºåˆ¶è§£é‡Šã€‚é€šè¿‡å°†å› å­å›¾æ¶ˆæ¯ä¼ é€’æ˜ å°„åˆ°è„‰å†²ä¿¡å·å¤„ç†ï¼Œå®ç°äº†ä¸‰ç§æ ¸å¿ƒçº¿æ€§è¿ç®—çš„ç¥ç»å®ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è´å¶æ–¯æ¨ç†ä¸ºè‡ªç„¶æ™ºèƒ½ä½“çš„ä¿¡æ¯å¤„ç†æä¾›äº†åŸåˆ™æ€§è§£é‡Šï¼Œä½†ç¥ç»ç½‘ç»œå¦‚ä½•æ‰§è¡Œè¿™äº›æŠ½è±¡è¿ç®—çš„ç¥ç»æœºåˆ¶ä»æ˜¯ä¸€ä¸ªå¼€æ”¾é—®é¢˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢åˆ†å¸ƒå¼è´å¶æ–¯æ¨ç†ï¼ˆç‰¹åˆ«æ˜¯å› å­å›¾ä¸Šçš„æ¶ˆæ¯ä¼ é€’ï¼‰æ˜¯å¦å¯ä»¥é€šè¿‡æ³„æ¼ç§¯åˆ†å‘æ”¾ç¥ç»å…ƒç½‘ç»œæ¥å®ç°ï¼Œä»è€Œå¼¥åˆæŠ½è±¡è®¡ç®—ç†è®ºä¸ç”Ÿç‰©ç¥ç»å®ç°ä¹‹é—´çš„é¸¿æ²Ÿã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨æ³„æ¼ç§¯åˆ†å‘æ”¾ç¥ç»å…ƒæ„å»ºçš„è„‰å†²ç¥ç»ç½‘ç»œå®ç°é«˜æ–¯ç½®ä¿¡åº¦ä¼ æ’­ï¼Œå°†å› å­èŠ‚ç‚¹ä¼ å…¥çš„æ¶ˆæ¯ç¼–ç ä¸ºè„‰å†²ä¿¡å·ï¼Œé€šè¿‡SNNä¼ æ’­è¿™äº›ä¿¡å·ï¼Œå†å°†è„‰å†²ä¿¡å·è§£ç ä¸ºä¼ å‡ºæ¶ˆæ¯ã€‚å…·ä½“å®ç°äº†ä¸‰ç§æ ¸å¿ƒçº¿æ€§è¿ç®—ï¼šç­‰å¼ï¼ˆåˆ†æ”¯ï¼‰ã€åŠ æ³•å’Œä¹˜æ³•ï¼Œè¿™äº›è¿ç®—åœ¨æ³„æ¼ç§¯åˆ†å‘æ”¾æ¨¡å‹ç½‘ç»œä¸­å¾—ä»¥å®ç°ï¼Œæ„æˆäº†å®Œæ•´çš„æ¶ˆæ¯ä¼ é€’æ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> éªŒè¯å®éªŒè¡¨æ˜ï¼Œè¯¥è„‰å†²ç¥ç»ç½‘ç»œå®ç°çš„æ¶ˆæ¯æ›´æ–°ä¸æ ‡å‡†å’Œç§¯ç®—æ³•ç›¸æ¯”å…·æœ‰é«˜å‡†ç¡®æ€§ã€‚åœ¨å¡å°”æ›¼æ»¤æ³¢å’Œè´å¶æ–¯çº¿æ€§å›å½’ç­‰åº”ç”¨ä¸­çš„è¡¨ç°è¯æ˜äº†è¯¥æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€æ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œå±•ç¤ºäº†ç¥ç»å®ç°èƒ½å¤Ÿæœ‰æ•ˆæ‰§è¡Œæ¦‚ç‡æ¨ç†æ“ä½œã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç”Ÿç‰©åŸºç¡€çš„ç¥ç»å½¢æ€æ¦‚ç‡æ¨ç†å®ç°æä¾›äº†é‡è¦è¿›å±•ï¼Œå±•ç¤ºäº†è„‰å†²ç¥ç»ç½‘ç»œå¦‚ä½•æ‰§è¡Œåˆ†å¸ƒå¼è´å¶æ–¯æ¨ç†çš„æ ¸å¿ƒè¿ç®—ã€‚è¿™ä¸€æ¡†æ¶ä¸ä»…ä¸ºç†è§£å¤§è„‘ä¸­çš„æ¦‚ç‡è®¡ç®—æä¾›äº†æ–°è§†è§’ï¼Œä¹Ÿä¸ºå¼€å‘ç¥ç»å½¢æ€è®¡ç®—ç³»ç»Ÿæ‰§è¡Œå¤æ‚æ¨ç†ä»»åŠ¡å¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†æŠ½è±¡ç®—æ³•ä¸ç¥ç»å®ç°ä¹‹é—´çš„æ¡¥æ¢æ„å»ºã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Bayesian inference offers a principled account of information processing in natural agents. However, it remains an open question how neural mechanisms perform their abstract operations. We investigate a hypothesis where a distributed form of Bayesian inference, namely message passing on factor graphs, is performed by a simulated network of leaky-integrate-and-fire neurons. Specifically, we perform Gaussian belief propagation by encoding messages that come into factor nodes as spike-based signals, propagating these signals through a spiking neural network (SNN) and decoding the spike-based signal back to an outgoing message. Three core linear operations, equality (branching), addition, and multiplication, are realized in networks of leaky integrate-and-fire models. Validation against the standard sum-product algorithm shows accurate message updates, while applications to Kalman filtering and Bayesian linear regression demonstrate the framework's potential for both static and dynamic inference tasks. Our results provide a step toward biologically grounded, neuromorphic implementations of probabilistic reasoning.</p>
  </article>
</body>
</html>
